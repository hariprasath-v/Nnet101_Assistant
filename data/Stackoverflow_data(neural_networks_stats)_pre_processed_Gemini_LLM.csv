question,q_link,tags,q_question_id,q_is_answered,q_accepted_answer_id,q_view_count,q_answer_count,q_score,q_last_activity_date,q_creation_date,a_score,a_creation_date,a_answer,answer
How to choose the number of hidden layers and nodes in a feedforward neural network?,https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw,model-selection|neural-networks,181,True,1097,1145801,10,820,1661947755,1279584902,671,1280715630,"I realize this question has been answered, but I don't think the extant answer really engages the question beyond pointing to a link generally related to the question's subject matter. In particular, the link describes one technique for programmatic network configuration, but that is not a ""[a] standard and accepted method"" for network configuration.
By following a small set of clear rules, one can programmatically set a competent network architecture (i.e., the number and type of neuronal layers and the number of neurons comprising each layer). Following this schema will give you a competent architecture but probably not an optimal one.
But once this network is initialized, you can iteratively tune the configuration during training using a number of ancillary algorithms; one family of these works by pruning nodes based on (small) values of the weight vector after a certain number of training epochs--in other words, eliminating unnecessary/redundant nodes (more on this below).
So every NN has three types of layers: input, hidden, and output.
Creating the NN architecture, therefore, means coming up with values for the number of layers of each type and the number of nodes in each of these layers.
The Input Layer
Simple--every NN has exactly one of them--no exceptions that I'm aware of.
With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term.
The Output Layer
Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.
Is your NN going to run in Machine Mode or Regression Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing)? Machine mode: returns a class label (e.g., ""Premium Account""/""Basic Account""). Regression Mode returns a value (e.g., price).
If the NN is a regressor, then the output layer has a single node.
If the NN is a classifier, then it also has a single node unless softmax is used
in which case the output layer has one node per class label in your model.
The Hidden Layers
So those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leaves the hidden layers.
How many hidden layers? Well, if your data is linearly separable (which you often know by the time you begin coding a NN), then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job.
Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ (http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) for an excellent summary (http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.
So what about the size of the hidden layer(s)--how many neurons? There are some empirically derived rules of thumb; of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, the author of Introduction to Neural Networks in Java (https://www.heatonresearch.com/book/), offers a few more.
In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) the number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers. 
Optimization of the Network Configuration
Pruning describes a set of techniques to trim network size (by nodes, not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look at weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training, then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on network architecture, err on the side of more neurons, if you add a pruning step.
Put another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single ""up-front"" (such as a genetic-algorithm-based algorithm), I don't know, though I do know that for now, this two-step optimization is more common.
","**Network Configuration in Neural Networks**

**Standardization**
There is no single standardized method for configuring networks. However, guidelines exist for setting the number and type of network layers, as well as the number of neurons in each layer.

**Initial Architecture Setup**
By following specific rules, one can establish a competent network architecture. This involves determining the number and type of neuronal layers and the number of neurons within each layer. This approach provides a foundational architecture but may not be optimal.

**Iterative Tuning**
Once the network is initialized, its configuration can be iteratively tuned during training. Ancillary algorithms, such as pruning, can be used to eliminate unnecessary nodes, optimizing the network's size and performance.

**Network Layer Types and Sizing**
Every neural network has input, hidden, and output layers.

* **Input Layer:** Number of neurons is determined by the number of features in the training data.
* **Output Layer:** Number of neurons is determined by the model configuration (regression mode has one node, classification mode uses softmax for multiple classes).
* **Hidden Layers:** Number of layers and neurons can be determined empirically, with one hidden layer often being sufficient. Rule of thumb suggests the hidden layer size should be between the input and output layer sizes.

**Optimization**
Pruning techniques can be employed during training to reduce network size and improve performance. This involves identifying and removing nodes that do not significantly impact network performance. Optimizing network configuration can be achieved by initially setting a larger number of neurons and then using pruning to refine the network."
What should I do when my neural network doesn&#39;t learn?,https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn,neural-networks|faq,352036,True,352037,365434,9,368,1701358003,1529367960,455,1529367960,"1.  Verify that your code is bug free
There's a saying among writers that ""All writing is re-writing"" -- that is, the greater part of writing is revising. For programmers (or at least data scientists) the expression could be re-phrased as ""All coding is debugging.""
Any time you're writing code, you need to verify that it works as intended. The best method I've ever found for verifying correctness is to break your code into small segments, and verify that each segment works. This can be done by comparing the segment output to what you know to be the correct answer. This is called unit testing (https://en.wikipedia.org/wiki/Unit_testing). Writing good unit tests is a key piece of becoming a good statistician/data scientist/machine learning expert/neural network practitioner. There is simply no substitute.
You have to check that your code is free of bugs before you can tune network performance! Otherwise, you might as well be re-arranging deck chairs on the RMS Titanic.
There are two features of neural networks that make verification even more important than for other types of machine learning or statistical models.

Neural networks are not ""off-the-shelf"" algorithms in the way that random forest or logistic regression are. Even for simple, feed-forward networks, the onus is largely on the user to make numerous decisions about how the network is configured, connected, initialized and optimized. This means writing code, and writing code means debugging.

Even when a neural network code executes without raising an exception, the network can still have bugs! These bugs might even be the insidious kind for which the network will train, but get stuck at a sub-optimal solution, or the resulting network does not have the desired architecture. (This is an example of the difference between a syntactic and semantic error (https://web.archive.org/web/20161201151434/https://wci.llnl.gov/codes/basis/manual/node53.html).)


This Medium post, ""How to unit test machine learning code (https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765),"" by Chase Roberts discusses unit-testing for machine learning models in more detail. I borrowed this example of buggy code from the article:
def make_convnet(input_image):
    net = slim.conv2d(input_image, 32, [11, 11], scope=""conv1_11x11"")
    net = slim.conv2d(input_image, 64, [5, 5], scope=""conv2_5x5"")
    net = slim.max_pool2d(net, [4, 4], stride=4, scope='pool1')
    net = slim.conv2d(input_image, 64, [5, 5], scope=""conv3_5x5"")
    net = slim.conv2d(input_image, 128, [3, 3], scope=""conv4_3x3"")
    net = slim.max_pool2d(net, [2, 2], scope='pool2')
    net = slim.conv2d(input_image, 128, [3, 3], scope=""conv5_3x3"")
    net = slim.max_pool2d(net, [2, 2], scope='pool3')
    net = slim.conv2d(input_image, 32, [1, 1], scope=""conv6_1x1"")
    return net

Do you see the error? Many of the different operations are not actually used because previous results are over-written with new variables. Using this block of code in a network will still train and the weights will update and the loss might even decrease -- but the code definitely isn't doing what was intended.  (The author is also inconsistent about using single- or double-quotes but that's purely stylistic.)
The most common programming errors pertaining to neural networks are

Variables are created but never used (usually because of copy-paste errors);
Expressions for gradient updates are incorrect;
Weight updates are not applied;
Loss functions are not measured on the correct scale (https://stats.stackexchange.com/questions/544337/same-loss-and-accuracy-on-epochs/544381#544381) (for example, cross-entropy loss can be expressed in terms of probability or logits)
The loss is not appropriate for the task (for example, using categorical cross-entropy loss for a regression task).
Dropout is used during testing, instead of only being used for training. (https://stats.stackexchange.com/questions/551144/why-am-i-getting-different-results-on-a-prediction-using-the-same-keras-model-an/551158#551158)
Make sure you're minimizing the loss function $L(x)$, instead of minimizing $-L(x)$ (https://stats.stackexchange.com/questions/562374/implementing-a-vae-in-pytorch-extremely-negative-training-loss/562402#562402).
Make sure your loss is computed correctly (https://stats.stackexchange.com/questions/565956/confused-with-binary-crossentrophy-vs-categorical-crossentropy/566066#566066).

Unit testing is not just limited to the neural network itself. You need to test all of the steps that produce or transform data and feed into the network. Some common mistakes here are

NA or NaN or Inf values in your data creating NA or NaN or Inf values in the output, and therefore in the loss function.
Shuffling the labels independently from the samples (for instance, creating train/test splits for  the labels and samples separately);
Accidentally assigning the training data as the testing data;
When using a train/test split, the model references the original, non-split data instead of the training partition or the testing partition.
Forgetting to scale the testing data;
Scaling the testing data using the statistics of the test partition instead of the train partition;
Forgetting to un-scale the predictions (e.g. pixel values are in [0,1] instead of [0, 255]).
Here's an example of a question where the problem appears to be one of model configuration or hyperparameter choice, but actually the problem was a subtle bug in how gradients were computed. Is this drop in training accuracy due to a statistical or programming error? (https://stats.stackexchange.com/questions/527045/drop-in-training-accuracy#comment970055_527045)

2. For the love of all that is good, scale your data
The scale of the data can make an enormous difference on training. Sometimes, networks simply won't reduce the loss if the data isn't scaled. Other networks will decrease the loss, but only very slowly. Scaling the inputs (and certain times, the targets) can dramatically improve the network's training.

Prior to presenting data to a neural network, standardizing the data to have 0 mean and unit variance, or to lie in a small interval like $[-0.5, 0.5]$ can improve training. This amounts to pre-conditioning, and removes the effect that a choice in units has on network weights. For example, length in millimeters and length in kilometers both represent the same concept, but are on different scales. The exact details of how to standardize the data depend on what your data look like.


Data normalization and standardization in neural networks (https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks)

Why does $[0,1]$ scaling dramatically increase training time for feed forward ANN (1 hidden layer)? (https://stats.stackexchange.com/questions/364735/why-does-0-1-scaling-dramatically-increase-training-time-for-feed-forward-an/364776#364776)




Batch or Layer normalization can improve network training. Both seek to improve the network by keeping a running mean and standard deviation for neurons' activations as the network trains. It is not well-understood why this helps training, and remains an active area of research.

""Understanding Batch Normalization (https://arxiv.org/abs/1806.02375v1)"" by Johan Bjorck, Carla Gomes, Bart Selman
""Towards a Theoretical Understanding of Batch Normalization (https://arxiv.org/abs/1805.10694v1)"" by Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, Thomas Hofmann
""How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) (https://arxiv.org/abs/1805.11604v2)"" by Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry



3. Crawl Before You Walk; Walk Before You Run
Wide and deep neural networks, and neural networks with exotic wiring, are the Hot Thing right now in machine learning. But these networks didn't spring fully-formed into existence; their designers built up to them from smaller units. First, build a small network with a single hidden layer and verify that it works correctly. Then incrementally add additional model complexity, and verify that each of those works as well.

Too few neurons in a layer can restrict the representation that the network learns, causing under-fitting. Too many neurons can cause over-fitting because the network will ""memorize"" the training data.
Even if you can prove that there is, mathematically, only a small number of neurons necessary to model a problem, it is often the case that having ""a few more"" neurons makes it easier for the optimizer to find a ""good"" configuration. (But I don't think anyone fully understands why this is the case.) I provide an example of this in the context of the XOR problem here: Aren't my iterations needed to train NN for XOR with MSE < 0.001 too high? (https://stats.stackexchange.com/questions/351216/arent-my-iterations-needed-to-train-nn-for-xor-with-mse-0-001-too-high/351713#351713).

Choosing the number of hidden layers lets the network learn an abstraction from the raw data. Deep learning is all the rage these days, and networks with a large number of layers have shown impressive results. But adding too many hidden layers can make risk overfitting or make it very hard to optimize the network.

Choosing a clever network wiring can do a lot of the work for you. Is your data source amenable to specialized network architectures? Convolutional neural networks can achieve impressive results on ""structured"" data sources, image or audio data. Recurrent neural networks can do well on sequential data types, such as natural language or time series data. Residual connections can improve deep feed-forward networks.


4. Neural Network Training Is Like Lock Picking
To achieve state of the art, or even merely good, results, you have to set up all of the parts configured to work well together. Setting up a neural network configuration that actually learns is a lot like picking a lock: all of the pieces have to be lined up just right. Just as it is not sufficient to have a single tumbler in the right place, neither is it sufficient to have only the architecture, or only the optimizer, set up correctly.
Tuning configuration choices is not really as simple as saying that one kind of configuration choice (e.g. learning rate) is more or less important than another (e.g. number of units), since all of these choices interact with all of the other choices, so one choice can do well in combination with another choice made elsewhere.
This is a non-exhaustive list of the configuration options which are not also regularization options or numerical optimization options.
All of these topics are active areas of research.

The network initialization is often overlooked as a source of neural network bugs. Initialization over too-large an interval can set initial weights too large, meaning that single neurons have an outsize influence over the network behavior.

The key difference between a neural network and a regression model is that a neural network is a composition of many nonlinear functions, called activation functions. (See: What is the essential difference between neural network and linear regression (https://stats.stackexchange.com/questions/259950/what-is-the-essential-difference-between-neural-network-and-linear-regression))
Classical neural network results focused on sigmoidal activation functions (logistic or $\tanh$ functions). A recent result has found that ReLU (or similar) units tend to work better because the have steeper gradients, so updates can be applied quickly. (See: Why do we use ReLU in neural networks and how do we use it? (https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)) One caution about ReLUs is the ""dead neuron"" phenomenon, which can stymie learning; leaky relus and similar variants avoid this problem. See

Why can't a single ReLU learn a ReLU? (https://stats.stackexchange.com/questions/379884/why-cant-a-single-relu-learn-a-relu)

My ReLU network fails to launch (https://stats.stackexchange.com/questions/188040/my-relu-network-fails-to-launch/)


There are a number of other options. See: Comprehensive list of activation functions in neural networks with pros/cons (https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)

Residual connections are a neat development that can make it easier to train neural networks. ""Deep Residual Learning for Image Recognition"" (https://arxiv.org/abs/1512.03385)
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun In: CVPR. (2016). Additionally, changing the order of operations within the residual block can further improve the resulting network. ""Identity Mappings in Deep Residual Networks (https://arxiv.org/pdf/1603.05027v3.pdf)"" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

5. Non-convex optimization is hard
The objective function of a neural network is only convex when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem.
In all other cases, the optimization problem is non-convex, and non-convex optimization is hard. The challenges of training neural networks are well-known (see: Why is it hard to train deep neural networks? (https://stats.stackexchange.com/questions/262750/why-is-it-hard-to-train-deep-neural-networks)). Additionally, neural networks have a very large number of parameters, which restricts us to solely first-order methods (see: Why is Newton's method not widely used in machine learning? (https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning)). This is a very active area of research.

Setting the learning rate too large will cause the optimization to diverge, because you will leap from one side of the ""canyon"" to the other. Setting this too small will prevent you from making any real progress, and possibly allow the noise inherent in SGD to overwhelm your gradient estimates. See:

How can change in cost function be positive? (https://stats.stackexchange.com/questions/364360/how-can-change-in-cost-function-be-positive/364366#364366)


Gradient clipping re-scales the norm of the gradient if it's above some threshold. I used to think that this was a set-and-forget parameter, typically at 1.0, but I found that I could make an LSTM language model dramatically better by setting it to 0.25. I don't know why that is.

Learning rate scheduling can decrease the learning rate over the course of training. In my experience, trying to use scheduling is a lot like regex (https://blog.codinghorror.com/regular-expressions-now-you-have-two-problems/): it replaces one problem (""How do I get learning to continue after a certain epoch?"") with two problems (""How do I get learning to continue after a certain epoch?"" and ""How do I choose a good schedule?""). Other people insist that scheduling is essential. I'll let you decide.

Choosing a good minibatch size can influence the learning process indirectly, since a larger mini-batch will tend to have a smaller variance (law-of-large-numbers (/questions/tagged/law-of-large-numbers)) than a smaller mini-batch. You want the mini-batch to be large enough to be informative about the direction of the gradient, but small enough that SGD can regularize your network.

There are a number of variants on stochastic gradient descent which use momentum, adaptive learning rates, Nesterov updates and so on to improve upon vanilla SGD. Designing a better optimizer is very much an active area of research. Some examples:

No change in accuracy using Adam Optimizer when SGD works fine (https://stats.stackexchange.com/questions/313278/no-change-in-accuracy-using-adam-optimizer-when-sgd-works-fine)
How does the Adam method of stochastic gradient descent work? (https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work/220563#comment661981_220563)
Why does momentum escape from a saddle point in this famous image? (https://stats.stackexchange.com/questions/308835/why-does-momentum-escape-from-a-saddle-point-in-this-famous-image)


When it first came out, the Adam optimizer generated a lot of interest. But some recent research has found that SGD with momentum can out-perform adaptive gradient methods for neural networks. ""The Marginal Value of Adaptive Gradient Methods in Machine Learning (https://arxiv.org/abs/1705.08292)"" by Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht

But on the other hand, this very recent paper proposes a new adaptive learning-rate optimizer which supposedly closes the gap between adaptive-rate methods and SGD with momentum. ""Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks (https://arxiv.org/abs/1806.06763v1)"" by Jinghui Chen, Quanquan Gu

Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes ""over adapted"". We design a new algorithm, called Partially adaptive momentum estimation method (Padam), which unifies the Adam/Amsgrad with SGD to achieve the best from both worlds. Experiments on standard benchmarks show that Padam can maintain fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.


Specifically for triplet-loss (/questions/tagged/triplet-loss) models, there are a number of tricks which can improve training time and generalization. See: In training a triplet network, I first have a solid drop in loss, but eventually the loss slowly but consistently increases. What could cause this? (https://stats.stackexchange.com/questions/475655/in-training-i-first-have-a-solid-drop-in-loss-but-eventually-the-loss-slowly-b)


6. Regularization
Choosing and tuning network regularization is a key part of building a model that generalizes well (that is, a model that is not overfit to the training data). However, at the time that your network is struggling to decrease the loss on the training data -- when the network is not learning -- regularization can obscure what the problem is.
When my network doesn't learn, I turn off all regularization and verify that the non-regularized network works correctly. Then I add each regularization piece back, and verify that each of those works along the way.
This tactic can pinpoint where some regularization might be poorly set. Some examples are

$L^2$ regularization (aka weight decay) or $L^1$ regularization is set too large, so the weights can't move.

Two parts of regularization are in conflict. For example, it's widely observed that layer normalization and dropout are difficult to use together. Since either on its own is very useful, understanding how to use both is an active area of research.

""Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift (https://arxiv.org/abs/1801.05134v1)"" by Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang
""Adjusting for Dropout Variance in Batch Normalization and Weight Initialization (https://arxiv.org/abs/1607.02488v2)"" by Dan Hendrycks, Kevin Gimpel.
""Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515)"" by GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr and Sepp Hochreiter



7. Keep a Logbook of Experiments
When I set up a neural network, I don't hard-code any parameter settings. Instead, I do that in a configuration file (e.g., JSON) that is read and used to populate network configuration details at runtime. I keep all of these configuration files. If I make any parameter modification, I make a new configuration file. Finally, I append as comments all of the per-epoch losses for training and validation.
The reason that I'm so obsessive about retaining old results is that this makes it very easy to go back and review previous experiments. It also hedges against mistakenly repeating the same dead-end experiment. Psychologically, it also lets you look back and observe ""Well, the project might not be where I want it to be today, but I am making progress compared to where I was $k$ weeks ago.""
As an example, I wanted to learn about LSTM language models, so I decided to make a Twitter bot that writes new tweets in response to other Twitter users. I worked on this in my free time, between grad school and my job. It took about a year, and I iterated over about 150 different models before getting to a model that did what I wanted: generate new English-language text that (sort of) makes sense. (One key sticking point, and part of the reason that it took so many attempts, is that it was not sufficient to simply get a low out-of-sample loss, since early low-loss models had managed to memorize the training data, so it was just reproducing germane blocks of text verbatim in reply to prompts -- it took some tweaking to make the model more spontaneous and still have low loss.)
","**Key Considerations for Neural Network Development**

1. **Verify Code Correctness:** Break down code into segments and test each segment to ensure it functions as intended. Unit testing is crucial in verifying code accuracy.

2. **Data Scaling and Standardization:** Standardizing data to have a mean of 0 and unit variance improves network training by removing the effect of unit choice on weights. Batch or layer normalization can enhance training by maintaining a running mean and standard deviation.

3. **Incremental Model Building:** Start with small, simple networks and gradually add complexity. Too few neurons can underfit the model, while too many can lead to overfitting.

4. **Configuration and Optimization:** Optimizing neural network configuration is crucial for achieving good results. Consider network initialization, activation functions, residual connections, and non-convex optimization challenges. Setting appropriate learning rates, gradient clipping, and learning rate scheduling are also important.

5. **Regularization:** Choosing and tuning the right regularization methods helps prevent overfitting. Consider $L^2$ or $L^1$ regularization, but be mindful of conflicts between regularization techniques, such as layer normalization and dropout.

6. **Experiment Logging:** Keeping a record of experiments with configuration details and per-epoch losses allows for easy review and comparison. This helps identify progress and avoid repeating previous mistakes.

7. **Gradual Learning:** Complex networks can require gradual learning. Test simple models first, then increase complexity while ensuring each step functions correctly. Over-complexity can hinder optimization."
"What exactly are keys, queries, and values in attention mechanisms?",https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms,neural-networks|natural-language|attention|machine-translation,421935,True,424127,261109,11,309,1708928023,1565686855,281,1567068576,"The key/value/query formulation of attention is from the paper Attention Is All You Need (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).

How should one understand the queries, keys, and values

The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).
The attention operation can be thought of as a retrieval process as well.
As mentioned in the paper you referenced (Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473)), attention by definition is just a weighted average of values,
$$c=\sum_{j}\alpha_jh_j$$
where $\sum \alpha_j=1$.
If we restrict $\alpha$ to be a one-hot vector, this operation becomes the same as retrieving from a set of elements $h$ with index $\alpha$. With the restriction removed, the attention operation can be thought of as doing ""proportional retrieval"" according to the probability vector $\alpha$.
It should be clear that $h$ in this context is the value. The difference between the two papers lies in how the probability vector $\alpha$ is calculated. The first paper (Bahdanau et al. 2015) computes the score through a neural network $$e_{ij}=a(s_i,h_j), \qquad \alpha_{i,j}=\frac{\exp(e_{ij})}{\sum_k\exp(e_{ik})}$$
where $h_j$ is from the encoder sequence, and $s_i$ is from the decoder sequence. One problem of this approach is, say the encoder sequence is of length $m$ and the decoding sequence is of length $n$, we have to go through the network $m*n$ times to acquire all the attention scores $e_{ij}$.
A more efficient model would be to first project $s$ and $h$ onto a common space, then choose a similarity measure (e.g. dot product) as the attention score, like
$$e_{ij}=f(s_i)g(h_j)^T$$
so we only have to compute $g(h_j)$ $m$ times and $f(s_i)$ $n$ times to get the projection vectors and $e_{ij}$ can be computed efficiently by matrix multiplication.
This is essentially the approach proposed by the second paper (Vaswani et al. 2017), where the two projection vectors are called query (for decoder) and key (for encoder), which is well aligned with the concepts in retrieval systems. (There are later techniques to further reduce the computational complexity, for example Reformer (https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), Linformer (https://arxiv.org/pdf/2006.04768.pdf), FlashAttention (https://arxiv.org/abs/2307.08691).)

How are the queries, keys, and values obtained

The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.

$$
\begin{align}\text{MultiHead($Q$, $K$, $V$)} & = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^{O} \\
\text{where head$_i$} &  = \text{Attention($QW_i^Q$, $KW_i^K$, $VW_i^V$)}
\end{align}$$
Where the projections are parameter matrices:
$$
\begin{align}
W_i^Q & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^K & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^V & \in \mathbb{R}^{d_\text{model} \times d_v}, \\
W_i^O & \in \mathbb{R}^{hd_v \times d_{\text{model}}}.
\end{align}$$

For unsupervised language model training like GPT (https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), $Q, K, V$ are usually from the same source, so such operation is also called self-attention.
For the machine translation task in the second paper, it first applies self-attention separately to source and target sequences, then on top of that it applies another attention where $Q$ is from the target sequence and $K, V$ are from the source sequence.
For recommendation systems, $Q$ can be from the target items, $K, V$ can be from the user profile and history.
","In the key/value/query formulation of attention, the query represents the input to be matched, while the keys and values store the candidate matches and their associated information.
Like a search engine, attention's query is matched against a set of keys, and the best matched values are returned.
This retrieval process involves calculating a probability vector alpha, which determines the proportion of each value to include in the output.

The first paper (Bahdanau et al. 2015) calculates alpha using a neural network, which is computationally expensive.
The second paper (Vaswani et al. 2017) proposes a more efficient approach, where the keys and values are first projected onto a common space, and then a similarity measure is used to calculate alpha.
This approach corresponds to the retrieval system concept, where the query and key projections are analogous to the user query and video metadata, respectively.

Multihead attention extends this key/value/query formulation by splitting the inputs into multiple heads and applying attention to each head independently.
The outputs of each head are then concatenated to produce the final attention output.

The source of the queries, keys, and values depends on the application.
For self-attention (as in language models), they all come from the same source.
For tasks like machine translation, queries and keys can come from different sources (e.g., target and source sequences, respectively).
In recommendation systems, queries can represent target items, while keys and values can represent user profiles and history."
What is batch size in neural network?,https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network,neural-networks|python|terminology|keras,153531,True,153535,731148,6,305,1650529048,1432286121,421,1432288067,"The batch size defines the number of samples that will be propagated through the network.
For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.
Advantages of using a batch size < number of all samples:

It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.
Typically networks train faster with mini-batches. That's because we update the weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated our network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.

Disadvantages of using a batch size < number of all samples:

The smaller the batch the less accurate the estimate of the gradient will be. In the figure below, you can see that the direction of the mini-batch gradient (green color) fluctuates much more in comparison to the direction of the full batch gradient (blue color).

 (https://i.sstatic.net/lU3sx.png)
Stochastic is just a mini-batch with batch_size equal to 1. In that case, the gradient changes its direction even more often than a mini-batch gradient.
","**Summary**

**Batch Size**

Batch size determines the number of samples trained through a neural network at once. A smaller batch size requires less memory but produces a less accurate gradient estimate.

**Advantages of Smaller Batch Sizes:**
* Reduced memory consumption
* Faster training due to more frequent weight updates

**Disadvantages of Smaller Batch Sizes:**
* Less accurate gradient estimation leading to potential fluctuation in gradient direction

**Special Case: Stochastic Gradient Descent**

Stochastic gradient descent uses a batch size of 1, resulting in even more frequent and potentially erratic gradient direction changes."
What are the advantages of ReLU over sigmoid function in deep neural networks?,https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks,machine-learning|neural-networks|sigmoid-curve|relu,126238,True,126362,290897,9,234,1723495231,1417486429,205,1417567286,"Two additional major benefits of ReLUs are sparsity and a reduced likelihood of vanishing gradient. But first recall the definition of a ReLU is $h = \max(0, a)$ where $a = Wx + b$.
One major benefit is the reduced likelihood of the gradient to vanish. This arises when $a > 0$. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.
The other benefit of ReLUs is sparsity. Sparsity arises when $a \le 0$. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations.
","**Summary:**

Rectified Linear Units (ReLUs) offer two key advantages:

* **Reduced Vanishing Gradient:** Unlike sigmoids, the gradient of ReLUs remains constant when the input is positive ($a > 0$). This constant gradient facilitates faster learning.

* **Sparsity:** When the input is non-positive ($a \le 0$), ReLUs produce zero output, leading to sparse representations. This sparsity is beneficial as it promotes efficient computation and representation learning compared to dense representations generated by sigmoids.

Overall, ReLUs are advantageous for their improved gradient flow and sparsity, enhancing the performance of deep learning models."
What does 1x1 convolution mean in a neural network?,https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network,neural-networks|deep-learning|convolution|convolutional-neural-network,194142,True,194450,218271,8,214,1722919050,1454643197,189,1454863550,"Suppose that I have a conv layer which outputs an $(N, F, H, W)$ shaped tensor where:

$N$ is the batch size
$F$ is the number of convolutional filters
$H, W$ are the spatial dimensions

Suppose the input is fed into a conv layer with $F_1$ 1x1 filters, zero padding and stride 1. Then the output of this 1x1 conv layer will have shape $(N, F_1, H , W)$.
So 1x1 conv filters can be used to change the dimensionality in the filter space. If $F_1 > F$ then we are increasing dimensionality, if $F_1 < F$ we are decreasing dimensionality, in the filter dimension.
Indeed, in the Google Inception article Going Deeper with Convolutions (http://arxiv.org/abs/1409.4842), they state (bold is mine, not by original authors):

One big problem with the above modules, at least in this naive form, is that even a modest number of 5x5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters.
This leads to the second idea of the proposed architecture:  judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch...1x1
convolutions are used to compute reductions before the expensive 3x3 and 5x5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose.

So in the Inception architecture, we use the 1x1 convolutional filters to reduce dimensionality in the filter dimension. As I explained above, these 1x1 conv layers can be used in general to change the filter space dimensionality (either increase or decrease) and in the Inception architecture we see how effective these 1x1 filters can be for dimensionality reduction, explicitly in the filter dimension space, not the spatial dimension space.
Perhaps there are other interpretations of 1x1 conv filters, but I prefer this explanation, especially in the context of the Google Inception architecture.
","**Summary:**

1x1 convolutional filters are used to manipulate the number of filters in a tensor, which is referred to as ""filter space dimensionality.""

When used with a stride of 1 and zero padding, 1x1 convolutions can increase or decrease the number of filters in a tensor. This technique is particularly useful for dimensionality reduction, as seen in the Google Inception architecture.

In Inception, 1x1 convolutions are applied before more computationally expensive 3x3 and 5x5 convolutions. This reduces the number of filters, making the subsequent large-kernel convolutions more efficient.

1x1 convolutions also serve as a dual-purpose layer, performing both dimensionality reduction and rectified linear activation (ReLU).

In summary, 1x1 convolutional filters are primarily used to change the filter space dimensionality, enabling efficient computation and dimensionality reduction in neural network architectures like Inception."
What does the hidden layer in a neural network compute?,https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute,machine-learning|neural-networks|nonlinear-regression,63152,True,63163,202690,4,212,1518901377,1372780747,257,1372790462,"Three sentence version:

Each layer can apply any function you want to the previous layer (usually a linear transformation followed by a squashing nonlinearity). 
The hidden layers' job is to transform the inputs into something that the output layer can use.
The output layer transforms the hidden layer activations into whatever scale you wanted your output to be on.

Like you're 5:
If you want a computer to tell you if there's a bus in a picture, the computer might have an easier time if it had the right tools.
So your bus detector might be made of a wheel detector (to help tell you it's a vehicle) and a box detector (since the bus is shaped like a big box) and a size detector (to tell you it's too big to be a car).  These are the three elements of your hidden layer: they're not part of the raw image, they're tools you designed to help you identify busses.
If all three of those detectors turn on (or perhaps if they're especially active), then there's a good chance you have a bus in front of you.
Neural nets are useful because there are good tools (like backpropagation) for building lots of detectors and putting them together.

Like you're an adult
A feed-forward neural network applies a series of functions to the data.  The exact functions will depend on the neural network you're using: most frequently, these functions each compute a linear transformation of the previous layer, followed by a squashing nonlinearity.  Sometimes the functions will do something else (like computing logical functions in your examples, or averaging over adjacent pixels in an image).  So the roles of the different layers could depend on what functions are being computed, but I'll try to be very general.
Let's call the input vector $x$, the hidden layer activations $h$, and the output activation $y$.  You have some function $f$ that maps from $x$ to $h$ and another function $g$ that maps from $h$ to $y$.  
So the hidden layer's activation is $f(x)$ and the output of the network is $g(f(x))$.
Why have two functions ($f$ and $g$) instead of just one?
If the level of complexity per function is limited, then $g(f(x))$ can compute things that $f$ and $g$ can't do individually.  

An example with logical functions:
For example, if we only allow $f$ and $g$ to be simple logical operators like ""AND"", ""OR"", and ""NAND"", then you can't compute other functions like ""XOR"" with just one of them.  On the other hand, we could compute ""XOR"" if we were willing to layer these functions on top of each other: 
First layer functions:

Make sure that at least one element is ""TRUE"" (using OR)
Make sure that they're not all ""TRUE"" (using NAND)

Second layer function:

Make sure that both of the first-layer criteria are satisfied (using AND)

The network's output is just the result of this second function.  The first layer transforms the inputs into something that the second layer can use so that the whole network can perform XOR.

An example with images:
Slide 61 from this talk (http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/CVPR2012-Tutorial_lee.pdf)--also available here (http://youqianhaozhe.com/deep%20learning.jpg) as a single image--shows (one way to visualize) what the different hidden layers in a particular neural network are looking for.
The first layer looks for short pieces of edges in the image: these are very easy to find from raw pixel data, but they're not very useful by themselves for telling you if you're looking at a face or a bus or an elephant.
The next layer composes the edges: if the edges from the bottom hidden layer fit together in a certain way, then one of the eye-detectors in the middle of left-most column might turn on.  It would be hard to make a single layer that was so good at finding something so specific from the raw pixels: eye detectors are much easier to build out of edge detectors than out of raw pixels.
The next layer up composes the eye detectors and the nose detectors into faces.  In other words, these will light up when the eye detectors and nose detectors from the previous layer turn on with the right patterns.  These are very good at looking for particular kinds of faces: if one or more of them lights up, then your output layer should report that a face is present.
This is useful because face detectors are easy to build out of eye detectors and nose detectors, but really hard to build out of pixel intensities.
So each layer gets you farther and farther from the raw pixels and closer to your ultimate goal (e.g. face detection or bus detection).

Answers to assorted other questions
""Why are some layers in the input layer connected to the hidden layer and some are not?""
The disconnected nodes in the network are called ""bias"" nodes. There's a really nice explanation here (https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks). The short answer is that they're like intercept terms in regression.
""Where do the ""eye detector"" pictures in the image example come from?""
I haven't double-checked the specific images I linked to, but in general, these visualizations show the set of pixels in the input layer that maximize the activity of the corresponding neuron.  So if we think of the neuron as an eye detector, this is the image that the neuron considers to be most eye-like.  Folks usually find these pixel sets with an optimization (hill-climbing) procedure.
In this paper (https://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf) by some Google folks with one of the world's largest neural nets, they show a ""face detector"" neuron and a ""cat detector"" neuron this way, as well as a second way: They also show the actual images that activate the neuron most strongly (figure 3, figure 16).  The second approach is nice because it shows how flexible and nonlinear the network is--these high-level ""detectors"" are sensitive to all these images, even though they don't particularly look similar at the pixel level.

Let me know if anything here is unclear or if you have any more questions.
","**Three-sentence summary:**

Neural networks apply functions such as linear transformations and nonlinearities to data, with each layer building upon the previous one. Hidden layers transform the data for easier processing by the output layer, which produces the final result.

**Like you're 5:**

Imagine you want a computer to recognize buses. You can create detectors for wheels, boxes, and size. These detectors work together in hidden layers to form a toolset for bus recognition. If all detectors are active, there's a good chance a bus is present. Neural networks provide easy ways to combine many detectors.

**Like you're an adult:**

Neural networks apply functions (e.g., linear transformations and nonlinearities) to data, with subsequent layers building upon each other. The hidden layer's activation (transformation of input) is fed as input to the output layer, which produces the network's output. The combination of multiple functions allows the network to perform complex tasks."
"What is the difference between a neural network and a deep neural network, and why do the deep ones work better?",https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w,neural-networks|deep-learning,182734,True,184921,141573,11,178,1588573172,1448018730,236,1449182141,"Let's start with a triviliaty: Deep neural network is simply a feedforward network with many hidden layers.
This is more or less all there is to say about the definition. Neural networks can be recurrent or feedforward; feedforward ones do not have any loops in their graph and can be organized in layers. If there are ""many"" layers, then we say that the network is deep.
How many layers does a network have to have in order to qualify as deep? There is no definite answer to this  (it's a bit like asking how many grains make a heap (https://en.wikipedia.org/wiki/Sorites_paradox)), but usually having two or more hidden layers counts as deep. In contrast, a network with only a single hidden layer is conventionally called ""shallow"". I suspect that there will be some inflation going on here, and in ten years people might think that anything with less than, say, ten layers is shallow and suitable only for kindergarten exercises. Informally, ""deep"" suggests that the network is tough to handle.
Here is an illustration, adapted from here (http://neuralnetworksanddeeplearning.com/chap5.html):
 (https://i.sstatic.net/OH3gI.png)
But the real question you are asking is, of course, Why would having many layers be beneficial?
I think that the somewhat astonishing answer is that nobody really knows. There are some common explanations that I will briefly review below, but none of them has been convincingly demonstrated to be true, and one cannot even be sure that having many layers is really beneficial.
I say that this is astonishing, because deep learning is massively popular, is breaking all the records (from image recognition, to playing Go, to automatic translation, etc.) every year, is getting used by the industry, etc. etc. And we are still not quite sure why it works so well.
I base my discussion on the Deep Learning (http://www.deeplearningbook.org/) book by Goodfellow, Bengio, and Courville which went out in 2017 and is widely considered to be the book on deep learning. (It's freely available online.) The relevant section is 6.4.1 Universal Approximation Properties and Depth.
You wrote that 

10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a neural network is able to represent [...]

You must be referring to the so called Universal approximation theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem), proved by Cybenko in 1989 and generalized by various people in the 1990s. It basically says that a shallow neural network (with 1 hidden layer) can approximate any function, i.e. can in principle learn anything. This is true for various nonlinear activation functions, including rectified linear units that most neural networks are using today (the textbook references Leshno et al. 1993 (http://www2.math.technion.ac.il/~pinkus/papers/neural.pdf) for this result).
If so, then why is everybody using deep nets?
Well, a naive answer is that because they work better. Here is a figure from the Deep Learning book showing that it helps to have more layers in one particular task, but the same phenomenon is often observed across various tasks and domains:
 (https://i.sstatic.net/trj4L.png)
We know that a shallow network could perform as good as the deeper ones. But it does not; and they usually do not. The question is --- why? Possible answers:

Maybe a shallow network would need more neurons then the deep one?
Maybe a shallow network is more difficult to train with our current algorithms (e.g. it has more nasty local minima, or the convergence rate is slower, or whatever)?
Maybe a shallow architecture does not fit to the kind of problems we are usually trying to solve (e.g. object recognition is a quintessential ""deep"", hierarchical process)?
Something else?

The Deep Learning book argues for bullet points #1 and #3. First, it argues that the number of units in a shallow network grows exponentially with task complexity. So in order to be useful a shallow network might need to be very big; possibly much bigger than a deep network. This is based on a number of papers proving that shallow networks would in some cases need exponentially many neurons; but whether e.g. MNIST classification or Go playing are such cases is not really clear. Second, the book says this:

Choosing a deep model encodes a very general belief that the function we
  want to learn should involve composition of several simpler functions. This can be
  interpreted from a representation learning point of view as saying that we believe
  the learning problem consists of discovering a set of underlying factors of variation
  that can in turn be described in terms of other, simpler underlying factors of
  variation.

I think the current ""consensus"" is that it's a combination of bullet points #1 and #3: for real-world tasks deep architecture are often beneficial and shallow architecture would be inefficient and require a lot more neurons for the same performance.
But it's far from proven. Consider e.g. Zagoruyko and  Komodakis, 2016, Wide Residual Networks (https://arxiv.org/abs/1605.07146). Residual networks with 150+ layers appeared in 2015 (https://arxiv.org/abs/1512.03385) and won various image recognition contests. This was a big success and looked like a compelling argument in favour of deepness; here is one figure from a presentation (http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf) by the first author on the residual network paper (note that the time confusingly goes to the left here):
 (https://i.sstatic.net/iVURh.png)
But the paper linked above shows that a ""wide"" residual network with ""only"" 16 layers can outperform ""deep"" ones with 150+ layers. If this is true, then the whole point of the above figure breaks down.
Or consider Ba and Caruana, 2014, Do Deep Nets Really Need to be Deep? (https://arxiv.org/abs/1312.6184):

In this paper we provide empirical evidence that shallow nets are capable of learning the same
  function as deep nets, and in some cases with the same number of parameters as the deep nets. We
  do this by first training a state-of-the-art deep model, and then training a shallow model to mimic the
  deep model. The mimic model is trained using the model compression scheme described in the next
  section. Remarkably, with model compression we are able to train shallow nets to be as accurate
  as some deep models, even though we are not able to train these shallow nets to be as accurate as
  the deep nets when the shallow nets are trained directly on the original labeled training data. If a
  shallow net with the same number of parameters as a deep net can learn to mimic a deep net with
  high fidelity, then it is clear that the function learned by that deep net does not really have to be deep.

If true, this would mean that the correct explanation is rather my bullet #2, and not #1 or #3.
As I said --- nobody really knows for sure yet.

Concluding remarks
The amount of progress achieved in the deep learning over the last ~10 years is truly amazing, but most of this progress was achieved by trial and error, and we still lack very basic understanding about what exactly makes deep nets to work so well. Even the list of things that people consider to be crucial for setting up an effective deep network seems to change every couple of years.
The deep learning renaissance started in 2006 when Geoffrey Hinton (who had been working on neural networks for 20+ years without much interest from anybody) published a couple of breakthrough papers offering an effective way to train deep networks (Science paper (https://www.cs.toronto.edu/~hinton/science.pdf), Neural computation paper (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)). The trick was to use unsupervised pre-training before starting the gradient descent. These papers revolutionized the field, and for a couple of years people thought that unsupervised pre-training was the key. 
Then in 2010 Martens showed that deep neural networks can be trained with second-order methods (so called Hessian-free methods) and can outperform networks trained with pre-training: Deep learning via Hessian-free optimization (http://www.cs.toronto.edu/~asamir/cifar/HFO_James.pdf). Then in 2013 Sutskever et al. showed that  stochastic gradient descent with some very clever tricks can outperform  Hessian-free methods: On the importance of initialization and momentum in deep learning (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf). Also, around 2010 people realized that using rectified linear units instead of sigmoid units makes a huge difference for gradient descent. Dropout appeared in 2014. Residual networks appeared in 2015. People keep coming up with more and more effective ways to train deep networks and what seemed like a key insight 10 years ago is often considered a nuisance today. All of that is largely driven by trial and error and there is little understanding of what makes some things work so well and some other things not. Training deep networks is like a big bag of tricks. Successful tricks are usually rationalized post factum.
We don't even know why deep networks reach a performance plateau; just 10 years people used to blame local minima, but the current thinking is that this is not the point (when the perfomance plateaus, the gradients tend to stay large). This is such a basic question about deep networks, and we don't even know this.
Update: This is more or less the subject of Ali Rahimi's NIPS 2017 talk on machine learning as alchemy: https://www.youtube.com/watch?v=Qi1Yry33TQE (https://www.youtube.com/watch?v=Qi1Yry33TQE).

[This answer was entirely re-written in April 2017, so some of the comments below do not apply anymore.]
","Deep neural networks are feedforward networks with numerous hidden layers. While there is no precise definition of ""many,"" networks with two or more hidden layers are generally considered deep.

The benefits of deep networks are not fully understood, but they often outperform shallow networks with a single hidden layer. One possible explanation is that deep networks require fewer neurons to achieve the same performance as shallow networks. Another possibility is that deep networks are more suitable for representing complex relationships in data.

Despite the success of deep learning, research into the reasons for their effectiveness is ongoing. Different theories propose that deep networks benefit from faster training, reduced local minima, and better representations for complex data. However, none of these theories has been conclusively proven.

Significant progress has been made in deep learning through trial and error, but fundamental understanding remains elusive. The field is constantly evolving, with new techniques and insights emerging regularly."
"Why normalize images by subtracting dataset&#39;s image mean, instead of the current image mean in deep learning?",https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current,deep-learning|image-processing,211436,True,220970,224759,5,158,1610822271,1462705915,116,1467098699,"Subtracting the dataset mean serves to ""center"" the data. Additionally, you ideally would like to divide by the sttdev of that feature or pixel as well if you want to normalize each feature value to a z-score. 
The reason we do both of those things is because in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. 
We'd like in this process for each feature to have a similar range so that our gradients don't go out of control (and that we only need one global learning rate multiplier). 
Another way you can think about it is deep learning networks traditionally share many parameters - if you didn't scale your inputs in a way that resulted in similarly-ranged feature values (ie: over the whole dataset by subtracting mean) sharing wouldn't happen very easily because to one part of the image weight w is a lot and to another it's too small.
You will see in some CNN models that per-image whitening is used, which is more along the lines of your thinking.  
","**Summary:**

Data preprocessing is crucial for training deep learning models. Centering data by subtracting the mean ""normalizes"" it by aligning its values around zero. Normalizing data further by dividing by the standard deviation ensures that each feature value has a similar range.

These preprocessing steps facilitate training by preventing gradients from becoming too large or too small. Additionally, they promote parameter sharing in the network, as weights and biases are applied to similarly scaled feature values, regardless of the image or location within the image.

Some CNN models use per-image whitening as an alternative normalization technique, which aligns the distribution of each image individually rather than the entire dataset."
"Why are neural networks becoming deeper, but not wider?",https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider,machine-learning|classification|neural-networks|deep-learning|convolutional-neural-network,222883,True,223637,91197,6,152,1629227205,1468046112,172,1468440687,"As a disclaimer, I work on neural nets in my research, but I generally use relatively small, shallow neural nets rather than the really deep networks at the cutting edge of research you cite in your question.  I am not an expert on the quirks and peculiarities of very deep networks and I will defer to someone who is.
First, in principle, there is no reason you need deep neural nets at all.  A sufficiently wide neural network with just a single hidden layer can approximate any (reasonable) function given enough training data.  There are, however, a few difficulties with using an extremely wide, shallow network.  The main issue is that these very wide, shallow networks are very good at memorization, but not so good at generalization.  So, if you train the network with every possible input value, a super wide network could eventually memorize the corresponding output value that you want.  But that's not useful because for any practical application you won't have every possible input value to train with.
The advantage of multiple layers is that they can learn features at various levels of abstraction.  For example, if you train a deep convolutional neural network to classify images, you will find that the first layer will train itself to recognize very basic things like edges, the next layer will train itself to recognize collections of edges such as shapes, the next layer will train itself to recognize collections of shapes like eyes or noses, and the next layer will learn even higher-order features like faces.  Multiple layers are much better at generalizing because they learn all the intermediate features between the raw data and the high-level classification.
So that explains why you might use a deep network rather than a very wide but shallow network.  But why not a very deep, very wide network?  I think the answer there is that you want your network to be as small as possible to produce good results.  As you increase the size of the network, you're really just introducing more parameters that your network needs to learn, and hence increasing the chances of overfitting.  If you build a very wide, very deep network, you run the chance of each layer just memorizing what you want the output to be, and you end up with a neural network that fails to generalize to new data.
Aside from the specter of overfitting, the wider your network, the longer it will take to train.  Deep networks already can be very computationally expensive to train, so there's a strong incentive to make them wide enough that they work well, but no wider.
","**Summary:**

Deep neural networks are more effective than shallow networks because:

* **Multi-layer architecture:** Layers learn features at various levels of abstraction, improving generalization.
* **Feature extraction:** Each layer captures different aspects of the input data, leading to a hierarchical representation.
* **Avoids overfitting:** Shallow and wide networks tend to memorize training data instead of generalizing to new inputs.

Despite the benefits, it's crucial to balance depth and width:

* **Excessive width:** Increases memorization and computational cost without improving generalization.
* **Excessive depth:** Introduces additional parameters, increasing the risk of overfitting.

Therefore, it's optimal to design neural networks that are as narrow and shallow as possible while achieving satisfactory performance. This minimizes overfitting, reduces computational cost, and improves generalization."
Difference between neural net weight decay and learning rate,https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate,neural-networks|terminology,29130,True,31334,184361,4,146,1608146542,1337923047,225,1340942319,"The learning rate is a parameter that determines how much an updating step influences the current value of the weights. While weight decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled.
So let's say that we have a cost or error function $E(\mathbf{w})$ that we want to minimize. Gradient descent tells us to modify the weights $\mathbf{w}$ in the direction of steepest descent in $E$:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i},
\end{equation}
where $\eta$ is the learning rate, and if it's large you will have a correspondingly large modification of the weights $w_i$ (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).
In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function. An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to $\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2}\mathbf{w}^2$. In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter $\lambda$ determines how you trade off the original cost $E$ with the large weights penalization.
Applying gradient descent to this new cost function we obtain:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}-\eta\lambda w_i.
\end{equation}
The new term $-\eta\lambda w_i$ coming from the regularization causes the weight to decay in proportion to its size.
","**Summary:**

**Learning Rate and Weight Decay**

The learning rate controls the magnitude of weight adjustments during training. Small learning rates result in gradual changes, while large learning rates can lead to overfitting.

**Weight Decay**

Weight decay is a regularization technique that prevents overfitting by penalizing large weight values. It modifies the update rule to include a term that causes weights to decay exponentially towards zero.

**Regularization**

Regularization limits model complexity and reduces overfitting. One common regularization method is to introduce a Gaussian prior over the weights, which adds a penalty term for large weights to the cost function.

**Impact of Weight Decay on Regularization**

When weight decay is applied to the regularized cost function, it causes weights to decay in proportion to their size. This further restricts model complexity and reduces overfitting."
"What is the difference between convolutional neural networks, restricted Boltzmann machines, and auto-encoders?",https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma,neural-networks|deep-learning|convolutional-neural-network|autoencoders|restricted-boltzmann-machine,114385,True,117188,118446,4,140,1505214911,1409863944,252,1411998350,"Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. E.g. in a network like this:

output[i] has edge back to input[i] for every i. Typically, number of hidden units is much less then number of visible (input/output) ones. As a result, when you pass data through such a network, it first compresses (encodes) input vector to ""fit"" in a smaller representation, and then tries to reconstruct (decode) it back. The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation (encoding) for input data. 
RBM shares similar idea, but uses stochastic approach. Instead of deterministic (e.g. logistic or ReLU) it uses stochastic units with particular (usually binary of Gaussian) distribution. Learning procedure consists of several steps of Gibbs sampling (propagate: sample hiddens given visibles; reconstruct: sample visibles given hiddens; repeat) and adjusting the weights to minimize reconstruction error. 

Intuition behind RBMs is that there are some visible random variables (e.g. film reviews from different users) and some hidden variables (like film genres or other internal features), and the task of training is to find out how these two sets of variables are actually connected to each other (more on this example may be found here (http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)). 
Convolutional Neural Networks are somewhat similar to these two, but instead of learning single global weight matrix between two layers, they aim to find a set of locally connected neurons. CNNs are mostly used in image recognition. Their name comes from ""convolution"" operator (http://en.wikipedia.org/wiki/Convolution) or simply ""filter"". In short, filters are an easy way to perform complex operation by means of simple change of a convolution kernel. Apply Gaussian blur kernel and you'll get it smoothed. Apply Canny kernel and you'll see all edges. Apply Gabor kernel to get gradient features. 

(image from here (https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html))
The goal of convolutional neural networks is not to use one of predefined kernels, but instead to learn data-specific kernels.  The idea is the same as with autoencoders or RBMs - translate many low-level features (e.g. user reviews or image pixels) to the compressed high-level representation (e.g. film genres or edges) - but now weights are learned only from neurons that are spatially close to each other. 
 
All three models have their use cases, pros and cons, but probably the most important properties are: 

Autoencoders are simplest ones. They are intuitively understandable, easy to implement and to reason about (e.g. it's much easier to find good meta-parameters for them than for RBMs).
RBMs are generative. That is, unlike autoencoders that only discriminate some data vectors in favour of others, RBMs can also generate new data with given joined distribution. They are also considered more feature-rich and flexible. 
CNNs are very specific model that is mostly used for very specific task (though pretty popular task). Most of the top-level algorithms in image recognition are somehow based on CNNs today, but outside that niche they are hardly applicable (e.g. what's the reason to use convolution for film review analysis?). 

UPD. 
Dimensionality reduction
When we represent some object as a vector of $n$ elements, we say that this is a vector in $n$-dimensional space. Thus, dimensionality reduction refers to a process of refining data in such a way, that each data vector $x$ is translated into another vector $x'$ in an $m$-dimensional space (vector with $m$ elements), where $m < n$. Probably the most common way of doing this is PCA (http://en.wikipedia.org/wiki/Principal_component_analysis). Roughly speaking, PCA finds ""internal axes"" of a dataset (called ""components"") and sorts them by their importance. First $m$ most important components are then used as new basis. Each of these components may be thought of as a high-level feature, describing data vectors better than original axes. 
Both - autoencoders and RBMs - do the same thing. Taking a vector in $n$-dimensional space they translate it into an $m$-dimensional one, trying to keep as much important information as possible and, at the same time, remove noise. If training of autoencoder/RBM was successful, each element of resulting vector (i.e. each hidden unit) represents something important about the object - shape of an eyebrow in an image, genre of a film, field of study in scientific article, etc. You take lots of noisy data as an input and produce much less data in a much more efficient representation. 
Deep architectures
So, if we already had PCA, why the hell did we come up with autoencoders and RBMs? It turns out that PCA only allows linear transformation of a data vectors. That is, having $m$ principal components $c_1..c_m$, you can represent only vectors $x=\sum_{i=1}^{m}w_ic_i$. This is pretty good already, but not always enough. No matter, how many times you will apply PCA to a data - relationship will always stay linear. 
Autoencoders and RBMs, on other hand, are non-linear by the nature, and thus, they can learn more complicated relations between visible and hidden units. Moreover, they can be stacked, which makes them even more powerful. E.g. you train RBM with $n$ visible and $m$ hidden units, then you put another RBM with $m$ visible and $k$ hidden units on top of the first one and train it too, etc. And exactly the same way with autoencoders. 
But you don't just add new layers. On each layer you try to learn best possible representation for a data from the previous one: 

On the image above there's an example of such a deep network. We start with ordinary pixels, proceed with simple filters, then with face elements and finally end up with entire faces! This is the essence of deep learning. 
Now note, that at this example we worked with image data and sequentially took
larger and larger areas of spatially close pixels. Doesn't it sound similar? Yes, because it's an example of deep convolutional network. Be it based on autoencoders or RBMs, it uses convolution to stress importance of locality. That's why CNNs are somewhat distinct from autoencoders and RBMs. 
Classification
None of models mentioned here work as classification algorithms per se. Instead, they are used for pretraining - learning transformations from low-level and hard-to-consume representation (like pixels) to a high-level one. Once deep (or maybe not that deep) network is pretrained, input vectors are transformed to a better representation and resulting vectors are finally passed to real classifier (such as SVM or logistic regression). In an image above it means that at the very bottom there's one more component that actually does classification. 
","**Autoencoders:**

* Three-layer neural networks that connect output units back to input units.
* Compress and then reconstruct input data, aiming for efficient data representation.
* Simple and easy to train.

**Restricted Boltzmann Machines (RBMs):**

* Similar to autoencoders but use stochastic units and a Gibbs sampling approach.
* Can generate new data based on learned distributions.
* Considered more feature-rich and flexible than autoencoders.

**Convolutional Neural Networks (CNNs):**

* Focus on extracting local features rather than global representations.
* Use filters (convolution kernels) to learn data-specific features.
* Primarily used in image recognition, where they excel at detecting patterns and edges.

**Dimensionality Reduction:**

* Autoencoders and RBMs perform dimensionality reduction by translating data from a high-dimensional space to a lower-dimensional space.
* This helps remove noise and extract important features.

**Deep Architectures:**

* Autoencoders and RBMs can be stacked to create deep neural networks.
* Non-linear transformations allow them to learn complex relationships between data.

**Classification:**

* Autoencoders, RBMs, and CNNs are typically not used directly for classification.
* They are often used for pretraining, transforming data into a more suitable representation for subsequent classification algorithms."
How is it possible that validation loss is increasing while validation accuracy is increasing as well,https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy,neural-networks|deep-learning|convolutional-neural-network|overfitting,282160,True,341054,207687,6,140,1711361645,1495980810,91,1523972701,"Other answers explain well how accuracy and loss are not necessarily exactly (inversely) correlated, as loss measures a difference between raw output (float) and a class (0 or 1 in the case of binary classification), while accuracy measures the difference between thresholded output (0 or 1) and class. So if raw outputs change, loss changes but accuracy is more ""resilient"" as outputs need to go over/under a threshold to actually change accuracy.
However, accuracy and loss intuitively seem to be somewhat (inversely) correlated, as better predictions should lead to lower loss and higher accuracy, and the case of higher loss and higher accuracy shown by OP is surprising. I have myself encountered this case several times, and I present here my conclusions based on the analysis I had conducted at the time. I stress that this answer is therefore purely based on experimental data I encountered, and there may be other reasons for OP's case.

Let's consider the case of binary classification, where the task is to predict whether an image is a cat or a dog, and the output of the network is a sigmoid (outputting a float between 0 and 1), where we train the network to output 1 if the image is one of a cat and 0 otherwise. In this case, two phenomenons can happen at the same time :
 (https://i.sstatic.net/gppxn.png)

Some images with borderline predictions get predicted better and so their output class changes to the correct one (image C in the figure). This is the classic ""loss decreases while accuracy increases"" behavior that we expect when training is going well.

Some images with very bad predictions keep getting worse (image D). This leads to a less classic ""loss increases while accuracy stays the same"".
You might think that this effect should be ""countered"" by the inverse phenomenon of images with very good predictions getting better (images A and B, note that their output has increased the same amount  than the one of image D has decreased (0.02)). However, when using cross-entropy loss for classification (as it is usually done), bad predictions are penalized much more strongly than good predictions are rewarded. For a cat image    (ground truth : 1), the loss is $-log(output)$, so slightly better predictions for images already close to 1 don't change the mean loss much (as the difference between log(<number close to 1>) and log(<number slightly closer to 1>) is low), while a single misclassified cat image getting worse can ""blow up"" your mean loss (as the difference between log(<number close to 0>) and log(<number slightly closer to 0>) is high). See this answer (https://stats.stackexchange.com/a/448033/201218) for further illustration of this phenomenon.
(Getting increasing loss and stable accuracy could also be caused by good predictions being classified a little worse, but I find it less likely because of this loss ""asymmetry"").


So I think that when both accuracy and loss are increasing, the network is starting to overfit, and both phenomena are happening at the same time. The network is starting to learn patterns only relevant for the training set and not great for generalization, leading to phenomenon 2 where some images from the validation set get predicted really wrong (image D), with an effect amplified by the ""loss asymmetry"". However, it is at the same time still learning some patterns which are useful for generalization (phenomenon one, ""good learning"") as more and more images are being correctly classified (images A, B, C).
I sadly have no answer for whether or not this ""overfitting"" is a bad thing in this case: should we stop the learning once the network is starting to learn spurious patterns, even though it's continuing to learn useful ones along the way?
Finally, I think this effect can be further obscured in the case of multi-class classification, where the network at a given epoch might be severely overfit on some classes but still learning on others.
","Accuracy and loss functions are not directly inversely correlated. Accuracy measures the difference between predicted and actual class values, while loss measures the difference between predicted and actual values (which may be decimals in the case of a sigmoid function).

In binary classification, where the network predicts whether an image is a cat or dog, two phenomena can occur simultaneously:

* Improved predictions (increased accuracy) and decreased loss
* Worsening predictions (decreased accuracy) but stable loss due to the penalization of incorrect predictions in the loss function.

When both accuracy and loss increase, the network may be overfitting, learning both useful patterns for generalization and irrelevant patterns that lead to incorrect predictions.

Whether this overfitting is beneficial or not is unclear, as the network may continue to learn useful patterns even as it develops irrelevant ones. This effect may be less noticeable in multi-class classification, where overfitting can occur on specific classes while learning continues on others."
tanh activation function vs sigmoid activation function,https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function,machine-learning|neural-networks|optimization|sigmoid-curve,101560,True,101563,157154,3,123,1723522466,1402207884,120,1402210058,"Yes it matters for technical reasons. Basically for optimization. It is worth reading Efficient Backprop (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by LeCun et al.
There are two reasons for that choice (assuming you have normalized your data, and this is very important):

Having stronger gradients: since data is centered around 0, the derivatives are higher. To see this, calculate the derivative of the tanh function and notice that its range (output values) is [0,1].

The range of the tanh function is [-1,1] and that of the sigmoid function is [0,1]

Avoiding bias in the gradients. This is explained very well in the paper, and it is worth reading it to understand these issues.

","Using the tanh instead of sigmoid for activation functions is advantageous for optimization. Firstly, tanh has a symmetric range of [-1,1], while sigmoid has a range of [0,1]. This leads to stronger gradients in the tanh case, as the derivatives are higher when data is centered around zero. Secondly, tanh avoids bias in the gradients, ensuring that the backpropagation algorithm converges more effectively. For a detailed explanation of these benefits, refer to ""Efficient Backprop"" by LeCun et al. (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)."
Validation Error less than training error?,https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error,machine-learning|mathematical-statistics|neural-networks|cross-validation,187335,True,187404,216324,10,121,1614730827,1450389872,118,1450446690,"It is difficult to be certain without knowing your actual methodology (e.g. cross-validation method, performance metric, data splitting method, etc.).
Generally speaking though, training error will almost always underestimate your validation error.  However it is possible for the validation error to be less than the training.  You can think of it two ways:

Your training set had many 'hard' cases to learn
Your validation set had mostly 'easy' cases to predict

That is why it is important that you really evaluate your model training methodology.  If you don't split your data for training properly your results will lead to confusing, if not simply incorrect, conclusions.
I think of model evaluation in four different categories:

Underfitting â Validation and training error high
Overfitting â Validation error is high, training error low
Good fit â Validation error low, slightly higher than the training error
Unknown fit - Validation error low, training error 'high'

I say 'unknown' fit because the result is counter intuitive to how machine learning works.  The essence of ML is to predict the unknown.  If you are better at predicting the unknown than what you have 'learned', AFAIK the data between training and validation must be different in some way.  This could mean you either need to reevaluate your data splitting method, adding more data, or possibly changing your performance metric (are you actually measuring the performance you want?).
EDIT
To address the OP's reference to a previous python lasagne question (https://stats.stackexchange.com/questions/178371/python-lasagne-tutorial-validation-error-lower-than-training-error?lq=1).
This suggests that you have sufficient data to not require cross-validation and simply have your training, validation, and testing data subsets.  Now, if you look at the lasagne tutorial (http://lasagne.readthedocs.org/en/latest/user/tutorial.html) you can see that the same behavior is seen at the top of the page.  I would find it hard to believe the authors would post such results if it was strange but instead of just assuming they are correct let's look further.  The section of most interest to us here is in the training loop section, just above the bottom you will see how the loss parameters are calculated.
The training loss is calculated over the entire training dataset.  Likewise, the validation loss is calculated over the entire validation dataset.  The training set is typically at least 4 times as large as the validation (80-20).  Given that the error is calculated over all samples, you could expect up to approximately 4X the loss measure of the validation set.  You will notice, however, that the training loss and validation loss are approaching one another as training continues.  This is intentional as if your training error begins to get lower than your validation error you would be beginning to overfit your model!!!
I hope this clarifies these errors.  
","**Summary:**

Predicting the validation error of a model is difficult without knowing the specific methodology used. However, it is generally expected that the training error will underestimate the validation error. This is because the training data may contain difficult cases that the model has not learned well, while the validation data may contain easier cases that the model can predict accurately.

Model evaluation can be classified into four categories:

* **Underfitting:** Both validation and training errors are high.
* **Overfitting:** Validation error is high, while training error is low.
* **Good fit:** Validation error is low and slightly higher than the training error.
* **Unknown fit:** Validation error is low, while training error is counterintuitively high. This suggests that the data between training and validation may differ in some way.

In the context of a specific question about the Lasagne Python library, it is explained that the training error is calculated over the entire training dataset, which is typically much larger than the validation dataset. Therefore, the training error can be expected to be approximately 4 times higher than the validation error. However, as training progresses, the training and validation errors should approach each other. If the training error starts to fall below the validation error, it indicates the model may be overfitting."
Is it possible to train a neural network without backpropagation?,https://stats.stackexchange.com/questions/235862/is-it-possible-to-train-a-neural-network-without-backpropagation,machine-learning|neural-networks|optimization|backpropagation,235862,True,235868,54448,6,121,1537898940,1474336101,94,1474338646,"The first two algorithms you mention (Nelder-Mead and Simulated Annealing) are generally considered pretty much obsolete in optimization circles, as there are much better alternatives which are both more reliable and less costly. Genetic algorithms covers a wide range, and some of these can be reasonable.
However, in the broader class of derivative-free optimization (DFO) algorithms, there are many which are significantly better than these ""classics"", as this has been an active area of research in recent decades. So, might some of these newer approaches be reasonable for deep learning?
A relatively recent paper comparing the state of the art is the following:

Rios, L. M., & Sahinidis, N. V. (2013) Derivative-free optimization: a review of algorithms and comparison of software implementations. (https://scholar.google.com/scholar?cluster=13996631775177561404) Journal of Global Optimization.

This is a nice paper which has many interesting insights into recent techniques. For example, the results clearly show that the best local optimizers are all ""model-based"", using different forms of sequential quadratic programming (https://en.wikipedia.org/wiki/Sequential_quadratic_programming) (SQP). 
However, as noted in their abstract ""We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size."" To give an idea of the numbers, for all problems the solvers were given a budget of 2500 function evaluations, and problem sizes were a maximum of ~300 parameters to optimize. Beyond O[10] parameters, very few of these optimizers performed very well, and even the best ones showed a noticable decay in performance as problem size was increased.
So for very high dimensional problems, DFO algorithms just are not competitive with derivative based ones. To give some perspective, PDE (partial differential equation)-based optimization (https://scholar.google.com/scholar?q=pde%20based%20optimization) is another area with very high dimensional problems (e.g. several parameter for each cell of a large 3D finite element grid). In this realm, the ""adjoint method (https://scicomp.stackexchange.com/questions/14259/understanding-the-cost-of-adjoint-method-for-pde-constrained-optimization)"" is one of the most used methods. This is also a gradient-descent optimizer based on automatic differentiation of a forward model code.
The closest to a high-dimensional DFO optimizer is perhaps the Ensemble Kalman Filter (https://en.wikipedia.org/wiki/Ensemble_Kalman_filter), used for assimilating data into complex PDE simulations, e.g. weather models. Interestingly, this is essentially an SQP approach, but with a Bayesian-Gaussian interpretation (so the quadratic model is positive definite, i.e. no saddle points). But I do not think that the number of parameters or observations in these applications is comparable to what is seen in deep learning.
Side note (local minima): From the little I have read on deep learning, I think the consensus is that it is saddle points rather than local minima, which are most problematic for high dimensional NN-parameter spaces.
For example, the recent review (https://scholar.google.com/scholar?cluster=5362332738201102290) in Nature says ""Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general. Instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the remainder.""
A related concern is about local vs. global optimization (for example this question (https://stats.stackexchange.com/questions/207450/in-neural-nets-why-use-gradient-methods-rather-than-other-metaheuristics) pointed out in the comments). While I do not do deep learning, in my experience overfitting is definitely a valid concern. In my opinion, global optimization methods are most suited for engineering design problems that do not strongly depend on ""natural"" data. In data assimilation problems, any current global minima could easily change upon addition of new data (caveat: My experience is concentrated in geoscience problems, where data is generally ""sparse"" relative to model capacity).
An interesting perspective is perhaps

O. Bousquet & L. Bottou (2008) The tradeoffs of large scale learning. (https://scholar.google.com/scholar?cluster=1338734004801715686) NIPS.

which provides semi-theoretical arguments on why and when approximate optimization may be preferable in practice.
End note (meta-optimization): While gradient based techniques seem likely to be dominant for training networks, there may be a role for DFO in associated meta-optimization tasks.
One example would be hyper-parameter tuning. (Interestingly, the successful model-based DFO optimizers from Rios & Sahinidis (https://scholar.google.com/scholar?cluster=13996631775177561404) could be seen as essentially solving a sequence of design-of-experiments/response-surface (https://en.wikipedia.org/wiki/Response_surface_methodology) problems.)
Another example might be designing architectures, in terms of the set-up of layers (e.g. number, type, sequence, nodes/layer). In this discrete-optimization context genetic-style algorithms may be more appropriate. Note that here I am thinking of the case where connectivity is determined implicitly by these factors (e.g. fully-connected layers, convolutional layers, etc.). In other words the $\mathrm{O}[N^2]$ connectivity is $not$ meta-optimized explicitly. (The connection strength would fall under training, where e.g. sparsity could be promoted by $L_1$ regularization and/or ReLU activations ... these choices could be meta-optimized however.)
","**Summary:**

Current optimization algorithms for deep learning include Nelder-Mead, Simulated Annealing, Genetic Algorithms, and Derivative-Free Optimization (DFO) algorithms. However, most of these, including the ""classics"" Nelder-Mead and Simulated Annealing, are considered obsolete due to better alternatives.

Among the newer DFO algorithms, model-based optimizers using sequential quadratic programming (SQP) are the best local optimizers. However, their performance diminishes significantly with increasing problem size. For very high-dimensional problems, DFO algorithms are not competitive with derivative-based ones.

The Ensemble Kalman Filter (EnKF) is a high-dimensional DFO optimizer used in weather modeling, but it is not directly applicable to deep learning due to the difference in problem scale.

In deep learning, it is generally agreed that saddle points rather than local minima are the primary challenge. While gradient-based methods are typically used, DFO algorithms may have a role in meta-optimization tasks such as hyper-parameter tuning and architecture design."
What is an embedding layer in a neural network?,https://stats.stackexchange.com/questions/182775/what-is-an-embedding-layer-in-a-neural-network,machine-learning|neural-networks|python|word-embeddings,182775,True,188603,110599,2,116,1726967059,1448037792,82,1451405367,"Relation to Word2Vec

Word2Vec in a simple picture:
 (https://i.sstatic.net/MfyBJ.png)
(source: netdna-ssl.com (https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Word2Vec-Training-Models-768x468.png))
More in-depth explanation:
I believe it's related to the recent Word2Vec innovation in natural language processing. Roughly, Word2Vec means our vocabulary is discrete and we will learn an map which will embed each word into a continuous vector space. Using this vector space representation will allow us to have a continuous, distributed representation of our vocabulary words. If for example our dataset consists of n-grams, we may now use our continuous word features to create a distributed representation of our n-grams. In the process of training a language model we will learn this word embedding map. The hope is that by using a continuous representation, our embedding will map similar words to similar regions. For example in the landmark paper Distributed Representations of Words and Phrases
and their Compositionality (https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf), observe in Tables 6 and 7 that certain phrases have very good nearest neighbour phrases from a semantic point of view. Transforming into this continuous space allows us to use continuous metric notions of similarity to evaluate the semantic quality of our embedding.
Explanation using Lasagne code
Let's break down the Lasagne code snippet:
x = T.imatrix()

x is a matrix of integers. Okay, no problem. Each word in the vocabulary can be represented an integer, or a 1-hot sparse encoding. So if x is 2x2, we have two datapoints, each being a 2-gram.
l_in = InputLayer((3, ))

The input layer. The 3 represents the size of our vocabulary. So we have words $w_0, w_1, w_2$ for example.
W = np.arange(3*5).reshape((3, 5)).astype('float32')

This is our word embedding matrix. It is a 3 row by 5 column matrix with entries 0 to 14.
Up until now we have the following interpretation. Our vocabulary has 3 words and we will embed our words into a 5 dimensional vector space. For example, we may represent one word $w_0 = (1,0,0)$, and another word $w_1 = (0, 1, 0)$ and the other word $w_2 = (0, 0, 1)$, e.g. as hot sparse encodings.  We can view the $W$ matrix as embedding these words via matrix multiplication. Therefore the first word $w_0 \rightarrow w_0W = [0, 1, 2, 3, 4].$ Similarly, $w_1 \rightarrow w_1W = [5, 6, 7, 8, 9]$.
It should be noted, due to the one-hot sparse encoding we are using, you also see this referred to as table lookups.
l1 = EmbeddingLayer(l_in, input_size=3, output_size=5, W=W)

The embedding layer
output = get_output(l1, x)

Symbolic Theano expression for the embedding.
f = theano.function([x], output)

Theano function which computes the embedding.
x_test = np.array([[0, 2], [1, 2]]).astype('int32')

It's worth pausing here to discuss what exactly x_test means. First notice that all of x_test entries are in $\{0, 1, 2\}$, i.e. range(3). x_test has 2 datapoints. The first datapoint [0, 2] represents the 2-gram $(w_0, w_2)$ and the second datapoint represents the 2-gram $(w_1, w_2)$.
We wish to embed our 2-grams using our word embedding layer now. Before we do that, let's make sure we're clear about what should be returned by our embedding function f. The 2 gram $(w_0, w_2)$ is equivalent to a [[1, 0, 0], [0, 0, 1]] matrix. Applying our embedding matrix $W$ to this sparse matrix should yield: [[0, 1, 2, 3, 4], [10, 11, 12, 13, 14]]. Note in order to have the matrix multiplication work out, we have to apply the word embedding matrix $W$ via right multiplication to the sparse matrix representation of our 2-gram.
f(x_test) 

returns:
array([[[  0.,   1.,   2.,   3.,   4.],
        [ 10.,  11.,  12.,  13.,  14.]],
       [[  5.,   6.,   7.,   8.,   9.],
        [ 10.,  11.,  12.,  13.,  14.]]], dtype=float32)

To convince you that the 3 does indeed represent the vocabulary size, try inputting a matrix x_test = [[5, 0], [1, 2]]. You will see that it raises a matrix mis-match error.
","**Relation to Word2Vec:**
Word2Vec embodies words as vectors in a continuous space, allowing for similarity comparisons based on vector distance. This enables the distributed representation of n-grams and the analysis of semantic relationships.

**Lasagne Code Explanation:**
The code snippet demonstrates the use of Lasagne to create a word embedding layer. The vocabulary size is defined as 3, and each word is embedded as a 5-dimensional vector. The word embedding matrix is initialized with values from 0 to 14.

The data is then represented as 2-grams in a sparse matrix format, where each row represents a 2-gram. The embedding function is applied to these 2-grams, resulting in a matrix that represents each 2-gram as a pair of 5-dimensional vectors. The dimensions of the output matrix are consistent with the vocabulary size and embedding size specified in the code."
What&#39;re the differences between PCA and autoencoder?,https://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder,machine-learning|pca|neural-networks|autoencoders,120080,True,120096,50984,4,91,1587820574,1413358015,81,1413363301,"PCA is restricted to a linear map, while auto encoders can have nonlinear enoder/decoders.
A single layer auto encoder with linear transfer function is nearly equivalent to PCA, where nearly means that the $W$ found by AE and PCA won't necessarily be the same - but the subspace spanned by the respective $W$'s will.
","**Summary:**

Principal Component Analysis (PCA) and autoencoders are both techniques for dimensionality reduction. PCA is a linear transformation that identifies the directions of maximum variance in the data, while autoencoders are nonlinear models that learn a compressed representation of the input.

Autoencoders have the advantage over PCA in that they can capture nonlinear relationships in the data. However, when the autoencoder has a single layer with a linear transfer function, it becomes nearly equivalent to PCA. This means that the subspaces spanned by the linear transformations learned by PCA and the autoencoder will be the same.

In other words, while PCA is restricted to linear transformations, autoencoders can handle nonlinear transformations. However, when the autoencoder is simplified to a linear model, its behavior closely resembles that of PCA."
How to apply Neural Network to time series forecasting?,https://stats.stackexchange.com/questions/10162/how-to-apply-neural-network-to-time-series-forecasting,time-series|forecasting|neural-networks,10162,True,10196,88115,1,91,1531649411,1304122279,110,1304231803,"Here is a simple recipe that may help you get started writing code and testing ideas...
Let's assume you have monthly data recorded over several years, so you have 36 values. Let's also assume that you only care about predicting one month (value) in advance.

Exploratory data analysis: Apply some of the traditional time series analysis methods to estimate the lag dependence in the data (e.g. auto-correlation and partial auto-correlation plots, transformations, differencing).
Let's say that you find a given month's value is correlated with the past three month's data but not much so beyond that.
Partition your data into training and validation sets: Take the first 24 points as your training values and the remaining points as the validation set.
Create the neural network layout: You'll take the past three month's values as inputs and you want to predict the next month's value. So, you need a neural network with an input layer containing three nodes and an output layer containing one node. You should probably have a hidden layer with at least a couple of nodes. Unfortunately, picking the number of hidden layers, and their respective number of nodes, is not something for which there are clear guidelines. I'd start small, like 3:2:1.
Create the training patterns: Each training pattern will be four values, with the first three corresponding to the input nodes and the last one defining what the correct value is for the output node. For example, if your training data are values $$x_1,x_2\dots,x_{24}$$ then $$pattern 1:  x_1,x_2,x_3,x_4$$ $$pattern 2: x_2,x_3,x_4,x_5$$ $$\dots$$ $$pattern 21: x_{21},x_{22},x_{23},x_{24}$$ 
Train the neural network on these patterns
Test the network on the validation set (months 25-36): Here you will pass in the three values the neural network needs for the input layer and see what the output node gets set to. So, to see how well the trained neural network can predict month 32's value you'll pass in values for months 29, 30, and 31

This recipe is obviously high level and you may scratch your head at first when trying to map your context into different software libraries/programs. But, hopefully this sketches out the main point: you need to create training patterns that reasonably contain the correlation structure of the series you are trying to forecast. And whether you do the forecasting with a neural network or an ARIMA model, the exploratory work to determine what that structure is is often the most time consuming and difficult part.
In my experience, neural networks can provide great classification and forecasting functionality but setting them up can be time consuming. In the example above, you may find that 21 training patterns is not enough; different input data transformations lead to a better/worse forecasts; varying the number of hidden layers and hidden layer nodes greatly affects forecasts; etc.   
I highly recommend looking at the neural_forecasting (http://www.neural-forecasting-competition.com/index.htm) website, which contains tons of information on neural network forecasting competitions. The Motivations (http://www.neural-forecasting-competition.com/motivation.htm) page is especially useful.
","**Summary:**

To write code and test forecasting ideas, start with exploratory data analysis to determine the lag dependence in the time series. For a simple recipe, assume monthly data, predicting one month ahead.

**Steps:**

1. **Exploratory Data Analysis:** Use autocorrelation/partial autocorrelation plots to identify correlation with past values (e.g., find a correlation with the past three months).

2. **Data Partitioning:** Divide data into training (first 24 values) and validation (remaining) sets.

3. **Neural Network Layout:** Create a neural network with an input layer with three nodes (past three months' values) and an output layer with one node (predicted value). Add a hidden layer with a couple of nodes to start (e.g., 3:2:1).

4. **Training Patterns:** Create patterns with the past three months' values as inputs and the next month's value as the correct output.

5. **Training:** Train the neural network on these patterns.

6. **Validation:** Test the trained network on the validation set by using the three input months to predict the next month's value.

**Considerations:**

* Determining the number of hidden layers and nodes is not straightforward.
* Neural networks can provide accurate forecasts, but setup can be time-consuming.
* Explore neural_forecasting (http://www.neural-forecasting-competition.com/index.htm) for additional resources and competition information."
ImageNet: what is top-1 and top-5 error rate?,https://stats.stackexchange.com/questions/156471/imagenet-what-is-top-1-and-top-5-error-rate,classification|neural-networks|error|measurement-error|image-processing,156471,True,156515,104520,2,90,1630208443,1434021990,103,1434036122,"
[...] where the top-5 error rate is the fraction of test images for which
the correct label is not among the five labels considered most
probable by the mode.

First, you make a prediction using the CNN and obtain the predicted class multinomial distribution ($\sum p_{class} = 1$).
Now, in the case of the top-1 score, you check if the top class (the one with the highest probability) is the same as the target label.
In the case of the top-5 score, you check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).
In both cases, the top score is computed as the number of times a predicted label matched the target label, divided by the number of data points evaluated.
Finally, when 5-CNNs are used, you first average their predictions and follow the same procedure for calculating the top-1 and top-5 scores.
","**Summary:**

To evaluate the performance of a Convolutional Neural Network (CNN) model, two metrics are used: top-1 error rate and top-5 error rate.

The top-1 error rate measures the percentage of test images for which the correct label is not the predicted most probable class.

The top-5 error rate measures the percentage of test images for which the correct label is not among the five most probable classes predicted by the model.

To calculate these scores, the model first predicts the probability distribution of each class for each input image. The top-1 score is then computed by comparing the predicted most probable class with the true label. The top-5 score is computed by checking if the true label is among the five most probable predictions.

When using multiple CNNs, their predictions are averaged before calculating the top-1 and top-5 scores."
What is global max pooling layer and what is its advantage over maxpooling layer?,https://stats.stackexchange.com/questions/257321/what-is-global-max-pooling-layer-and-what-is-its-advantage-over-maxpooling-layer,neural-networks|convolutional-neural-network|pooling,257321,True,257325,95093,2,86,1510308459,1484931313,99,1484932035,"Global max pooling  =  ordinary max pooling layer with pool size equals to the size of the input (minus filter size + 1, to be precise). You can see that MaxPooling1D takes a pool_length argument, whereas GlobalMaxPooling1D does not.
For example, if the input of the max pooling layer  is $0,1,2,2,5,1,2$, global max pooling outputs $5$, whereas  ordinary max pooling layer with pool size equals to 3 outputs $2,2,5,5,5$ (assuming stride=1).
This can be seen in the code (https://github.com/fchollet/keras/blob/3d176e926f848c5aacd036d6095ab015a2f8cc83/keras/layers/pooling.py#L433):
class GlobalMaxPooling1D(_GlobalPooling1D):
    """"""Global max pooling operation for temporal data.
    # Input shape
        3D tensor with shape: `(samples, steps, features)`.
    # Output shape
        2D tensor with shape: `(samples, features)`.
    """"""

    def call(self, x, mask=None):
        return K.max(x, axis=1)

In some domains, such as natural language processing, it is common to use global max pooling. In some other domains, such as computer vision, it is common to use a max pooling that isn't global.
","**Summary:**

Global max pooling is a type of max pooling where the pool size is equal to the input size. Unlike regular max pooling, which produces a smaller output, global max pooling produces an output with the same dimensionality as the input.

In global max pooling, the maximum value across the entire input is extracted, providing a representation that focuses on the most prominent feature. This is useful in applications like natural language processing, where the most important words in a sentence are often indicative of its meaning.

In contrast, regular max pooling divides the input into smaller segments and extracts the maximum value from each segment, reducing the output size. This is more common in computer vision, where spatial information is important and reducing the size of the representation can be beneficial for computational efficiency."
Why do neural network researchers care about epochs?,https://stats.stackexchange.com/questions/242004/why-do-neural-network-researchers-care-about-epochs,neural-networks|gradient-descent,242004,True,242125,15254,3,84,1639229321,1477277099,75,1477331809,"In addition to Franck's answer about practicalities, and David's answer about looking at small subgroups â both of which are important points â there are in fact some theoretical reasons to prefer sampling without replacement. The reason is perhaps related to David's point (which is essentially the coupon collector's problem (https://en.wikipedia.org/wiki/Coupon_collector%27s_problem)).
In 2009, LÃ©on Bottou compared the convergence performance on a particular text classification problem ($n = 781,265$).

Bottou (2009). Curiously Fast Convergence of some
  Stochastic Gradient Descent Algorithms. Proceedings of the
  symposium on learning and data science. (author's pdf (http://leon.bottou.org/publications/pdf/slds-2009.pdf))

He trained a support vector machine via SGD with three approaches:

Random: draw random samples from the full dataset at each iteration.
Cycle: shuffle the dataset before beginning the learning process, then walk over it sequentially, so that in each epoch you see the examples in the same order.
Shuffle: reshuffle the dataset before each epoch, so that each epoch goes in a different order.

He empirically examined the convergence $\mathbb E[ C(\theta_t) - \min_\theta C(\theta) ]$, where $C$ is the cost function, $\theta_t$ the parameters at step $t$ of optimization, and the expectation is over the shuffling of assigned batches.

For Random, convergence was approximately on the order of $t^{-1}$ (as expected by existing theory at that point).
Cycle obtained convergence on the order of $t^{-\alpha}$ (with $\alpha > 1$ but varying depending on the permutation, for example $\alpha \approx 1.8$ for his Figure 1).
Shuffle was more chaotic, but the best-fit line gave $t^{-2}$, much faster than Random.

This is his Figure 1 illustrating that:
 (https://i.sstatic.net/bl3eQ.png)
This was later theoretically confirmed by the paper:

GÃ¼rbÃ¼zbalaban, Ozdaglar, and Parrilo (2015). Why Random Reshuffling Beats Stochastic Gradient Descent. arXiv:1510.08560 (https://arxiv.org/abs/1510.08560). (video of invited talk at NIPS 2015 (https://www.microsoft.com/en-us/research/video/invited-talk-incremental-methods-for-additive-cost-convex-optimization/))

Their proof only applies to the case where the loss function is strongly convex, i.e. not to neural networks. It's reasonable to expect, though, that similar reasoning might apply to the neural network case (which is much harder to analyze).
","Sampling without replacement, where each data point is selected once, has theoretical advantages over sampling with replacement, where data points can be selected multiple times. This is because it reduces bias and variance in the sample, leading to more accurate results.

A study by Bottou (2009) compared sampling methods in text classification. Random sampling, where data points are drawn randomly, converged at a rate of $t^{-1}$. Cycle sampling, where data points are iterated over in a fixed order, converged at a slower rate. However, shuffle sampling, where data points are reshuffled before each epoch, converged much faster at a rate of $t^{-2}$.

This observation was theoretically confirmed by GÃ¼rbÃ¼zbalaban et al. (2015), who showed that shuffle sampling converges faster for strongly convex loss functions. While their proof only applies to specific cases, it suggests that similar reasoning may hold for neural networks, which are more complex and challenging to analyze."
"How can an artificial neural network ANN, be used for unsupervised clustering?",https://stats.stackexchange.com/questions/140148/how-can-an-artificial-neural-network-ann-be-used-for-unsupervised-clustering,clustering|neural-networks|unsupervised-learning|self-organizing-maps,140148,True,140165,137968,3,83,1713284764,1425399661,79,1425405711,"Neural networks are widely used in unsupervised learning in order to learn better representations of the input data. For example, given a set of text documents, NN can learn a mapping from document to real-valued vector in such a way that resulting vectors are similar for documents with similar content, i.e. distance preserving. This can be achieved using, for example, auto-encoders - a model that is trained to reconstruct the original vector from a smaller representation (hidden layer activations) with reconstruction error (distance from the ID function) as cost function. This process doesn't give you clusters, but  it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.
Clustering: There are a number of different NN architectures specifically designed for clustering. The most widely known is probably self organizing maps. A SOM is a NN that has a set of neurons connected to form a topological grid (usually rectangular). When some pattern is presented to an SOM, the neuron with closest weight vector is considered a winner and its weights are adapted to the pattern, as well as the weights of its neighbourhood. In this way an SOM naturally finds data clusters. A somewhat related algorithm is growing neural gas (it is not limited to predefined number of neurons). 
Another approach is Adaptive Resonance Theory where we have two layers: ""comparison field"" and ""recognition field"". Recognition field also determines the best match (neuron) to the vector transferred from the comparison field and also have lateral inhibitory connections. Implementation details and exact equations can readily found by googling the names of these models, so I won't put them here.
","**Summary of Neural Networks in Unsupervised Learning**

Neural networks (NNs) play a major role in unsupervised learning, particularly in discovering meaningful representations of data. These representations aim to preserve the similarities between data points, benefiting tasks like clustering.

**Autoencoders for Representation Learning:**

Using autoencoders, NNs can learn representations by compressing the input data into a smaller hidden representation and then reconstructing the original input. The reconstruction error serves as the training cost function. While not directly providing clusters, the learned representations can be used for clustering algorithms.

**Clustering Architectures:**

Specific NN architectures are designed for clustering, including:

* **Self Organizing Maps (SOMs):** These NNs use a topological grid of neurons. When presented with data, the neuron with the closest weight vector is selected as the winner, and its weights and neighbors are adjusted, facilitating cluster discovery.
* **Growing Neural Gas (GNG):** A variant of SOMs that dynamically adds neurons as needed, avoiding the limitation of predefining the number of neurons.
* **Adaptive Resonance Theory (ART):** ART has two layers: the comparison field, which identifies the best match to an input vector, and the recognition field, which applies lateral inhibition to enhance cluster formation."
What are good initial weights in a neural network?,https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network,neural-networks|normalization,47590,True,47604,113712,7,82,1681489004,1358025999,59,1358033749,"I assume you are using logistic neurons, and that you are training by gradient descent/back-propagation. 
The logistic function is close to flat for large positive or negative inputs. The derivative at an input of $2$ is about $1/10$, but at $10$ the derivative is about $1/22000$ . This means that if the input of a logistic neuron is $10$ then, for a given training signal, the neuron will learn about $2200$ times slower that if the input was $2$. 
If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range $[-4,4]$. 
The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean $0$ and standard deviation $1$, then a random sum of $d$ terms with weights uniform on $(\frac{-1}{\sqrt{d}},\frac{1}{\sqrt{d}})$ will have mean $0$ and variance $\frac{1}{3}$, independent of $d$. The probability that you get a sum outside of $[-4,4]$ is small. That means as you increase $d$, you are not causing the neurons to start out saturated so that they don't learn. 
With inputs which are not normalized, those weights may not be effective at avoiding saturation. 
","**Summary:**

Logistic neurons can learn slowly when their inputs are large because the derivative of the logistic function becomes small. To accelerate learning, it is recommended to use a large training signal or set the initial weights to obtain inputs in the range [-4, 4], where the derivative is larger.

If the inputs are normalized with mean 0 and standard deviation 1, a random sum of weights uniformly distributed between (-1/âd, 1/âd) will effectively create inputs within this range. However, for unnormalized inputs, these weights may not prevent saturation and slow learning."
How and why do normalization and feature scaling work?,https://stats.stackexchange.com/questions/41704/how-and-why-do-normalization-and-feature-scaling-work,machine-learning|neural-networks|covariance|normalization,41704,True,41926,119997,8,81,1554361072,1351801248,25,1352130475,"It's simply a case of getting all your data on the same scale: if the scales for different features are wildly different, this can have a knock-on effect on your ability to learn (depending on what methods you're using to do it). Ensuring standardised feature values implicitly weights all features equally in their representation.
","**Summary:**

Data standardization involves adjusting feature values to have a consistent scale. This is important for machine learning algorithms because different feature scales can bias the learning process and affect algorithm performance.

By standardizing feature values, all features are given equal representation in the data. This ensures that algorithms do not prioritize features with larger scales and helps prevent overfitting or underfitting. Standardization enables algorithms to learn more effectively and make more accurate predictions.

In essence, standardization ensures that the relative importance of features is determined by their inherent characteristics rather than by differences in their scales."
Which activation function for output layer?,https://stats.stackexchange.com/questions/218542/which-activation-function-for-output-layer,neural-networks,218542,True,218589,188189,7,75,1646063416,1465742531,72,1465773585,"
Regression: linear (because values are unbounded)
Classification: softmax (simple sigmoid works too but softmax works better)

Use simple sigmoid only if your output admits multiple ""true"" answers, for instance, a network that checks for the presence of various objects in an image. In other words, the output is not a probability distribution (does not need to sum to 1).
","**Summary:**

For regression tasks, where continuous value predictions are made, a linear activation function should be used. This is because the values to be predicted are unbounded.

For classification tasks, a softmax activation function is generally preferred over a sigmoid function. Softmax ensures that the output probabilities for each class sum to 1, making it a valid probability distribution. However, a sigmoid function can be used in specific cases where multiple ""true"" answers are possible for a single input. In such scenarios, the output is not constrained to sum to 1 and does not represent a probability distribution."
What&#39;s the difference between feed-forward and recurrent neural networks?,https://stats.stackexchange.com/questions/2213/whats-the-difference-between-feed-forward-and-recurrent-neural-networks,machine-learning|neural-networks|terminology|recurrent-neural-network|topologies,2213,True,2218,106825,3,75,1578419125,1283182408,70,1283192604,"Feed-forward (http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.multil.jpg) ANNs allow signals to travel one way only: from input to output. There are no feedback (loops); i.e., the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straightforward networks that associate inputs with outputs. They are extensively used in pattern recognition. This type of organisation is also referred to as bottom-up or top-down.

Feedback (http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.neural2.jpg) (or recurrent or interactive) networks can have signals traveling in both directions by introducing loops in the network. Feedback networks are powerful and can get extremely complicated. Computations derived from earlier input are fed back into the network, which gives them a kind of memory. Feedback networks are dynamic; their 'state' is changing continuously until they reach an equilibrium point. They remain at the equilibrium point until the input changes and a new equilibrium needs to be found.

Feedforward neural networks are ideally suitable for modeling relationships between a set of predictor or input variables and one or more response or output variables. In other words, they are appropriate for any functional mapping problem where we want to know how a number of input variables affect the output variable. The multilayer feedforward neural networks, also called multi-layer perceptrons (http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP), are the most widely studied and used neural network model in practice.
As an example of feedback network, I can recall Hopfieldâs network (http://en.wikipedia.org/wiki/Hopfield_net). The main use of Hopfieldâs network is as associative memory. An associative memory is a device which accepts an input pattern and generates an output as the stored pattern which is most closely associated with the input. The function of the associate memory is to recall the corresponding stored pattern, and then produce a clear version of the pattern at the output. Hopfield networks are typically used for those problems with binary pattern vectors and the input pattern may be a noisy version of one of the stored patterns. In the Hopfield network, the stored patterns are encoded as the weights of the network.
Kohonenâs self-organizing maps (SOM) represent another neural network type that is markedly different from the feedforward multilayer networks. Unlike training in the feedforward MLP, the SOM training or learning is often called unsupervised because there are no known target outputs associated with each input pattern in SOM and during the training process, the SOM processes the input patterns and learns to cluster or segment the data through adjustment of weights (that makes it an important neural network model for dimension reduction and data clustering). A two-dimensional map is typically created in such a way that the orders of the interrelationships among inputs are preserved. The number and composition of clusters can be visually determined based on the output distribution generated by the training process. With only input variables in the training sample, SOM aims to learn or discover the underlying structure of the data.
(The diagrams are from Dana Vrajitoru's C463 / B551 Artificial Intelligence web site (http://www.cs.iusb.edu/~danav/teach/c463/12_nn.html).)
","**Feed-forward ANNs**

Feed-forward ANNs transmit signals unidirectionally from input to output, allowing for straightforward pattern recognition tasks. They lack feedback loops, meaning each layer's output does not influence itself.

**Feedback (Recurrent) ANNs**

Feedback ANNs enable signals to travel both ways through feedback loops. This complexity allows for memory-based computations, where previously processed data is fed back into the network. These dynamic networks constantly update their state until equilibrium is reached and maintained unless input changes.

**Types of Feedback ANNs**

* **Hopfield's Network:** An associative memory that retrieves and enhances stored patterns based on input.
* **Kohonen's Self-Organizing Maps (SOM):** An unsupervised learning model that clusters data into visually recognizable maps."
Why is tanh almost always better than sigmoid as an activation function?,https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function,machine-learning|neural-networks|backpropagation|sigmoid-curve,330559,True,330885,82103,7,75,1691573034,1519634703,45,1519760687,"Yan LeCun and others argue in Efficient BackProp (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) that

Convergence is usually faster if the average of each input variable over the training set is close to zero. To see this, consider the extreme case where all the inputs are positive. Weights to a particular node in the first weight layer are updated by an amount proportional to $\delta x$ where $\delta$ is the (scalar) error at that node and $x$ is the input vector (see equations (5) and (10)). When all of the components of an input vector are positive, all of the updates of weights that feed into a node will have the same sign (i.e. sign($\delta$)). As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow.

This is why you should normalize your inputs so that the average is zero.
The same logic applies to middle layers:

This heuristic should be applied at all layers which means that we want the average of the outputs of a node to be close to zero because these outputs are the inputs to the next layer.

Postscript @craq makes the point that this quote doesn't make sense for ReLU(x)=max(0,x) which has become a widely popular activation function. While ReLU does avoid the first zigzag problem mentioned by LeCun, it doesn't solve this second point by LeCun who says it is important to push the average to zero. I would love to know what LeCun has to say about this. In any case, there is a paper called Batch Normalization (https://arxiv.org/pdf/1502.03167.pdf), which builds on top of the work of LeCun and offers a way to address this issue:

It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened â i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.


By the way, this video by Siraj (https://www.youtube.com/watch?v=-7scQpJT7uo) explains a lot about activation functions in 10 fun minutes.

@elkout says ""The real reason that tanh is preferred compared to sigmoid (...) is that the derivatives of the tanh are larger than the derivatives of the sigmoid.""
I think this is a non-issue. I never seen this being a problem in the literature. If it bothers you that one derivative is smaller than another, you can just scale it.
The logistic function has the shape $\sigma(x)=\frac{1}{1+e^{-kx}}$. Usually, we use $k=1$, but nothing forbids you from using another value for $k$ to make your derivatives wider, if that was your problem.

Nitpick: tanh is also a sigmoid function. Any function with a S shape is a sigmoid. What you guys are calling sigmoid is the logistic function. The reason why the logistic function is more popular is historical reasons. It has been used for a longer time by statisticians. Besides, some feel that it is more biologically plausible.
","**Summary:**

Normalizing input data by zero-centering the average improves the convergence rate of neural network training. This is because when input values are all positive, weights that feed into a node will update consistently in the same direction, hindering the efficient exploration of the weight space.

The same principle applies to intermediate layers: nodes' output averages should be near zero to facilitate efficient weight adjustments in subsequent layers.

Although ReLU activation functions address the issue of constant-sign weight updates in the first layer, they do not eliminate the need for zero-centering average outputs.

Whitening, a transformation that standardizes input data to have zero mean and unit variance, accelerates convergence further by promoting zero-centered input distributions in all layers.

While tanh activation functions have larger derivatives than sigmoid functions, this difference is typically not significant and can be adjusted by scaling. The preference for sigmoid functions is historical and may be rooted in biological plausibility."
Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions?,https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary,machine-learning|neural-networks|loss-functions|tensorflow|cross-entropy,260505,True,260537,148414,4,74,1692118738,1486479736,82,1486487846,"Bernoulli$^*$ cross-entropy loss is a special case of categorical cross-entropy loss for $m=2$.
$$
\begin{align}
\mathcal{L}(\theta)
&= -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^m y_{ij}\log(p_{ij}) \\
&= -\frac{1}{n}\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right] 
\end{align}
$$
Where $i$ indexes samples/observations and $j$ indexes classes, and $y$ is the sample label (binary for LSH, one-hot vector on the RHS) and $p_{ij}\in(0,1):\sum_{j} p_{ij} =1\forall i,j$ is the prediction for a sample.

I write ""Bernoulli cross-entropy"" because this loss arises from a Bernoulli probability model. There is not a ""binary distribution."" A ""binary cross-entropy"" doesn't tell us if the thing that is binary is the one-hot vector of $k \ge 2$ labels, or if the author is using binary encoding for each trial (success or failure). This isn't a general convention, but it makes clear that these formulae arise from particular probability models. Conventional jargon is not clear in that way.
","**Summary:**

Bernoulli cross-entropy loss is a special case of categorical cross-entropy loss used for binary classification. It calculates the loss of a model's predictions for binary labels ($y_{ij}$). The loss is formulated as the negative log-likelihood of the Bernoulli distribution, which models the probability of success or failure in a binary trial.

The loss function for Bernoulli cross-entropy loss is given by the following equation:

```
\mathcal{L}(\theta) = -\frac{1}{n}\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right]
```

where:

* $i$ indexes samples/observations
* $y_i$ is the binary sample label (0 or 1)
* $p_i$ is the model's predicted probability of success for sample $i$

The term ""Bernoulli cross-entropy"" emphasizes that the loss arises from a Bernoulli probability model, which models the likelihood of success or failure in a binary trial. This distinction is made to clarify the specific probability model involved, as opposed to more general terms like ""binary cross-entropy"" or ""binary distribution."""
Data normalization and standardization in neural networks,https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks,machine-learning|neural-networks|normalization|standardization,7757,True,7759,152521,7,73,1605011737,1299005584,54,1299011251,"A standard approach is to scale the inputs to have mean 0 and a variance of 1. Also linear decorrelation/whitening/pca helps a lot.
If you are interested in the tricks of the trade, I can recommend LeCun's efficient backprop paper. (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
","**Summary:**

To improve neural network performance, it is beneficial to preprocess the input data by:

* **Standardization:** Rescaling the inputs to have a mean of 0 and variance of 1.
* **Linear decorrelation, whitening, or principal component analysis (PCA):** Techniques that remove correlations between input features, making the network more efficient at learning.

These preprocessing steps help the network converge faster and achieve better accuracy. For more detailed information on these techniques, refer to Yann LeCun's paper on efficient backpropagation."
Why is logistic regression a linear classifier?,https://stats.stackexchange.com/questions/93569/why-is-logistic-regression-a-linear-classifier,logistic|classification|neural-networks,93569,True,93570,84481,3,72,1667922882,1397331269,61,1397331930,"Logistic regression is linear in the sense that the predictions can be written as
$$ \hat{p} = \frac{1}{1 + e^{-\hat{\mu}}}, \text{ where } \hat{\mu} = \hat{\theta} \cdot x. $$
Thus, the prediction can be written in terms of $\hat{\mu}$, which is a linear function of $x$. (More precisely, the predicted log-odds is a linear function of $x$.)
Conversely, there is no way to summarize the output of a neural network in terms of a linear function of $x$, and that is why neural networks are called non-linear.
Also, for logistic regression, the decision boundary $\{x:\hat{p} = 0.5\}$ is linear: it's the solution to $\hat{\theta} \cdot x = 0$. The decision boundary of a neural network is in general not linear.
","**Summary:**

Logistic regression is a linear model because its predictions are a function of a linear combination of input features ($x$). The predicted probability ($\hat{p}$) is calculated using a logistic function, which transforms the linear combination into a probability between 0 and 1.

In contrast, neural networks are non-linear models. Their predictions cannot be expressed as a linear function of the input features. This is because the output of a neural network is typically calculated through multiple layers of non-linear activation functions.

Additionally, the decision boundary of a logistic regression model, where the predicted probability is 0.5, is a linear boundary. This means that the model separates data points into two classes by a straight line. However, the decision boundary of a neural network is generally non-linear, allowing for more complex data separation."
What is the definition of a &quot;feature map&quot; (aka &quot;activation map&quot;) in a convolutional neural network?,https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio,neural-networks|deep-learning|convolutional-neural-network,291820,True,292064,106565,4,71,1622891571,1500214560,62,1500338311,"A feature map, or activation map, is the output activations for a given filter (a1 in your case) and the definition is the same regardless of what layer you are on. 
Feature map and activation map mean exactly the same thing. It is called an activation map because it is a mapping that corresponds to the activation of different parts of the image, and also a feature map because it is also a mapping of where a certain kind of feature is found in the image. A high activation means a certain feature was found.  
A ""rectified feature map"" is just a feature map that was created using Relu. You could possibly see the term ""feature map"" used for the result of the dot products (z1) because this is also really a map of where certain features are in the image, but that is not common to see. 
","**Summary:**

A feature map, also known as an activation map, is the output of a neural network filter. It shows the locations within an image where specific features are activated. A high activation indicates a strong presence of that feature.

Feature maps are created using activation functions, and the term ""rectified feature map"" refers to maps created using the ReLU activation function.

It's possible to use the term ""feature map"" for the result of dot products before applying the activation function, but this is not common practice."
What should I do when my neural network doesn&#39;t generalize well?,https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well,neural-networks|overfitting|faq,365778,True,365806,64340,5,70,1681575887,1536311529,86,1536318458,"First of all, let's mention what does ""my neural network doesn't generalize well"" mean and what's the difference with saying ""my neural network doesn't perform well"".
When training a Neural Network, you are constantly evaluating it on a set of labelled data called the training set. If your model isn't working properly and doesn't appear to learn from the training set, you don't have a generalization issue yet, instead please refer to this post (https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn). However, if your model is achieving a satisfactory performance on the training set, but cannot perform well on previously unseen data (e.g. validation/test sets), then you do have a generalization problem.
Why is your model not generalizing properly?
The most important part is understanding why your network doesn't generalize well. High-capacity Machine Learning models have the ability to memorize the training set, which can lead to overfitting.
Overfitting is the state where an estimator has begun to learn the training set so well that it has started to model the noise in the training samples (besides all useful relationships).
For example, in the image below we can see how the blue on the right line has clearly overfit.

But why is this bad?
When attempting to evaluate our model on new, previously unseen data (i.e. validation/test set), the model's performance will be much worse than what we expect.
How to prevent overfitting?
In the beginning of the post I implied that the complexity of your model is what is actually causing the overfitting, as it is allowing the model to extract unnecessary relationships from the training set, that map its inherent noise. The easiest way to reduce overfitting is to essentially limit the capacity of your model. These techniques are called regularization techniques.

Parameter norm penalties. These add an extra term to the weight update function of each model, that is dependent on the norm of the parameters. This term's purpose is to counter the actual update (i.e. limit how much each weight can be updated). This makes the models more robust to outliers and noise. Examples of such regularizations are L1 and L2 (http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/) regularizations, which can be found on the Lasso (https://en.wikipedia.org/wiki/Lasso_(statistics)), Ridge (https://en.wikipedia.org/wiki/Tikhonov_regularization) and Elastic Net (https://en.wikipedia.org/wiki/Elastic_net_regularization) regressors.
Since each (fully connected) layer in a neural network functions much like a simple linear regression, these are used in Neural Networks. The most common use is to regularize each layer individually.
keras implementation (https://keras.io/regularizers/).

Early stopping. This technique attempts to stop an estimator's training phase prematurely, at the point where it has learned to extract all meaningful relationships from the data, before beginning to model its noise. This is done by monitoring the validation loss (or a validation metric of your choosing) and terminating the training phase when this metric stops improving. This way we give the estimator enough time to learn the useful information but not enough to learn from the noise.
keras implementation (https://keras.io/callbacks/#earlystopping).




Neural Network specific regularizations. Some examples are:

Dropout. Dropout (http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) is an interesting technique that works surprisingly well. Dropout is applied between two successive layers in a network. At each iteration a specified percentage of the connections (selected randomly), connecting the two layers, are dropped. This causes the subsequent layer rely on all of its connections to the previous layer.
keras implementation (https://keras.io/layers/core/#dropout)
Transfer learning. This is especially used in Deep Learning. This is done by initializing the weights of your network to the ones of another network with the same architecture pre-trained on a large, generic dataset.
Other things that may limit overfitting in Deep Neural Networks are: Batch Normalization (https://en.wikipedia.org/wiki/Batch_normalization), which can act as a regulizer and in some cases (e.g. inception modules) works as well as dropout; relatively small sized batches (https://arxiv.org/pdf/1705.08741.pdf) in SGD, which can also prevent overfitting; adding small random noise to weights (https://pdfs.semanticscholar.org/1023/043d2a5a76a07388c3e17c1284018937dbfc.pdf) in hidden layers.



Another way of preventing overfitting, besides limiting the model's capacity, is by improving the quality of your data. The most obvious choice would be outlier/noise removal, however in practice their usefulness is limited. A more common way (especially in image-related tasks) is data augmentation. Here we attempt randomly transform the training examples so that while they appear to the model to be different, they convey the same semantic information (e.g. left-right flipping on images).
Data augmentation overview (http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)
Practical suggestions:

By far the most effective regularization technique is dropout, meaning that it should be the first you should use. However, you don't need to (and probably shouldn't) place dropout everywhere! The most prone layers to overfitting are the Fully Connected (FC) layers, because they contain the most parameters. Dropout should be applied to these layers (impacting their connections to the next layer).
Batch normalization, besides having a regularization effect aids your model in several other ways (e.g. speeds up convergence, allows for the use of higher learning rates). It too should be used in FC layers.
As mentioned previously it also may be beneficial to stop your model earlier in the training phase than scheduled. The problem with early stopping is that there is no guarantee that, at any given point, the model won't start improving again. A more practical approach than early stopping is storing the weights of the model that achieve the best performance on the validation set. Be cautious, however, as this is not an unbiased estimate of the performance of your model (just better than the training set). You can also overfit on the validation set. More on that later.
keras implementation (https://keras.io/callbacks/#modelcheckpoint)
In some applications (e.g. image related tasks), it is highly recommended to follow an already established architecture (e.g. VGG, ResNet, Inception), that you can find ImageNet weights for. The generic nature of this dataset, allows the features to be in turn generic enough to be used for any image related task. Besides being robust to overfitting this will greatly reduce the training time.
Another use of the similar concept is the following: if your task doesn't have much data, but you can find another similar task that does, you can use transfer learning to reduce overfitting. First train your network for the task that has the larger dataset and then attempt to fine-tune the model to the one you initially wanted. The initial training will, in most cases, make your model more robust to overfitting.
Data augmentation. While it always helps to have a larger dataset, data augmentation techniques do have their shortcomings. More specifically, you have to be careful not to augment too strongly, as this might ruin the semantic content of the data. For example in image augmentation if you translate/shift/scale or adjust the brighness/contrast the image too much you'll lose much of the information it contains. Furthermore, augmentation schemes need to be implemented for each task in an ad-hoc fashion (e.g. in handwritten digit recognition the digits are usually aligned and shouldn't be rotated too much; also they shouldn't be flipped in any direction, as they aren't horizontally/vertically symetric. Same goes for medical images).
In short be careful not to produce non realistic images through data augmentation. Moreover, an increased dataset size will require a longer training time. Personally, I start considering using data augmentation when I see that my model is reaching near $0$ loss on the training set.

","**Summary:**

Generalization in neural networks refers to a model's ability to perform well on unseen data. When a model fails to generalize, it may exhibit overfitting, where it memorizes the training data but struggles with new input.

Overfitting occurs when a model's capacity (number of parameters) is too large, allowing it to extract unnecessary patterns from the training data. To prevent overfitting, regularization techniques can be employed:

* **Parameter Norm Penalties:** Add a penalty term to weight updates, limiting parameter changes and improving robustness to noise.
* **Early Stopping:** Terminate training when the validation loss stops improving, preventing the model from learning from noise.
* **Dropout:** Randomly drop connections between layers, forcing them to learn from all connections.

Other overfitting mitigation strategies include:

* **Data Augmentation:** Randomly transforming training data to improve model generalization.
* **Transfer Learning:** Using pre-trained weights from a larger dataset to reduce overfitting.
* **Data Cleaning:** Removing outliers and noise from the training data.

Key points to remember:

* For effective regularization, prioritize dropout and batch normalization in Fully Connected layers.
* Store model weights with the best validation performance to minimize overfitting on the validation set.
* Consider using established network architectures with large ImageNet datasets to enhance generalization."
Proper way of using recurrent neural network for time series analysis,https://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis,time-series|machine-learning|neural-networks,8000,True,8014,48326,3,70,1615808611,1299568561,52,1299588689,"What you describe is in fact a ""sliding time window"" approach and is different to recurrent networks. You can use this technique with any regression algorithm. There is a huge limitation to this approach: events in the inputs can only be correlatd with other inputs/outputs which lie at most t timesteps apart, where t is the size of the window.
E.g. you can think of a Markov chain of order t. RNNs don't suffer from this in theory, however in practice learning is difficult.
It is best to illustrate an RNN in contrast to a feedfoward network. Consider the (very) simple feedforward network $y = Wx$ where $y$ is the output, $W$ is the weight matrix, and $x$ is the input.
Now, we use a recurrent network. Now we have a sequence of inputs, so we will denote the inputs by $x^{i}$ for the ith input. The corresponding ith output is then calculated via $y^{i} = Wx^i + W_ry^{i-1}$.
Thus, we have another weight matrix $W_r$ which incorporates the output at the previous step linearly into the current output.
This is of course a simple architecture. Most common is an architecture where you have a hidden layer which is recurrently connected to itself. Let $h^i$ denote the hidden layer at timestep i. The formulas are then:
$$h^0 = 0$$
$$h^i = \sigma(W_1x^i + W_rh^{i-1})$$
$$y^i = W_2h^i$$
Where $\sigma$ is a suitable non-linearity/transfer function like the sigmoid. $W_1$ and $W_2$ are the connecting weights between the input and the hidden and the hidden and the output layer. $W_r$ represents the recurrent weights.
Here is a diagram of the structure: 

","**Summary:**

A ""sliding time window"" approach uses a fixed-size window to capture temporal dependencies in a time series. However, it's limited by the window size, as events can only be correlated within that range.

Unlike sliding time windows, recurrent neural networks (RNNs) can theoretically learn long-term dependencies. They do this by incorporating an internal memory that stores past information, allowing them to consider events from distant timesteps.

RNNs differ from feedforward neural networks by adding recurrent connections, which feed the previous output back into the network. This creates a feedback loop that preserves temporal information.

A common RNN architecture consists of a hidden layer with recurrent connections and an input and output layer. Inputs are processed through the hidden layer, which is updated at each timestep based on the current input and the hidden state from the previous step. The hidden state is then used to generate the output.

This recurrent architecture allows RNNs to capture and exploit sequential dependencies in time series, making them suitable for tasks such as time series prediction, language modeling, and natural language processing."
What is the difference between a neural network and a deep belief network?,https://stats.stackexchange.com/questions/51273/what-is-the-difference-between-a-neural-network-and-a-deep-belief-network,machine-learning|neural-networks|deep-learning|deep-belief-networks,51273,True,59854,54197,2,70,1514911542,1362370722,63,1369325939,"""Neural networks"" is a term usually used to refer to feedforward neural networks. Deep Neural Networks are feedforward Neural Networks with many layers. 
A Deep belief network is not the same as a Deep Neural Network.
As you have pointed out a deep belief network has undirected connections between some layers. This means that the topology of the DNN and DBN is different by definition.
The undirected layers in the DBN are called Restricted Boltzmann Machines. This layers can be trained using an unsupervised learning algorithm (Contrastive Divergence) that is very fast (Here's a link (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)! with details).
Some more comments:
The solutions obtained with deeper neural networks correspond to solutions that perform worse than the solutions obtained for networks with 1 or 2 hidden layers. As the architecture gets deeper, it becomes more difficult to obtain good generalization using a Deep NN.
In 2006 Hinton  discovered that much better results could be achieved in deeper architectures when each layer (RBM) is pre-trained with an unsupervised learning algorithm (Contrastive Divergence). Then the Network can be trained in a supervised way using backpropagation in order to ""fine-tune"" the weights.
","**Summary:**

**Neural Networks**

- ""Neural networks"" typically refers to feedforward networks with one hidden layer.
- Deep Neural Networks (DNNs) are feedforward networks with multiple hidden layers.

**Deep Belief Networks (DBNs)**

- DBNs are distinct from DNNs, having undirected connections between some layers.
- These undirected layers (Restricted Boltzmann Machines) can be trained unsupervisedly using the fast Contrastive Divergence algorithm.

**Training Deep Networks**

- Deeper networks tend to perform worse than shallow networks with few hidden layers.
- To improve performance, layers can be pre-trained unsupervisedly (Contrastive Divergence) and then fine-tuned supervisedly (backpropagation).
- This pre-training approach was introduced by Hinton in 2006."
Cost function of neural network is non-convex?,https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex,machine-learning|neural-networks|loss-functions,106334,True,106343,60585,7,69,1660505215,1404914378,50,1404916975,"The cost function of a neural network is in general neither convex nor concave. This means that the matrix of all second partial derivatives (the Hessian) is neither positive semidefinite, nor negative semidefinite. Since the second derivative is a matrix, it's possible that it's neither one or the other.
To make this analogous to one-variable functions, one could say that the cost function is neither shaped like the graph of $x^2$ nor like the graph of $-x^2$. Another example of a non-convex, non-concave function is $\sin(x)$ on $\mathbb{R}$. One of the most striking differences is that $\pm x^2$ has only one extremum, whereas $\sin$ has infinitely many maxima and minima.
How does this relate to our neural network? A cost function $J(W,b)$ has also a number of local maxima and minima, as you can see in this picture (http://www.holehouse.org/mlclass/09_Neural_Networks_Learning_files/Image%20%5B35%5D.png), for example.
The fact that $J$ has multiple minima can also be interpreted in a nice way. In each layer, you use multiple nodes which are assigned different parameters to make the cost function small. Except for the values of the parameters, these nodes are the same. So you could exchange the parameters of the first node in one layer with those of the second node in the same layer, and accounting for this change in the subsequent layers. You'd end up with a different set of parameters, but the value of the cost function can't be distinguished by (basically you just moved a node, to another place, but kept all the inputs/outputs the same).
","**Summary:**

The cost function of neural networks is typically non-convex and non-concave, meaning it has neither a consistent upward or downward curvature. This complexity allows for multiple local minima and maxima, unlike single-variable functions like x^2 or -x^2 which have only one extremum.

In neural networks, this non-convexity results in numerous local minima. This is visualized in the cost function's multiple peaks and valleys. Interestingly, exchanging the parameters of nodes within the same layer, while adjusting the subsequent layers accordingly, doesn't alter the cost function value. This suggests that the choice of which node to adjust in a layer is arbitrary concerning the cost function's minimization."
Cross-Entropy or Log Likelihood in Output layer,https://stats.stackexchange.com/questions/198038/cross-entropy-or-log-likelihood-in-output-layer,neural-networks|maximum-likelihood|softmax,198038,True,198047,96802,3,68,1664304875,1456205823,90,1456209438,"The negative log likelihood (eq.80) is also known as the multiclass cross-entropy (ref: Pattern Recognition and Machine Learning Section 4.3.4), as they are in fact two different interpretations of the same formula.
eq.57 is the negative log likelihood of the Bernoulli distribution, whereas eq.80 is the negative log likelihood of the multinomial distribution with one observation (a multiclass version of Bernoulli). 
For binary classification problems, the softmax function outputs two values (between 0 and 1 and sum to 1) to give the prediction of each class. While the sigmoid function outputs one value (between 0 and 1) to give the prediction of one class (so the other class is 1-p).  
So eq.80 can't be directly applied to the sigmoid output, though it is essentially the same loss as eq.57.
Also see this answer (https://stats.stackexchange.com/a/224491/95569).

Following is a simple illustration of the connection between (sigmoid + binary cross-entropy) and (softmax + multiclass cross-entropy) for binary classification problems. 
Say we take $0.5$ as the split point of the two categories, for sigmoid
output it follows,
$$\sigma(wx+b)=0.5$$
$$wx+b=0$$
which is the decision boundary in the feature space.
For softmax output it follows
$$\frac{e^{w_1x+b_1}}{e^{w_1x+b_1}+e^{w_2x+b_2}}=0.5$$
$$e^{w_1x+b_1}=e^{w_2x+b_2}$$
$$w_1x+b_1=w_2x+b_2$$
$$(w_1-w_2)x+(b_1-b_2)=0$$
so it remains the same model although there are twice as many parameters.
The followings show the decision boundaries obtained using theses two methods, which are almost identical.


","**Summary:**

The negative log likelihood (NLL), also known as multiclass cross-entropy, is a measure of the discrepancy between predicted and actual class distributions.

For binary classification, where the sigmoid function is used, the NLL is defined in equation 57. For multiclass problems, where the softmax function is used, it is defined in equation 80.

While these two formulas have different interpretations, they represent the same loss function. Equation 80 cannot be directly applied to sigmoid outputs, but it is essentially equivalent to equation 57.

For binary classification, the sigmoid and softmax functions produce nearly identical decision boundaries, even though the softmax model has twice as many parameters.

The decision boundary for the sigmoid function is given by wx+b=0, while for the softmax function it is given by (w1-w2)x+(b1-b2)=0. These equations show that the decision boundaries are parallel, indicating that both models make similar predictions."
Why do neural networks need so many training examples to perform?,https://stats.stackexchange.com/questions/394118/why-do-neural-networks-need-so-many-training-examples-to-perform,neural-networks|neuroscience,394118,True,394128,18827,12,66,1551634625,1551017231,103,1551023084,"I caution against expecting strong resemblance between biological and artificial neural networks. I think the name ""neural networks"" is a bit dangerous, because it tricks people into expecting that neurological processes and machine learning should be the same. The differences between biological and artificial neural networks outweigh the similarities.
As an example of how this can go awry, you can also turn the reasoning in the original post on its head. You can train a neural network to learn to recognize cars in an afternoon, provided you have a reasonably fast computer and some amount of training data. You can make this a binary task (car/not car) or a multi-class task (car/tram/bike/airplane/boat) and still be confident in a high level of success. 
By contrast, I wouldn't expect a child to be able to pick out a car the day - or even the week - after it's born, even after it has seen ""so many training examples."" Something is obviously different between a two-year-old and an infant that accounts for the difference in learning ability, whereas a vanilla image classification neural network is perfectly capable of picking up object classification immediately after ""birth."" I think that there are two important differences: (1) the relative volumes of training data available and (2) a self-teaching mechanism that develops over time because of abundant training data.

The original post exposes two questions. The title and body of the question ask why neural networks need ""so many examples."" Relative to a child's experience, neural networks trained using common image benchmarks have comparatively little data.
I will re-phrases the question in the title to 
""How does training a neural network for a common image benchmark compare & contrast to the learning experience of a child?""
For the sake of comparison I'll consider the CIFAR-10 data because it is a common image benchmark. The labeled portion is composed of 10 classes of images with 6000 images per class. Each image is 32x32 pixels. If you somehow stacked the labeled images from CIFAR-10 and made a standard 48 fps video, you'd have about 20 minutes of footage.
A child of 2 years who observes the world for 12 hours daily has roughly 263000 minutes (more than 4000 hours) of direct observations of the world, including feedback from adults (labels). (These are just ballpark figures -- I don't know how many minutes a typical two-year-old has spent observing the world.) Moreover, the child will have exposure to many, many objects beyond the 10 classes that comprise CIFAR-10.
So there are a few things at play. One is that the child has exposure to more data overall and a more diverse source of data than the CIFAR-10 model has. Data diversity and data volume are well-recognized as pre-requisites for robust models in general. In this light, it doesn't seem surprising that a neural network is worse at this task than the child, because a neural network trained on CIFAR-10 is positively starved for training data compared to the two-year-old. The image resolution available to a child is better than the 32x32 CIFAR-10 images, so the child is able to learn information about the fine details of objects.
The CIFAR-10 to two-year-old comparison is not perfect because the CIFAR-10 model will likely be trained with multiple passes over the same static images, while the child will see, using binocular vision, how objects are arranged in a three-dimensional world while moving about and with different lighting conditions and perspectives on the same objects.
The anecdote about OP's child implies a second question, 
""How can neural networks become self-teaching?""
A child is endowed with some talent for self-teaching, so that new categories of objects can be added over time without having to start over from scratch. 

OP's remark about transfer-learning (/questions/tagged/transfer-learning) names one kind of model adaptation in the machine learning context.
In comments, other users have pointed out that one- and few-shot learning* is another machine learning research area.
Additionally, reinforcement-learning (/questions/tagged/reinforcement-learning) addresses self-teaching models from a different perspective, essentially allowing robots to undertake trial-and-error experimentation to find optimal strategies for solving specific problems (e.g. playing chess).

It's probably true that all three of these machine learning paradigms are germane to improving how machines adapt to new computer vision tasks. Quickly adapting machine learning models to new tasks is an active area of research. However, because the practical goals of these projects (identify new instances of malware, recognize imposters in passport photos, index the internet) and criteria for success differ from the goals of a child learning about the world, and the fact that one is done in a computer using math and the other is done in organic material using chemistry, direct comparisons between the two will remain fraught.

As an aside, it would be interesting to study how to flip the CIFAR-10 problem around and train a neural network to recognize 6000 objects from 10 examples of each. But even this wouldn't be a fair comparison to 2-year-old, because there would still be a large discrepancy in the total volume, diversity and resolution of the training data.
*We don't presently have a tags for one-shot learning or few-shot learning.
","Biological neural networks differ significantly from artificial ones, and it's misleading to expect a strong resemblance between them. Biological systems have far more training data and a self-teaching mechanism that develops over time.

Children experience a much greater volume and diversity of data than neural networks trained on image benchmarks like CIFAR-10. This data disparity contributes to the neural network's inability to match a child's learning abilities.

To address the question of how neural networks can become self-teaching, researchers explore methods such as transfer learning, one- and few-shot learning, and reinforcement learning. These approaches aim to improve machine adaptation to new tasks, but direct comparisons to children's learning remain challenging due to fundamental differences in methodology and goals.

Despite the limitations in drawing parallels between biological and artificial neural networks, it remains an interesting avenue of study to explore how neural networks can recognize objects with limited data."
What are alternatives of Gradient Descent?,https://stats.stackexchange.com/questions/97014/what-are-alternatives-of-gradient-descent,machine-learning|svm|neural-networks,97014,True,97026,46540,7,64,1677401546,1399620098,50,1399628964,"This is more a problem to do with the function being minimized than the method used, if finding the true global minimum is important, then use a method such a simulated annealing (http://en.wikipedia.org/wiki/Simulated_annealing).  This will be able to find the global minimum, but may take a very long time to do so.
In the case of neural nets, local minima are not necessarily that much of a problem.  Some of the local minima are due to the fact that you can get a functionally identical model by permuting the hidden layer units, or negating the inputs and output weights of the network etc.  Also if the local minima is only slightly non-optimal, then the difference in performance will be minimal and so it won't really matter.  Lastly, and this is an important point, the key problem in fitting a neural network is over-fitting, so aggressively searching for the global minima of the cost function is likely to result in overfitting and a model that performs poorly.
Adding a regularisation term, e.g. weight decay, can help to smooth out the cost function, which can reduce the problem of local minima a little, and is something I would recommend anyway as a means of avoiding overfitting.
The best method however of avoiding local minima in neural networks is to use a Gaussian Process model (or a Radial Basis Function neural network), which have fewer problems with local minima.
","Local minima are not a significant problem for neural networks due to:
- Functional equivalence of permuting hidden layer units or negating weights.
- Minimal performance differences between local and global minima.
- Overfitting concerns, where searching for the global minimum may lead to poor performance.

To mitigate local minima:
- Add a regularization term (e.g., weight decay) to smooth the cost function.
- Use Gaussian Process models or Radial Basis Function neural networks, which are less prone to local minima.

Simulated annealing can find the global minimum but may be time-consuming."
Why sigmoid function instead of anything else?,https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else,logistic|neural-networks|least-squares,162988,True,163152,54457,4,63,1650293613,1437736470,31,1437840026,"Quoting myself from this answer (https://stats.stackexchange.com/a/156422/17023) to a different question:

In section 4.2 of Pattern Recognition and Machine Learning (http://research.microsoft.com/en-us/um/people/cmbishop/PRML/) (Springer 2006), Bishop shows that the logit arises naturally as the form of the posterior probability distribution in a Bayesian treatment of two-class classification. He then goes on to show that the same holds for discretely distributed features, as well as a subset of the family of exponential distributions. For multi-class classification the logit generalizes to the normalized exponential or softmax function.

This explains why this sigmoid is used in logistic regression.
Regarding neural networks, this blog post (http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/) explains how different nonlinearities including the logit / softmax and the probit used in neural networks can be given a statistical interpretation and thereby a motivation. The underlying idea is that a multi-layered neural network can be regarded as a hierarchy of generalized linear models; according to this, activation functions are link functions, which in turn correspond to different distributional assumptions.
","**Summary:**

Bishop's influential work demonstrates that the logit function emerges naturally as the posterior probability distribution in Bayesian two-class classification. This applies to both discrete and certain exponential distributions. For multi-class problems, the logit generalizes to the softmax function.

This explains the widespread use of the logit function in logistic regression. In the context of neural networks, the logit and probit functions can be statistically interpreted as link functions in generalized linear models. This suggests that neural networks can be understood as hierarchies of these models, with activation functions corresponding to distributional assumptions.

In essence, the logit function is a natural choice for classification tasks due to its statistical underpinnings. Its use in logistic regression and neural networks is supported by both mathematical theory and statistical reasoning."
Why do Convolutional Neural Networks not use a Support Vector Machine to classify?,https://stats.stackexchange.com/questions/168064/why-do-convolutional-neural-networks-not-use-a-support-vector-machine-to-classif,machine-learning|neural-networks|svm|deep-learning|convolutional-neural-network,168064,True,168073,25788,3,63,1635380791,1440081828,73,1440086453,"What is an SVM, anyway?
I think the answer for most purposes is âthe solution to the following optimization problemâ:
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal H} \frac{1}{n} \sum_{i=1}^n \ell_\mathit{hinge}(f(x_i), y_i) \, + \lambda \lVert f \rVert_{\mathcal H}^2
\\ \ell_\mathit{hinge}(t, y) = \max(0, 1 - t y)
,\end{split}
\tag{SVM}
$$
where $\mathcal H$ is a reproducing kernel Hilbert space, $y$ is a label in $\{-1, 1\}$, and $t = f(x) \in \mathbb R$ is a âdecision valueâ; our final prediction will be $\operatorname{sign}(t)$. In the simplest case, $\mathcal H$ could be the space of affine functions $f(x) = w \cdot x + b$, and $\lVert f \rVert_{\mathcal H}^2 = \lVert w \rVert^2 + b^2$. (Handling of the offset $b$ varies depending on exactly what youâre doing, but thatâs not important for our purposes.)
In the â90s through the early â10s, there was a lot of work on solving this particular optimization problem in various smart ways, and indeed thatâs what LIBSVM / LIBLINEAR / SVMlight / ThunderSVM / ... do. But I donât think that any of these particular algorithms are fundamental to âbeing an SVM,â really.
Now, how do we train a deep network? Well, we try to solve something like, say,
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal F} \frac1n \sum_{i=1}^n \ell_\mathit{CE}(f(x_i), y) + R(f)
\\
\ell_\mathit{CE}(p, y) = - y \log(p) - (1-y) \log(1 - p)
,\end{split}
   \tag{$\star$}
$$
where now $\mathcal F$ is the set of deep nets we consider, which output probabilities $p = f(x) \in [0, 1]$. The explicit regularizer $R(f)$ might be an L2 penalty on the weights in the network, or we might just use $R(f) = 0$. Although we could solve (SVM) up to machine precision if we really wanted, we usually canât do that for $(\star)$ when $\mathcal F$ is more than one layer; instead we use stochastic gradient descent to attempt at an approximate solution.
If we take $\mathcal F$ as a reproducing kernel Hilbert space and $R(f) = \lambda \lVert f \rVert_{\mathcal F}^2$, then $(\star)$ becomes very similar to (SVM), just with cross-entropy loss instead of hinge loss: this is also called kernel logistic regression. My understanding is that the reason SVMs took off in a way kernel logistic regression didnât is largely due to a slight computational advantage of the former (more amenable to these fancy algorithms), and/or historical accident; there isnât really a huge difference between the two as a whole, as far as I know. (There is sometimes a big difference between an SVM with a fancy kernel and a plain linear logistic regression, but thatâs comparing apples to oranges.)
So, what does a deep network using an SVM to classify look like? Well, that could mean some other things, but I think the most natural interpretation is just using $\ell_\mathit{hinge}$ in $(\star)$.
One minor issue is that $\ell_\mathit{hinge}$ isnât differentiable at $\hat y = y$; we could instead use $\ell_\mathit{hinge}^2$, if we want. (Doing this in (SVM) is sometimes called âL2-SVMâ or similar names.) Or we can just ignore the non-differentiability; the ReLU activation isnât differentiable at 0 either, and this usually doesnât matter. This can be justified via subgradients (https://en.wikipedia.org/wiki/Subderivative), although note that the correctness here is actually quite subtle (https://proceedings.neurips.cc/paper/2020/hash/4aaa76178f8567e05c8e8295c96171d8-Abstract.html) when dealing with deep networks.
An ICML workshop paper â Tang, Deep Learning using Linear Support Vector Machines (http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf), ICML 2013 workshop Challenges in Representation Learning â found using $\ell_\mathit{hinge}^2$ gave small but consistent improvements over $\ell_\mathit{CE}$ on the problems they considered. Iâm sure others have tried (squared) hinge loss since in deep networks, but it certainly hasnât taken off widely.
(You have to modify both $\ell_\mathit{CE}$ as Iâve written it and $\ell_\mathit{hinge}$ to support multi-class classification, but in the one-vs-rest scheme used by Tang, both are easy to do.)

Another thing thatâs sometimes done is to train CNNs in the typical way, but then take the output of a late layer as ""features"" and train a separate SVM on that. This was common in early days of transfer learning with deep features, but is I think less common now.

Something like this is also done sometimes in other contexts, e.g. in meta-learning by Lee et al., Meta-Learning with Differentiable Convex Optimization (https://arxiv.org/abs/1904.03758), CVPR 2019, who actually solved (SVM) on deep network features and backpropped through the whole thing. (They didn't, but you can even do this with a nonlinear kernel in $\mathcal H$; this is also done in some other ""deep kernels"" contexts.) Itâs a very cool approach âÂ one that I've also worked on â and in certain domains this type of approach makes a ton of sense, but there are some pitfalls, and I donât think itâs very applicable to a typical ""plain classification"" problem.
","**Summary:**

SVMs (Support Vector Machines) involve solving an optimization problem to find a decision boundary that separates data points with different labels. The objective function consists of a loss term that penalizes misclassifications and a regularization term that prevents overfitting.

In a typical SVM, the decision boundary is defined by a hyperplane in a high-dimensional feature space, where the data points are mapped using a kernel function. The optimization problem seeks to maximize the margin, or distance, between the decision boundary and the closest data points.

Deep networks can also be trained using a similar optimization framework, with a cross-entropy loss function and a regularization term such as weight decay. However, due to the non-convex nature of deep networks, training typically involves stochastic gradient descent instead of the exact optimization methods used in SVMs.

In practice, deep networks can be used with SVMs by replacing the cross-entropy loss with the hinge loss used in SVMs. Alternatively, the output of a deep network can be used as features for a separate SVM classifier. This approach combines the strengths of deep learning and SVMs, but it requires careful consideration of potential pitfalls such as overfitting and scalability."
How does LSTM prevent the vanishing gradient problem?,https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem,neural-networks|lstm,185639,True,263956,59357,4,63,1564498983,1449565307,35,1488017915,"The vanishing gradient is best explained in the one-dimensional case. The multi-dimensional is more complicated but essentially analogous. You can review it in this excellent paper [1].
Assume we have a hidden state $h_t$ at time step $t$. If we make things simple and remove biases and inputs, we have
$$h_t = \sigma(w h_{t-1}).$$
Then you can show that 
\begin{align}
\frac{\partial h_{t'}}{\partial h_t} 
&= \prod_{k=1}^{t' - t} w \sigma'(w h_{t'-k})\\
&= \underbrace{w^{t' - t}}_{!!!}\prod_{k=1}^{t' - t} \sigma'(w h_{t'-k})
\end{align}
The factored marked with !!! is the crucial one. If the weight is not equal to 1, it will either decay to zero exponentially fast in $t'-t$, or grow exponentially fast.
In LSTMs, you have the cell state $s_t$. The derivative there is of the form
$$\frac{\partial s_{t'}}{\partial s_t} = \prod_{k=1}^{t' - t} \sigma(v_{t+k}).$$
Here $v_t$ is the input to the forget gate. As you can see, there is no exponentially fast decaying factor involved. Consequently, there is at least one path where the gradient does not vanish. For the complete derivation, see [2].
[1] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. ""On the difficulty of training recurrent neural networks."" ICML (3) 28 (2013): 1310-1318.
[2] Bayer, Justin Simon. Learning Sequence Representations. Diss. MÃ¼nchen, Technische UniversitÃ¤t MÃ¼nchen, Diss., 2015, 2015.
","**Summary:**

The vanishing gradient problem occurs when the gradients of a neural network become exponentially small as we move back in time. This can prevent the network from learning long-term dependencies.

In the one-dimensional case, the vanishing gradient is caused by a weight term that is less than 1. This term causes the gradients to decay exponentially fast with time. In contrast, in Long Short-Term Memory (LSTM) networks, there is no such exponentially decaying term, allowing gradients to flow over longer time periods.

This difference is due to the presence of a cell state in LSTMs. The derivative of the cell state with respect to itself does not have an exponentially decaying term, which ensures that at least one gradient path does not vanish. Consequently, LSTMs are better suited for learning long-term dependencies than vanilla recurrent neural networks."
Recurrent vs Recursive Neural Networks: Which is better for NLP?,https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp,machine-learning|neural-networks|deep-learning|natural-language,153599,True,158995,57199,4,62,1586887349,1432317020,54,1435446936,"Recurrent Neural networks are recurring over time. For example if you have a sequence
x = ['h', 'e', 'l', 'l']
This sequence is fed to a single neuron which has a single connection to itself.
At time step 0, the letter 'h' is given as input.At time step 1, 'e' is given as input. The network when unfolded over time will look like this.

A recursive network is just a generalization of a recurrent network. In a recurrent network the weights are shared (and dimensionality remains constant) along the length of the sequence because how would you deal with position-dependent weights when you encounter a sequence at test-time of different length to any you saw at train-time. In a recursive network the weights are shared (and dimensionality remains constant) at every node for the same reason.
This means that all the W_xh weights will be equal(shared) and so will be the W_hh weight. This is simply because it is a single neuron which has been unfolded in time.
This is what a Recursive Neural Network looks like.

It is quite simple to see why it is called a Recursive Neural Network. Each parent node's children are simply a node similar to that node.
The Neural network you want to use depends on your usage. In Karpathy's blog (https://karpathy.github.io/2015/05/21/rnn-effectiveness/), he is generating characters one at a time so a recurrent neural network is good.
But if you want to generate a parse tree, then using a Recursive Neural Network is better because it helps to create better hierarchical representations. 
If you want to do deep learning in c++, then use CUDA. It has a nice user-base, and is fast. I do not know more about that so cannot comment more.
In python, Theano is the best option because it provides automatic differentiation, which means that when you are forming big, awkward NNs, you don't have to find gradients by hand. Theano does it automatically for you. This feature is lacked by Torch7. 
Theano is very fast as it provides C wrappers to python code and can be implemented on GPUs.  It also has an awesome user base, which is very important while learning something new.
","**Summary:**

**Recurrent Neural Networks (RNNs)** are neural networks that process sequential data by feeding previous hidden states back into the network at each time step. This allows them to learn long-term dependencies in the data.

**Recursive Neural Networks (RNNs)** are a generalization of RNNs where the weights are shared across all time steps and nodes, forming a tree-like structure. This makes them suitable for hierarchical data.

**Choice of Neural Network:**

* **Recurrent Neural Networks** are effective for tasks involving sequential generation (e.g., character generation).
* **Recursive Neural Networks** are better suited for tasks requiring hierarchical representations (e.g., parse tree generation).

**Implementation Options:**

* **CUDA (C++)**: Fast and widely used, but requires specific hardware.
* **Theano (Python)**: Provides automatic differentiation, reducing manual gradient calculation. Supports GPU implementation.

Theano is generally preferred in Python due to its ease of use and performance optimizations."
Neural networks vs support vector machines: are the second definitely superior?,https://stats.stackexchange.com/questions/30042/neural-networks-vs-support-vector-machines-are-the-second-definitely-superior,machine-learning|svm|neural-networks,30042,True,30100,57274,5,61,1563481549,1339124379,51,1339185721,"It is a matter of trade-offs. SVMs are in right now, NNs used to be in. You'll find a rising number of papers that claim Random Forests, Probabilistic Graphic Models or Nonparametric Bayesian methods are in. Someone should publish a forecasting model in the Annals of Improbable Research on what models will be considered hip.
Having said that for many famously difficult supervised problems the best performing single models are some type of NN, some type of SVMs or a problem specific stochastic gradient descent method implemented using signal processing methods.

Pros of NN:

They are extremely flexible in the types of data they can support. NNs do a decent job at learning the important features from basically any data structure, without having to manually derive features.



NN still benefit from feature engineering, e.g. you should have an area feature if you have a length and width. The model will perform better for the same computational effort.
Most of supervised machine learning requires you to have your data structured in a observations by features matrix, with the labels as a vector of length observations. This restriction is not necessary with NN. There is fantastic work with structured SVM, but it is unlikely it will ever be as flexible as NNs.



Pros of SVM:

Fewer hyperparameters. Generally SVMs require less grid-searching to get a reasonably accurate model. SVM with a RBF kernel usually performs quite well.
Global optimum guaranteed.


Cons of NN and SVM:

For most purposes they are both black boxes. There is some research on interpreting SVMs, but I doubt it will ever be as intuitive as GLMs. This is a serious problem in some problem domains.



If you're going to accept a black box then you can usually squeeze out quite a bit more accuracy by bagging/stacking/boosting many many models with different trade-offs.

Random forests are attractive because they can produce out-of-bag predictions(leave-one-out predictions) with no extra effort, they are very interpretable, they have an good bias-variance trade-off(great for bagging models) and they are relatively robust to selection bias. Stupidly simple to write a parallel implementation of.
Probabilistic graphical models are attractive because they can incorporate domain-specific-knowledge directly into the model and are interpretable in this regard.
Nonparametric(or really extremely parametric) Bayesian methods are attractive because they produce confidence intervals directly. They perform very well on small sample sizes and very well on large sample sizes. Stupidly simple to write a linear algebra implementation of.



","The choice between different machine learning models involves trade-offs. Support Vector Machines (SVMs) and Neural Networks (NNs) dominate in supervised learning but have limitations.

NNs excel in feature learning and handling various data types. However, they require feature engineering for optimal performance and can be black boxes. On the other hand, SVMs have fewer hyperparameters and guarantee a global optimum but lack the flexibility of NNs.

Random Forests offer interpretability, bias-variance balance, and out-of-bag predictions. Probabilistic Graphic Models enable domain-specific knowledge integration and interpretability. Nonparametric Bayesian methods provide confidence intervals and perform well with both small and large datasets.

In complex domains, bagging/stacking/boosting multiple models with varying trade-offs can enhance accuracy. Ultimately, the choice of model depends on the specific problem requirements, including data structure, interpretability needs, and desired performance characteristics."
CNN architectures for regression?,https://stats.stackexchange.com/questions/335836/cnn-architectures-for-regression,regression|machine-learning|neural-networks|convolutional-neural-network|tensorflow,335836,True,336077,90964,1,59,1565009830,1521636786,101,1521727701,"First of all a general suggestion: do a literature search before you start making experiments on a topic you're not familiar with. You'll save yourself a lot of time. 
In this case, looking at existing papers you may have noticed that 

CNNs have been used multiple times for regression: this (https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf) is a classic but it's old (yes, 3 years is old in DL). A more modern paper wouldn't have used AlexNet for this task. This (https://arxiv.org/abs/1708.05628) is more recent, but it's for a vastly more complicated problem (3D rotation), and anyway I'm not familiar with it.
Regression with CNNs is not a trivial problem. Looking again at the first paper, you'll see that they have a problem where they can basically generate infinite data. Their objective is to predict the rotation angle needed to rectify 2D pictures. This means that I can basically take my training set and augment it by rotating each image by arbitrary angles, and I'll obtain a valid, bigger training set. Thus the problem seems relatively simple, as far as Deep Learning problems go. By the way, note the other data augmentation tricks they use: 

We use translations (up to 5% of the image width), brightness
  adjustment in the range [â0.2, 0.2], gamma adjustment with Î³ â [â0.5,
  0.1] and Gaussian pixel noise with a standard deviation in the range [0, 
  0.02].

I don't know your problem well enough to say if it makes sense to consider 
variations in position, brightness and gamma noise for your pictures, carefully 
shot in a lab. But you can always try, and remove it if it doesn't improve your test set loss. Actually, you should really use a validation set or $k-$fold cross-validation for these kinds of experiments, and don't look at the test set until you have defined your setup, if you want the test set loss to be representative of the generalization error.
Anyway, even in their ideal conditions, the naive approach didn't work that well (section 4.2). They stripped out the output layer (the softmax layer) and substituted it with a layer with two units which would predict the sine $y$ and cosine $x$ of the rotation angle. The actual angle would then be computed as $\alpha=\text{atan2}(y,x)$. The neural network was also pretrained on ImageNet (this is called transfer learning). Of course the training on ImageNet had been for a different task (classification), but still training the neural network from scratch must have given such horrible results that they decided not to publish them. So you had all ingredients to make a good omelette: potentially infinite training data, a pretrained network and an apparently simple regression problem (predict two numbers between -1 and 1). Yet, the best they could get with this approach was a 21Â° error. It's not clear if this is an RMSE error, a MAD error or what, but still it's not great: since the maximum error you can make is 180Â°, the average error is $>11\%$ of the maximum possible error. They did slightly better by using two networks in series: the first one would perform  classification (predict whether the angle would be in the $[-180Â°,-90Â°],[-90Â°,0Â°],[0Â°,90Â°]$ or $[90Â°,180Â°]$ class), then the image, rotated by the amount predicted by the first network, would be fed to another neural network (for regression, this time), which would predict the final additional rotation in the $[-45Â°,45Â°]$ range.
On a much simpler (rotated MNIST) problem, you can get something better (https://it.mathworks.com/help/nnet/examples/train-a-convolutional-neural-network-for-regression.html?requestedDomain=true), but still you don't go below an RMSE error which is $2.6\%$ of the maximum possible error. 

So, what can we learn from this? First of all, that 5000 images is a small data set for your task. The first paper used a network which was pretrained on images similar to that for which they wanted to learn the regression task: not only you need to learn a different task from that for which the architecture was designed (classification), but your training set doesn't look anything at all like the training sets on which these networks are usually trained (CIFAR-10/100 or ImageNet). So you probably won't get any benefits from transfer learning. The MATLAB example had 5000 images, but they were black and white and semantically all very similar (well, this could be your case too). 
Then, how realistic is doing better than 0.3? We must first of all understand what do you mean by 0.3 average loss. Do you mean that the RMSE error is 0.3,
$$\frac{1}{N}\sum_{i=1}^N (h(\mathbf{x}_i)-y_i)^2$$
where $N$ is the size of your training set (thus, $N< 5000$), $h(\mathbf{x}_i)$ is the output of your CNN for image $\mathbf{x}_i$ and $y_i$ is the corresponding concentration of the chemical? Since $y_i\in[80,350]$, then assuming that you clip the predictions of your CNN between 80 and 350 (or you just use a logit to make them fit in that interval), you're getting less than $0.12\%$ error. Seriously, what do you expect? it doesn't seem to me a big error at all. 
Also, just try to compute the number of parameters in your network: I'm in a hurry and I may be making silly mistakes, so by all means double check my computations with some summary function from whatever framework you may be using. However, roughly I would say you have
$$9\times(3\times 32 + 2\times 32\times 32 + 32\times64+2\times64\times64+ 64\times128+2\times128\times128) +128\times128+128\times32+32 \times32\times32=533344$$
(note I skipped the parameters of the batch norm layers, but they're just 4 parameters for layer so they don't make a difference). You have half a million parameters and 5000 examples...what would you expect? Sure, the number of parameters is not a good indicator for the capacity of a neural network (it's a non-identifiable model), but still...I don't think you can do much better than this, but you can try a few things:

normalize all inputs (for example, rescale the RGB intensities of each pixel between -1 and 1, or use standardization) and all outputs. This will especially help if you have convergence issues.
go to grayscale: this would reduce your input channels from 3 to 1. All your images seem (to my highly untrained eye) to be of relatively similar colors. Are you sure it's the color that it's needed to predict $y$, and not the existence of darker or brighter areas? Maybe you're sure (I'm not an expert): in this case skip this suggestion.
data augmentation: since you said that flipping, rotating by an arbitrary angle or mirroring your images should result in the same output, you can increase the size of your data set a lot. Note that with a bigger dataset the error on the training set will go up: what we're looking for here is a smaller gap between training set loss and test set loss. Also, if the training set loss increases a lot, this could be good news: it may mean that you can train a deeper network on this bigger training set without the risk of overfitting. Try adding more layers and see if now you get a smaller training set and test set loss. Finally, you could try also the other data augmentation tricks I quoted above, if they make sense in the context of your application.
use the classification-then-regression trick: a first network only determines if $y$ should be in one of, say, 10 bins, such as $[80,97],[97,124]$,etc. A second network then computes a $[0,27]$ correction: centering and normalizing may help here too. Can't say without trying.
try using a modern architecture (Inception or ResNet) instead than a vintage one. ResNet has actually less parameters than VGG-net. Of course, you want to use the small ResNets here - I don't think ResNet-101 could help on a 5000 images data set. You can augment the data set a lot, though....
Since your output is invariant to rotation, another great idea would be to use either group equivariant CNNs (http://proceedings.mlr.press/v48/cohenc16.html), whose output (when used as classifiers) is invariant to discrete rotations, or steerable CNNs (https://arxiv.org/pdf/1612.08498.pdf) whose output is invariant to continuous rotations. The invariance property would allow you to get good results with much less data augmentation, or ideally none at all (for what it concerns rotations: of course you still need the other types of d. a.). Group equivariant CNNs are more mature than steerable CNNs from an implementation point of view, so Iâd try group CNNs first. You can try the classification-then-regression, using the G-CNN for the classification part, or you may experiment with the pure regression approach. Remember to change the top layer accordingly.
experiment with the batch size (yeah, yeah, I know hyperparameters-hacking is not cool, but this is the best I could come with in a limited time frame & for free :-)
finally, there are architectures which have been especially developed to make accurate predictions with small data sets. Most of them used dilated convolutions: one famous example is the mixed-scale dense convolutional neural network (http://www.pnas.org/content/115/2/254). The implementation is not trivial, though.

","**Summary:**

* Conduct a literature search before experimenting on unfamiliar topics to save time.
* Regression with CNNs is challenging, especially with small data sets and unfamiliar domains.
* Pretrained networks may not benefit regression tasks with different data characteristics.
* The number of parameters in a network should be considered in relation to the data set size.
* Data augmentation techniques can increase the size of training sets and reduce overfitting.
* A two-step approach of classification followed by regression can improve results.
* Using modern CNN architectures or group equivariant CNNs can provide better performance with small data sets.
* Experimenting with hyperparameters, such as batch size, can also impact accuracy.
* Architectures specifically designed for small data sets, like mixed-scale dense convolutional neural networks, should be considered."
Why do we use ReLU in neural networks and how do we use it?,https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it,neural-networks,226923,True,226927,137178,6,59,1616778699,1470158770,55,1470160787,"The ReLU function is $f(x)=\max(0, x).$ Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like.
One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of $x$). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations.
Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is  has its own problem, called ""dead neurons,"" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU, or PReLU, etc.) can ameliorate this.
$\frac{d}{dx}\text{ReLU}(x)=1\forall x > 0$ . By contrast, the gradient of a sigmoid unit is at most $0.25$; on the other hand, $\tanh$ fares better for inputs in a region near 0 since $0.25 < \frac{d}{dx}\tanh(x) \le 1 \forall x \in [-1.31, 1.31]$ (approximately).
","The ReLU (Rectified Linear Unit) function replaces other activation functions in neural networks, improving training speed due to its simple gradient calculation and computational step. The gradient of ReLU is 0 for negative inputs and 1 for positive inputs, which accelerates learning in the positive region. However, it also introduces ""dead neurons"" where the output is always zero.

To address this, modified ReLU units like ELU and Leaky ReLU have been developed. Compared to the sigmoid function, which has a gradient of at most 0.25, ReLU provides a larger gradient for positive inputs, speeding up learning. However, tanh performs better for inputs near zero, as its gradient is greater than 0.25 within a specific range."
Adam optimizer with exponential decay,https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay,neural-networks|deep-learning|gradient-descent|tensorflow|adam,200063,True,200105,90406,6,59,1572756842,1457166121,46,1457194373,"Empirically speaking: definitely try it out, you may find some very useful training heuristics, in which case, please do share!

Usually people use some kind of decay, for Adam it seems uncommon. Is there any theoretical reason for this? Can it be useful to combine Adam optimizer with decay?

I haven't seen enough people's code using ADAM optimizer to say if this is true or not. If it is true, perhaps it's because ADAM is relatively new and learning rate decay ""best practices"" haven't been established yet. 
I do want to note however that learning rate decay is actually part of the theoretical guarantee for ADAM. Specifically in Theorem 4.1 of their ICLR article (http://arxiv.org/abs/1412.6980), one of their hypotheses is that the learning rate has a square root decay, $\alpha_t = \alpha/\sqrt{t}$. Furthermore, for their logistic regression experiments they use the square root decay as well. 
Simply put: I don't think anything in the theory discourages using learning rate decay rules with ADAM. I have seen people report some good results using ADAM and finding some good training heuristics would be incredibly valuable. 
","**Summary:**

The author suggests trying out learning rate decay with the Adam optimizer, as it may be beneficial.

Despite being relatively new, Adam's theoretical foundation supports the use of decay. The ICLR article introducing Adam includes decay as a hypothesis and uses it in experiments.

The author encourages sharing any successful training heuristics discovered using Adam and decay."
What are the main theorems in Machine (Deep) Learning?,https://stats.stackexchange.com/questions/321851/what-are-the-main-theorems-in-machine-deep-learning,machine-learning|deep-learning|mathematical-statistics,321851,True,322934,9863,6,58,1595866955,1515253072,65,1515856887,"As I wrote in the comments, this question seems too broad to me, but I'll make an attempt to an answer. In order to set some boundaries, I will start with a little math which underlies most of ML, and then concentrate on recent results for DL.

The bias-variance tradeoff is referred to in countless books, courses, MOOCs, blogs, tweets, etc. on ML, so we can't start without mentioning it:
$$\mathbb{E}[(Y-\hat{f}(X))^2|X=x_0]=\sigma_{\epsilon}^2+\left(\mathbb{E}\hat{f}(x_0)-f(x_0)\right)^2+\mathbb{E}\left[\left(\hat{f}(x_0)-\mathbb{E}\hat{f}(x_0)\right)^2\right]=\text{Irreducible error + Bias}^2 \text{ + Variance}$$
Proof here: https://web.stanford.edu/~hastie/ElemStatLearn/ (https://web.stanford.edu/%7Ehastie/ElemStatLearn/)

The Gauss-Markov Theorem (yes, linear regression will remain an important part of Machine Learning, no matter what: deal with it) clarifies that, when the linear model is true and some assumptions on the error term are valid, OLS has the minimum mean squared error (which in the above expression is just $\text{Bias}^2 \text{ + Variance}$) only among the unbiased linear estimators of the linear model. Thus there could well be linear estimators with bias (or nonlinear estimators) which have a better mean square error, and thus a better expected prediction error, than OLS. And this paves the way to all the regularization arsenal (ridge regression, LASSO, weight decay, etc.) which is a workhorse of ML. A proof is given here (and in countless other books):
https://www.amazon.com/Linear-Statistical-Models-James-Stapleton/dp/0470231467 (https://rads.stackoverflow.com/amzn/click/com/0470231467)
Probably more relevant to the explosion of regularization approaches, as noted by Carlos Cinelli in the comments, and definitely more fun to learn about, is the James-Stein theorem. Consider $n$ independent, same variance but not same mean Gaussian random variables:
$$X_i|\mu_i\sim \mathcal{N}(\theta_i,\sigma^2), \quad i=1,\dots,n$$
in other words, we have an $n-$components Gaussian random vector $\mathbf{X}\sim \mathcal{N}(\boldsymbol{\theta},\sigma^2I)$. We have one sample $\mathbf{x}$ from $\mathbf{X}$ and we want to estimate $\boldsymbol{\theta}$. The MLE (and also UMVUE) estimator is obviously $\hat{\boldsymbol{\theta}}_{MLE}=\mathbf{x}$. Consider the James-Stein estimator
$$\hat{\boldsymbol{\theta}}_{JS}= \left(1-\frac{(n-2)\sigma^2}{||\mathbf{x}||^2}\right)\mathbf{x} $$
Clearly, if $(n-2)\sigma^2\leq||\mathbf{x}||^2$, $\hat{\boldsymbol{\theta}}_{JS}$ shrinks the MLE estimate towards zero. The James-Stein theorem states that for $n\geq4$, $\hat{\boldsymbol{\theta}}_{JS}$ strictly dominates $\hat{\boldsymbol{\theta}}_{MLE}$, i.e., it has lower MSE $\forall \ \boldsymbol{\theta}$. Pheraps surprisingly, even if we shrink towards any other constant $\boldsymbol{c}\neq \mathbf{0}$, $\hat{\boldsymbol{\theta}}_{JS}$ still dominates $\hat{\boldsymbol{\theta}}_{MLE}$. Since the $X_i$ are independent, it may seem weird that, when trying to estimate  the height of three unrelated persons, including a sample from the number of apples produced in Spain, may improve our estimate on average. The key point here is ""on average"": the mean square error for the simultaneous estimation of all the components of the parameter vector is smaller, but the square error for one or more components  may well be larger, and indeed it often is, when you have ""extreme"" observations.
Finding out that MLE, which was indeed the ""optimal"" estimator for the univariate estimation case, was dethroned for multivariate estimation, was quite a shock at the time, and led to a great interest in shrinkage, better known as regularization in ML parlance. One could note some similarities with mixed models and the concept of ""borrowing strength"": there is indeed some connection, as discussed here
Unified view on shrinkage: what is the relation (if any) between Stein's paradox, ridge regression, and random effects in mixed models? (https://stats.stackexchange.com/questions/122062/unified-view-on-shrinkage-what-is-the-relation-if-any-between-steins-paradox)
Reference: James, W., Stein, C., Estimation with Quadratic Loss. Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 361--379, University of California Press, Berkeley, Calif., 1961 (https://projecteuclid.org/euclid.bsmsp/1200512173)

Principal Component Analysis is key to the important topic of dimension reduction, and it's based on the Singular Value Decomposition: for each $N\times p$ real matrix $X$ (although the theorem easily generalizes to complex matrices) we can write
$$X=UDV^T$$
where $U$ of size $N \times p$ is orthogonal, $D$ is a $p \times p$ diagonal matrix with nonnegative diagonal elements and $U$ of size $p \times p$ is again orthogonal. For proofs and algorithms on how to compute it see: Golub, G., and Van Loan, C. (1983), Matrix computations, John Hopkins University press, Baltimore.

Mercer's theorem is the founding stone for a lot of different ML methods: thin plate splines, support vector machines, the Kriging estimate of a Gaussian random process, etc. Basically, is one of the two theorems behind the so-called kernel trick. Let $K(x,y):[a,b]\times[a,b]\to\mathbb{R}$ be a symmmetric continuous function or kernel. if $K$ is positive semidefinite, then it admits an orthornormal basis of eigenfunctions corresponding to nonnegative eigenvalues:
$$K(x,y)=\sum_{i=1}^\infty\gamma_i \phi_i(x)\phi_i(y)$$
The importance of this theorem for ML theory is testified by the number of references it gets in famous texts, such as for example Rasmussen & Williams text on Gaussian processes (http://www.gaussianprocess.org/gpml/chapters/RW4.pdf).
Reference: J. Mercer, Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209:415-446, 1909 (http://rsta.royalsocietypublishing.org/content/roypta/209/441-458/415.full.pdf)
There is also a simpler presentation in Konrad JÃ¶rgens, Linear integral operators, Pitman, Boston, 1982.

The other theorem which, together with Mercer's theorem, lays out the theoretical foundation of the kernel trick, is the representer theorem. Suppose you have a sample space $\mathcal{X}$ and a symmetric positive semidefinite kernel $K: \mathcal{X} \times \mathcal{X}\to \mathbb{R}$. Also let $\mathcal{H}_K$ be the RKHS associated with $K$. Finally, let $S=\{\mathbb{x}_i,y_i\}_{i=1}^n$ be a training sample. The theorem says that among all functions $f\in \mathcal{H}_K$, which all admit an infinite representation in terms of eigenfunctions of $K$ because of Mercer's theorem, the one that minimizes the regularized risk always has a finite representation in the basis formed by the kernel evaluated at the $n$ training points, i.e.
$$\min_{f \in \mathcal{H}_K} \sum_{i=1}^n L(y_i,f(x_i))+\lambda||f||^2_{\mathcal{H}_K}=\min_{\{c_j\}_1^\infty} \sum_{i=1}^n L(y_i,\sum_j^\infty c_j\phi_j(x_i))+\lambda\sum_j^\infty \frac{c_j^2}{\gamma_j}=\sum_{i=1}^n\alpha_i K(x,x_i)$$
(the theorem is the last equality). References: Wahba, G. 1990, Spline Models for Observational Data, SIAM, Philadelphia.

The universal approximation theorem has been already cited by user Tobias Windisch and is much less relevant to Machine Learning than it is to functional analysis, even if it may not seem so at a first glance. The problem is that the theorem only says that such a network exists, but:

it doesn't give any correlation between the size $N$ of the hidden layer and some measure of complexity of the target function $f(x)$, such as for example Total Variation. If $f(x)=\sin(\omega x):[0,2\pi]\to[-1,1]$ and the $N$ required for a fixed error $\epsilon$ growed exponentially with $\omega$, then single hidden layer neural networks would be useless.
it doesn't say if the network $F(x)$ is learnable. In other words assume that given $f$ and $\epsilon$, we know that a size $N$ NN will approximate $f$ with the required tolerance in the hypercube. Then by using training sets of size $M$  and a learning procedure such as for example back-prop, do we have any guarantee that by increasing $M$ we can recover $F$?
finally, and worse of them all, it doesn't say anything about the prediction error of neural networks. What we're really interested in is an estimate of the prediction error, at least averaged over all training sets of size $M$. The theorem doesn't help in this respect.

A smaller pain point with the Hornik's version of this theorem is that it doesn't hold for ReLU activation functions. However, Bartlett has since proved  an extended version which covers this gap.

Until now, I guess all the theorems I considered were well-known to anybody. So now it's time for the fun stuff :-) Let's see a few Deep Learning theorems:
Assumptions:

the deep neural network $\Phi(X,W)$ (for fixed $W$, $\Phi_W(X)$ is the function which associates the inputs of the neural network with its outputs) and the regularization loss $\Theta(W)$ are both sums of positively homogeneous functions of the same degree
the loss function $L(Y,\Phi(X,W))$ is convex and at least once differentiable in $X$, in a compact set $S$

Then:

any local minimum for $L(Y,\Phi(X,W))+\lambda\Theta(W)$ such that a subnetwork of $\Phi(X,W)$ has zero weights, is a global minimum (Theorem 1)
above a critical network size, local descent will always converge to a global minimum from any initialization (Theorem 2).

This is very interesting: CNNs made only of convolutional layers, ReLU, max-pooling, fully connected ReLU and linear layers are positively homogenous functions, while if we include sigmoid activation functions, this isn't true anymore, which may partly explain the superior performance in some applications of ReLU + max pooling with respect to sigmoids. What's more, the theorems only hold if also $\Theta$ is positively homogeneous in $W$ of the same degree as $\Phi$. Now, the fun fact is that  $l_1$ or $l_2$ regularization, although positively homogeneous, don't have the same degree of $\Phi$ (the degree of $\Phi$, in the simple CNN case mentioned before, increases with the number of layers). Instead, more modern regularization methods such as batch normalization and path-SGD do correspond to a positively homogeneous regularization function of the same degree as $\Phi$, and dropout, while not fitting this framework exactly, holds strong similarities to it. This may explain why, in order to get high accuracy with CNNs, $l_1$ and $l_2$ regularization are not enough, but we need to employ all kinds of devilish tricks, such as dropout and batch normalization! To the best of my knowledge, this is the closest thing to an explanation of the efficacy of batch normalization, which is otherwise very obscure, as correctly noted by Al Rahimi in his talk.
Another observation that some people make, based on Theorem 1, is that it could explain why ReLU work well, even with the problem of dead neurons. According to this intuition, the fact that, during training, some ReLU neurons ""die"" (go to zero activation and then never recover from that, since for $x<0$ the gradient of ReLU is zero) is ""a feature, not a bug"", because if we have reached a minimum and a full subnetwork has died, then we're provably reached a global minimum (under the hypotheses of Theorem 1). I may be missing something, but I think this interpretation is far-fetched. First of all, during training ReLUs can ""die"" well before we have reached a local minimun. Secondly, it has to be proved that when ReLU units ""die"", they always do it over a full subnetwork: the only case where this is trivially true is when you have just one hidden layer, in which case of course each single neuron is a subnetwork. But in general I would be very cautious in seeing ""dead neurons"" as a good thing.
References:
B. Haeffele and R. Vidal, Global optimality in neural network training,
In IEEE Conference on Computer Vision and Pattern Recognition,
2017.
B. Haeffele and R. Vidal. Global optimality in tensor factorization,
deep learning, and beyond, arXiv, abs/1506.07540, 2015.

Image classification requires learning representations which are invariant (or at least robust, i.e., very weakly sensitive) to various transformations such as  location, pose, viewpoint, lighting, expression, etc. which are commonly present in natural images, but do not contain info for the classification task. Same thing for speech recognition: changes in pitch, volume, pace, accent. etc. should not lead to a change in the classification of the word. Operations such as convolution, max pooling, average pooling, etc., used in CNNs, have exactly this goal, so intuitively we expect that they would work for these applications. But do we have theorems to support this intuition? There is a vertical translation invariance theorem, which, notwithstanding the name, has nothing to do with translation in the vertical direction, but it's basically a result which says that features learnt in following layers get more and more invariant, as the number of layers grows. This is opposed to an older horizontal translation invariance theorem which however holds for scattering networks, but not for CNNs.
The theorem is very technical, however:

assume $f$ (your input image) is square-integrable
assume your filter commutes with the translation operator $T_t$, which maps the input image $f$ to a translated copy of itself $T_t f$. A learned convolution kernel (filter) satisfies this hypothesis.
assume all filters, nonlinearities and pooling in your network satisfy a so-called weak admissibility condition, which is basically some sort of weak regularity and boundedness conditions. These conditions are satisfied by learned convolution kernel (as long as some normalization operation is performed on each layer), ReLU, sigmoid, tanh, etc, nonlinearities, and by average pooling, but not by max-pooling. So it covers some (not all) real world CNN architectures.
Assume finally that each layer $n$ has a pooling factor $S_n> 1$, i.e., pooling is applied in each layer and effectively discards information. The condition $S_n\geq 1 $ would also suffice for a weaker version of the theorem.

Indicate with $\Phi^n(f)$ the output of layer $n$ of the CNN, when the input is $f$. Then finally:
$$\lim_{n\to\infty}|||\Phi^n(T_f f)-\Phi^n(f)|||=0$$
(the triple bars are not an error) which basically means that each layer learns features which become more and more invariant, and in the limit of an infinitely deep network we have a perfectly invariant architecture. Since CNNs have a finite number of layers, they're not perfectly translation-invariant, which is something well-known to practitioners.
Reference: T. Wiatowski and H. Bolcskei, A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction, arXiv:1512.06293v3 (https://arxiv.org/pdf/1512.06293.pdf).

To conclude, numerous bounds for the generalization error of a Deep Neural Network based on its Vapnik-Chervonkensis dimension or on the Rademacher complexity grow with the number of parameters (some even exponentially), which means they can't explain why DNNs work so well in practice even when the number of parameters is considerably larger than the number of training samples. As a matter of fact, VC theory is not very useful in Deep Learning.
Conversely, some results from last year bound the generalization error of a DNN classifier with a quantity which is independent of the neural network's depth and size, but depends only on the structure of the training set and the input space. Under some pretty technical assumptions on the learning procedure, and on the training set and input space, but with very little assumptions on the DNN (in particular, CNNs are fully covered), then with probability at least $1-\delta$, we have
$$\text{GE} \leq \sqrt{2\log{2}N_y\frac{\mathcal{N_{\gamma}}}{m}}+\sqrt{\frac{2\log{(1/\delta)}}{m}}$$
where:

$\text{GE}$ is the generalization error, defined as the difference between the expected loss (the average loss of the learned classifier on all possible test points) and the empirical loss (just the good ol' training set error)
$N_y$ is the number of classes
$m$ is the size of the training set
$\mathcal{N_{\gamma}}$ is the covering number of the data, a quantity related to the structure of the input space and to the the minimal separation among points of different classes in the training set. Reference:

J. Sokolic, R. Giryes, G. Sapiro, and M. Rodrigues. Generalization
error of invariant classifiers. In AISTATS, 2017
","**Summary:**

This summary covers foundational concepts and recent theorems in machine learning, particularly focusing on deep learning.

**Bias-Variance Tradeoff:**
The bias-variance tradeoff explains the relationship between model complexity, bias, and variance in prediction errors.

**Regularization:**
Regularization techniques like ridge regression and LASSO reduce model complexity by introducing penalties to prevent overfitting. The James-Stein theorem shows that shrinkage estimators can outperform maximum likelihood estimators in certain cases.

**Dimension Reduction:**
Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) reduce dimensionality by finding orthogonal directions that capture most of the data's variance.

**Kernel Methods:**
Mercer's theorem and the representer theorem provide the theoretical basis for the kernel trick, which enables the use of nonlinear functions without explicitly mapping data to a higher-dimensional space.

**Universal Approximation Theorem:**
This theorem states that any continuous function can be approximated by a neural network with a single hidden layer, but it has limitations in terms of network size and learnability.

**Deep Learning Theorems:**
Recent theorems for deep neural networks include:
- Global optimality under certain conditions, explaining potential advantages of ReLU activations and batch normalization.
- Translation invariance, indicating that CNNs learn features that become increasingly invariant to transformations like translation.

**Generalization Error Bounds:**
Bounds for the generalization error of deep neural networks have been developed, indicating that network size and depth may not be critical factors for successful learning."
Danger of setting all initial weights to zero in Backpropagation,https://stats.stackexchange.com/questions/27112/danger-of-setting-all-initial-weights-to-zero-in-backpropagation,neural-networks|backpropagation,27112,True,27152,65636,8,58,1686547160,1335378073,26,1335404453,"edit see alfa's comment below. I'm not an expert on neural nets, so I'll defer to him. 
My understanding is different from the other answers that have been posted here.
I'm pretty sure that backpropagation involves adding to the existing weights, not multiplying.  The amount that you add is specified by the delta rule (https://en.wikipedia.org/wiki/Delta_rule).  Note that wij doesn't appear on the right-hand-side of the equation.
My understanding is that there are at least two good reasons not to set the initial weights to zero:

First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values.  You can't do that if they all start at zero.
Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another.

","Neural networks are powerful computational models that can be used to solve a variety of problems, such as image recognition and natural language processing. The performance of a neural network depends on the weights of its connections. The process of adjusting these weights is called backpropagation.
Backpropagation involves calculating the error between the output of the network and the desired output, and then using this error to adjust the weights. The error is calculated by taking the difference between the output of the network and the desired output, and then squaring the result. The weights are then adjusted by a small amount, proportional to the error and the gradient of the error with respect to the weight.
The gradient is calculated using the chain rule, which allows us to calculate the derivative of the error with respect to the weight. The derivative is a vector that points in the direction of the steepest descent of the error, and the amount that the weight is adjusted is proportional to the magnitude of the gradient.
Backpropagation can be a very slow process, especially for large networks. However, it is a powerful technique that can be used to train neural networks to solve a wide variety of problems."
Can a deep neural network approximate multiplication function?,https://stats.stackexchange.com/questions/217703/can-a-deep-neural-network-approximate-multiplication-function,regression|machine-learning|neural-networks,217703,True,218659,25368,7,57,1678355826,1465281487,22,1465820516,"A big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have a zero gradient (because of neural network implementation details and limitations). We can use two approaches:

Divide by a constant. We are just dividing everything before the learning and multiply after.

Use log-normalization. It makes multiplication into addition:
\begin{align}
m &= x \cdot y\\
&\Rightarrow  \\
\ln(m) &= \ln(x) + \ln(y)
\end{align}


","**Summary:**

Gradient issues in neural networks can lead to vanishing gradients, where hidden nodes have zero gradients. To address this, two methods are proposed:

**1. Divide by a Constant:**
Before training, divide all weights and activations by a constant. After training, multiply them by the same constant.

**2. Log-Normalization:**
Transform multiplication into addition by applying the natural logarithm function:
* Original multiplication: `m = x * y`
* Log-normalized addition: `ln(m) = ln(x) + ln(y)`

This conversion allows for more stable gradient flow and prevents vanishing gradients."
What is pre training a neural network?,https://stats.stackexchange.com/questions/193082/what-is-pre-training-a-neural-network,neural-networks|pre-training,193082,True,193451,62127,3,57,1538072535,1454073171,58,1454324111,"The usual way of training a network:
You want to train a neural network to perform a task (e.g. classification) on a data set (e.g. a set of images). You start training by initializing the weights randomly. As soon as you start training, the weights are changed in order to perform the task with less mistakes (i.e. optimization).
Once you're satisfied with the training results you save the weights of your network somewhere.
You are now interested in training a network to perform a new task (e.g. object detection) on a different data set (e.g. images too but not the same as the ones you used before). Instead of repeating what you did for the first network and start from training with randomly initialized weights, you can use the weights you saved from the previous network as the initial weight values for your new experiment. Initializing the weights this way is referred to as using a pre-trained network. The first network is your pre-trained network. The second one is the network you are fine-tuning.
The idea behind pre-training is that random initialization is...well...random, the values of the weights have nothing to do with the task you're trying to solve. Why should a set of values be any better than another set? But how else would you initialize the weights? If you knew how to initialize them properly for the task, you might as well set them to the optimal values (slightly exaggerated). No need to train anything. You have the optimal solution to your problem.
Pre-training gives the network a head start. As if it has seen the data before.
What to watch out for when pre-training:
The first task used in pre-training the network can be the same as the fine-tuning stage. The datasets used for pre-training vs. fine-tuning can also be the same, but can also be different. It's really interesting to see how pre-training on a different task and different dataset can still be transferred to a new dataset and new task that are slightly different.
Using a pre-trained network generally makes sense if both tasks or both datasets have something in common. The bigger the gap, the less effective pre-training will be. It makes little sense to pre-train a network for image classification by training it on financial data first. In this case there's too much disconnect between the pre-training and fine-tuning stages.
","**Summary:**

Pre-training is a technique used to initialize the weights of a neural network for a new task by transferring knowledge from a previously trained network. This is done by using the weights from the pre-trained network as the starting point for training the new network.

Pre-training provides a head start to the new network by giving it weights that have already learned general features from the data. This reduces the time and effort required for training and can improve the performance of the network on the new task.

The pre-training and fine-tuning stages can involve the same or different tasks and datasets. Pre-training can be beneficial even when there are differences between the two stages, but it is most effective when there is some overlap between the tasks or datasets.

When selecting a pre-trained network, consider the similarity between the pre-training and fine-tuning tasks and datasets. If there is too much disconnect, pre-training may not be effective."
How to weight KLD loss vs reconstruction loss in variational auto-encoder?,https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder,machine-learning|neural-networks|tensorflow|autoencoders|variational-bayes,332179,True,333176,43648,5,57,1693338250,1520417952,34,1520941971,"For anyone stumbling on this post also looking for an answer, this twitter thread (https://twitter.com/memotv/status/973323454350090240) has added a lot of very useful insight.
Namely:
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (https://openreview.net/forum?id=Sy2fzU9gl)
discusses my exact question with a few experiments. Interestingly, it seems their $\beta_{norm}$ (which is similar to my normalised KLD weight) is also centred around 0.1, with higher values giving more structured latent space at the cost of poorer reconstruction, and lower values giving better reconstruction with less structured latent space (though their focus is specifically on learning disentangled representations).
and related reading (where similar issues are discussed)

Semi-Supervised Learning with Deep Generative Models (https://arxiv.org/abs/1406.5298)
https://github.com/dpkingma/nips14-ssl (https://github.com/dpkingma/nips14-ssl)
InfoVAE: Information Maximizing Variational Autoencoders (https://arxiv.org/abs/1706.02262)
Density estimation using Real NVP (https://arxiv.org/abs/1605.08803)
Neural Discrete Representation Learning (https://arxiv.org/abs/1711.00937)

","**Summary:**

The provided Twitter thread offers valuable insights into the impact of a parameter called $\beta_{norm}$ in beta-Variational AutoEncoders (beta-VAEs) on the structure and reconstruction quality of the latent space. According to the study ""Learning Basic Visual Concepts with a Constrained Variational Framework,"" $\beta_{norm}$ is similar to a normalized Kullback-Leibler divergence (KLD) weight.

Higher $\beta_{norm}$ values promote structured latent spaces, leading to better disentanglement of representations but poorer reconstruction. Conversely, lower $\beta_{norm}$ values favor better reconstruction with less structured latent spaces.

The summary also points to a range of related research, including:
- Semi-Supervised Learning with Deep Generative Models
- InfoVAE: Information Maximizing Variational Autoencoders
- Density Estimation using Real NVP
- Neural Discrete Representation Learning"
What does the term saturating nonlinearities mean?,https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean,machine-learning|neural-networks|terminology|convolutional-neural-network,174295,True,174438,29327,3,57,1617097774,1443296715,56,1443399709,"Intuition
A saturating activation function squeezes the input.

Definitions

$f$ is non-saturating iff $ (|\lim_{z\to-\infty} f(z)| = +\infty) \vee (|\lim_{z\to+\infty} f(z)| = +\infty) $ 
$f$ is saturating iff $f$ is not non-saturating. 

These definitions are not specific to convolutional neural networks.

Examples
The Rectified Linear Unit (ReLU) activation function, which is defined as $f(x)=max(0,x)$ is non-saturating because $\lim_{z\to+\infty} f(z) = +\infty$:
 (https://i.sstatic.net/liEMr.png)
The sigmoid activation function, which is defined as $f(x) = \frac{1}{1 + e^{-x}}$ is saturating, because it squashes real numbers to range between $[0,1]$: 
 (https://i.sstatic.net/LUJKU.png)
The tanh (hyperbolic tangent) activation function is saturating as it squashes real numbers to range between $[-1,1]$:
 (https://i.sstatic.net/tjl2T.png)
(figures are from CS231n (http://cs231n.github.io/neural-networks-1/),  MIT License)
","**Summary:**

**Definition:**
* Non-saturating activation functions allow the input to become arbitrarily large or negative.
* Saturating activation functions restrict the input to a limited range.

**Examples:**
* Rectified Linear Unit (ReLU) is non-saturating because the output can grow indefinitely.
* Sigmoid and tanh are saturating because they limit the output to a range between 0-1 and -1-1, respectively.

Saturating functions ""squeeze"" the input by restricting its range. This can make them more robust to large input values but can also limit their expressiveness. Non-saturating functions allow for a wider range of outputs, providing more flexibility but potentially making them more sensitive to noise and extreme values."
R libraries for deep learning,https://stats.stackexchange.com/questions/41771/r-libraries-for-deep-learning,r|neural-networks|deep-learning|restricted-boltzmann-machine|deep-belief-networks,41771,True,118266,47764,8,56,1487428280,1351877756,19,1412753479,"OpenSource h2o.deepLearning() is package for deeplearning in R from h2o.ai
here's a write up http://www.r-bloggers.com/things-to-try-after-user-part-1-deep-learning-with-h2o/ (http://www.r-bloggers.com/things-to-try-after-user-part-1-deep-learning-with-h2o/)
And code: https://gist.github.com/woobe/3e728e02f6cc03ab86d8#file-link_data-r (https://gist.github.com/woobe/3e728e02f6cc03ab86d8#file-link_data-r)
######## *Convert Breast Cancer data into H2O*
dat <- BreastCancer[, -1]  # remove the ID column
dat_h2o <- as.h2o(localH2O, dat, key = 'dat')

######## *Import MNIST CSV as H2O*
dat_h2o <- h2o.importFile(localH2O, path = "".../mnist_train.csv"")

######## *Using the DNN model for predictions*
h2o_yhat_test <- h2o.predict(model, test_h2o)

######## *Converting H2O format into data frame*
df_yhat_test <- as.data.frame(h2o_yhat_test)

######## Start a local cluster with 2GB RAM
library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE, 
                    Xmx = '2g') 
########Execute deeplearning

model <- h2o.deeplearning( x = 2:785,  # column numbers for predictors
               y = 1,   # column number for label
               data = train_h2o, # data in H2O format
               activation = ""TanhWithDropout"", # or 'Tanh'
               input_dropout_ratio = 0.2, # % of inputs dropout
               hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
               balance_classes = TRUE, 
               hidden = c(50,50,50), # three layers of 50 nodes
               epochs = 100) # max. no. of epochs

","**Summary:**

OpenSource h2o.deepLearning() package in R provides a comprehensive set of functions for deep learning.

**Key Features:**

* **Data Preprocessing:** Facilitates data conversion into H2O format for deep learning models.
* **Model Creation and Training:** Allows for training deep neural network (DNN) models with customizable network architecture, activation functions, and dropout ratios.
* **Prediction:** Enables prediction using trained DNN models.
* **Result Extraction:** Provides easy conversion of H2O prediction results into data frames for further analysis.

**Examples:**

* Importing Breast Cancer and MNIST datasets into H2O format.
* Training a DNN model with specific network parameters.
* Generating predictions and extracting results as a data frame.
* Starting a local H2O cluster with customized settings."
Understanding &quot;almost all local minimum have very similar function value to the global optimum&quot;,https://stats.stackexchange.com/questions/203288/understanding-almost-all-local-minimum-have-very-similar-function-value-to-the,machine-learning|neural-networks|optimization|deep-learning,203288,True,203300,25004,2,56,1722430669,1458752525,83,1458755872,"A recent paper The Loss Surfaces of Multilayer Networks (http://arxiv.org/abs/1412.0233) offers some possible explanations for this. From their abstract (bold is mine):

""We conjecture that both simulated annealing and SGD converge
to the band of low critical points, and that all critical points found there are local minima of  high  quality  measured  by  the  test  error.
This emphasizes a major difference between large- and small-size networks where for the latter  poor  quality  local  minima  have  non-zero probability of being recovered.  Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.""

A lot of the influential people in deep learning (Yann LeCunn and Yoshua Bengio to name a few) and some researchers coming more from the mathematical angle (Rong Ge and other Sanjeev Arora collaborators) have been discussing and exploring these ideas.
In the above referenced paper, see Figure 3, which shows a banding/concentration phenomenon of the local minima values as the nets have more hidden units. The banding/concentration represents some empirical evidence that for deeper or larger models, a local minima is ""good enough"", since their loss values are roughly similar. And most importantly, they have a loss which is closer to the global minimum as the model gets more complex (in this case wider, but in practice, deeper).
Furthermore, they use a spin-glass model, which they even state is just a model and not necessarily indicative of the true picture, to show that reaching the global minimizer from a local minima may take exponentially long:

""In order to find a further low lying minimum we must pass through a saddle point. Therefore we must go up at  least to the level where there is an equal  amount of saddle points to have a decent chance of finding a
path that might possibly take us to another local minimum. This process takes an exponentially long time so in practice finding the global minimum is not feasible.""

The Rong Ge research is centered around breaking through saddle points. Yoshua Bengio and his collaborators have posed a pretty bold Saddle Point Hypothesis:

Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.

source here: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. (http://arxiv.org/abs/1406.2572)
To some extent, the above two approaches aren't exactly the same (the Saddle Point Hypothesis might question what is really a local minima and what is merely a poorly conditioned saddle point with a very long plateau region?). The idea behind the Saddle Point Hypothesis is that it is possible to design optimization methods to break through saddle points, for example Saddle-Free Newton from the Bengio article, to potentially speed up convergence and maybe even reach the global optimum. The first Multilayer Loss Surface article is not really concerned with reaching the global optimum and actually believes it to have some poor overfitting properties. Curiously, both articles use ideas from statistical physics and spin-glass models.
But they are sort of related in that both articles believe that in order to reach the global minimizer, one must overcome the optimization challenge of saddle points. The first article just believes that local minima are good enough.
It is fair to wonder if momentum methods and other new optimization algorithms, which can estimate some 2nd order curvature properties can escape saddle points. A famous animation by Alec Radford here (https://web.archive.org/web/20160725050440/https://imgur.com/a/Hqolp).
To answer your question: ""where does this belief come from"" I personally think it comes from the fact that it's possible to use different random seeds to learn different weights, but the corresponding nets have similar quantitative performance. For example, if you set two different random seeds for Glorot weight initialization, you will probably learn different weights, but if you train using similar optimization methods, the nets will have similar performance. One common folklore belief is that the optimization landscape is similar to that of an egg carton, another good blog post on this here: No more local minima? (http://blog.terminal.com/no-more-local-minima/) with the egg-carton analogy.
Edit: I just wanted to be clear that the egg carton analogy is not true, otherwise there would be no need for momentum or other more advanced optimization techniques. But it is known that SGD does not perform as well as SGD+Momentum or more modern optimization algorithms, perhaps due to the existence of saddle points.
","Recent research in deep learning suggests that the loss surfaces of multilayer networks exhibit a phenomenon of ""banding,"" where local minima cluster together. This suggests that for larger models, local minima are typically of high quality, and the global minimum is less important, as it often leads to overfitting.

Overcoming saddle points is seen as a key challenge in optimization. The Saddle Point Hypothesis proposes that saddle points can slow down learning and create the illusion of local minima. Methods like Saddle-Free Newton aim to break through saddle points and potentially reach the global optimum.

However, the ""Multilayer Loss Surface"" paper argues that local minima are sufficient for good performance, and reaching the global minimum is impractical due to the exponential time required.

Overall, the belief that multiple local minima with similar performance exist stems from observations that different random initializations often lead to models with similar quantitative performance, suggesting a relatively smooth optimization landscape. However, the egg carton analogy is not fully accurate, as momentum and advanced optimization techniques still provide benefits."
What is a latent space?,https://stats.stackexchange.com/questions/442352/what-is-a-latent-space,machine-learning|neural-networks|definition,442352,True,442360,31446,3,56,1616964773,1577423495,56,1577431881,"Latent space refers to an abstract multi-dimensional space containing feature values that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.
Just as we, humans, have an understanding of a broad range of topics and the events belonging to those topics, latent space aims to provide a similar understanding to a computer through a quantitative spatial representation/modeling. 
The motivation to learn a latent space (set of hidden topics/ internal representations) over the observed data (set of events) is that large differences in observed space/events could be due to small variations in latent space (for the same topic). Hence, learning a latent space would help the model make better sense of observed data than from observed data itself, which is a very large space to learn from.
Some examples of latent space are:
1) Word Embedding Space - consisting of word vectors where words similar in meaning have vectors that lie close to each other in space (as measured by cosine-similarity or euclidean-distance) and words that are unrelated lie far apart (Tensorflow's Embedding Projector (http://projector.tensorflow.org/) provides a good visualization of word embedding spaces).
2) Image Feature Space - CNNs in the final layers encode higher-level features in the input image that allows it to effectively detect, for example, the presence of a cat in the input image under varying lighting conditions, which is a difficult task in the raw pixel space.
3) Topic Modeling (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) methods such as LDA (https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), PLSA (https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) use statistical approaches to obtain a latent set of topics from an observed set of documents and word distribution. (PyLDAvis (https://medium.com/@sherryqixuan/topic-modeling-and-pyldavis-visualization-86a543e21f58) provides a good visualization of topic models)
4) VAEs & GANs (https://www.youtube.com/watch?v=yFBFl1cLYx8&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=4) aim to obtain a latent space/distribution that closely approximates the real latent space/distribution of the observed data.
In all the above examples, we quantitatively represent the complex observation space with a (relatively simple) multi-dimensional latent space that approximates the real latent space of the observed data.
The terms ""high dimensional"" and ""low dimensional"" help us define how specific or how general the kinds of features we want our latent space to learn and represent. High dimensional latent space is sensitive to more specific features of the input data and can sometimes lead to overfitting when there isn't sufficient training data. Low dimensional latent space aims to capture the most important features/aspects required to learn and represent the input data (a good example is a low-dimensional bottleneck layer in VAEs). 
If this answer helped, please don't forget to up-vote it :)
","Latent space is an abstract concept that represents hidden features and relationships in complex data. It allows computers to develop an internal understanding of the data, similar to human comprehension.

By learning a latent space, models can capture underlying patterns and variations in data that may not be immediately apparent. This simplifies learning and enhances model performance. Examples of latent space in practice include:

- **Word embedding:** Representing words as vectors that capture their meaning and relationships.
- **Image feature space:** Identifying high-level features in images to aid in object recognition.
- **Topic modeling:** Discovering hidden topics from documents based on word distributions.
- **Generative models (VAEs and GANs):** Approximating the real latent distribution of data to generate new samples.

Latent spaces can be categorized as high-dimensional or low-dimensional based on their level of detail. High-dimensional spaces capture more specific features, while low-dimensional spaces focus on essential aspects. The choice depends on the specific learning task and data characteristics."
Understanding LSTM units vs. cells,https://stats.stackexchange.com/questions/241985/understanding-lstm-units-vs-cells,neural-networks|terminology|lstm|recurrent-neural-network|tensorflow,241985,True,241996,61877,6,56,1604898021,1477265830,30,1477272608,"The terminology is unfortunately inconsistent. num_units in TensorFlow is the number of hidden states, i.e. the dimension of $h_t$ in the equations you gave.
Also, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md) :

The definition of cell in this package differs from the definition used in the literature. In the literature, cell refers to an object with a single scalar output. The definition in this package refers to a horizontal array of such units.

""LSTM layer"" is probably more explicit, example (http://deeplearning.net/tutorial/code/lstm.py):
def lstm_layer(tparams, state_below, options, prefix='lstm', mask=None):
    nsteps = state_below.shape[0]
    if state_below.ndim == 3:
        n_samples = state_below.shape[1]
    else:
        n_samples = 1

    assert mask is not None
    [â¦]

","**Summary:**

The terminology in RNNs can be inconsistent. In TensorFlow, ""num_units"" refers to the dimension of hidden states ($h_t$), while in the literature, ""cell"" typically refers to a single scalar output.

The term ""LSTM layer"" is more precise and refers to an array of LSTM cells. An example from the literature shows how to define an LSTM layer in TensorFlow, including the number of time steps, input dimension, and output dimension."
Multivariate linear regression vs neural network?,https://stats.stackexchange.com/questions/41289/multivariate-linear-regression-vs-neural-network,regression|multiple-regression|neural-networks,41289,True,41290,42486,3,56,1461229848,1351325183,31,1351329963,"Neural networks can in principle model nonlinearities automatically (see the universal approximation theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem)), which you would need to explicitly model using transformations (splines etc.) in linear regression.
The caveat: the temptation to overfit can be (even) stronger in neural networks than in regression, since adding hidden layers or neurons looks harmless. So be extra careful to look at out-of-sample prediction performance.
","**Summary:**

Neural networks have the advantage of automatically capturing nonlinearities, eliminating the need for manual modeling using transformations. However, this capability also introduces the challenge of overfitting due to the easy addition of hidden layers and neurons.

To mitigate overfitting, it is crucial to focus on out-of-sample prediction performance. This involves evaluating a neural network's ability to predict on unseen data and avoiding excessive reliance on training data performance.

In essence, neural networks offer a powerful tool for modeling complex relationships, but their use requires careful attention to preventing overfitting and ensuring robust prediction performance in practice."
How large should the batch size be for stochastic gradient descent?,https://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent,machine-learning|neural-networks|gradient-descent|backpropagation,140811,True,141265,94456,1,55,1498935327,1425763116,79,1426056940,"The ""sample size"" you're talking about is referred to as batch size, $B$. The batch size parameter is just one of the hyper-parameters you'll be tuning when you train a neural network with mini-batch Stochastic Gradient Descent (SGD) and is data dependent. The most basic method of hyper-parameter search is to do a grid search over the learning rate and batch size to find a pair which makes the network converge.
To understand what the batch size should be, it's important to see the relationship between batch gradient descent, online SGD, and mini-batch SGD. Here's the general formula for the weight update step in mini-batch SGD, which is a generalization of all three types. [2 (http://arxiv.org/abs/1206.5533)]
$$
\theta_{t+1} \leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}
$$

Batch gradient descent, $B = |x|$
Online stochastic gradient descent: $B = 1$
Mini-batch stochastic gradient descent: $B > 1$ but $B < |x|$.

Note that with 1, the loss function is no longer a random variable and is not a stochastic approximation.
SGD converges faster than normal ""batch"" gradient descent because it updates the weights after looking at a randomly selected subset of the training set. Let $x$ be our training set and let $m \subset x$. The batch size $B$ is just the cardinality of $m$: $B = |m|$.
Batch gradient descent updates the weights $\theta$ using the gradients of the entire dataset $x$; whereas SGD updates the weights using an average of the gradients for a mini-batch $m$. (Using the average as opposed to a sum prevents the algorithm from taking steps that are too large if the dataset is very large. Otherwise, you would need to adjust your learning rate based on the size of the dataset.) The expected value of this stochastic approximation of the gradient used in SGD is equal to the deterministic gradient used in batch gradient descent. $\mathbb{E}[\nabla \mathcal{L}_{SGD}(\theta, \textbf{m})] = \nabla \mathcal{L}(\theta, \textbf{x})$.
Each time we take a sample and update our weights it is called a mini-batch. Each time we run through the entire dataset, it's called an epoch.
Let's say that we have some data vector $\textbf{x} : \mathbb{R}^D$, an initial weight vector that parameterizes our neural network, $\theta_0 : \mathbb{R}^{S}$, and a loss function $\mathcal{L}(\theta, \textbf{x}) : \mathbb{R}^{S} \rightarrow \mathbb{R}^{D} \rightarrow \mathbb{R}^S$ that we are trying to minimize. If we have $T$ training examples and a batch size of $B$, then we can split those training examples into C mini-batches:
$$
C = \lceil T / B \rceil
$$
For simplicity we can assume that T is evenly divisible by B. Although, when this is not the case, as it often is not, proper weight should be assigned to each mini-batch as a function of its size.
An iterative algorithm for SGD with $M$ epochs is given below:
\begin{align*}
t &\leftarrow 0 \\
\textrm{while } t &< M \\
\theta_{t+1} &\leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}  \\
t &\leftarrow t + 1
\end{align*}
Note: in real life we're reading these training example data from memory and, due to cache pre-fetching and other memory tricks done by your computer, your algorithm will run faster if the memory accesses are coalesced, i.e. when you read the memory in order and don't jump around randomly. So, most SGD implementations shuffle the dataset and then load the examples into memory in the order that they'll be read.
The major parameters for the vanilla (no momentum) SGD described above are:

Learning Rate: $\epsilon$

I like to think of epsilon as a function from the epoch count to a learning rate. This function is called the learning rate schedule.
$$
    \epsilon(t) : \mathbb{N} \rightarrow \mathbb{R}
$$
If you want to have the learning rate fixed, just define epsilon as a constant function.

Batch Size

Batch size determines how many examples you look at before making a weight update. The lower it is, the noisier the training signal is going to be, the higher it is, the longer it will take to compute the gradient for each step.
Citations & Further Reading:

Introduction to Gradient Based Learning (http://www-labs.iro.umontreal.ca/~bengioy/ift6266/H12/html/gradient_en.html)
Practical recommendations for gradient-based training of deep architectures (http://arxiv.org/abs/1206.5533)
Efficient Mini-batch Training for Stochastic Optimization (http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf)

","**Summary:**

When training neural networks using Stochastic Gradient Descent (SGD), the ""sample size"" refers to the batch size, $B$. The batch size determines the number of training examples considered before updating the network weights.

SGD involves splitting the training data into mini-batches of size $B$. The weight update formula uses the average gradient over the batch, as opposed to the entire dataset. This makes SGD faster than ""batch"" gradient descent, which uses the full dataset for weight updates.

The batch size is a hyper-parameter that influences training performance. Finding an optimal batch size typically involves grid search over different values of $B$ and the learning rate.

SGD can be categorized into three types based on batch size:

* Batch gradient descent: $B = |x|$, using the entire dataset
* Online stochastic gradient descent: $B = 1$, updating weights for each example
* Mini-batch stochastic gradient descent: $B > 1$ but $B < |x|$

The expected value of the SGD gradient approximation is equal to the true gradient, ensuring convergence to a minimum.

Typical SGD training involves multiple epochs (iterations through the entire dataset) and updates weights after each mini-batch. Common SGD parameters include the learning rate, which determines the magnitude of weight updates, and the batch size."
What is the derivative of the ReLU activation function?,https://stats.stackexchange.com/questions/333394/what-is-the-derivative-of-the-relu-activation-function,self-study|neural-networks,333394,True,333400,120557,1,54,1521134938,1521016323,63,1521018898,"The derivative is:
$$ f(x)=
\begin{cases} 
0 & \text{if  }  x < 0 \\
1 & \text{if  }  x > 0 \\
\end{cases}
$$
And undefined in $x=0$.
The reason for it being undefined at $x=0$ is that its left- and right derivative (https://en.wikipedia.org/wiki/Semi-differentiability) are not equal.
","**Summary:**

The derivative of the given piecewise-defined function is defined as follows:

* For all negative values of x, the derivative is 0.
* For all positive values of x, the derivative is 1.
* At x = 0, the derivative is undefined.

The undefined nature of the derivative at x = 0 stems from the fact that the left-hand and right-hand derivatives do not converge. The left-hand derivative is 0, while the right-hand derivative is 1. This discontinuity in the derivative results in the function being non-differentiable at x = 0."
Dice-coefficient loss function vs cross-entropy,https://stats.stackexchange.com/questions/321460/dice-coefficient-loss-function-vs-cross-entropy,neural-networks|loss-functions|cross-entropy,321460,True,344244,75956,3,53,1629643417,1515035521,53,1525361752,"One compelling reason for using cross-entropy over dice-coefficient or the similar IoU metric is that the gradients are nicer.
The gradients of cross-entropy wrt the logits is something like $p - t$, where $p$ is the softmax outputs and $t$ is the target. Meanwhile, if we try to write the dice coefficient in a differentiable form: $\frac{2pt}{p^2+t^2}$ or $\frac{2pt}{p+t}$, then the resulting gradients wrt $p$ are much uglier: $\frac{2t(t^2-p^2)}{(p^2+t^2)^2}$ and $\frac{2t^2}{(p+t)^2}$. It's easy to imagine a case where both $p$ and $t$ are small, and the gradient blows up to some huge value. In general, it seems likely that training will become more unstable.

The main reason that people try to use dice coefficient or IoU directly is that the actual goal is maximization of those metrics, and cross-entropy is just a proxy which is easier to maximize using backpropagation. In addition, Dice coefficient performs better at class imbalanced problems by design: 
However, class imbalance is typically taken care of simply by assigning loss multipliers to each class, such that the network is highly disincentivized to simply ignore a class which appears infrequently, so it's unclear that Dice coefficient is really necessary in these cases. 

I would start with cross-entropy loss, which seems to be the standard loss for training segmentation networks, unless there was a really compelling reason to use Dice coefficient.
","**Summary:**

Cross-entropy loss is preferred over dice coefficient or IoU for training segmentation networks due to its smoother gradients. The gradients of cross-entropy are linear, making training more stable. In contrast, the gradients of dice coefficient are non-linear and can become unstable when both target and output probabilities are low.

While dice coefficient aligns with the actual goal of maximizing overlap metrics, cross-entropy serves as an effective proxy that is easier to optimize. Additionally, class imbalance can be addressed by adjusting loss multipliers, making dice coefficient less necessary in such cases.

Therefore, cross-entropy loss is generally recommended as the preferred option unless there are specific reasons to use dice coefficient."
How does the Adam method of stochastic gradient descent work?,https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work,neural-networks|optimization|gradient-descent|adam,220494,True,220563,39803,1,52,1533259432,1466783107,50,1466822956,"The Adam paper says, ""...many objective functions are composed of a sum of subfunctions evaluated at different subsamples of data; in this case optimization can be made more efficient by taking gradient steps w.r.t.  individual subfunctions..."" Here, they just mean that the objective function is a sum of errors over training examples, and training can be done on individual examples or minibatches. This is the same as in stochastic gradient descent (SGD), which is more efficient for large scale problems than batch training because parameter updates are more frequent.
As for why Adam works, it uses a few tricks.
One of these tricks is momentum, which can give faster convergence. Imagine an objective function that's shaped like a long, narrow canyon that gradually slopes toward a minimum. Say we want to minimize this function using gradient descent. If we start from some point on the canyon wall, the negative gradient will point in the direction of steepest descent, i.e. mostly toward the canyon floor. This is because the canyon walls are much steeper than the gradual slope of the canyon toward the minimum. If the learning rate (i.e. step size) is small, we could descend to the canyon floor, then follow it toward the minimum. But, progress would be slow. We could increase the learning rate, but this wouldn't change the direction of the steps. In this case, we'd overshoot the canyon floor and end up on the opposite wall. We would then repeat this pattern, oscillating from wall to wall while making slow progress toward the minimum. Momentum can help in this situation.
Momentum simply means that some fraction of the previous update is added to the current update, so that repeated updates in a particular direction compound; we build up momentum, moving faster and faster in that direction. In the case of the canyon, we'd build up momentum in the direction of the minimum, since all updates have a component in that direction. In contrast, moving back and forth across the canyon walls involves constantly reversing direction, so momentum would help to damp the oscillations in those directions.
Another trick that Adam uses is to adaptively select a separate learning rate for each parameter. Parameters that would ordinarily receive smaller or less frequent updates receive larger updates with Adam (the reverse is also true). This speeds learning in cases where the appropriate learning rates vary across parameters. For example, in deep networks, gradients can become small at early layers, and it make sense to increase learning rates for the corresponding parameters. Another benefit to this approach is that, because learning rates are adjusted automatically, manual tuning becomes less important. Standard SGD requires careful tuning (and possibly online adjustment) of learning rates, but this less true with Adam and related methods. It's still necessary to select hyperparameters, but performance is less sensitive to them than to SGD learning rates.
Related methods:
Momentum is often used with standard SGD. An improved version is called Nesterov momentum or Nesterov accelerated gradient. Other methods that use automatically tuned learning rates for each parameter include: Adagrad, RMSprop, and Adadelta. RMSprop and Adadelta solve a problem with Adagrad that could cause learning to stop. Adam is similar to RMSprop with momentum. Nadam modifies Adam to use Nesterov momentum instead of classical momentum.
References:
Kingma and Ba (2014) (https://arxiv.org/abs/1412.6980). Adam: A Method for Stochastic Optimization.
Goodfellow et  al. (2016) (http://www.deeplearningbook.org/contents/optimization.html). Deep learning, chapter 8.
Slides (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) from Geoff Hinton's course
Dozat (2016) (https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ). Incorporating Nesterov Momentum into Adam.
","**Summary:**

The Adam optimization algorithm efficiently trains machine learning models by optimizing a series of subfunctions. It employs several techniques to achieve this efficiency:

* **Momentum:** Adam uses momentum to accelerate convergence by accumulating previous gradients, allowing it to move more quickly towards the minimum in complex landscapes.
* **Adaptive Learning Rates:** Adam dynamically adjusts the learning rates for individual parameters, boosting learning in cases where different parameters require different rates. This reduces the need for manual tuning.

Adam is similar to other optimization methods like Adagrad, RMSprop, and Adadelta, which also use adaptive learning rates. However, Adam incorporates momentum, making it a more effective choice in many situations. Nadam is a variant of Adam that uses Nesterov momentum for further acceleration.

Overall, Adam is an efficient optimization algorithm that can handle large-scale problems and automatically adjusts learning rates, making it a popular choice for training deep neural networks."
Difference between GradientDescentOptimizer and AdamOptimizer (TensorFlow)?,https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow,machine-learning|neural-networks|error|gradient-descent|supervised-learning,184448,True,184497,63787,1,50,1467878619,1448977698,85,1448994447,"The tf.train.AdamOptimizer (http://www.tensorflow.org/api_docs/python/train.html#AdamOptimizer) uses Kingma and Ba's Adam algorithm (http://arxiv.org/pdf/1412.6980v8.pdf) to control the learning rate. Adam offers several advantages over the simple tf.train.GradientDescentOptimizer. Foremost is that it uses moving averages of the parameters (momentum); Bengio discusses the reasons for why this is beneficial in Section 3.1.1 of this paper (http://arxiv.org/pdf/1206.5533.pdf). Simply put, this enables Adam to use a larger effective step size, and the algorithm will converge to this step size without fine tuning.
The main down side of the algorithm is that Adam requires more computation to be performed for each parameter in each training step (to maintain the moving averages and variance, and calculate the scaled gradient); and more state to be retained for each parameter (approximately tripling the size of the model to store the average and variance for each parameter). A simple tf.train.GradientDescentOptimizer could equally be used in your MLP, but would require more hyperparameter tuning before it would converge as quickly.
","**Summary:**

TensorFlow's AdamOptimizer employs the Adam algorithm, which provides advantages over the GradientDescentOptimizer. Adam uses moving averages (momentum) for parameter updates, allowing for larger effective step sizes and automatic convergence.

However, Adam has computational and memory drawbacks: it requires more computations for moving averages and variance calculations, and it triples the model size for storing averages and variances. While it offers faster convergence, GradientDescentOptimizer may require more manual tuning for similar performance."
Importance of local response normalization in CNN,https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn,deep-learning|convolution|convolutional-neural-network,145768,True,191203,47973,7,50,1661008030,1428710705,25,1453120120,"It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods. This is what is written in the lecture notes for the Stanford Course CS321n (http://cs231n.github.io/convolutional-networks/) on ConvNets: 

Normalization Layer
Many types of normalization layers have been proposed for use in
  ConvNet architectures, sometimes with the intentions of implementing
  inhibition schemes observed in the biological brain. However, these
  layers have recently fallen out of favor because in practice their
  contribution has been shown to be minimal, if any. For various types
  of normalizations, see the discussion in Alex Krizhevsky's
  cuda-convnet library API.

","Normalization layers were once popular for ConvNets, but their impact has been diminished. This is due to the emergence of more effective regularization techniques such as dropout and batch normalization, as well as improved initialization methods and training algorithms. As a result, these normalization layers are now rarely used."
How are kernels applied to feature maps to produce other feature maps?,https://stats.stackexchange.com/questions/85767/how-are-kernels-applied-to-feature-maps-to-produce-other-feature-maps,machine-learning|neural-networks|deep-learning|convolutional-neural-network,85767,True,86879,26286,4,50,1538069749,1391785265,21,1392654106,"The kernels are 3-dimensional, where width and height can be chosen, while the depth is equal to the number of maps in the input layer - in general. 
They are certainly not 2-dimensional and replicated across the input feature maps at the same 2D location! That would mean a kernel wouldn't be able to distinguish between its input features at a given location, since it would use one and the same weight across the input feature maps!
","**Summary:**

Convolutional neural networks (CNNs) employ kernels that are three-dimensional. The width and height of the kernel are adjustable, while the depth corresponds to the number of feature maps in the input layer.

Unlike traditional 2D kernels, which apply the same weights across all input feature maps, CNN kernels can distinguish between features at specific locations. This is because each location within the kernel has a unique weight for each input feature map. This allows kernels to capture spatial relationships between features and learn more complex patterns."
How to set up neural network to output ordinal data?,https://stats.stackexchange.com/questions/140061/how-to-set-up-neural-network-to-output-ordinal-data,neural-networks|ordinal-data|softmax,140061,True,494965,23038,2,49,1716202430,1425347148,17,1604444248,"I think the approach to only encode the ordinal labels as

class 1 is represented as [0 0 0 0 ...]

class 2 is represented as [1 0 0 0 ...]

class 3 is represented as [1 1 0 0 ...]


and use binary cross-entropy as the loss function is suboptimal. As mentioned in the comments, it might happen that the predicted vector is for example [1 0 1 0 ...]. This is undesirable for making predictions.
The paper Rank-consistent ordinal regression for neural networks (https://arxiv.org/pdf/1901.07884.pdf) describes how to restrict the neural network to make rank-consistent predictions. You have to make sure that the last layer shares its weights, but should have different biases. You can implement this in Tensorflow by adding the following as the last part of the network (credits for https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers (https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers)):
class BiasLayer(tf.keras.layers.Layer):
    def __init__(self, units, *args, **kwargs):
        super(BiasLayer, self).__init__(*args, **kwargs)
        self.bias = self.add_weight('bias',
                                    shape=[units],
                                    initializer='zeros',
                                    trainable=True)

    def call(self, x):
        return x + self.bias


# Add the following as the output of the Sequential model
model.add(keras.layers.Dense(1, use_bias=False))
model.add(BiasLayer(4))
model.add(keras.layers.Activation(""sigmoid""))

Note that the number of ordinal classes here is 5, hence the $K-1$ biases.
I tested the difference in performance on actual data, and the predictive accuracy improved substantially. Hope this helps.
","**Summary:**

Encoding ordinal labels (class levels) as simple binary vectors (e.g., [0 0 0 1] for class 4) and using binary cross-entropy loss is inefficient. This approach can result in undesirable predictions (e.g., [1 0 1 0]), where the predicted label is not consistent with the order of classes.

To address this issue, ""rank-consistent ordinal regression"" was proposed in the ""Rank-consistent Ordinal Regression for Neural Networks"" paper. This technique restricts the neural network to make rank-consistent predictions.

To implement this in TensorFlow, add a custom layer called ""BiasLayer"" as the last layer of the network, with shared weights but different biases for each ordinal class. This layer adds biases to the output of the network, ensuring that the predicted labels maintain the correct class order (e.g., class 1 < class 2 < class 3).

The number of biases in the BiasLayer should be equal to the number of ordinal classes minus one (K-1)."
Why are non zero-centered activation functions a problem in backpropagation?,https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation,neural-networks|deep-learning|backpropagation,237169,True,237282,33337,1,48,1641700377,1474985560,60,1475032111,"$$f=\sum w_ix_i+b$$ $$\frac{df}{dw_i}=x_i$$ $$\frac{dL}{dw_i}=\frac{dL}{df}\frac{df}{dw_i}=\frac{dL}{df}x_i$$
because $x_i>0$, the gradient $\dfrac{dL}{dw_i}$ always has the same sign as $\dfrac{dL}{df}$ (all positive or all negative).
Update
Say there are two parameters $w_1$ and $w_2$. If the gradients of two dimensions are always of the same sign (i.e., either both are positive or both are negative), it means we can only move roughly in the direction of northeast or southwest in the parameter space.
If our goal happens to be in the northwest, we can only move in a zig-zagging fashion to get there, just like parallel parking in a narrow space. (forgive my drawing)
 (https://i.sstatic.net/eWcLR.png)
Therefore all-positive or all-negative activation functions (relu, sigmoid) can be difficult for gradient based optimization. To solve this problem we can normalize the data in advance  to be zero-centered as in batch/layer normalization.
Also another solution I can think of is to add a bias term for each input so the layer becomes
$$f=\sum w_i(x_i+b_i).$$
The gradients is then
$$\frac{dL}{dw_i}=\frac{dL}{df}(x_i-b_i)$$
the sign won't solely depend on $x_i$.
","**Summary:**

The gradient descent algorithm updates parameters (weights) of a linear model to optimize a loss function. The gradient, which indicates the direction of steepest loss decrease, is determined by the partial derivatives of the loss function with respect to the weights.

In the case of linear functions, the gradient for each weight is proportional to the corresponding input feature. If all input features have the same sign (positive or negative), the gradient will also have the same sign. This means the optimization algorithm can only move in particular directions in the parameter space.

This can hinder optimization if the optimal solution is in a different direction. To address this, data normalization or adding bias terms to the input features can be employed. Normalization centers the data around zero, reducing the impact of large positive or negative inputs on the gradient. Bias terms introduce additional flexibility in the gradient calculation, allowing optimization to proceed in more desirable directions."
Neural Networks: weight change momentum and weight decay,https://stats.stackexchange.com/questions/70101/neural-networks-weight-change-momentum-and-weight-decay,neural-networks|optimization|regularization|gradient-descent,70101,True,70146,42476,1,47,1589313650,1379296588,51,1379344580,"Yes, it's very common to use both tricks.  They solve different problems and can work well together.
One way to think about it is that weight decay changes the function that's being optimized, while momentum changes the path you take to the optimum.
Weight decay, by shrinking your coefficients toward zero, ensures that you find a local optimum with small-magnitude parameters.  This is usually crucial for avoiding overfitting (although other kinds of constraints on the weights can work too).  As a side benefit, it can also make the model easier to optimize, by making the objective function more convex.
Once you have an objective function, you have to decide how to move around on it.  Steepest descent on the gradient is the simplest approach, but you're right that fluctuations can be a big problem. Adding momentum helps solve that problem.  If you're working with batch updates (which is usually a bad idea with neural networks) Newton-type steps are another option. The new ""hot"" approaches are based on Nesterov's accelerated gradient and so-called ""Hessian-Free"" optimization.
But regardless of which of these update rules you use (momentum, Newton, etc.), you're still working with the same objective function, which is determined by your error function (e.g. squared error) and other constraints (e.g. weight decay).  The main question when deciding which of these to use is how quickly you'll get to a good set of weights.
","**Summary:**

Weight decay and momentum are two common optimization techniques used to improve the training of machine learning models.

**Weight Decay:**

* Adjusts the objective function to penalize large coefficients (weights).
* Ensures small-magnitude weights, reducing overfitting and improving model optimization.

**Momentum:**

* Modifies the optimization path by adding a moving average of previous gradients.
* Reduces fluctuations and speeds up convergence to a local optimum.

Both techniques solve different problems and can be combined effectively. Weight decay shapes the objective function, while momentum influences the optimization trajectory. The choice of update rule depends on the desired convergence speed and can include momentum, Newton-type steps, or accelerated gradient methods."
What are the differences between hidden Markov models and neural networks?,https://stats.stackexchange.com/questions/20429/what-are-the-differences-between-hidden-markov-models-and-neural-networks,data-mining|algorithms|neural-networks|markov-process,20429,True,25999,51331,3,46,1526768767,1325365415,36,1333749546,"What is hidden and what is observed
The thing that is hidden in a hidden Markov model is the same as the thing that is hidden in a discrete mixture model, so for clarity, forget about the hidden state's dynamics and stick with a finite mixture model as an example.  The 'state' in this model is the identity of the component that caused each observation.  In this class of model such causes are never observed, so 'hidden cause' is translated statistically into the claim that the observed data have marginal dependencies which are removed when the source component is known.  And the source components are estimated to be whatever makes this statistical relationship true.
The thing that is hidden in a feedforward multilayer neural network with sigmoid middle units is the states of those units, not the outputs which are the target of inference.  When the output of the network is a classification, i.e., a probability distribution over possible output categories, these hidden units values define a space within which categories are separable.  The trick in learning such a model is to make a hidden space (by adjusting the mapping out of the input units) within which the problem is linear.  Consequently, non-linear decision boundaries are possible from the system as a whole.
Generative versus discriminative
The mixture model (and HMM) is a model of the data generating process, sometimes called a likelihood or 'forward model'.  When coupled with some assumptions about the prior probabilities of each state you can infer a distribution over possible values of the hidden state using Bayes theorem (a generative approach).  Note that, while called a 'prior', both the prior and the parameters in the likelihood are usually learned from data.
In contrast to the mixture model (and HMM) the neural network learns a posterior distribution over the output categories directly (a discriminative approach).  This is possible because the output values were observed during estimation.  And since they were observed, it is not necessary to construct a posterior distribution from a prior and a specific model for the likelihood such as a mixture.  The posterior is learnt directly from data, which is more efficient and less model dependent. 
Mix and match
To make things more confusing, these approaches can be mixed together, e.g. when mixture model (or HMM) state is sometimes actually observed.  When that is true, and in some other circumstances not relevant here, it is possible to train discriminatively in an otherwise generative model.  Similarly it is possible to replace the mixture model mapping of an HMM with a more flexible forward model, e.g., a neural network.  
The questions
So it's not quite true that both models predict hidden state.  HMMs can be used to predict hidden state, albeit only of the kind that the forward model is expecting.  Neural networks can be used to predict a not yet observed state, e.g. future states for which predictors are available.  This sort of state is not hidden in principle, it just hasn't been observed yet.
When would you use one rather than the other?  Well, neural networks make rather awkward time series models in my experience.  They also assume you have observed output.  HMMs don't but you don't really have any control of what the hidden state actually is.  Nevertheless they are proper time series models.
","**Hidden and Observed in Statistical Models**

In hidden Markov models and discrete mixture models, the hidden component is the underlying cause of observed data, which has marginal dependencies removed when the cause is known. Neural networks, on the other hand, have hidden states that define a space within which output categories are separable.

**Generative vs. Discriminative Models**

Mixture models and HMMs are generative, modeling the data-generating process and inferring hidden state distributions using Bayes' theorem. Neural networks are discriminative, learning posterior distributions over output categories directly without relying on a specific likelihood model.

**Mixing Approaches**

These approaches can be combined, with mixture models sometimes having observed states, allowing for discriminative training. Neural networks can also replace the forward model in HMMs for increased flexibility.

**Use Cases**

HMMs excel as proper time series models despite not providing control over hidden states, while neural networks are better suited for predicting unobserved states when output values are available."
Softmax layer in a neural network,https://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network,neural-networks,79454,True,92309,65997,3,45,1512816502,1386853020,45,1396474114,"I feel a little bit bad about providing my own answer for this because it is pretty well captured by amoeba and juampa, except for maybe the final intuition about how the Jacobian (http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)  can be reduced back to a vector.
You correctly derived the gradient of the diagonal of the Jacobian matrix, which is to say that 
$ {\partial h_i \over \partial z_j}= h_i(1-h_j)\;\;\;\;\;\;: i = j $
and as amoeba stated it, you also have to derive the off diagonal entries of the Jacobian, which yield
$ {\partial h_i \over \partial z_j}= -h_ih_j\;\;\;\;\;\;: i \ne j $
These two concepts definitions can be conveniently combined using a construct called the Kronecker Delta (http://en.wikipedia.org/wiki/Kronecker_delta), so the definition of the gradient becomes
$ {\partial h_i \over \partial z_j}= h_i(\delta_{ij}-h_j) $
So the Jacobian is a square matrix $ \left[J \right]_{ij}=h_i(\delta_{ij}-h_j) $
All of the information up to this point is already covered by amoeba and juampa. The problem is of course, that we need to get the input errors from the output errors that are already computed. Since the gradient of the output error $\nabla h_i$ depends on all of the inputs, then the gradient of the input $x_i$ is
$[\nabla x]_k = \sum\limits_{i=1} \nabla h_{i,k}  $
Given the Jacobian matrix defined above, this is implemented trivially as the product of the matrix and the output error vector:
$ \vec{\sigma_l} = J\vec{\sigma_{l+1}} $
If the softmax layer is your output layer, then combining it with the cross-entropy cost model simplifies the computation to simply
$ \vec{\sigma_l} = \vec{h}-\vec{t} $
where $\vec{t}$ is the vector of labels, and $\vec{h}$ is the output from the softmax function. Not only is the simplified form convenient, it is also extremely useful from a numerical stability standpoint.
","**Summary:**

Computing the gradient for a neural network involves deriving the Jacobian matrix, which captures the relationship between changes in the input and output layer errors. The Jacobian's diagonal elements represent the gradient of the diagonal of the matrix, while the off-diagonal elements represent the gradient of the off-diagonal entries. These elements can be combined using the Kronecker Delta function, resulting in the following definition for the gradient:

```
âh_i / âz_j = h_i(Î´_{ij} - h_j)
```

To obtain the input errors from the output errors, the gradient of the output error (âh_i) is computed for each input dimension. These gradients are then summed and multiplied by the Jacobian matrix (J) to obtain the gradient of the input (âx).

In the case of a softmax output layer and cross-entropy cost model, the calculation simplifies to:

```
Ï_l = h - t
```

where Ï_l is the gradient of the logarithmic likelihood error, h is the output of the softmax function, and t is the vector of labels. This simplified form is convenient and numerically stable for neural network training."
Training loss increases with time,https://stats.stackexchange.com/questions/324896/training-loss-increases-with-time,machine-learning|neural-networks|loss-functions|recurrent-neural-network|training-error,324896,True,325255,117095,3,45,1573140634,1516822787,19,1517002089,"I had such a similar behavior when training a CNN, it was because I used the gradient descent with decaying learning rate for the error calculation. Have you significantly increased the number of iterations and checked if this behavior comes much later with the new low learning rate?
","During CNN training, the training process exhibited a similar behavior, attributed to the use of gradient descent with a decaying learning rate for error calculation. It is recommended to significantly increase the number of iterations and observe if the behavior persists at a lower learning rate. This approach helps determine if the issue is solely due to the low learning rate or other factors in the training process. Further exploration is necessary to identify the underlying cause and implement appropriate adjustments to optimize the training."
Why is max pooling necessary in convolutional neural networks?,https://stats.stackexchange.com/questions/288261/why-is-max-pooling-necessary-in-convolutional-neural-networks,deep-learning|convolutional-neural-network|pooling,288261,True,288445,47723,4,44,1651610822,1498872952,38,1499007908,"You can indeed do that, see Striving for Simplicity: The All Convolutional Net (https://arxiv.org/abs/1412.6806). Pooling gives you some amount of translation invariance, which may or may not be helpful. Also, pooling is faster to compute than convolutions. Still, you can always try replacing pooling by convolution with stride and see what works better. 
Some current works use average pooling (Wide Residual Networks (https://arxiv.org/abs/1605.07146), DenseNets (https://arxiv.org/abs/1608.06993)), others use convolution with stride (DelugeNets (https://arxiv.org/abs/1611.05552))
","**Summary:**

Pooling layers in convolutional neural networks (CNNs) offer some translation invariance and computational efficiency. However, they can be replaced by convolutions with stride, which may yield superior results.

Some recent CNN architectures, such as Wide Residual Networks and DenseNets, employ average pooling. Others, like DelugeNets, utilize convolutions with stride. The optimal choice between pooling and convolutions can vary depending on the network architecture and task.

Overall, the use of convolutions with stride can be a viable alternative to pooling layers in CNNs, offering the potential for enhanced performance."
What exactly is the difference between a parametric and non-parametric model?,https://stats.stackexchange.com/questions/268638/what-exactly-is-the-difference-between-a-parametric-and-non-parametric-model,machine-learning|neural-networks|terminology|nonparametric,268638,True,268646,70279,4,44,1623694514,1490018086,39,1490020547,"In a parametric model, the number of parameters is fixed with respect to the sample size.  In a nonparametric model, the (effective) number of parameters can grow with the sample size.  
In an OLS regression, the number of parameters will always be the length of $\beta$, plus one for the variance.  
A neural net with fixed architecture and no weight decay would be a parametric model.  
But if you have weight decay, then the value of the decay parameter selected by cross-validation will generally get smaller with more data.  This can be interpreted as an increase in the effective number of parameters with increasing sample size.
","**Summary**

Statistical models can be categorized as parametric or nonparametric based on the behavior of their parameters with increasing sample size:

* **Parametric models:** Have a fixed number of parameters (e.g., OLS regression with a fixed number of coefficients).

* **Nonparametric models:** Have an effective number of parameters that can increase with sample size (e.g., neural nets with weight decay).

In an OLS regression, the number of parameters is determined by the number of coefficients plus one for the variance. Neural nets without weight decay are parametric, but weight decay introduces an effective parameter that decreases with increasing sample size, effectively increasing the model's flexibility.

The selection of model type depends on the nature of the data and the desired level of flexibility. Parametric models provide simpler inference and interpretation, while nonparametric models can better capture complex relationships with potentially large datasets."
Training loss goes down and up again. What is happening?,https://stats.stackexchange.com/questions/201129/training-loss-goes-down-and-up-again-what-is-happening,machine-learning|neural-networks|loss-functions|lstm,201129,True,206238,112112,1,44,1497128812,1457691522,31,1460118041,"Your learning rate could be to big after the 25th epoch. This problem is easy to identify. You just need to set up a smaller value for your learning rate. If the problem related to your learning rate than NN should reach a lower error despite that it will go up again after a while. The main point is that the error rate will be lower in some point in time.
If you observed this behaviour you could use two simple solutions. First one is a simplest one. Set up a very small step and train it. The second one is to decrease your learning rate monotonically. Here is a simple formula:
$$
\alpha(t + 1) = \frac{\alpha(0)}{1 + \frac{t}{m}}
$$
Where $a$ is your learning rate, $t$ is your iteration number and $m$ is a coefficient that identifies learning rate decreasing speed. It means that your step will minimise by a factor of two when $t$ is equal to $m$.
","**Summary:**

Overfitting can occur when the learning rate is too high. To identify this, monitor the error rate. If it initially decreases but then increases, the learning rate may be too high.

Two solutions to address this issue are:

1. **Set a smaller learning rate:** Use a very small learning rate and train the model.

2. **Monotonically decrease the learning rate:** Use the formula:
    ```
    Î±(t + 1) = Î±(0) / (1 + t/m)
    ```
    where Î± is the learning rate, t is the iteration number, and m is a coefficient that controls the speed of decrease. This formula gradually reduces the learning rate, which helps prevent overfitting."
What are the advantages of stacking multiple LSTMs?,https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms,classification|neural-networks|deep-learning|lstm|recurrent-neural-network,163304,True,179817,43383,4,43,1583844632,1437962278,28,1446479118,"I think that you are referring to vertically stacked LSTM layers (assuming the horizontal axes is the time axis. 
In that case the main reason for stacking LSTM is to allow for greater model complexity. In case of a simple feedforward net we stack layers to create a hierarchical feature representation of the input data to then use for some machine learning task. The same applies for stacked LSTM's. 
At every time step an LSTM, besides the recurrent input. If the input is already the result from an LSTM layer (or a feedforward layer) then the current LSTM can create a more complex feature representation of the current input. 
Now the difference between having a feedforward layer between the feature input and the LSTM layer and having another LSTM layer is that a feed forward layer (say a fully connected layer) does not receive feedback from its previous time step and thus can not account for certain patterns. Having an LSTM in stead (e.g. using a stacked LSTM representation) more complex input patterns can be described at every layer
","Stacking LSTM layers allows for increased model complexity. Similar to stacking layers in feedforward networks, stacked LSTM layers create a hierarchical feature representation of input data.

At each time step, an LSTM receives recurrent input. When the input is the output of a previous LSTM layer, the current LSTM can create a more complex feature representation.

Unlike feedforward layers, stacked LSTM layers can capture complex input patterns due to their feedback mechanism. This enables the model to learn from past time steps and account for longer-term dependencies."
Why is it that my colleagues and I learned opposite definitions for test and validation sets?,https://stats.stackexchange.com/questions/525697/why-is-it-that-my-colleagues-and-i-learned-opposite-definitions-for-test-and-val,machine-learning|neural-networks|cross-validation|terminology|validation,525697,True,525810,5223,3,42,1621967341,1621864755,25,1621930291,"For machine learning, I've predominantly seen the usage OP describes, but I've also encountered lots of confusion coming from this usage.

Historically, I guess what happened (at least in my field, analytical chemistry) is that as models became more complex, at some point people noticed that independent data is needed for verification and validation purposes (in our terminology, almost all testing that is routinely done with models would be considered part of verification which in turn is part of the much wider task of method validation). Enter the validation set and methods such as cross validation (with its original purpose of estimating generalization error).
Later, people started to use generalization error estimates from what we call internal verification/validation such as cross validation or a random split to refine/optimize their models. Enter hyperparameter tuning.
Again, it was realized that estimating generalization error of the refined model needs independent data. And a new name was needed as well, as the usage of ""validation set"" for the data used for refining/optimizing had already been established. Enter the test set.
Thus we have the situation where a so-called validation set is used for model development/optimization/refining and is therefore not suitable any more for the purpose of model verification and validation.

Someone with e.g. an analytical chemistry (or engineering) background will certainly refer to the data they use/acquire for method validation purposes as their validation data* - and that is correct usage of the terms in these fields.
*(unless they know the different use of terminology in machine learning, in which case they'd usually explain what exactly they are talking about).

Personally, in order to avoid the ongoing confusion that comes from this clash of terminology between fields, I've moved to using ""optimization data/set"" for the data used for hyperparameter tuning (Andrew Ng's development set is fine with me as well) and ""verification data/set"" for the final independent test data (the testing we typically do is actually verification rather than validation, so that avoids another common mistake: the testing we typically do is not even close to a full method validation in analytical chemistry, and it's good to be aware of that)
Another strategy I find helpful to avoid confusion is moving from splitting into 3 data sets back to splitting into training and verification data, and then describing the hyperparameter tuning as part of the training procedure which happens to include another split into data used to fit the model parameters and data used to optimize the hyperparameters.
","**Summary:**

In machine learning, there is a historical confusion in terminology regarding data used for model validation and optimization.

**Evolution of Terminology:**

1. **Validation Set:** Initially used for independent verification and validation purposes.
2. **Hyperparameter Tuning:** Generalization error estimates from internal verification datasets (e.g., cross-validation) were used to refine models. This led to the need for a separate dataset for final verification.
3. **Test Set:** Introduced as the independent dataset for final verification after model refinement.

**Clash of Terminology:**

The term ""validation set"" in machine learning now refers to the data used for optimization, while in other fields (e.g., analytical chemistry), it refers to independent verification data.

**Suggested Solutions:**

1. **Rename Optimization Data:** Use ""optimization data/set"" or ""development set"" to avoid confusion.
2. **Use Verification Data:** Refer to the final independent test data as ""verification data/set"" to clarify its purpose.
3. **Consolidate Data Split:** Consider splitting data into two sets: training and verification, with hyperparameter tuning as part of the training process involving an internal split for optimization."
Difference between feedback RNN and LSTM/GRU,https://stats.stackexchange.com/questions/222584/difference-between-feedback-rnn-and-lstm-gru,neural-networks|lstm|recurrent-neural-network|gru,222584,True,222587,100674,5,41,1633072823,1467896015,59,1467897431,"All RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies. GRUs are similar to LSTMs, but use a simplified structure. They also use a set of gates to control the flow of information, but they don't use separate memory cells, and they use fewer gates.
This paper gives a good overview:

Chung et al. (2014) (http://arxiv.org/abs/1412.3555).  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.

","**Main Ideas:**

* Recurrent neural networks (RNNs) excel at handling sequential data due to their feedback loops.
* Standard RNNs struggle with long-term temporal dependencies due to vanishing gradients.

**LSTM Networks:**

* Special RNN units that address the vanishing gradient problem.
* Include memory cells that retain information over long periods.
* Gates control information flow into, out of, and within the memory cell.

**GRU Networks:**

* Simplified version of LSTMs.
* Utilize gates to regulate information flow but without separate memory cells.
* Employ fewer gates than LSTMs.

**Conclusion:**

LSTM and GRU networks overcome the limitations of standard RNNs by effectively handling long-term dependencies. LSTM networks offer superior performance but greater complexity, while GRUs strike a balance between efficiency and effectiveness."
Why do we need to normalize the images before we put them into CNN?,https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn,deep-learning|convolutional-neural-network|image-processing,185853,True,185857,62913,1,41,1539676793,1449644046,55,1449645855,"First note: you really should be also dividing by the standard deviation of each feature (pixel) value as well. Subtracting the mean centers the input to 0, and dividing by the standard deviation makes any scaled feature value the number of standard deviations away from the mean. 
To answer your question: Consider how a neural network learns its weights. C(NN)s learn by continually adding gradient error vectors (multiplied by a learning rate) computed from backpropagation to various weight matrices throughout the network as training examples are passed through. 
The thing to notice here is the ""multiplied by a learning rate"". 
If we didn't scale our input training vectors, the ranges of our distributions of feature values would likely be different for each feature, and thus the learning rate would cause corrections in each dimension that would differ (proportionally speaking) from one another. We might be over compensating a correction in one weight dimension while undercompensating in another. 
This is non-ideal as we might find ourselves in a oscillating (unable to center onto a better maxima in cost(weights) space) state or in a slow moving (traveling too slow to get to a better maxima) state. 
It is of course possible to have a per-weight learning rate, but it's yet more hyperparameters to introduce into an already complicated network that we'd also have to optimize to find. Generally learning rates are scalars. 
Thus we try to normalize images before using them as input into NN (or any gradient based) algorithm. 
","**Summary:**

Data normalization, involving mean subtraction and division by standard deviation, enhances neural network performance by:

* **Uniformizing Feature Distributions:** Normalization scales different feature values to a common range, ensuring that each feature contributes equally to the gradient calculation.
* **Optimizing Gradient Corrections:** Without normalization, the learning rate can lead to disproportionate weight adjustments across dimensions, hindering convergence.
* **Preventing Oscillations and Slow Training:** Normalization stabilizes the training process by centering the input at zero and ensuring a consistent learning rate.
* **Simplifying Hyperparameter Optimization:** Normalization eliminates the need for per-weight learning rates, reducing the number of hyperparameters that need to be adjusted.

Overall, data normalization promotes efficient and effective learning by creating a consistent and well-behaved input for gradient-based algorithms like neural networks."
"What&#39;s the relation between hierarchical models, neural networks, graphical models, bayesian networks?",https://stats.stackexchange.com/questions/4498/whats-the-relation-between-hierarchical-models-neural-networks-graphical-mode,causality|neural-networks|multilevel-analysis|graphical-model,4498,True,4502,9873,3,41,1449584398,1289626995,34,1289637446,"A Bayesian network is a type of graphical model. The other ""big"" type of graphical model is a Markov Random Field (MRF).  Graphical models are used for inference, estimation and in general, to model the world.
The term hierarchical model is used to mean many things in different areas.
While neural networks come with ""graphs"" they generally don't encode dependence information, and the nodes don't represent random variables. NNs are different because they are discriminative. Popular neural networks are used for classification and regression.
Kevin Murphy has an excellent introduction to these topics available here (http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html).
","**Summary:**

Graphical models represent dependencies among variables by connecting nodes. The two main types are Bayesian networks (BNs) and Markov Random Fields (MRFs).

BNs use directed edges to model causal relationships, while MRFs use undirected edges to model pairwise dependencies.

Both BNs and MRFs are used for various tasks, including inference, estimation, and representing the world.

Hierarchical models can have different meanings depending on the context.

Neural networks also have graphs, but they generally do not encode dependencies or represent random variables. Instead, they are discriminative models used for classification and regression."
How do bottleneck architectures work in neural networks?,https://stats.stackexchange.com/questions/205150/how-do-bottleneck-architectures-work-in-neural-networks,residuals|deep-learning|convolutional-neural-network,205150,True,221466,45334,3,41,1621935154,1459624000,12,1467294310,"The bottleneck architecture is used in very deep networks due to computational considerations.
To answer your questions:

56x56 feature maps are not represented in the above image. This block is taken from a ResNet with input size 224x224. 56x56 is the downsampled version of the input at some intermediate layer.
64-d refers to the number of feature maps(filters). The bottleneck architecture has 256-d, simply because it is meant for much deeper network, which possibly take higher resolution image as input and hence require more feature maps.
Refer this figure (http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) for parameters of each bottleneck layer in ResNet 50.

","**Summary:**

The bottleneck architecture is employed in deep neural networks to reduce computational complexity. It features a narrow ""bottleneck"" layer between two wider layers, where the number of feature maps (filters) is significantly reduced.

The example given illustrates a bottleneck block with 64 feature maps at the downsampled resolution of 56x56. The original image input size was likely 224x224. Deeper networks typically require more feature maps (e.g., 256) due to higher input resolutions.

To visualize the parameters of each bottleneck layer in a specific network (e.g., ResNet 50), refer to the provided figure, which provides detailed information for each layer."
Is overfitting &quot;better&quot; than underfitting?,https://stats.stackexchange.com/questions/521835/is-overfitting-better-than-underfitting,machine-learning|neural-networks|overfitting|bias-variance-tradeoff,521835,True,521842,9861,9,40,1619799926,1619607816,67,1619611071,"Overfitting is likely to be worse than underfitting.  The reason is that there is no real upper limit to the degradation of generalisation performance that can result from over-fitting, whereas there is for underfitting.
Consider a non-linear regression model, such as a neural network or polynomial model.  Assume we have standardised the response variable.  A maximally underfitted solution might completely ignore the training set and have a constant output regardless of the input variables.  In this case the expected mean squared error on test data will be approximately the variance of the response variable in the training set.
Now consider an over-fitted model that exactly interpolates the training data.  To do so, this may require large excursions from the true conditional mean of the data generating process between points in the training set, for example the spurious peak at about x = -5.  If the first three training points were closer together on the x-axis, the peak would be likely to be even higher.  As a result, the test error for such points can be arbitrarily large, and hence the expected MSE on test data can similarly be arbitrarily large.
 (https://i.sstatic.net/hxmlA.png)
Source: https://en.wikipedia.org/wiki/Overfitting (https://en.wikipedia.org/wiki/Overfitting) (it is actually a polynomial model in this case, but see below for an MLP example)
Edit: As @Accumulation suggests, here is an example where the extent of overfitting is much greater (10 randomly selected data points from a linear model with Gaussian noise, fitted by a 10th order polynomial fitted to the utmost degree).  Happily the random number generator gave some points that were not very well spaced out first time!
 (https://i.sstatic.net/oJOq8.png)
It is worth making a distinction between ""overfitting"" and ""overparameterisation"".  Overparameterisation means you have used a model class that is more flexible than necessary to represent the underlying structure of the data, which normally implies a larger number of parameters.  ""Overfitting"" means that you have optimised the parameters of a model in a way that gives a better ""fit"" to the training sample (i.e. a better value of the training criterion), but to the detriment of generalisation performance.  You can have an over-parameterised model that does not overfit the data.  Unfortunately the two terms are often used interchangeably, perhaps because in earlier times the only real control of overfitting was achieved by limiting the number of parameters in the model (e.g. feature selection for linear regression models).  However regularisation (c.f. ridge regression) decouples overparameterisation from overfitting, but our use of the terminology has not reliably adapted to that change (even though ridge regression is almost as old as I am!).
Here is an example that was actually generated using an (overparameterised) MLP
 (https://i.sstatic.net/XINi2.png)
","**Summary of Overfitting and Underfitting**

Overfitting occurs when a model fits the training data too closely, resulting in poor performance on unseen data. Conversely, underfitting occurs when a model fails to capture the underlying patterns in the training data.

**Consequences of Overfitting and Underfitting**

Overfitting is generally more detrimental than underfitting because it can lead to arbitrarily large errors on test data. Underfitting, on the other hand, typically results in a constant error that is approximately equal to the variance of the response variable.

**Examples of Overfitting and Underfitting**

An overfitted model may perfectly interpolate the training data but fail to generalize to new data, resulting in large errors on test points outside the training set. An underfitted model may ignore the training data and produce a constant output regardless of the input, leading to a relatively low error but poor performance on real-world applications.

**Overfitting vs. Overparameterization**

Overfitting refers to optimizing a model's parameters to fit the training data too closely, while overparameterization means using a model with more parameters than necessary. It is possible to have an overparameterized model that does not overfit, as the parameters can be regularized to prevent excessive fitting."
What does kernel size mean?,https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean,machine-learning|neural-networks,296679,True,296701,101656,2,40,1586520801,1502131280,37,1502138617,"Deep neural networks, more concretely convolutional neural networks (CNN), are basically a stack of layers which are defined by the action of a number of filters on the input. Those filters are usually called kernels.
For example, the kernels in the convolutional layer, are the convolutional filters. Actually no convolution is performed, but a cross-correlation. The kernel size here refers to the widthxheight of the filter mask.
The max pooling layer, for example, returns the pixel with maximum value from a set of pixels within a mask (kernel). That kernel is swept across the input, subsampling it.
So nothing to do with the concept of kernels in support vector machines or regularization networks. You can think of them as feature extractors.
","Deep neural networks, particularly convolutional neural networks (CNNs), consist of layers defined by the application of filters (kernels) on the input.

These convolutional kernels apply cross-correlation operations, not convolution, to extract features from the input.

Max pooling layers use kernels to select the maximum value within a mask, subsampling the input.

The term ""kernel"" in CNNs differs from its usage in support vector machines or regularization networks.

Instead, CNN kernels act as feature extractors, identifying patterns and characteristics within the input data."
What is the &quot;capacity&quot; of a machine learning model?,https://stats.stackexchange.com/questions/312424/what-is-the-capacity-of-a-machine-learning-model,machine-learning|deep-learning|autoencoders|variational-bayes,312424,True,312578,30711,3,40,1692802473,1510054001,48,1510123776,"Capacity is an informal term. It's very close (if not a synonym) for model complexity. It's a way to talk about how complicated a pattern or relationship a model can express. You could expect a model with higher capacity to be able to model more relationships between more variables than a model with a lower capacity.
Drawing an analogy from the colloquial definition of capacity, you can think of it as the ability of a model to learn from more and more data, until it's been completely ""filled up"" with information.
There are various ways to formalize capacity and compute a numerical value for it, but importantly these are just some possible ""operationalizations"" of capacity (in much the same way that, if someone came up with a formula to compute beauty, you would realize that the formula is just one fallible interpretation of beauty).

VC dimension (https://en.wikipedia.org/wiki/VC_dimension) is a mathematically rigorous formulation of capacity. However, there can be a large gap between the VC dimension of a model and the model's actual ability to fit the data. Even though knowing the VC dim gives a bound on the generalization error of the model, this is usually too loose to be useful with neural networks.
Another line of research see here (http://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf) is to use the spectral norm of the weight matrices in a neural network as a measure of capacity. One way to understand this is that the spectral norm bounds the Lipschitz constant of the network. 
The most common way to estimate the capacity of a model is to count the number of parameters. The more parameters, the higher the capacity in general. Of course, often a smaller network learns to model more complex data better than a larger network, so this measure is also far from perfect.
Another way to measure capacity might be to train your model with random labels (Neyshabur et. al (https://arxiv.org/abs/1706.08947)) -- if your network can correctly remember a bunch of inputs along with random labels, it essentially shows that the model has the ability to remember all those data points individually. The more input/output pairs which can be ""learned"", the higher the capacity. 
Adapting this to an auto-encoder, you might 
generate random inputs, train the network to reconstruct them, and then count how many random inputs you can successfully reconstruct with less than $\epsilon$ error. 
","**Summary:**

Capacity in machine learning refers to a model's ability to capture complex relationships between variables. Models with higher capacity can learn from more data and capture more intricate patterns.

VC dimension is a mathematical measure of capacity, but it may not accurately reflect a model's real-world performance with neural networks. 

Another approach is to use the spectral norm of weight matrices to bound the model's Lipschitz constant. 

A simple measure of capacity is the number of parameters, although this does not always correlate with a model's ability to model complex data. 

Training a model with random labels can also serve as a capacity measure, where higher capacity models can remember more input-output pairs. 

Capacity estimation in auto-encoders involves generating random inputs, training the network to reconstruct them, and counting successful reconstructions with low error."
Why should we shuffle data while training a neural network?,https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network,machine-learning|neural-networks,245502,True,311318,47262,6,38,1583788951,1478905829,33,1509566175,"Note: throughout this answer I refer to minimization of training loss and I do not discuss stopping criteria such as validation loss. The choice of stopping criteria does not affect the process/concepts described below.
The process of training a neural network is to find the minimum value of a loss function $â_X(W)$, where $W$ represents a matrix (or several matrices) of weights between neurons and $X$ represents the training dataset. I use a subscript for $X$ to indicate that our minimization of $â$ occurs only over the weights $W$ (that is, we are looking for $W$ such that $â$ is minimized) while $X$ is fixed.
Now, if we assume that we have $P$ elements in $W$ (that is, there are $P$ weights in the network), $â$ is a surface in a $P+1$-dimensional space. To give a visual analogue, imagine that we have only two neuron weights ($P=2$). Then $â$ has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights $W$, the loss function can be evaluated on $X$ and that value becomes the elevation of the surface.
But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming ""stuck"" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if $X$ is unchanged over all training iterations, because the surface is fixed for a given $X$; all its features are static, including its various minima.
A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, $X$ changes with every iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same $X$. The effect is that the solver can easily ""bounce"" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration $i$ with training mini-batch $X_i$. This local minimum corresponds to $â$ evaluated at a particular value of weights; we'll call it $â_{X_i}(W_i)$. On the next iteration the shape of our loss surface actually changes because we are using $X_{i+1}$, that is, $â_{X_{i+1}}(W_i)$ may take on a very different value from $â_{X_i}(W_i)$ and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of $â_{X_{i+1}}$ will -- in general -- be different from that of $â_{X_{i}}$. Note that here I am referring to the loss function $â$ evaluated on a training set $X$; it is a complete surface defined over all possible values of $W$, rather than the evaluation of that loss (which is just a scalar) for a specific value of $W$. Note also that if mini-batches are used without shuffling there is still a degree of ""diversification"" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch).
One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because $â$ is evaluated by computing a value for each row of $X$ (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices $W$, the arrangement of the rows of $X$ has no effect when using full-batch gradient descent (that is, when each batch is the full $X$, and iterations and epochs are the same thing).
","Training a neural network involves minimizing a loss function, which represents a surface in a multi-dimensional space. However, this surface often has multiple local minima, making it difficult for gradient descent algorithms to find the optimal solution.

Mini-batch training combined with shuffling addresses this problem. By randomly shuffling the rows of the training data and training on only a subset of them in each iteration, the shape of the loss surface changes continuously. This allows the solver to ""bounce"" out of local minima and potentially find a deeper, better solution.

Note that the shape of the loss surface is different for each mini-batch, as the loss function is evaluated on a different subset of the training data. This diversification of loss surfaces helps the solver escape local minima and converge to a more optimal solution.

It's worth noting that mini-batch training does not use the full training data in each iteration, which can impact parallelization and convergence speed. However, the effect of row arrangement in the training data on the loss function is negligible when using full-batch gradient descent."
What are the differences between sparse coding and autoencoder?,https://stats.stackexchange.com/questions/118199/what-are-the-differences-between-sparse-coding-and-autoencoder,machine-learning|neural-networks|unsupervised-learning|deep-learning|autoencoders,118199,True,118490,34596,4,38,1509118246,1412703894,36,1412880717,"Finding the differences can be done by looking at the models. Let's look at sparse coding first. 
Sparse coding
Sparse coding minimizes the objective
$$
\mathcal{L}_{\text{sc}} = \underbrace{||WH - X||_2^2}_{\text{reconstruction term}} + \underbrace{\lambda  ||H||_1}_{\text{sparsity term}}
$$
where $W$ is a matrix of bases, H is a matrix of codes and $X$ is a matrix of the data we wish to represent. $\lambda$ implements a trade of between sparsity and reconstruction. Note that if we are given $H$, estimation of $W$ is easy via least squares. 
In the beginning, we do not have $H$ however. Yet, many algorithms exist that can solve the objective above with respect to $H$. Actually, this is how we do inference: we need to solve an optimisation problem if we want to know the $h$ belonging to an unseen $x$.
Auto encoders
Auto encoders are a family of unsupervised neural networks. There are quite a lot of them, e.g. deep auto encoders or those having different regularisation tricks attached--e.g. denoising, contractive, sparse. There even exist probabilistic ones, such as generative stochastic networks or the variational auto encoder. Their most abstract form is
$$
D(d(e(x;\theta^r); \theta^d), x)
$$
but we will go along with a much simpler one for now:
$$
\mathcal{L}_{\text{ae}} = ||W\sigma(W^TX) - X||^2
$$
where $\sigma$ is a nonlinear function such as the logistic sigmoid $\sigma(x) = {1 \over 1 + \exp(-x)}$. 
Similarities
Note that $\mathcal{L}_{sc}$ looks almost like $\mathcal{L}_{ae}$ once we set $H = \sigma(W^TX)$. The difference of both is that i) auto encoders do not encourage sparsity in their general form ii) an autoencoder uses a model for finding the codes, while sparse coding does so by means of optimisation.
For natural image data, regularized auto encoders and sparse coding tend to yield very similar $W$. However, auto encoders are much more efficient and are easily generalized to much more complicated models. E.g. the decoder can be highly nonlinear, e.g. a deep neural network. Furthermore, one is not tied to the squared loss (on which the estimation of $W$ for $\mathcal{L}_{sc}$ depends.)
Also, the different methods of regularisation yield representations with different characteristica. Denoising auto encoders have also been shown to be equivalent to a certain form of RBMs etc.
But why?
If you want to solve a prediction problem, you will not need auto encoders unless you have only little labeled data and a lot of unlabeled data. Then you will generally be better of to train a deep auto encoder and put a linear SVM on top instead of training a deep neural net.
However, they are very powerful models for capturing characteristica of distributions. This is vague, but research turning this into hard statistical facts is currently conducted. Deep latent Gaussian models aka Variational Auto encoders or generative stochastic networks are pretty interesting ways of obtaining auto encoders which provably estimate the underlying data distribution.
","Sparse coding and auto encoders are techniques for finding representations of data. Sparse coding minimizes a function that encourages a sparse representation of data in a basis, while auto encoders use a neural network to learn a representation that minimizes the reconstruction error.

Both methods can learn similar representations for natural image data, but auto encoders are more efficient and can be generalized to more complex models. Regularized auto encoders yield representations with different characteristics, such as denoising auto encoders that are equivalent to certain RBMs.

Auto encoders are useful for capturing characteristics of distributions, especially when there is limited labeled data. They can also be used for more complex models, such as deep latent Gaussian models, which can estimate the underlying data distribution."
Gradient backpropagation through ResNet skip connections,https://stats.stackexchange.com/questions/268820/gradient-backpropagation-through-resnet-skip-connections,machine-learning|neural-networks|convolutional-neural-network|gradient-descent|backpropagation,268820,True,268824,24592,2,38,1628862856,1490083792,24,1490085777,"Add sends the gradient back equally to both inputs. You can convince yourself of this by running the following in tensorflow:
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    x1_tf = tf.Variable(1.5, name='x1')
    x2_tf = tf.Variable(3.5, name='x2')
    out_tf = x1_tf + x2_tf

    grads_tf = tf.gradients(ys=[out_tf], xs=[x1_tf, x2_tf])
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        fd = {
            out_tf: 10.0
        }
        print(sess.run(grads_tf, feed_dict=fd))

Output:
[1.0, 1.0]

So, the gradient will be:

passed back to previous layers, unchanged, via the skip-layer connection, and also
passed to the block with weights, and used to update those weights

Edit: there is a question: ""what is the operation at the point where the highway connection and the neural net block join back together again, at the bottom of Figure 2?""
There answer is: they are summed. You can see this from Figure 2's formula:
$$
\mathbf{\text{output}} \leftarrow \mathcal{F}(\mathbf{x}) + \mathbf{x}
$$
What this says is that:

the values in the bus ($\mathbf{x}$)
are added to the results of passing the bus values, $\mathbf{x}$, through the network, ie $\mathcal{F}(\mathbf{x})$
to give the output from the residual block, which I've labelled here as $\mathbf{\text{output}}$

Edit 2:
Rewriting in slightly different words:

in the forwards direction, the input data flows down the bus


at points along the bus, residual blocks can learn to add/remove values to the bus vector

in the backwards direction, the gradients flow back down the bus


along the way, the gradients update the residual blocks they move past
the residual blocks will themselves modify the gradients slightly too


The residual blocks do modify the gradients flowing backwards, but there are no 'squashing' or 'activation' functions that the gradients flow through. 'squashing'/'activation' functions are what causes the exploding/vanishing gradient problem, so by removing those from the bus itself, we mitigate this problem considerably.
Edit 3: Personally I imagine a resnet in my head as the following diagram. Its topologically identical to figure 2, but it shows more clearly perhaps how the bus just flows straight through the network, whilst the residual blocks just tap the values from it, and add/remove some small vector against the bus:
 (https://i.sstatic.net/ei9GQ.png)
","**Summary:**

Residual neural networks (ResNets) alleviate the vanishing/exploding gradient problem in deep networks by utilizing a ""highway connection"" or ""skip-layer connection."" This connection allows the input data to flow directly through the network without any modifications.

During forward propagation, input data flows down the highway connection. Residual blocks, located along the connection, can learn to add or remove values from the data.

During backpropagation, gradients flow back down the highway connection and through the residual blocks. The blocks modify the gradients slightly, but there are no ""squashing"" or ""activation"" functions that could cause gradient issues.

In other words, the highway connection allows input data to bypass the complex transformations in the residual blocks, ensuring that gradients flow smoothly back through the network. This design mitigates the gradient problem and enhances the network's ability to learn."
How to visualize/understand what a neural network is doing?,https://stats.stackexchange.com/questions/11764/how-to-visualize-understand-what-a-neural-network-is-doing,data-visualization|neural-networks,11764,True,11790,8733,5,38,1455682217,1307639959,12,1307687345,"Neural networks are sometimes called ""differentiable function approximators"". So what you can do is to differentiate any unit with respect to any other unit to see what their relationshsip is.
You can check how sensitive the error of the network is wrt to a specific input as well with this.
Then, there is something called ""receptive fields"", which is just the visualization of the connections going into a hidden unit. This makes it easy to understand what particular units do for image data, for example. This can be done for higher levels as well. See Visualizing Higher-Level Features of a Deep Network (http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247).
","**Summary:**

Neural networks are described as ""differentiable function approximators,"" meaning that their relationships can be analyzed by calculating derivatives between units. This allows for understanding how each unit affects the network's error and the overall behavior.

Additionally, ""receptive fields"" visualize the connections between units, providing insights into their function. This visualization is particularly useful for interpreting image data, as it shows how units respond to specific input patterns. The concept of receptive fields can be extended to higher levels of the network, enabling the understanding of complex feature extraction in deep neural networks."
"What is the difference between kernel, bias, and activity regulizers, and when to use which?",https://stats.stackexchange.com/questions/383310/what-is-the-difference-between-kernel-bias-and-activity-regulizers-and-when-t,neural-networks|regularization|keras,383310,True,383326,30979,3,37,1599410632,1545003348,51,1545013207,"What is the difference between them?
You have the regression equation $y = Wx+b$, where $x$ is the input, $W$ the weights matrix and $b$ the bias.

Kernel Regularizer: Tries to reduce the weights $W$ (excluding bias).
Bias Regularizer: Tries to reduce the bias $b$.
Activity Regularizer: Tries to reduce the layer's output $y$, thus will reduce the weights and adjust bias so $Wx+b$ is smallest.

When to use which?
Usually, if you have no prior on the distribution that you wish to model, you would only use the kernel regularizer, since a large enough network can still model your function even if the regularization on the weights are big.
If you want the output function to pass through (or have an intercept closer to) the origin, you can use the bias regularizer.
If you want the output to be smaller (or closer to 0), you can use the activity regularizer.
$L_1$ versus $L_2$ regularization
Now, for the $L_1$ versus $L_2$ loss for weight decay (not to be confused with the outputs loss function).

$L_2$ loss is defined as $w^2$
$L_1$ loss is defined as $|w|$.

where $w$ is a component of the matrix $W$.

The gradient of $L_2$ will be: $2w$
The gradient of $L_1$ will be: $sign(w)$

Thus, for each gradient update with a learning rate $a$, in $L_2$ loss, the weights will be subtracted by $aW$, while in $L_1$ loss they will be subtracted by $a \cdot sign(W)$.
The effect of $L_2$ loss on the weights is a reduction of large components in the matrix $W$, while $L_1$ loss will make the weights matrix sparse, with many zero values. The same applies to the bias and output respectively using the bias and activity regularizer.
","**Summary:**

* **Regularizers:**
    * Kernel: Reduces weights, preventing overfitting.
    * Bias: Adjusts bias, ensuring the output passes through or near the origin.
    * Activity: Minimizes output, reducing weights and adjusting bias to lower the overall function value.

* **When to Use:**
    * Kernel: Default option for modeling functions without prior knowledge.
    * Bias: When the output should pass through or near the origin.
    * Activity: When a smaller output is desired.

* **L1 vs. L2 Regularization:**
    * L2 loss penalizes larger weights, reducing them gradually.
    * L1 loss sets smaller weights to zero, creating a sparse weight matrix.

In L2 regularization, weights are subtracted by a value proportional to their magnitude. In L1 regularization, weights are subtracted by a constant equal to the sign of their value. This results in L2 regularization favoring smaller weights and L1 regularization creating a sparse weight matrix."
What are variational autoencoders and to what learning tasks are they used?,https://stats.stackexchange.com/questions/321841/what-are-variational-autoencoders-and-to-what-learning-tasks-are-they-used,machine-learning|bayesian|deep-learning|autoencoders|variational-bayes,321841,True,328181,9217,2,37,1696412147,1515249119,58,1518445757,"Even though variational autoencoders (VAEs) are easy to implement and train, explaining them is not simple at all, because they blend concepts from Deep Learning and Variational Bayes, and the Deep Learning and Probabilistic Modeling communities use different terms for the same concepts. Thus when explaining VAEs you risk either concentrating on the statistical model part, leaving the reader without a clue about how to actually implement it, or vice versa to concentrate on the network architecture and loss function, in which the Kullback-Leibler term seems to be pulled out of thin air. I'll try to strike a middle ground here, starting from the model but giving enough details to actually implement it in practice, or understand someone's else implementation.
VAEs are generative models
Unlike classical (sparse, denoising, etc.) autoencoders, VAEs are generative models, like GANs. With generative model I mean a model which learns the probability distribution $p(\mathbf{x})$ over the input space $\mathcal{x}$. This means that after we have trained such a model, we can then sample from (our approximation of) $p(\mathbf{x})$. If our training set is made of handwritten digits (MNIST), then after training the generative model is able to create images which look like handwritten digits, even though they're not ""copies"" of the images in the training set.
Learning the distribution of the images in the training set implies that images which look like handwritten digits should have an high probability of being generated, while images which look like the Jolly Roger or random noise should have a low probability. In other words, it means learning about the dependencies among pixels: if our image is a $28\times 28=784$ pixels grayscale image from MNIST, the model should learn that if a pixel is very bright, then there's a significant probability that some neighboring pixels are bright too, that if we have a long, slanted line of bright pixels we may have another smaller, horizontal line of pixels above this one (a 7), etc.
VAEs are latent variable models
The VAE is a latent variables model: this means that $\mathbf{x}$, the random vector of the 784 pixel intensities (the observed variables), is modeled as a (possibly very complicated) function of a random vector $\mathbf{z}\in\mathcal{Z}$ of lower dimensionality, whose components are unobserved (latent) variables. When does such a model make sense? For example, in the MNIST case we think that the handwritten digits belong to a manifold of dimension much smaller than the dimension of $\mathcal{x}$, because the vast majority of random arrangements of 784 pixel intensities, don't look at all like handwritten digit. Intuitively we would expect the dimension to be at least 10 (the number of digits), but it's most likely larger because each digit can be written in different ways. Some differences are unimportant for the quality of the final image (for example, global rotations and translations), but others are important. So in this case the latent model makes sense. More on this later. Note that, amazingly, even if our intuition tells us that the dimension should about 10, we can definitely use just 2 latent variables to encode the MNIST dataset with a VAE (though results won't be pretty). The reason is that even a single real variable can encode infinitely many classes, because it can assume all possible integer values and more. Of course, if the classes have significant overlap among them (such as 9 and 8 or 7 and I in MNIST), even the most complicated function of just two latent variables will do a poor job of generating clearly discernible samples for each class. More on this later.
VAEs assume a multivariate parametric distribution $q(\mathbf{z}\vert\mathbf{x},\boldsymbol{\lambda})$ (where $\boldsymbol{\lambda}$ are the parameters of $q$), and they learn the parameters of the multivariate distribution. The use of a parametric pdf for $\mathbf{z}$, which prevents the number of parameters of a VAE to grow without bounds with the growth of the training set, is called amortization in VAE lingo (yeah, I know...).
The decoder network
 (https://i.sstatic.net/Hqauz.png)
We start from the decoder network because the VAE is a generative model, and the only part of the VAE which is actually used to generate new images is the decoder. The encoder network is only used at inference (training) time.
The goal of the decoder network is to generate new random vectors $\mathbf{x}$ belonging to the input space $\mathcal{X}$, i.e., new images, starting from realizations of the latent vector $\mathbf{z}$. This means clearly that it must learn the conditional distribution $p(\mathbf{x}\vert\mathbf{z})$. For VAEs this distribution is often assumed to be a multivariate Gaussian1:
$$p_{\boldsymbol{\phi}}(\mathbf{x}\vert\mathbf{z}) = \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}(\mathbf{z}; \boldsymbol{\phi}), \boldsymbol{\sigma}(\mathbf{z}; \boldsymbol{\phi})^2I) $$
$\boldsymbol{\phi}$ is the vector of weights (and biases) of the encoder network. The vectors $\boldsymbol{\mu}(\mathbf{z};\boldsymbol{\phi})$ and $\boldsymbol{\sigma}(\mathbf{z}; \boldsymbol{\phi})$ are complex, unknown nonlinear functions, modeled by the decoder network: neural networks are powerful nonlinear functions approximators.

As noted by @amoeba in the comments, there is a striking similarity between the decoder and a classic latent variables model: Factor Analysis. In Factor Analysis, you assume the model:
$$ \mathbf{x}\vert\mathbf{z}\sim\mathcal{N}(\mathbf{W}\mathbf{z}+\boldsymbol{\mu}, \boldsymbol{\sigma}^2I),\ \mathbf{z}\sim\mathcal{N}(0,I)$$
Both models (FA & the decoder) assume that the conditional distribution of the observable variables $\mathbf{x}$ on the latent variables $\mathbf{z}$ is Gaussian, and that the $\mathbf{z}$ themselves are standard Gaussians. The difference is that the decoder doesn't assume that the mean of $p(\mathbf{x}|\mathbf{z})$ is linear in $\mathbf{z}$, nor it assumes that the standard deviation is a constant vector. On the contrary, it models them as  complex nonlinear functions of the $\mathbf{z}$. In this respect, it can be seen as nonlinear Factor Analysis. See here (https://www.cs.cmu.edu/%7Ebhiksha/courses/deeplearning/Spring.2017/slides/lec12.vae.pdf) for an insightful discussion of this connection between FA and VAE. Since FA with an isotropic covariance matrix is just PPCA, this also ties in to the well-known result that a linear autoencoder reduces to PCA.

Let's go back to the decoder: how do we learn $\boldsymbol{\phi}$? Intuitively we want latent variables $\mathbf{z}$ which maximize the likelihood of generating the $\mathbf{x}_i$ in the training set $D_n$. In other words we want to compute the posterior probability distribution of the $\mathbf{z}$, given the data:
$$p(\mathbf{z}\vert\mathbf{x})=\frac{p_{\boldsymbol{\phi}}(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})}{p(\mathbf{x})}$$
We assume a $\mathcal{N}(0,I)$ prior on $\mathbf{z}$, and we're left with the usual issue in Bayesian inference that computing $p(\mathbf{x})$ (the evidence) is hard (a multidimensional integral). What's more, since here $\boldsymbol{\mu}(\mathbf{z};\boldsymbol{\phi})$ is unknown, we can't compute it anyway. Enter Variational Inference, the tool which gives Variational Autoencoders their name.
Variational Inference for the VAE model
Variational Inference is a tool to perform approximate Bayesian Inference for very complex models. It's not an overly complex tool, but my answer is already too long and I won't go into a detailed explanation of VI. You can have a look at this answer and the references therein if you're curious:
https://stats.stackexchange.com/a/270569/58675 (https://stats.stackexchange.com/a/270569/58675)
It suffices to say that VI looks for an approximation to $p(\mathbf{z}\vert \mathbf{x})$ in a parametric family of distributions $q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})$, where, as noted above, $\boldsymbol{\lambda}$ are the parameters of the family. We look for the parameters which minimize the Kullback-Leibler divergence between our target distribution $p(\mathbf{z}\vert \mathbf{x})$ and $q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})$:
$$\min_{\boldsymbol{\lambda}}\mathcal{D}[p(\mathbf{z}\vert \mathbf{x})\vert\vert q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})]$$
Again, we cannot minimize this directly because the definition of Kullback-Leibler divergence includes the evidence. Introducing the ELBO (Evidence Lower BOund) and after some algebraic manipulations, we finally get at:
$$ELBO(\boldsymbol{\lambda})= E_{q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})}[\log p(\mathbf{x}\vert\boldsymbol{z})]-\mathcal{D}[(q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})\vert\vert p(\boldsymbol{z})]$$
Since the ELBO is a lower bound on evidence (see the above link), maximizing the ELBO is not exactly equivalent to maximizing the likelihood of data given $\boldsymbol{\lambda}$ (after all, VI is a tool for approximate Bayesian inference), but it goes in the right direction.
In order to make inference, we need to specify the parametric family $q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$. In most VAEs we choose a multivariate, uncorrelated Gaussian distribution
$$q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda}) = \mathcal{N}(\mathbf{z}\vert\boldsymbol{\mu}(\mathbf{x}), \boldsymbol{\sigma}^2(\mathbf{x})I) $$
This is the same choice we made for $p(\mathbf{x}\vert\mathbf{z})$, though we may have chosen a different parametric family. As before, we can estimate these complex nonlinear functions by introducing a neural network model. Since this model accepts input images and returns parameters of the distribution of the latent variables we call it the encoder network.
The encoder network
Also called the inference network, this is only used at training time.
 (https://i.sstatic.net/XHxi8.png)
As noted above, the encoder must approximate $\boldsymbol{\mu}(\mathbf{x})$ and  $\boldsymbol{\sigma}(\mathbf{x})$, thus if we have, say, 24 latent variables, the output of the encoder is a $d=48$ vector. The encoder has weights (and biases) $\boldsymbol{\theta}$. To learn $\boldsymbol{\theta}$, we can finally write the ELBO in terms of the parameters $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ of the encoder and decoder network, as well as the training set points:
$$ELBO(\boldsymbol{\theta},\boldsymbol{\phi})= \sum_i E_{q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})}[\log p_{\boldsymbol{\phi}}(\mathbf{x}_i\vert\boldsymbol{z})]-\mathcal{D}[(q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})\vert\vert p(\boldsymbol{z})]$$
We can finally conclude. The opposite of the ELBO, as a function of $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$, is used as the loss function of the VAE. We use SGD to minimize this loss, i.e., maximize the ELBO. Since the ELBO is a lower bound on the evidence, this goes in the direction of maximizing the evidence, and thus generating new images which are optimally similar to those in the training set. The first term in the ELBO is the expected negative log-likelihood of the training set points, thus it encourages the decoder to produce images which are similar to the training ones. The second term can be interpreted as a regularizer: it encourages the encoder to generate a distribution for the latent variables which is similar to $p(\boldsymbol{z})=\mathcal{N}(0,I)$. But by introducing the probability model first, we understood where the whole expression comes from: the minimization of the Kullabck-Leibler divergence between the approximate posterior $q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$ and the model posterior $p(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$.2
Once we have learned $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ by maximizing $ELBO(\boldsymbol{\theta},\boldsymbol{\phi})$, we can throw away the encoder. From now on, to generate new images just sample $\boldsymbol{z}\sim \mathcal{N}(0,I)$ and propagate it through the decoder. The decoder outputs will be images similar to those in the training set.
References and further reading

the original paper: Auto-Encoding Variational Bayes (https://arxiv.org/abs/1312.6114)
a nice tutorial, with a few minor imprecisions: Tutorial on Variational Autoencoders (https://arxiv.org/pdf/1606.05908.pdf)
how to reduce the blurriness of the images generated by your VAE, while at the same time getting latent variables which have a visual (perceptual) meaning, so that you can ""add"" features (smile, sunglasses, etc.) to your generated images: Deep Feature Consistent Variational
Autoencoder (https://arxiv.org/pdf/1610.00291.pdf)
improving the quality of VAE-generated images even more, by using Gaussian versions of autoregressive autoencoders:Improved Variational Inference
with Inverse Autoregressive Flow (https://arxiv.org/pdf/1606.04934.pdf)
new directions of research and a deeper understanding of pros & cons of the VAE model: Towards a Deeper Understanding of Variational Autoencoding Models (https://arxiv.org/pdf/1702.08658.pdf) & INFERENCE SUBOPTIMALITY
IN VARIATIONAL AUTOENCODERS (https://arxiv.org/pdf/1801.03558.pdf)


1 This assumption is not strictly necessary, though it simplifies our description of VAEs. However, depending on applications, you may assume a different distribution for $p_{\phi}(\mathbf{x}\vert\mathbf{z})$. For example, if $\mathbf{x}$ is a vector of binary variables, a Gaussian $p$ makes no sense, and a multivariate Bernoulli can be assumed.
2 The ELBO expression, with its mathematical elegance, conceals two major sources of pain for the VAE practitioners. One is the average term $E_{q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})}[\log p_{\boldsymbol{\phi}}(\mathbf{x}_i\vert\boldsymbol{z})]$. This effectively requires computing an expectation, which requires taking multiple samples from $q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})$. Given the sizes of the involved neural networks, and the low convergence rate of the SGD algorithm, having to draw multiple random samples at each iteration (actually, for each minibatch, which is even worse) is very time-consuming. VAE users solve this problem very pragmatically by computing that expectation with a single (!) random sample. The other issue is that to train two neural networks (encoder & decoder) with the backpropagation algorithm, I need to be able to differentiate all steps involved in forward propagation from the encoder to the decoder. Since the decoder is not deterministic (evaluating its output requires drawing from a multivariate Gaussian), it doesn't even make sense to ask if it's a differentiable architecture. The solution to this is the reparametrization trick (https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important).
","Variational autoencoders (VAEs) are generative models that learn to capture the probability distribution of a dataset. Unlike traditional autoencoders, VAEs represent data as a function of a lower-dimensional ""latent"" space, assuming that the data points near each other in latent space belong to similar classes.

VAEs consist of two networks: an encoder and a decoder. The encoder estimates the distribution of the latent variables given an input image, while the decoder generates new images from sampled latent variables.

To train a VAE, the Kullback-Leibler divergence is minimized between the approximate posterior distribution of the latent variables and the prior distribution, while maximizing the likelihood of the input data.

The ELBO (Evidence Lower BOund) is used as the loss function for VAE training. The ELBO consists of two terms: the expected negative log-likelihood of the data and a regularization term that encourages the latent distribution to match the prior. By maximizing the ELBO, the VAE learns to generate images that are similar to those in the training set while minimizing the information loss in the latent representation."
Explanation of Spikes in training loss vs. iterations with Adam Optimizer,https://stats.stackexchange.com/questions/303857/explanation-of-spikes-in-training-loss-vs-iterations-with-adam-optimizer,neural-networks|deep-learning|adam,303857,True,304150,56463,2,37,1543935120,1505813424,33,1505931359,"The spikes are an unavoidable consequence of Mini-Batch Gradient Descent in Adam (batch_size=32). 
Some mini-batches have 'by chance' unlucky data for the optimization, inducing those spikes you see in your cost function using Adam. If you try stochastic gradient descent (same as using batch_size=1) you will see that there are even more spikes in the cost function. The same doesnÂ´t happen in (Full) Batch GD because it uses all training data (i.e the batch size is equal to the cardinality of your training set) each optimization epoch. As in your first graphic the cost is monotonically decreasing smoothly it seems the title (i) With SGD) is wrong and you are using (Full) Batch Gradient Descent instead of SGD.
On his great Deep Learning course at Coursera (https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent), Andrew Ng explains in great details this using the image below:
 (https://i.sstatic.net/8xKxN.png) 
","Mini-batch gradient descent with Adam often produces spikes in the cost function due to unlucky data in some mini-batches. This issue is amplified in stochastic gradient descent but absent in full batch gradient descent, which uses the entire training dataset each epoch.

The first graph incorrectly labeled as ""With SGD"" actually represents full batch gradient descent, as evidenced by its smooth, monotonic decrease in cost. This demonstrates that batch size significantly affects the optimization process, with full batch gradient descent providing the most stable progress."
What is the difference between dropout and drop connect?,https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect,neural-networks|dropout,201569,True,201891,21428,3,37,1658164203,1457951530,41,1458070846,"DropOut and DropConnect are both methods intended to prevent ""co-adaptation"" of units in a neural network. In other words, we want units to independently extract features from their inputs instead of relying on other neurons to do so. 
Suppose we have a multilayered feedforward network like this one (the topology doesn't really matter). We're worried about the yellow hidden units in the middle layer co-adapting. 
 (https://i.sstatic.net/DfKTA.png)
 DropOut 
To apply DropOut, we randomly select a subset of the units and clamp their output to zero, regardless of the input; this effectively removes those units from the model. A different subset of units is randomly selected every time we present a training example. 
Below are two possible network configurations. On the first presentation (left), the 1st and 3rd units are disabled, but the 2nd and 3rd units have been randomly selected on a subsequent presentation. At test time, we use the complete network but rescale the weights to compensate for the fact that all of them can now become active (e.g., if you drop half of the nodes, the weights should also be halved).
 (https://i.sstatic.net/CewjH.png)
DropConnect
DropConnect works similarly, except that we disable individual weights (i.e., set them to zero), instead of nodes, so a node can remain partially active. Schematically, it looks like this:
 (https://i.sstatic.net/D1QC7.png)
 Comparison 
These methods both work because they effectively let you train several models at the same time, then average across them for testing. For example, the yellow layer has four nodes, and thus 16 possible DropOut states (all enabled, #1 disabled, #1 and #2 disabled, etc). 
DropConnect is a generalization of DropOut because it produces even more possible models, since there are almost always more connections than units. However, you can get similar outcomes on an individual trial. For example, the DropConnect network on the right has effectively dropped Unit #2 since all of the incoming connections have been removed.
Further Reading
The original papers are pretty accessible and contain more details and empirical results.

DropOut: Hinton et al., 2012 (http://www.cs.toronto.edu/~hinton/absps/dropout.pdf), Srivasta et al., 2014; JMLR (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
DropConnect: Wan et al., 2013 (http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf)

","**Summary**

DropOut and DropConnect are regularization techniques in neural networks that prevent co-adaptation of units, ensuring they independently extract features.

* **DropOut:** Randomly disables entire nodes (output set to zero). Different nodes are disabled for each training example. At test time, all nodes are active, but weights are rescaled to compensate for inactive nodes during training.


* **DropConnect:** Similar to DropOut, but disables individual weights instead of nodes. This allows nodes to remain partially active.


* **Comparison:** Both methods train multiple models simultaneously and average their results. DropConnect is more versatile as it creates more possible models. However, both techniques can achieve similar results on specific trials."
Backpropagation vs Genetic Algorithm for Neural Network training,https://stats.stackexchange.com/questions/55887/backpropagation-vs-genetic-algorithm-for-neural-network-training,neural-networks|genetic-algorithms|backpropagation,55887,True,56569,43014,6,37,1646330457,1365723761,28,1366360230,"If you look carefully at the scientific literature you'll find contrasting results. Obviously, in some cases GA (and more in general, Evolutionary Algorithms) may help you to find an optimal NN design but normally they have so many drawbacks (algorithm parameters' tuning, computational complexity etc) and their use is not feasible for real-world applications. Of course you can find a set of problems where GA/EAs is always better than backpropagation. Given that finding an optimal NN design is a complex multimodal optimization problem GA/EAs may help (as metaheuristics) to improve the results obtained with ""traditional"" algorithms, e.g. using GA/EAs to find only the initial weights configuration or helping traditional algorithms to escape from local minima (if you are interested I wrote a paper about this topic). 
I worked a lot on this field and I can tell you that there are many scientific works on GA/EAs applied to NNs because they are (or better, they used to be) an emerging research field. 
","**Summary:**

Genetic algorithms (GAs) and evolutionary algorithms (EAs) have been explored as tools to optimize neural network (NN) design. However, their use has limitations in real-world applications due to drawbacks such as parameter tuning and computational complexity.

GAs/EAs can be useful as metaheuristics to enhance the performance of traditional NN optimization algorithms. They can assist in finding initial weight configurations or aiding in escaping local minima.

While there is ample research on GA/EA applications in NN design, it's important to be aware of their limitations and consider them in the context of specific problems."
Should training samples randomly drawn for mini-batch training neural nets be drawn without replacement?,https://stats.stackexchange.com/questions/235844/should-training-samples-randomly-drawn-for-mini-batch-training-neural-nets-be-dr,machine-learning|neural-networks|optimization|deep-learning,235844,True,236186,23518,3,37,1635454091,1474325607,10,1474477147,"According to the code in Nielsen's repository, mini-batches are drawn without replacement:
    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
    n = len(training_data)
    for j in range(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in range(0, n, mini_batch_size)
            ]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)

We can see that there is no replacement of training samples within an epoch. Interestingly, we can also see that Nielsen chooses not to worry about adjusting eta (the learning rate) for the last mini_batch size, which may not have as many training samples as the previous mini-batches. Presumably this is an advanced modification he leaves for later chapters.**
** EDIT: Actually, this scaling occurs in the def update_mini_batch function. For example, with the weights:
self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]     

This is necessary because the last mini_batch may be smaller than the previous mini_batches if the number of training samples per mini_batch does not divide evenly into the total number of training samples available.
mylist = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
n = len(mylist)
mini_batch_size = 2
mini_batches = [
    mylist[k:k+mini_batch_size]
    for k in range(0, n, mini_batch_size)
    ]
for mini_batch in mini_batches:
    print(mini_batch)

Output:
['1', '2']
['3', '4']
['5', '6']
['7', '8']
['9', '10']

Changing mini_batch_size to 3, which does not divide evenly into our 10 training samples. For output we get:
['1', '2', '3']
['4', '5', '6']
['7', '8', '9']
['10']

When evaluating a range over list indices (something of the form [x:y] where x and y are some indices into the list), if our right-hand value exceeds the list length, python simply returns the items from the list up until the value goes out of index range. 
So the last mini-batch might be smaller than previous mini-batches, but if it is weighted by the same eta then those training samples will contribute more to the learning than samples in the other, larger mini-batches. Since this is just the last mini-batch it's probably not worth worrying about too much, but can easily be solved by scaling eta to the length of the mini-batch.
","Nielsen's SGD implementation uses mini-batches drawn without replacement, ensuring each training sample is considered once per epoch. Additionally, to address potential discrepancies in mini-batch sizes, the learning rate (eta) is scaled by the mini-batch size during weight updates. Specifically, for the last mini-batch, which may be smaller than others, eta is scaled to ensure equal contributions from all training samples. This adjustment compensates for the smaller size of the last mini-batch, preventing it from having a disproportionate influence on weight updates. Ultimately, this approach ensures consistent learning progress throughout the training process."
What&#39;s the difference between &quot;deep learning&quot; and multilevel/hierarchical modeling?,https://stats.stackexchange.com/questions/60153/whats-the-difference-between-deep-learning-and-multilevel-hierarchical-modeli,machine-learning|multilevel-analysis|hierarchical-bayesian|deep-learning,60153,True,61629,14954,2,36,1416843469,1369688707,41,1371113113,"Similarity
Fundamentally both types of algorithms were developed to answer one general question in machine learning applications:

Given predictors (factors) $x_1, x_2, \ldots, x_p$ - how to incorporate the interactions between this factors in order to increase the performance?

One way is to simply introduce new predictors: $x_{p+1} = x_1x_2, x_{p+2} = x_1x_3, \ldots$ But this proves to be bad idea due to huge number of parameters and very specific type of interactions.
Both Multilevel modelling and Deep Learning algorithms answer this question by introducing much smarter model of interactions. And from this point of view they are very similar.
Difference
Now let me try to give my understanding on what is the great conceptual difference between them. In order to give some explanation, let's see the assumptions that we make in each of the models:

Multilevel modelling:$^1$ (https://en.wikipedia.org/wiki/Multilevel_model) layers that reflect the data structure can be represented as a Bayesian Hierarchical Network (https://en.wikipedia.org/wiki/Hierarchical_Bayesian_model). This network is fixed and usually comes from domain applications.
Deep Learning:$^2$ (http://en.wikipedia.org/wiki/Deep_learning) the data were generated by the interactions of many factors. The structure of interactions is not known, but can be represented as a layered factorisation: higher-level interactions are obtained by transforming lower-level representations.

The fundamental difference comes from the phrase ""the structure of interactions is not known"" in Deep Learning. We can assume some priors on the type of interaction, but yet the algorithm defines all the interactions during the learning procedure. On the other hand, we have to define the structure of interactions for Multilevel modelling (we learn only vary the parameters of the model afterwards).
Examples
For example, let's assume we are given three factors $x_1, x_2, x_3$ and we define $\{x_1\}$ and $\{x_2, x_3\}$ as different layers.
In the Multilevel modelling regression, for example, we will get the interactions $x_1 x_2$ and $x_1 x_3$, but we will never get the interaction $x_2 x_3$. Of course, partly the results will be affected by the correlation of the errors, but this is not that important for the example.
In Deep learning, for example in multilayered Restricted Boltzmann machines (RBM (http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)) with two hidden layers and linear activation function, we will have all the possible polinomial interactions with the degree less or equal than three.
Common advantages and disadvantages
Multilevel modelling
(-) need to define the structure of interactions
(+) results are usually easier to interpret
(+) can apply statistics methods (evaluate confidence intervals, check hypotheses)
Deep learning
(-) requires huge amount of data to train (and time for training as well)
(-) results are usually impossible to interpret (provided as a black box)
(+) no expert knowledge required
(+) once well-trained, usually outperforms most other general methods (not application specific)
Hope it will help!
","**Summary:**

Multilevel modeling and deep learning algorithms share the goal of incorporating interactions between factors to enhance performance. However, they differ in their assumptions and approaches.

**Multilevel Modeling:**

* Assumes a fixed hierarchical structure of interactions, typically derived from domain knowledge.
* Requires defining the interaction structure upfront.
* Results are easily interpretable and allow for statistical testing.

**Deep Learning:**

* Assumes the interaction structure is unknown and emerges during the learning process.
* Captures complex and potentially non-linear interactions.
* Requires vast amounts of data and significant training time.
* Results are often difficult to interpret and may be considered ""black boxes.""

**Advantages and Disadvantages:**

**Multilevel Modeling:**

* **Advantages:**
    * Interpretable results
    * Statistical methods can be applied
* **Disadvantages:**
    * Requires defining interactions upfront

**Deep Learning:**

* **Advantages:**
    * Captures complex interactions
    * No expert knowledge required
* **Disadvantages:**
    * Data-intensive and time-consuming training
    * Difficult to interpret results"
How does batch size affect convergence of SGD and why?,https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why,machine-learning|neural-networks|optimization|gradient-descent|stochastic-gradient-descent,316464,True,316527,49634,4,36,1587628209,1512045314,21,1512070504,"Sure one update with a big minibatch is ""better"" (in terms of accuracy) than one update with a small minibatch.  This can be seen in the table you copied in your question (call $N$ the sample size):

batch size 1: number of updates $27N$
batch size 20,000: number of updates $8343\times\frac{N}{20000}\approx 0.47N$

You can see that with bigger batches you need much fewer updates for the same accuracy.
But it can't be compared because it's not processing the same amount of data. I'm quoting the first article:
""We compare the effect of executing $k$ SGD iterations with small minibatches
$B_j$ versus a single iteration with a large minibatch $\displaystyle\bigcup_{1\leq j\leq k} B_j$""
Here it's about processing the same amount of data and while there is small overhead for multiple mini-batches, this takes comparable processing resources.
There are several ways to understand why several updates is better (for the same amount of data being read). It's the key idea of stochastic gradient descent vs. gradient descent. Instead of reading everything and then correct yourself at the end, you correct yourself on the way, making the next reads more useful since you correct yourself from a better guess. Geometrically, several updates is better because you are drawing several segments, each in the direction of the (approximated) gradient at the start of each segment. while a single big update is a single segment from the very start in the direction of the (exact) gradient. It's better to change direction several times even if the direction is less precise.
The size of mini-batches is essentially the frequency of updates: the smaller minibatches the more updates. At one extreme (minibatch=dataset) you have gradient descent. At the other extreme (minibatch=one line) you have full per line SGD. Per line SGD is better anyway, but bigger minibatches are suited for more efficient parallelization.
At the end of the convergence process, SGD becomes less precise than (batch) GD. But at this point, things become (usually) a sort of uselessly precise fitting. While you get a slightly smaller loss function on the training set, you don't get real predictive power. You are only looking for the very precise optimum but it does not help. If the loss function is correctly regularized (which prevents over-fitting) you don't exactly ""over""-fit, you just uselessly ""hyper""-fit. This shows as a non significant change in accuracy on the test set.
","**Summary**

**Using larger minibatches in SGD:**

* Provides higher accuracy compared to smaller minibatches, reducing the required number of updates for comparable accuracy.

**Comparison with a single large batch:**

* SGD with multiple updates processes the same amount of data as a single large batch update.
* Multiple updates improve accuracy due to progressive corrections based on improved estimates of the gradient.

**Gradient descent vs. SGD:**

* Gradient descent uses the full dataset and corrects itself at the end, while SGD uses minibatches to make incremental corrections.
* Multiple updates in SGD are advantageous because each segment is aligned with the gradient at the start of the segment, even if the gradient estimation is less precise.

**Practical considerations:**

* Batches smaller than the entire dataset allow for more frequent updates.
* Per-line SGD (minibatch size of 1) provides the best accuracy but may not be practical.
* Larger minibatches enable efficient parallelization.

**Convergence and overfitting:**

* SGD becomes less precise at the end of convergence, but this does not usually result in overfitting.
* With proper regularization, SGD models may not ""over""fit but rather ""hyper""fit, with little impact on accuracy on the test set."
"Difference between Bayes network, neural network, decision tree and Petri nets",https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets,machine-learning|neural-networks|bayesian-network|fuzzy,94511,True,94518,34654,6,35,1538988815,1398053762,32,1398061836,"Wow, what a big question! The short version of the answer is that just because you can represent two models using diagrammatically similar visual representations, doesn't mean they are even remotely related structurally, functionally, or philosophically. I'm not familiar with FCM or NF, but I can speak to the other ones a bit. 
Bayesian Network
In a Bayesian network, the graph represents the conditional dependencies of different variables in the model. Each node represents a variable, and each directed edge represents a conditional relationship. Essentially, the graphical model is a visualization of the chain rule. 
Neural Network
In a neural network, each node is a simulated ""neuron"". The neuron is essentially on or off, and its activation is determined by a linear combination of the values of each output in the preceding ""layer"" of the network. 
Decision Tree
Let's say we are using a decision tree for classification. The tree essentially provides us with a flowchart describing how we should classify an observation. We start at the root of the tree, and the leaf where we end up determines the classification we predict.
As you can see, these three models really have basically nothing at all to do with each other besides being representable with boxes and arrows.
","**Summary:**

Diagrammatic similarity between models does not imply structural, functional, or philosophical equivalence.

* **Bayesian Network:** Models conditional dependencies between variables through directed edges.
* **Neural Network:** Simulates ""neurons"" whose activation determines the output based on preceding layers.
* **Decision Tree:** Provides a flowchart-like representation for classification.

Despite their visual similarities, these models differ significantly in their underlying principles:

* Bayesian Network: Conditional probabilities
* Neural Network: Non-linear relationships
* Decision Tree: Rule-based classification

Therefore, visual representation alone is insufficient to establish conceptual connections between models."
How to get started with neural networks,https://stats.stackexchange.com/questions/36247/how-to-get-started-with-neural-networks,machine-learning|neural-networks|references,36247,True,36257,22425,7,34,1628519789,1347555261,34,1347568180,"Neural networks have been around for a while, and they've changed dramatically over the years. If you only poke around on the web, you might end up with the impression that ""neural network"" means multi-layer feedforward network trained with back-propagation. Or, you might come across any of the dozens of rarely used, bizarrely named models and conclude that neural networks are more of a zoo than a research project. Or that they're a novelty. Or...
I could go on. If you want a clear explanation, I'd listen to Geoffrey Hinton (http://www.cs.toronto.edu/~hinton). He has been around forever and (therefore?) does a great job weaving all the disparate models he's worked on into one cohesive, intuitive (and sometimes theoretical) historical narrative. On his homepage, there are links to Google Tech Talks and Videolectures.net lectures he has done (on RBMs (http://www.youtube.com/watch?v=AyzOUbkUf3M) and Deep Learning (http://videolectures.net/jul09_hinton_deeplearn/), among others).
From the way I see it, here's a historical and pedagogical road map to understanding neural networks, from their inception to the state-of-the-art:

Perceptrons (http://en.wikipedia.org/wiki/Perceptron)

Easy to understand
Severely limited

Multi-layer, trained by back-propogation

Many resources (http://en.wikiversity.org/wiki/Learning_and_neural_networks) to learn these
Don't generally do as well as SVMs

Boltzmann machines (http://en.wikipedia.org/wiki/Boltzmann_machine)

Interesting way of thinking about the stability of a recurrent network in terms of ""energy""
Look at Hopfield networks (http://www.comp.leeds.ac.uk/ai23/reading/Hopfield.pdf) if you want an easy to understand (http://lcn.epfl.ch/tutorial/english/hopfield/html/index.html) (but not very practical) example of recurrent networks with ""energy"".
Theoretically interesting, useless in practice (training about the same speed as continental drift)

Restricted Boltzmann Machines

Useful!
Build off of the theory of Boltzmann machines
Some good introductions (http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/) on the web

Deep Belief Networks

So far as I can tell, this is a class of multi-layer RBMs for doing semi-supervised learning.
Some resources (http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial)


","Neural networks have evolved over time, from simple perceptrons to complex multi-layer models with back-propagation training. Despite the diverse range of models, Geoffrey Hinton provides a comprehensive roadmap for understanding neural networks.

Historical progression:

* **Perceptrons:** Simple models with limited abilities.
* **Multi-layer, back-propagation trained networks:** Widely used and well-documented, but often underperform compared to SVMs.
* **Boltzmann machines:** Explore network stability in terms of ""energy,"" but are impractical due to slow training.
* **Restricted Boltzmann Machines (RBMs):** Practical models based on Boltzmann machine theory.
* **Deep Belief Networks (DBNs):** Multi-layer RBMs for semi-supervised learning.

For a detailed understanding, it is recommended to refer to Geoffrey Hinton's lectures on Google Tech Talks and Videolectures.net, which provide an intuitive historical narrative connecting the various neural network models."
Why are rectified linear units considered non-linear?,https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear,neural-networks|deep-learning,141960,True,141978,10606,1,34,1536787757,1426521061,41,1426526521,"RELUs are nonlinearities. To help your intuition, consider a very simple network with 1 input unit $x$, 2 hidden units $y_i$, and 1 output unit $z$. With this simple network we could implement an absolute value function,
$$z = \max(0, x) + \max(0, -x),$$
or something that looks similar to the commonly used sigmoid function,
$$z = \max(0, x + 1) - \max(0, x - 1).$$
By combining these into larger networks/using more hidden units, we can approximate arbitrary functions.
$\hskip2in$
","**Summary:**

RELUs (Rectified Linear Units) are non-linear functions that are used as activation functions in artificial neural networks. They introduce non-linearity by thresholding the input at zero, allowing the network to learn complex functions.

By combining multiple RELUs and hidden units, it is possible to approximate a wide range of functions. This is crucial for neural networks to solve complex problems, as they enable the network to model non-linear relationships in the data. The specific combinations of RELUs and hidden units can be used to create activation functions that resemble common mathematical functions like the absolute value function and sigmoid function."
How to train and validate a neural network model in R?,https://stats.stackexchange.com/questions/21717/how-to-train-and-validate-a-neural-network-model-in-r,r|neural-networks,21717,True,21890,79873,1,34,1557411086,1327522887,36,1327796664,"Max Kuhn's caret Manual - Model Building (https://cran.r-project.org/web/packages/caret/vignettes/caret.html) is a great starting point. 
I would think of the validation stage as occurring within the caret train() call, since it is choosing your hyperparameters of decay and size via bootstrapping or some other approach that you can specify via the trControl parameter. I call the data set I use for characterizing the error of the final chosen model my test set. Since caret handles selection of hyperparameters for you, you just need a training set and a test set. 
You can use the createDataPartition() function in caret to split your data set into training and test sets. I tested this using the Prestige data set from the car package, which has information about income as related to level of education and occupational prestige:
library(car)
library(caret)
trainIndex <- createDataPartition(Prestige$income, p=.7, list=F)
prestige.train <- Prestige[trainIndex, ]
prestige.test <- Prestige[-trainIndex, ]

The createDataPartition() function seems a little misnamed because it doesn't create the partition for you, but rather provides a vector of indexes that you then can use to construct training and test sets. It's pretty easy to do this yourself (http://gettinggeneticsdone.blogspot.com/2011/02/split-data-frame-into-testing-and.html) in R using sample() but one thing createDataPartition() apparently does do is sample from within factor levels. Moreover, if your outcome is categorical, the distribution is maintained across the data partitions. It's not relevant in this case, however, since your outcome is continuous. 
Now you can train your model on the training set:
my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 6, 7))
prestige.fit <- train(income ~ prestige + education, data = prestige.train,
    method = ""nnet"", maxit = 1000, tuneGrid = my.grid, trace = F, linout = 1)    

Aside: I had to add the linout parameter to get nnet to work with a regression (vs. classification) problem. Otherwise I got all 1s as predicted values from the model. 
You can then call predict on the fit object using the test data set and calculate RMSE from the results:
prestige.predict <- predict(prestige.fit, newdata = prestige.test)
prestige.rmse <- sqrt(mean((prestige.predict - prestige.test$income)^2)) 

","**Summary:**

Max Kuhn's Caret Manual provides guidance on model building. The validation phase occurs within the Caret train() function, where hyperparameters are optimized using a technique like bootstrapping.

To split data into training and test sets, the createDataPartition() function can be used. It ensures that data is sampled from within factor levels and maintains the distribution of outcome variables.

For a regression model, the training set is used to train the model using a grid search approach. The trained model is then evaluated on the test set to calculate the root mean squared error (RMSE)."
What are attention mechanisms exactly?,https://stats.stackexchange.com/questions/344508/what-are-attention-mechanisms-exactly,time-series|deep-learning|lstm|recurrent-neural-network|attention,344508,True,345441,5818,1,34,1552529435,1525454446,30,1525927244,"Attention is a method for aggregating a set of vectors $v_i$ into just one vector, often via a lookup vector $u$. Usually, $v_i$ is either the inputs to the model or the hidden states of previous time-steps, or the hidden states one level down (in the case of stacked LSTMs). 
The result is often called the context vector $c$, since it contains the context relevant to the current time-step. 
This additional context vector $c$ is then fed into the RNN/LSTM as well (it can be simply concatenated with the original input). Therefore, the context can be used to help with prediction. 
The simplest way to do this is to compute probability vector $p = \text{softmax}(V^Tu)$ and $c = \sum_i p_i v_i$ where $V$ is the concatenation of all previous $v_i$. A common lookup vector $u$ is the current hidden state $h_t$.
There are many variations on this, and you can make things as complicated as you want. For example, instead using $v_i^T u$ as the logits, one may choose $f(v_i, u)$ instead, where $f$ is an arbitrary neural network. 
A common attention mechanism for sequence-to-sequence models uses $p = \text{softmax}(q^T \tanh(W_1 v_i + W_2 h_t))$, where $v$ are the hidden states of the encoder, and $h_t$ is the current hidden state of the decoder. $q$ and both $W$s are parameters.
Some papers which show off different variations on the attention idea:
Pointer Networks (https://arxiv.org/abs/1506.03134) use attention to reference inputs in order to solve combinatorial optimization problems.
Recurrent Entity Networks (https://arxiv.org/abs/1612.03969) maintain separate memory states for different entities (people/objects) while reading text, and update the correct memory state using attention.
Transformer (https://arxiv.org/pdf/1706.03762.pdf) models also make extensive use of attention. Their formulation of attention is slightly more general and also involves key vectors $k_i$: the attention weights $p$ are actually computed between the keys and the lookup, and the context is then constructed with the $v_i$.

Here is a quick implementation of one form of attention, although I can't guarantee correctness beyond the fact that it passed some simple tests.
Basic RNN:
def rnn(inputs_split):
    bias = tf.get_variable('bias', shape = [hidden_dim, 1])
    weight_hidden = tf.tile(tf.get_variable('hidden', shape = [1, hidden_dim, hidden_dim]), [batch, 1, 1])
    weight_input = tf.tile(tf.get_variable('input', shape = [1, hidden_dim, in_dim]), [batch, 1, 1])

    hidden_states = [tf.zeros((batch, hidden_dim, 1), tf.float32)]
    for i, input in enumerate(inputs_split):
        input = tf.reshape(input, (batch, in_dim, 1))
        last_state = hidden_states[-1]
        hidden = tf.nn.tanh( tf.matmul(weight_input, input) + tf.matmul(weight_hidden, last_state) + bias )
        hidden_states.append(hidden)
    return hidden_states[-1]

With attention, we add only a few lines before the new hidden state is computed:
        if len(hidden_states) > 1:
            logits = tf.transpose(tf.reduce_mean(last_state * hidden_states[:-1], axis = [2, 3]))
            probs = tf.nn.softmax(logits)
            probs = tf.reshape(probs, (batch, -1, 1, 1))
            context = tf.add_n([v * prob for (v, prob) in zip(hidden_states[:-1], tf.unstack(probs, axis = 1))])
        else:
            context = tf.zeros_like(last_state)

        last_state = tf.concat([last_state, context], axis = 1)

        hidden = tf.nn.tanh( tf.matmul(weight_input, input) + tf.matmul(weight_hidden, last_state) + bias )

the full code (https://gist.github.com/ricsonc/0ac6d0b9fd13c42c1c154eb4ccd7fab3)
","Attention is a mechanism that merges multiple vectors into a single context vector. This context vector incorporates relevant information from the input or hidden states of a model, enhancing predictions.

Attention can be implemented in various ways. One common method uses a lookup vector to compute weights for each vector, resulting in a probability vector. The context vector is then calculated as a weighted sum of the input or hidden state vectors. More complex approaches involve using arbitrary neural networks or incorporating key vectors.

Attention has been used in various applications, including combinatorial optimization (Pointer Networks), entity recognition (Recurrent Entity Networks), and sequence-to-sequence modeling (Transformer).

In a simple RNN example, attention is implemented by computing weights for previous hidden states. These weights are used to create a context vector that is concatenated with the current hidden state before computing the new hidden state. This process enhances the model's predictive capabilities by incorporating relevant context from previous time steps."
"Why are there no deep reinforcement learning engines for chess, similar to AlphaGo?",https://stats.stackexchange.com/questions/308777/why-are-there-no-deep-reinforcement-learning-engines-for-chess-similar-to-alpha,neural-networks|deep-learning|reinforcement-learning|games,308777,True,308784,23307,2,33,1512975104,1508398691,52,1508401596,"EDIT (after reading the paper):
I've read the paper thoughtfully. Let's start off with what Google claimed in the paper:

They defeated Stockfish with Monte-Carlo-Tree-Search + Deep neural networks
The match was absolutely one-sided, many wins for AlphaZero but none for Stockfish
They were able to do it in just four hours
AlphaZero played like a human

Unfortunately, I don't think it's a good journal paper. I'm going to explain with links (so you know I'm not dreaming):

https://chess.stackexchange.com/questions/19360/how-is-alpha-zero-more-human (https://chess.stackexchange.com/questions/19360/how-is-alpha-zero-more-human) has my answer on how AlphaZero played like a human
The match was unfair, strongly biased. I quote Tord Romstad, the original programmer for Stockfish.


https://www.chess.com/news/view/alphazero-reactions-from-top-gms-stockfish-author (https://www.chess.com/news/view/alphazero-reactions-from-top-gms-stockfish-author)
The match results by themselves are not particularly meaningful because of the rather strange choice of time controls and Stockfish parameter settings: The games were played at a fixed time of 1 minute/move, which means that Stockfish has no use of its time management heuristics (lot of effort has been put into making Stockfish identify critical points in the game and decide when to spend some extra time on a move; at a fixed time per move, the strength will suffer significantly).

Stockfish couldn't have played the best chess with only a minute per move. The program was not designed for that.

Stockfish was running on a regular commercial machine, while AlphaZero was on a 4 millions+ TPU machine tuned for AlphaZero. This is a like matching your high-end desktop against a cheap Android phone. Tord wrote:


One is a conventional chess program running on ordinary computers, the other uses fundamentally different techniques and is running on custom designed hardware that is not available for purchase (and would be way out of the budget of ordinary users if it were).


Google inadvertently gave 64 threads to a 32 core machine for Stockfish. I quote GM Larry Kaufman (world class computer chess expert):


http://talkchess.com/forum/viewtopic.php?p=741987&highlight=#741987 (http://talkchess.com/forum/viewtopic.php?p=741987&highlight=#741987)
I agree that the test was far from fair; another issue that hurt SF was that it was apparently run on 64 threads on a 32 core machine, but it would play much better running just 32 threads on that machine, since there is almost no SMP benefit to offset the roughly 5 to 3 slowdown. Also the cost ratio was more than I said; I was thinking it was a 64 core machine, but a 32 core machine costs about half what I guessed. So maybe all in all 30 to 1 isn't so bad an estimate. On the other hand I think you underestimate how much it could be further improved. 


Stockfish gave only 1GB hash table. This is a joke... I have a larger hash table for my Stockfish iOS app (http://www.smallchess.com/) (Disclaimer: I'm the author) on my iPhone! Tord wrote:


... way too small hash tables for the number of threads ...


1GB hash table is absolutely unacceptable for a match like this. Stockfish would frequently encounter hash collision. It takes CPU cycles to replace old hash entries.

Stockfish is not designed to run with that many number of threads. In my iOS chess app, only a few threads are used. Tord wrote:


... was playing with far more search threads than has ever received any significant amount of testing ...


Stockfish was running without an opening book or 6-piece Syzygy endgame tablebase. The sample size was insufficient. The Stockfish version was not the latest. Discussion here (http://talkchess.com/forum/viewtopic.php?t=65909&postdays=0&postorder=asc&highlight=syzygy%20stockfish&topic_view=flat&start=10).

CONCLUSION
Google has not proven without doubts their methods are superior to Stockfish. Their numbers are superficial and strongly biased to AlphaZero. Their methods are not reproducible by an independent third party. It's still a bit too early to say Deep Learning is a superior method to traditional chess programming.

EDIT (Dec 2017):
There is a new paper from Google Deepmind (https://arxiv.org/pdf/1712.01815.pdf (https://arxiv.org/pdf/1712.01815.pdf)) for deep reinforcement learning in chess. From the abstract, the world number one Stockfish chess engine was ""convincingly"" defeated. I think this is the most significant achievement in computer chess since the 1997 Deep Blue match. I'll update my answer once I read the paper in details.

Original (before Dec 2017)
Let's clarify your question:

No, chess engines don't use brute-force.
AlphaGo does use tree searching, it uses Monte Carlo Tree Search. Google ""Monte Carlo Tree Search alphaGo"" if you want to be convinced.

ANN can be used for chess engines:

Giraffe (https://chessprogramming.wikispaces.com/Giraffe) (the link posted by @Tim)
NeuroChess (https://chessprogramming.wikispaces.com/NeuroChess)


Would this program perform better than the top chess-engines (and chess players) of today?

Giraffe plays at about Internation Master level, which is about FIDE 2400 rating. However, Stockfish, Houdini and Komodo all play at about FIDE 3000. This is a big gap. Why? Why not Monte-Carlo Tree Search?

Material heuristic in chess is simple. Most of the time, a chess position is winning/losing by just counting materials on the board. Please recall counting materials doesn't work for Go. Material counting is orders of magnitude faster than running neural networks - this can be done by bitboards represented by a 64-bit integer. On the 64 bits system, it can be done by only several machine instructions. Searching with the traditional algorithm is much faster than machine learning. Higher nodes per second translate to deeper search.
Similarly, there're very useful and cheap techniques such as null move pruning, late move reduction and killer moves etc. They are cheap to run, and much efficient to the approach used in AlphaGo.
Static evaluation in chess is fast and useful
Machine learning is useful for optimizating parameters, but we also have SPSA and CLOP for chess.
There are lots of useful metrics for tree reduction in chess. Much less so for Go.

There was research that Monte Carlo Tree Search don't scale well for chess. Go is a different game to chess. The chess algorithms don't work for Go because chess relies on brutal tactics. Tactics is arguably more important in chess.
Now, we've established that MCTS work well for AlphaGo but less so for chess. Deep learning would be more useful if:

The tuned NN evaluation is better than the traditional algorithms. However ... deep learning is not magic, you as the programmer would still need to do the programming. As mentioned, we have something like SPSA for self-playing for parameters tuning in chess.
Investment, money! There's not much money for machine learning in chess. Stockfish is free and open source, but strong enough to defeat all human players. Why would Google spend millions if anybody can just download Stockfish for free? Why's going to pay for the CPU clusters? Who's going to pay for talents? Nobody wants to do it, because chess is considered a ""solved"" game.

If deep learning can achieve the following, it'll beat the traditional algorithm:

Given a chess position, ""feel"" it like a human grandmaster. For example, a human grandmaster wouldn't go into lines that are bad - by experience. Neither the traditional algorithm nor deep learning can achieve that. Your NN model might give you a probability [0..1] for your position, but that's not good enough.

Let me point out:

No. Giraffe (the link posted by @Tim) doesn't use Monte Carlo Tree Search. It uses the regular nega-max algorithm. All it does is replace the regular evaluation function with NN, and it's very slow.

one more:

Although Kasparov was beaten by Deep Blue in the 1997 match. ""Humanity"" was really lost around 2003-2005, when Kramnik lost a match to Deep Fritz without a win and Michael Adams lost to a cluster machine in a one-sided match. Around that time, Rybka proved too strong for even the best players in the world.

Reference:

http://www.talkchess.com/forum/viewtopic.php?t=64096&postdays=0&postorder=asc&highlight=alphago+chess&topic_view=flat&start=0 (http://www.talkchess.com/forum/viewtopic.php?t=64096&postdays=0&postorder=asc&highlight=alphago+chess&topic_view=flat&start=0)

I quote:

In chess we have the concept of materiality which already gives a resonable estimation of how well an engine is doing and can be computed quickly. Furthermore, there a lot of other aspects of the game that can be encoded in a static evaluation function which couldn`t be done in Go. Due to the many heuristics and good evaluation, the EBF (Effective-Branching-Factor) is quite small. Using a Neural Network as a replacement for the static evaluation function would definently slow down the engine by quite a lot. 

","**Summary:**

A research paper by Google DeepMind claims that their program, AlphaZero, defeated the top-ranked chess engine, Stockfish, using a combination of Monte-Carlo-Tree-Search and deep neural networks. However, there are concerns raised about the fairness and significance of the match.

**Concerns:**

* The match was biased in AlphaZero's favor due to:
    * Different time controls (1 minute per move for Stockfish vs. no time limit for AlphaZero)
    * Stockfish running on a regular machine while AlphaZero used custom hardware
    * Stockfish given only 1GB of hash table space
    * Stockfish not fully optimized for the conditions

* The sample size was insufficient, and the latest version of Stockfish was not used.

**Assessment of AlphaZero vs. Traditional Chess Engines:**

* Material heuristic is simpler and faster in chess compared to Go.
* Chess engines have efficient techniques like null move pruning and killer moves.
* Static evaluation is faster and more effective than deep neural networks for chess.

**Limitations of Machine Learning in Chess:**

* Deep neural networks are not inherently superior to traditional chess algorithms.
* Tuning neural networks requires significant investment of resources.
* Machine learning cannot replicate the ""feel"" of a human grandmaster.

**Conclusion:**

While AlphaZero's victory is impressive, the fairness of the match has been questioned, and it is too early to conclude that deep learning is superior to traditional chess programming. Further research and unbiased testing are needed to determine the true potential of deep learning in chess."
Why second order SGD convergence methods are unpopular for deep learning?,https://stats.stackexchange.com/questions/394083/why-second-order-sgd-convergence-methods-are-unpopular-for-deep-learning,neural-networks|optimization|convergence|gradient-descent|stochastic-gradient-descent,394083,True,394108,13489,2,33,1686779623,1550998678,28,1551008896,"
Should we go toward second order methods for deep learning?

TL;DR: No, especially now when the pace of innovation is slowing down, and we're seeing less new architectural innovations, and more ways to train what are basically just copies of existing architectures, on larger datasets (see OpenAI's GPT-2).
First, without even getting to second order, it's worth mentioning that in Deep Learning you don't even use (mini-batch) gradient descent to the fullest of its potential (i.e., you don't perform line search), because the optimization step in line search would be very costly. 
Second, second order methods are:

way more complex, i.e., harder to implement without bugs. DL systems are increasingly becoming a small part of huge data processing pipelines. Introducing further complexity and brittleness in a complex system is only wise if the gains largely offset the risks. I'll argue below that they don't.
harder to optimize for distributed computing on heterogeneous hardware, which is becoming more and more common. See how much work was required in order to make K-FAC work on distributed (non heterogeneous) systems, and performances are still no better than the best first-order methods: https://arxiv.org/pdf/1811.12019.pdf (https://arxiv.org/pdf/1811.12019.pdf). Instead, if just switching to distributed computing makes my first-order method as fast as, or faster, than second-order methods, I don't see the reason to use a more complicated optimization algorithm.
way more expensive in terms of iteration cost (not number) and memory occupation, thus they introduce a considerable overhead. Current architectures (GPUs) are more memory-bound that computation-bound. As explained very well here (https://stats.stackexchange.com/questions/320082/why-not-use-the-third-derivative-for-numerical-optimization), the increase in iteration cost and memory occupation is steeper, the more high-dimensional the problem is. Optimization in Deep Learning is arguably one of the most high-dimensional optimization problems, so it's not clear that second order methods would have a clear advantage in terms of computational time (not iteration count, which is not what we really care about) wrt first-order methods.
another issue with Deep Learning optimization are saddle points. It's becoming abundantly clear that ""bad"" local minima are not an issue in Deep Learning, but saddle points are. Newton's method does have a tendency to be attracted to saddle points (https://stats.stackexchange.com/a/301728/58675). If I remember correctly, Hessian approximating methods such as K-FAC don't have this issue, but I think the proof depends on the type of architecture, making the use of such methods brittle.
they don't fix the problems which make practitioners waste most of their time. Dead or saturated units are not solved by K-FAC, but by better initialization schemes, so that's what we should focus on, e.g., Fixup: https://arxiv.org/abs/1901.09321 (https://arxiv.org/abs/1901.09321)
another issue with second order methods is that for most common loss functions, it's easy to use mini-batches to get an estimator which converges to the actual gradient. It is much more complicated to build a sampling-based estimator for the approximation to the inverse of the Hessian. In other words, second order methods introduce a lot of complexity and extra memory occupation, but stochastic second order methods introduce even more complexity. Contrast that with stochastic first order methods, where the algorithm is just slightly more complicated than that of deterministic first order methods.
finally, they have a lot of moving parts, which are difficult to tune up in the best way. Your same paper leaves a lot of details to be specified. Do we need even more extra hyperparameters, or do we need robust optimization methods? Keep in mind that in Deep Learning, as explained very well by Shai Shalev-Shwartz, when something goes wrong, it's very difficult to understand how to fix it https://www.youtube.com/watch?v=1nvf_DBnsxo (https://www.youtube.com/watch?v=1nvf_DBnsxo) and more hyperparameters don't help in that respect.

","Despite advances in deep learning, second-order methods are not recommended at this stage due to various drawbacks.

First, current deep learning practices do not fully utilize first-order methods, so transitioning to second order is premature. Second-order methods pose challenges, including increased complexity, difficulty in optimizing for distributed computing, higher computational costs, and susceptibility to saddle points.

Additionally, these methods do not address common issues like dead or saturated units, which can be resolved through better initialization strategies. Furthermore, constructing sampling-based estimators for second-order methods is more complex than for first-order methods. Finally, second-order methods introduce extra hyperparameters, complicating tuning and potentially hindering troubleshooting.

In summary, given the current state of deep learning, the benefits of second-order methods do not outweigh their drawbacks, making them a less desirable choice for optimization. Instead, improving first-order methods and addressing practical challenges through techniques like Fixup are more promising avenues for progress."
What did my neural network just learn? What features does it care about and why?,https://stats.stackexchange.com/questions/322639/what-did-my-neural-network-just-learn-what-features-does-it-care-about-and-why,neural-networks|deep-learning,322639,True,322665,5315,2,33,1644033549,1515690049,30,1515696180,"It is true that it's hard to understand what a neural network is learning but there has been a lot of work on that front. We definitely can get some idea of what our network is looking for.
Let's consider the case of a convolutional neural net for images. We have the interpretation for our first layer that we are sliding $K$ filters over the image, so our first hidden layer corresponds to the agreement between small chunks of the image and our various filters. We can visualize these filters to see what our first layer of representation is:
 (https://i.sstatic.net/fPjH3.png)
This picture is of the first layer of filters from an AlexNet (https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) and is taken from this wonderful tutorial: http://cs231n.github.io/understanding-cnn/ (http://cs231n.github.io/understanding-cnn/). This lets us interpret the first hidden layer as learning to represent the image, consisting of raw pixels, as a tensor where each coordinate is the agreement of a filter with a small region of the image. The next layer then is working with these filter activations. 
It's not so hard to understand the first hidden layer because we can just look at the filters to see how they behave, because they're directly applied to an input image. E.g. let's say you're working with a black and white image (so our filters are 2D rather than 3D) and you have a filter that's something like
$$
\begin{bmatrix}0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0\end{bmatrix}.
$$
Imagine applying this to a 3x3 region of an image (ignoring the bias term). If every pixel was the same color then you'd get $0$ since they'd cancel out. But if the upper half is different from the lower half, say, then you'll get a potentially large value. This filter, in fact, is an edge detector, and we can figure that out by actually just applying it to images and seeing what happens. But it's a lot harder to understand the deeper layers because the whole problem is we don't know how to interpret what we're applying the filters to.
This paper (https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network) by Erhan et al (2009) agrees with this: they say that first hidden layer visualizations are common (and that was back in 2009) but visualizing the deeper layers is the hard part. From that paper:

The main experimental finding of this investigation is very
  surprising: the response of an internal unit to input images, as a
  function in image space, appears to be unimodal, or at least that the
  maximum is found reliably and consistently for all the random
  initializations tested. This is interesting because finding this
  dominant mode is relatively easy, and displaying it then provides a
  good characterization of what the unit does.

Chris Olah et al (https://distill.pub/2017/feature-visualization/ (https://distill.pub/2017/feature-visualization/)) build on this and discuss how in general you can (1) generate images that lead to large activations in order to get a sense of what the network is looking for; or (2) take actual input images and see how different parts of the image activate the network. That post focuses on (1).
In the image below, taken from that linked article by Olah et al., the authors discuss the different aspects of the network that you can inspect. The left-most image shows the result of optimizing the activation of a particular neuron over the input image space, and so on. 
 (https://i.sstatic.net/2YxfR.png)
I would highly recommend reading that article in its entirety if you want a deeper understanding of this, and by reading its references you should have a great grasp of what's been done with this.
Now of course this was all just for images where we as humans can make sense of the inputs. If you're working with something harder to interpret, like just a big vector of numbers, then you may not be able to make such cool visualizations, but in principle you could still consider these techniques for assessing the various neurons, layers, and etc.
","Understanding what neural networks learn is challenging. However, there are techniques to gain insights into their behavior.

**First Hidden Layer Visualization**

In convolutional neural networks for image classification, the first hidden layer consists of filter activations. Each filter corresponds to a specific pattern or feature in the image. By visualizing these filters, we can interpret the network's representation of the input.

**Understanding Deeper Layers**

Visualizing deeper layers is more difficult as the network's operations become more complex. However, researchers have developed methods to extract insights using:

* **Activation Maximization:** Generating images that maximize the activation of specific neurons, revealing what the network is sensitive to.
* **Layer Activation Visualization:** Examining how different parts of input images activate the network.

**Extending to Non-Image Data**

While these visualization techniques are primarily used for images, they can be applied to other types of data. However, the interpretation of the results may be more challenging when the input data is not easily human-interpretable.

**Summary**

Understanding neural network behavior is an ongoing challenge. However, visualization techniques provide insights into the first hidden layer and deeper layers, enabling researchers to assess how networks process and represent data. These methods can also be extended to non-image data, although interpretation may be more complex."
Why Not Prune Your Neural Network?,https://stats.stackexchange.com/questions/474192/why-not-prune-your-neural-network,machine-learning|neural-networks|optimization|pruning,474192,True,474203,4904,3,32,1593225695,1593183435,43,1593185385,"Pruning is indeed remarkably effective and I think it is pretty commonly used on networks which are ""deployed"" for use after training.
The catch about pruning is that you can only increase efficiency, speed, etc. after training is done. You still have to train with the full size network. Most computation time throughout the lifetime of a model's development and deployment is spent during development: training networks, playing with model architectures, tweaking parameters, etc. You might train a network several hundred times before you settle on the final model. Reducing computation of the deployed network is a drop in the bucket compared to this.
Among ML researchers, we're mainly trying to improve training techniques for DNN's. We usually aren't concerned with deployment, so pruning isn't used there.
There is some research on utilizing pruning techniques to speed up network training, but not much progress has been made. See, for example, my own paper from 2018 which experimented with training on pruned and other structurally sparse NN architectures: https://arxiv.org/abs/1810.00299 (https://arxiv.org/abs/1810.00299)
","**Summary:**

Pruning is a technique that improves efficiency and speed of neural networks after training. However, it is typically not used during training, which accounts for most of a model's development time.

ML researchers primarily focus on improving training techniques and are less concerned with deployment, where pruning is beneficial.

Some research explores the use of pruning to accelerate training, but progress in this area has been limited."
Is logistic regression a specific case of a neural network?,https://stats.stackexchange.com/questions/500948/is-logistic-regression-a-specific-case-of-a-neural-network,neural-networks|logistic|classification,500948,True,500973,14685,3,32,1608225234,1608041213,52,1608047866,"You have to be very specific about what you mean. We can show mathematically that a certain neural network architecture trained with a certain loss coincides exactly with logistic regression at the optimal parameters. Other neural networks will not.
A binary logistic regression makes predictions $\hat{y}$ using this equation:
$$
\hat{y}=\sigma(X \beta + \beta_0)
$$
where $X$ is a $n \times p$ matrix of features (predictors, independent variables) and vector $\beta$ is the vector of $p$ coefficients and $\beta_0$ is the intercept and $\sigma(z)=\frac{1}{\exp(-z)+1}$. Conventionally in a logistic regression, we would roll the $\beta_0$ scalar into the vector $\beta$ and append a column of 1s to $X$, but I've moved it out of $\beta$ for clarity of exposition.
A neural network with no hidden layers and one output neuron with a sigmoid activation makes predictions using the equation
$$
\hat{y}=\sigma(X \beta + \beta_0)
$$
with $\hat{y},\sigma,X, \beta, \beta_0$ as before. Clearly, the equation is exactly the same. In the neural-networks (/questions/tagged/neural-networks) literature, $\beta_0$ is usually called a ""bias,"" even though it has nothing to do with the statistical concept of bias (/questions/tagged/bias). Otherwise, the terminology is identical.
A logistic regression has the Bernoulli likelihood as its objective function, or, equivalently, the Bernoulli log-likelihood function. This objective function is maximized:
$$
\arg\max_{\beta,\beta_0} \sum_i \left[ y_i \log(\hat{y_i}) + (1-y_i)\log(1-\hat{y_i})\right]
$$ where $y \in \{0,1\}$.
We can motivate this objective function from a Bernoulli probability model where the probability of success depends on $X$.
A neural network can, in principle, use any loss function we like. It might use the so-called ""cross-entropy"" function (even though the ""cross-entropy"" can motivate any number of loss functions; see How to construct a cross-entropy loss for general regression targets? (https://stats.stackexchange.com/questions/378274/how-to-construct-a-cross-entropy-loss-for-general-regression-targets)), in which case the model minimizes this loss function:
$$
\arg\min_{\beta,\beta_0} -\sum_i \left[ y_i \log(\hat{y_i}) + (1-y_i)\log(1-\hat{y_i})\right]
$$
In both cases, these objective functions are strictly convex (concave) when certain conditions are met. Strict convexity implies that there is a single minimum and that this minimum is a global. Moreover, the objective functions are identical, since minimizing a strictly convex function $f$ is equivalent to maximizing $-f$. Therefore, these two models recover the same parameter estimates $\beta, \beta_0$. As long as the model attains the single optimum, it doesn't matter what optimizer is used, because there is only one optimum for these specific models.
However, a neural network is not required to optimize this specific loss function; for instance, a triplet-loss (/questions/tagged/triplet-loss) for this same model would likely recover different estimates $\beta,\beta_0$. And the MSE/least squares loss is not convex in this problem, so that neural network would differ from logistic regression as well (see: What is happening here, when I use squared loss in logistic regression setting? (https://stats.stackexchange.com/questions/326350/what-is-happening-here-when-i-use-squared-loss-in-logistic-regression-setting)).
","Under specific conditions, a neural network with a single output neuron and sigmoid activation coincides with logistic regression, both mathematically and in terms of predictions. Both models use the same linear equation, with identical coefficients and intercept.

The key factor is the choice of loss function. If the neural network is trained with the Bernoulli log-likelihood function, which is maximized in logistic regression, the two models recover the same parameter estimates. This is because the loss function is strictly convex, ensuring a single global minimum.

However, if the neural network uses a different loss function, such as a triplet loss or MSE loss, the parameter estimates will likely differ from logistic regression. Additionally, the MSE loss is not convex in this setting, further distinguishing the neural network from logistic regression."
Why use gradient descent with neural networks?,https://stats.stackexchange.com/questions/181629/why-use-gradient-descent-with-neural-networks,neural-networks|gradient-descent|backpropagation,181629,True,181630,13738,3,32,1499516071,1447426024,39,1447426533,"
Because we can't. The optimization surface $S(\mathbf{w})$ as a function of the weights $\mathbf{w}$ is nonlinear and no closed form solution exists for $\frac{d S(\mathbf{w})}{d\mathbf{w}}=0$.
Gradient descent, by definition, descends. If you reach a stationary point after descending, it has to be a (local) minimum or a saddle point, but never a local maximum.

","**Summary:**

Finding an optimal solution for complex functions, such as $S(\mathbf{w})$, is challenging due to the nonlinear nature of its optimization surface. This means there is no straightforward way to find points where the derivative of the function is zero.

Gradient descent, a common optimization technique, is designed to minimize the function. As a result, any stationary point reached after using gradient descent must be a local minimum or a saddle point. This means that gradient descent cannot find local maxima, which are desirable points for some optimization problems."
How to use early stopping properly for training deep neural network?,https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network,neural-networks|deep-learning,231061,True,231118,70598,2,32,1497206725,1471860116,23,1471882669,"
What would be a good validation frequency? Should I check my model on the validation data at the end of each epoch? (My batch size is 1)

There is no gold rule, computing the validation error after each epoch is quite common. Since your validation set much smaller than your training set, it will not slow down the training much.

Is it the case that the first few epochs might yield worse result before it starts converging to better value? 

yes

In that case, should we train our network for several epochs before checking for early stopping?

You could, but then the issue is how many epochs should you skip. So in practice, most of the time people do not skip any epoch.

How to handle the case when the validation loss might go up and down? In that case, early stopping might prevent my model from learning further, right?

People typically define a patience, i.e. the number of epochs to wait before early stop if no progress on the validation set. The patience is often set somewhere between 10 and 100 (10 or 20 is more common), but it really depends on your dataset and network. 
Example with patience = 10:
 (https://i.sstatic.net/No38I.png)
","**Summary:**

**Validation Frequency:**

* Computing the validation error after each epoch is common, as it has minimal impact on training speed due to the relatively small validation set size.

**Convergence and Early Stopping:**

* Early epochs may exhibit worse results before converging.
* Skipping epochs before early stopping is not recommended.
* Patience (number of epochs without progress) for early stopping should be set, typically within 10-100 epochs.

**Validation Loss Fluctuations:**

* Validation loss may fluctuate during training.
* Patience allows the model to recover from temporary setbacks."
Why is softmax function used to calculate probabilities although we can divide each value by the sum of the vector?,https://stats.stackexchange.com/questions/419751/why-is-softmax-function-used-to-calculate-probabilities-although-we-can-divide-e,machine-learning|neural-networks|softmax,419751,True,419753,7581,3,31,1599238521,1564448779,51,1564449198,"The function you propose has a singularity whenever the sum of the elements is zero.
Suppose your vector is $[-1, \frac{1}{3}, \frac{2}{3}]$. This vector has a sum of 0, so division is not defined. The function is not differentiable here.
Additionally, if one or more of the elements of the vector is negative but the sum is nonzero, your result is not a probability.
Suppose your vector is $[-1, 0, 2]$. This has a sum of 1, so applying your function results in $[-1, 0, 2]$, which is not a probability vector because it has negative elements, and elements exceeding 1.
Taking a wider view, we can motivate the specific form of the softmax function from the perspective of extending binary logistic regression to the case of three or more categorical outcomes.
Doing things like taking absolute values or squares, as suggested in comments, means that $-x$ and $x$ have the same predicted probability; this means the model is not identified (https://stats.stackexchange.com/questions/349418/definition-of-softmax-function). By contrast, $\exp(x)$ is monotonic and positive for all real $x$, so the softmax result is (1) a probability vector and (2) the multinomial logistic model is identified.
","**Summary:**

The proposed function faces challenges when applied to vectors with specific characteristics:

* **Singularity**: When the sum of the vector elements is zero, division becomes undefined, and the function is not differentiable.
* **Non-probabilities**: If elements are negative or exceed 1, the function may not produce a valid probability vector.

Motivated by extending binary logistic regression, the softmax function has been designed to address these issues:

* It ensures positivity and monotonicity, preventing cases where elements with opposite signs have the same predicted probability.
* It consistently generates probability vectors, ensuring elements sum to 1 and fall within the [0, 1] range.

In contrast, alternatives like absolute values or squares fail to maintain these properties, leading to potential identification issues."
Why doesn&#39;t backpropagation work when you initialize the weights the same value?,https://stats.stackexchange.com/questions/45087/why-doesnt-backpropagation-work-when-you-initialize-the-weights-the-same-value,machine-learning|neural-networks|backpropagation,45087,True,45092,12527,2,31,1443816559,1354623902,34,1354626939,"Symmetry breaking. 
If all weights start with equal values and if the solution requires that unequal weights be developed, the system can never learn.
This is because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units must always be the same. The system is starting out at a kind of unstable equilibrium point that keeps the weights equal, but it is higher than some neighboring points on the error surface, and once it moves away to one of these points, it will never return. We counteract this problem by starting the system with small random weights. Under these conditions symmetry problems of this kind do not arise. 
","**Summary:**

Symmetry breaking occurs when a system with initially equal weights cannot learn if unequal weights are needed for the solution. This is because error signals are propagated back through weights proportionally, resulting in identical error signals for hidden units connected to output units. Consequently, the weights from these hidden units to the output units remain equal, preventing the system from breaking symmetry.

To avoid this problem, the system is initialized with small random weights. This introduces asymmetry, allowing the error signals to propagate differently through the weights. As a result, different weights can be developed, enabling the system to escape the unstable equilibrium point where symmetry prevents learning."
Do neural networks learn a function or a probability density function?,https://stats.stackexchange.com/questions/347431/do-neural-networks-learn-a-function-or-a-probability-density-function,machine-learning|neural-networks,347431,True,347435,15898,3,31,1668467402,1526926649,27,1526928393,"Strictly speaking, neural networks are fitting a non-linear function. 
They can be interpreted as fitting a probability density function if suitable activation functions are chosen and certain conditions are respected (Values must be positive and $\leq$ 1, etc...). But that is a question of how you choose to interpret their output, not of what they are actually doing. Under the hood, they are still non-linear function estimators, which you are choosing to apply to the specific problem of PDF estimation. 
","**Summary:**

Neural networks are mathematical models that fit non-linear functions to data. While they can be used to estimate probability density functions (PDFs) by choosing appropriate activation functions and conditions, this is an interpretation of their output rather than their primary function. Fundamentally, neural networks remain estimators of non-linear functions that can be applied to various tasks, including PDF estimation."
Deep learning : How do I know which variables are important?,https://stats.stackexchange.com/questions/261008/deep-learning-how-do-i-know-which-variables-are-important,machine-learning|neural-networks|bias|tensorflow|theano,261008,True,261012,41752,7,30,1685727339,1486680632,20,1486682012,"What you describe is indeed one standard way of quantifying the importance of neural-net inputs.  Note that in order for this to work, however, the input variables must be normalized in some way.  Otherwise weights corresponding to input variables that tend to have larger values will be proportionally smaller.  There are different normalization schemes, such as for instance subtracting off a variable's mean and dividing by its standard deviation.  If the variables weren't normalized in the first place, you could perform a correction on the weights themselves in the importance calculation, such as multiplying by the standard deviation of the variable.
$I_i = \sigma_i\sum\limits_{j = 1}^{n_\text{hidden}}\left|w_{ij}\right|$.
Here $\sigma_i$ is the standard deviation of the $i$th input, $I_i$ is the $i$th input's importance, $w_{ij}$ is the weight connecting the $i$th input to the $j$th hidden node in the first layer, and $n_\text{hidden}$ is the number of hidden nodes in the first layer.
Another technique is to use the derivative of the neural-net mapping with respect to the input in question, averaged over inputs.
$I_i = \sigma_i\left\langle\left|\frac{dy}{dx_i}\right|\right\rangle$
Here $x_i$ is the $i$th input, $y$ is the output, and the expectation value is taken with respect to the vector of inputs $\mathbf{x}$.
","**Summary:**

To determine the importance of neural network inputs, various methods exist. One common approach involves quantifying the weights between input nodes and hidden nodes. However, this requires normalized input variables, as larger variable ranges can bias the weights. One normalization method is subtracting the mean and dividing by the standard deviation. Alternatively, weights can be adjusted using the standard deviation.

Another technique measures input importance via the derivative of the neural network mapping with respect to the input, averaged over all inputs. This method also requires normalized inputs.

Both approaches provide insights into the relative contributions of input variables to the neural network's output. The normalization step ensures that input variables with different scales are evaluated fairly."
Multi-layer perceptron vs deep neural network,https://stats.stackexchange.com/questions/315402/multi-layer-perceptron-vs-deep-neural-network,neural-networks|perceptron,315402,True,315411,42753,3,30,1551008339,1511503933,29,1511507795,"One can consider multi-layer perceptron (MLP) to be a subset of deep neural networks (DNN), but are often used interchangeably in literature.
The assumption that perceptrons are named based on their learning rule is incorrect. The classical ""perceptron update rule"" is one of the ways that can be used to train it. The early rejection of neural networks was because of this very reason, as the perceptron update rule was prone to vanishing and exploding gradients, making it impossible to train networks with more than a layer.
The use of back-propagation in training networks led to using alternate squashing activation functions such as tanh and sigmoid.
So, to answer the questions,

the question is. Is a ""multi-layer perceptron"" the same thing as a ""deep neural network""?

MLP is subset of DNN. While DNN can have loops and MLP are always feed-forward, i.e.,
A multi layer perceptrons (MLP)is a finite acyclic graph

why is this terminology used?

A lot of the terminologies used in the literature of science has got to do with trends of the time and has caught on.

How broad is this terminology? Would one use the term ""multi-layered perceptron"" when referring to, for example, Inception net? How about for a recurrent network using LSTM modules used in NLP?

So, yes inception, convolutional network, resnet etc are all MLP because there is no cycle between connections. Even if there is a shortcut connections skipping layers, as long as it is in forward direction, it can be called a multilayer perceptron. But, LSTMs, or Vanilla RNNs etc have cyclic connections, hence cannot be called MLPs but are a subset of DNN.
This is my understanding of things. Please correct me if I am wrong.
Reference Links:
https://cs.stackexchange.com/questions/53521/what-is-difference-between-multilayer-perceptron-and-multilayer-neural-network (https://cs.stackexchange.com/questions/53521/what-is-difference-between-multilayer-perceptron-and-multilayer-neural-network)
https://en.wikipedia.org/wiki/Multilayer_perceptron (https://en.wikipedia.org/wiki/Multilayer_perceptron)
https://en.wikipedia.org/wiki/Perceptron (https://en.wikipedia.org/wiki/Perceptron)
http://ml.informatik.uni-freiburg.de/former/_media/teaching/ss10/05_mlps.printer.pdf (http://ml.informatik.uni-freiburg.de/former/_media/teaching/ss10/05_mlps.printer.pdf)
","**Main Ideas:**

* Multi-layer perceptrons (MLPs) are a subset of deep neural networks (DNNs) that use feed-forward connections.
* The term ""perceptron"" does not refer exclusively to the classical perceptron update rule but is used more broadly for neural networks with layered architectures.
* MLPs are often used interchangeably with DNNs, but DNNs can include more complex architectures such as loops and recurrent connections.

**Terminology Usage:**

* Acyclic feed-forward networks with multiple layers can be referred to as MLPs, including architectures like Inception Net and ResNets.
* Networks with cyclic connections, such as LSTMs and Vanilla RNNs, are not considered MLPs but rather a subset of DNNs."
Can we use MLE to estimate Neural Network weights?,https://stats.stackexchange.com/questions/145902/can-we-use-mle-to-estimate-neural-network-weights,maximum-likelihood|neural-networks,145902,True,145907,10648,2,30,1675019643,1428779370,26,1428780357,"MLE estimates of artificial neural network weights (ANN) certainly are possible; indeed, it's entirely typical. For classification problems, a standard objective function is cross-entropy, which is the same as the negative log-likelihood of a binomial model. For regression problems, residual square error is used, which parallels the MLE of OLS regression. See: How to construct a cross-entropy loss for general regression targets? (https://stats.stackexchange.com/questions/378274/how-to-construct-a-cross-entropy-loss-for-general-regression-targets)
But there are some problems with assuming that the nice properties of MLEs derived in classical statistics, such as uniqueness, also hold for MLEs of neural networks.

There is a general problem with ANN estimation: there are many symmetric solutions to even single-layer ANNs. Reversing the signs of the weights for the hidden layer, and reversing the signs of the hidden layer activation parameters both have equal likelihood. Additionally, you can permute any of the hidden nodes and these permutations also have the same likelihood. This is consequential insofar as you must acknowledge that you are giving up identifiability. However, if identifiability is not important, then you can simply accept that these alternative solutions are just reflections and/or permutations of each other.
This is in contrast to classical usages of MLE in statistics, such as a OLS regression: the OLS problem is convex, and strictly convex when the design matrix is full rank. Strong convexity implies that there is a single, unique minimizer.


It's true that these solutions have the same quality (same loss, same accuracy), but a number of students who arrive at neural networks from an understanding of regression are surprised to learn that NNs are non-convex and do not have unique optimal parameter estimates.

ANNs will tend to overfit the data when using an unconstrained solution. The weights will tend to race away from the origin to implausibly large values which do not generalize well or predict new data with much accuracy. Imposing weight decay or other regularization methods has the effect of shrinking weight estimates toward zero. This doesn't necessarily resolve the indeterminacy issue from (1), but it can improve the generalization of the network.

The loss function is nonconvex and optimization can find locally optimal solutions which are not globally optimal. Or perhaps these solutions are saddle points, where some optimization methods stall. The results in this paper (http://arxiv.org/abs/1412.6544) find that modern estimation methods sidestep this issue.

In a classical statistical setting, penalized fit methods such as elastic net, $L^1$ or $L^2$ regularization can make convex a rank-deficient (i.e. non-convex) problem. This fact does not extend to the neural network setting, due to the permutation issue in (1). Even if you restrict the norm of your parameters, permuting the weights or symmetrically reversing signs won't change the norm of the parameter vector; nor will it change the likelihood. Therefore the loss will remain the same for the permuted or reflected models and the model is still non-identified.


","Maximum likelihood estimation (MLE) is a widely used method for estimating the parameters of artificial neural networks (ANNs). However, unlike in classical statistical models, MLEs of ANN weights are not necessarily unique.

This lack of uniqueness arises from the symmetry of ANN solutions. Reversing the signs of hidden layer weights and activation parameters, or permuting hidden nodes, results in equivalent solutions with the same likelihood. This contrasts with convex optimization problems like OLS regression, where there is a single optimal solution.

The non-convexity of ANNs also means that optimization algorithms may find locally optimal solutions that are not globally optimal. Regularization techniques, such as weight decay, can improve generalization but do not fully resolve the lack of identifiability.

Despite these issues, modern estimation methods have been found to mitigate the impact of non-uniqueness and non-convexity in ANN estimation. However, it is important to be aware of these limitations when interpreting and using ANN models."
What is the difference between episode and epoch in deep Q learning?,https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning,neural-networks|terminology|reinforcement-learning|q-learning,250943,True,250955,38715,1,30,1639015354,1481478309,31,1481482556,"
one episode = one a sequence of states, actions and rewards, which ends with terminal state. For example, playing an entire game can be considered as one episode, the terminal state being reached when one player loses/wins/draws. Sometime, one may prefer to define one episode as several games (example (http://karpathy.github.io/2016/05/31/rl/): ""each episode is a few dozen games, because the games go up to score of 21 for either player"").
one epoch = (https://stats.stackexchange.com/a/164875/12359) one forward pass and one backward pass of all the training examples, in the neural network terminology. 

In the paper you mention, they seem to be more flexible regarding the meaning of epoch, as they just define one epoch as being a certain amount of weight updates. You can therefore view one epoch as being an outer loop around the episode loop, as you mentioned in the question.
","**Summary:**

* **Episode:** A sequence of actions, states, and rewards that ends in a terminal state (e.g., a game). Each episode can consist of one or multiple games.
* **Epoch:** One forward and backward pass through all training examples in a neural network.

In the paper in question, the definition of epoch is flexible:

* It can be a specific number of weight updates, making it an outer loop surrounding the episode loop.
* It can also refer to one or more episodes, depending on the context."
What is the difference between convolutional neural networks and deep learning?,https://stats.stackexchange.com/questions/234891/what-is-the-difference-between-convolutional-neural-networks-and-deep-learning,machine-learning|neural-networks|deep-learning|terminology|convolutional-neural-network,234891,True,234894,54225,5,29,1556969066,1473832174,41,1473833037,"Deep Learning is the branch of Machine Learning based on Deep Neural Networks (DNNs), meaning neural networks with at the very least 3 or 4 layers (including the input and output layers). But for some people (especially non-technical), any neural net qualifies as Deep Learning, regardless of its depth. And others consider a 10-layer neural net as shallow.
Convolutional Neural Networks (CNNs) are one of the most popular neural network architectures. They are extremely successful at image processing, but also for many other tasks (such as speech recognition, natural language processing, and more). The state of the art CNNs are pretty deep (dozens of layers at least), so they are part of Deep Learning. But you can build a shallow CNN for a simple task, in which case it's not (really) Deep Learning.
But CNNs are not alone, there are many other neural network architectures out there, including Recurrent Neural Networks (RNN), Autoencoders, Transformers, Deep Belief Nets (DBN = a stack of Restricted Boltzmann Machines, RBM), and more. They can be shallow or deep. Note: even shallow RNNs can be considered part of Deep Learning since training them requires unrolling them through time, resulting in a deep net.
","**Summary:**

Deep Learning involves neural networks with multiple layers (at least 3 or 4). However, the term's usage can vary, with some considering any neural network as Deep Learning.

Convolutional Neural Networks (CNNs) are popular architectures often used in image processing. State-of-the-art CNNs are typically deep, while shallow CNNs for simpler tasks may not qualify as Deep Learning.

Other neural network architectures include Recurrent Neural Networks (RNNs), Autoencoders, Transformers, and Deep Belief Nets (DBNs). These architectures can range from shallow to deep. Notably, even shallow RNNs are considered part of Deep Learning due to the deep network structure created during training."
Can&#39;t deep learning models now be said to be interpretable? Are nodes features?,https://stats.stackexchange.com/questions/344498/cant-deep-learning-models-now-be-said-to-be-interpretable-are-nodes-features,neural-networks|deep-learning|interpretation,344498,True,344501,5555,3,29,1644033031,1525451239,32,1525451581,"Interpretation of deep models is still challenging. 

Your post only mentions CNNs for computer vision applications, but (deep or shallow) feed-forward networks and recurrent networks remain challenging to understand.
Even in the case of CNNs which have obvious ""feature detector"" structures, such as edges and orientation of pixel patches, it's not completely obvious how these lower-level features are aggregated upwards, or what, precisely, is going on when these vision features are aggregated in a fully-connected layer.
Adversarial examples show how interpretation of the network is difficult. An adversarial example has some tiny modification made to it, but results in a dramatic shift in the decision made by the model. In the context of image classification, a tiny amount of noise added to an image can change an image of a lizard to have a highly confident classification as another animal, like a (species of) dog.

This is related to interpretability in the sense that there is a strong, unpredictable relationship between the (small) amount of noise and the (large) shift in the classification decision. Thinking about how these networks operate, it makes some sense: computations at previous layers are propagated forward, so that a number of errors -- small, unimportant errors to a human -- are magnified and accumulate as more and more computations are performed using the ""corrupted"" inputs.
On the other hand, the existence of adversarial examples shows that the interpretation of any node as a particular feature or class is difficult, since the fact that the node is activated might have little to do with the actual content of the original image, and that this relationship is not really predictable in terms of the original image. But in the example images below, no humans are deceived about the content of the images: you wouldn't confuse the flag pole for a dog. How can we interpret these decisions, either in aggregate (a small noise pattern ""transmutes"" a lizard into dog, or a flagpole into a dog) or in smaller pieces (that several feature detectors are more sensitive to the noise pattern than the actual image content)?
HAAM is a promising new method to generate adversarial images using harmonic functions. (""Harmonic Adversarial Attack Method"" Wen Heng, Shuchang Zhou, Tingting Jiang.) Images generated using this method can be used to emulate lighting/shadow effects and are generally even more challenging for humans to detect as having been altered.
As an example, see this image, taken from ""Universal adversarial perturbations (http://openaccess.thecvf.com/content_cvpr_2017/papers/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.pdf)"", by
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. I chose this image just because it was one of the first adversarial images I came across. This image establishes that a particular noise pattern has a strange effect on the image classification decision, specifically that you can make a small modification to an input image and make the classifier think the result is a dog. Note that the underlying, original image is still obvious: in all cases, a human would not be confused into thinking that any of the non-dog images are dogs.
 (https://i.sstatic.net/4VLlm.jpg)
Here's a second example from a more canonical paper, ""EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES (https://arxiv.org/pdf/1412.6572.pdf)"" by Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy. The added noise is completely indistinguishable in the resulting image, yet the result is very confidently classified as the wrong result, a gibbon instead of a panda. In this case, at least, there is at least a passing similarity between the two classes, since gibbons and pandas are at least somewhat biologically and aesthetically similar in the broadest sense.
 (https://i.sstatic.net/gMCYU.png)
This third example is taken from ""Generalizable Adversarial Examples Detection Based on Bi-model Decision Mismatch (https://arxiv.org/pdf/1802.07770.pdf)"" by JoÃ£o Monteiro, Zahid Akhtar and Tiago H. Falk. It establishes that the noise pattern can be indistinguishable to a human yet still confuse the classifier.  (https://i.sstatic.net/hF4cN.png)
For reference, a mudpuppy is a dark-colored animal with four limbs and a tail, so it does not really have much resemblance to a goldfish.
 (https://i.sstatic.net/Etj0r.jpg)

I just found this paper today. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus. ""Intriguing properties of neural networks (https://arxiv.org/abs/1312.6199)"". The abstract includes this intriguing quotation:


First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. 

So, rather than having 'feature detectors' at the higher levels, the nodes merely represent coordinates in a feature space which the network uses to model the data.
","**Main Ideas:**

* Interpreting deep learning models, including Convolutional Neural Networks (CNNs), feed-forward networks, and recurrent networks, remains challenging.

* **CNNs:**
    * Lower-level features (e.g., edges, orientations) are aggregated upwards, but the aggregation process and the significance of features in fully-connected layers are not fully understood.

* **Adversarial examples:**
    * Tiny modifications to input data can dramatically alter model decisions, highlighting the difficulty of interpretation.
    * Accumulated errors in computations can amplify the impact of noise, making it unpredictable how small changes affect classification results.
    * The relationship between node activation and image content can be tenuous, making it hard to interpret node functions.

* **HAAM method:**
    * Generates adversarial images using harmonic functions, creating noise patterns that are difficult for humans to detect but significantly impact model decisions.

* **Neural network organization:**
    * High-level units in neural networks do not necessarily represent specific features but rather coordinates in a feature space.
    * The network's ability to model data lies within the high-level feature space, not in individual units."
"Does the image format (png, jpg, gif) affect how an image recognition neural net is trained?",https://stats.stackexchange.com/questions/285931/does-the-image-format-png-jpg-gif-affect-how-an-image-recognition-neural-net,neural-networks|deep-learning|image-processing,285931,True,287959,22492,4,29,1614951674,1497740099,25,1498736902,"Short answer is NO.  
The format in which the image is encoded has to do with its quality. Neural networks are essentially mathematical models that perform lots and lots of operations (matrix multiplications, element-wise additions and mapping functions). A neural network sees a Tensor (https://en.wikipedia.org/wiki/Tensor) as its input  (i.e. a multi-dimensional array). It's shape usually is 4-D (number of images per batch, image height, image width, number of channels).
Different image formats (especially lossy ones) may produce different input arrays but strictly speaking neural nets see arrays in their input, and NOT images.
","**Summary:**

Neural networks process data in the form of tensors, which are multi-dimensional arrays. When an image is fed into a neural network, it is converted into a tensor. The format of the image file (e.g., JPEG, PNG) does not directly impact the quality of the image as perceived by the neural network. However, the compression algorithms used in lossy image formats can introduce artifacts or distortions into the data, which may affect the network's performance.

In other words, neural networks do not inherently see images as visual representations but rather as mathematical data structures. The format of the image file only determines how the image is encoded into a tensor, and the neural network's performance depends on the specific characteristics of the tensor data, not on the original image format."
Are line search methods used in deep learning? Why not?,https://stats.stackexchange.com/questions/321592/are-line-search-methods-used-in-deep-learning-why-not,machine-learning|neural-networks|optimization|deep-learning,321592,True,321649,7787,3,29,1627380645,1515091280,21,1515115854,"Vanilla gradient descent can be made more reliable using line searches; I've written algorithms that do this and it makes for a very stable algorithm (although not necessarily fast). 
However, it makes almost no sense to do a line search for stochastic gradient methods. The reason I say this is that if we do a line search based on minimizing the full loss function, we've immediately lost one of the main motivations for doing stochastic methods; we now need to compute the full loss function for each update, which typically has computational cost comparable to computing the full first derivative. Given that we wanted to avoid computing the full gradient because of computational costs, it seems very unlikely that we want be okay with computing the full loss function. 
Alternatively, you might think of doing something like a line search based on your randomly sampled data point. However, this isn't a good idea either; this will tell you nothing about whether you have stepped too far (which is the main benefit of line searches). For example, suppose you are performing logistic regression. Then each outcome is simply a 0 or 1, and for any single sample, we trivially get perfect separation so the optimal solution for our regression parameters based on the sample of 1 is trivially $-\infty$ or $\infty$ by the Hauck Donner effect. That's not good. 
EDIT
@DeltaIV points out that this also applies to mini-batch, not just individual samples.
","Line searches can enhance vanilla gradient descent stability, but they are generally not recommended for stochastic gradient methods.

Using line searches with stochastic methods negates their primary advantage of avoiding full loss function computation, as line searches require evaluating the entire loss function.

Line searches based on randomly sampled data points are also ineffective. In logistic regression, for instance, line searches using a single sample can lead to extreme parameter values due to the Hauck-Donner effect.

This issue also applies to line searches using mini-batches, as they can still yield misleading results due to the stochastic nature of the data."
Hidden Markov Model vs Recurrent Neural Network,https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network,time-series|neural-networks|hidden-markov-model|recurrent-neural-network,282987,True,366430,29746,3,29,1585458365,1496324614,28,1536683418,"Summary
Hidden Markov Models (HMMs) are much simpler than Recurrent Neural Networks (RNNs), and rely on strong assumptions which may not always be true. If the assumptions are true then you may see better performance from an HMM since it is less finicky to get working.
An RNN may perform better if you have a very large dataset, since the extra complexity can take better advantage of the information in your data. This can be true even if the HMMs assumptions are true in your case.
Finally, don't be restricted to only these two models for your sequence task, sometimes simpler regressions (e.g. ARIMA) can win out, and sometimes other complicated approaches such as Convolutional Neural Networks might be the best. (Yes, CNNs can be applied to some kinds of sequence data just like RNNs.)
As always, the best way to know which model is best is to make the models and measure performance on a held out test set.
Strong Assumptions of HMMs
State transitions only depend on the current state, not on anything in the past.
This assumption does not hold in a lot of the areas I am familiar with. For example, pretend you are trying to predict for every minute of the day whether a person was awake or asleep from movement data. The chance of someone transitioning from asleep to awake increases the longer the person has been in the asleep state. An RNN could theoretically learn this relationship and exploit it for higher predictive accuracy.
You can try to get around this, for example by including the previous state as a feature, or defining composite states, but the added complexity does not always increase an HMM's predictive accuracy, and it definitely doesn't help computation times.
You must pre-define the total number of states.
Returning to the sleep example, it may appear as if there are only two states we care about. However, even if we only care about predicting awake vs. asleep, our model may benefit from figuring out extra states such as driving, showering, etc. (e.g. showering usually comes right before sleeping). Again, an RNN could theoretically learn such a relationship if showed enough examples of it.
Difficulties with RNNs
It may seem from the above that RNNs are always superior. I should note, though, that RNNs can be difficult to get working, especially when your dataset is small or your sequences very long. I've personally had troubles getting RNNs to train on some of my data, and I have a suspicion that most published RNN methods/guidelines are tuned to text data. When trying to use RNNs on non-text data I have had to perform a wider hyperparameter search than I care to in order to get good results on my particular datasets.
In some cases, I've found the best model for sequential data is actually a UNet style (https://arxiv.org/pdf/1505.04597.pdf (https://arxiv.org/pdf/1505.04597.pdf)) Convolutional Neural Network model since it is easier and faster to train, and is able to take the full context of the signal into account.
","**Summary:**

**Model Comparison for Sequence Tasks:**

Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs) are commonly used for sequence prediction.

**Advantages of HMMs:**

* Simpler than RNNs
* Rely on simplifying assumptions
* May perform better with smaller datasets or stronger assumptions

**Advantages of RNNs:**

* Can capture long-term dependencies not possible with HMMs
* May perform better with larger datasets, even with true HMM assumptions

**Considerations:**

* Other models, e.g., ARIMA or CNNs, may also be suitable for sequence tasks.
* The best model depends on the specific dataset and task.
* It is important to evaluate performance on held-out test sets to determine the optimal model.

**Limitations of HMMs:**

* Strong assumptions:
    * State transitions depend solely on the current state
    * Total number of states must be predefined
* These assumptions may not always hold, leading to lower performance.

**Challenges with RNNs:**

* Can be difficult to train, especially with small datasets or long sequences
* Hyperparameter tuning may be required for non-text data
* Training can be slow and computationally expensive"
What&#39;s the intuition behind contrastive learning or approach?,https://stats.stackexchange.com/questions/451827/whats-the-intuition-behind-contrastive-learning-or-approach,neural-networks|unsupervised-learning|intuition|semi-supervised-learning|transfer-learning,451827,True,453196,15407,2,29,1632230110,1582905846,25,1583670754,"Contrastive learning is very intuitive. If I ask you to find the matching animal in the photo below, you can do so quite easily. You understand the animal on left is a ""cat"" and you want to find another ""cat"" image on the right side. So, you can contrast between similar and dissimilar things.  
 (https://i.sstatic.net/7yV4F.png) 
Contrastive learning is an approach to formulate this task of finding similar and dissimilar things for a machine. You can train a machine learning model to classify between similar and dissimilar images. There are various choices to make ranging from:  

Encoder Architecture: To convert the image into representations  
Similarity measure between two images: mean squared error, cosine similarity, content loss  
Generating the Training Pairs: manual annotation, self-supervised methods

This blog post (https://amitness.com/2020/03/illustrated-simclr/) explains the intuition behind contrastive learning and how it is applied in recent papers like SimCLR in more detail.
","**Summary:**

Contrastive learning aims to identify similarities and differences between data, similar to how humans distinguish objects. This approach enables machines to learn by comparing examples and contrasting features.

In contrastive learning, a machine learning model is trained to determine whether pairs of images are similar or dissimilar. The model learns by utilizing an encoder to convert images into representations and a similarity measure to quantify the level of similarity between pairs.

Key implementation aspects of contrastive learning include the choice of:

* **Encoder Architecture:** Converts images into representations.
* **Similarity Measure:** Quantifies the similarity between representations.
* **Training Pair Generation:** Determines how to group images for training.

Contrastive learning has proven effective in various applications, including image classification, object detection, and representation learning. It represents a powerful approach for machines to learn from data by leveraging the principles of similarity and contrast."
What is the role of temperature in Softmax?,https://stats.stackexchange.com/questions/527080/what-is-the-role-of-temperature-in-softmax,machine-learning|neural-networks|softmax,527080,True,527082,27384,4,28,1708442918,1622667056,36,1622669376,"The temperature is a way to control the entropy of a distribution, while preserving the relative ranks of each event.

If two events $i$ and $j$ have probabilities $p_i$ and $p_j$ in your softmax, then adjusting the temperature preserves this relationship, as long as the temperature is finite:
$$p_i > p_j \Longleftrightarrow p'_i > p'_j$$

Heating a distribution increases the entropy, bringing it closer to a uniform distribution. (Try it for yourself: construct a simple distribution like $\mathbf{y}=(3, 4, 5)$, then divide all $y_i$ values by $T=1000000$ and see how the distribution changes.)
Cooling it decreases the entropy, accentuating the common events.
Iâll put that another way. Itâs common to talk about the inverse temperature $\beta=1/T$. If $\beta = 0$, then you've attained a uniform distribution. As $\beta \to \infty$, you reach a trivial distribution with all mass concentrated on the highest-probability class. This is why softmax is considered a soft relaxation of argmax.
","**Summary:**

Temperature in softmax functions controls the entropy of a distribution while preserving relative event probabilities.

* Higher temperatures (lower inverse temperature) increase entropy, making distributions more uniform.
* Lower temperatures (higher inverse temperature) decrease entropy, amplifying the likelihood of common events.
* Adjusting temperature scales all probabilities by the same factor, maintaining the ranking of events.
* An inverse temperature of 0 yields a uniform distribution, while an infinite inverse temperature results in all probability mass concentrated on the most likely event.
* Thus, softmax can be viewed as a ""soft"" version of argmax, with temperature controlling the degree of relaxation."
What does a bottleneck layer mean in neural networks?,https://stats.stackexchange.com/questions/262044/what-does-a-bottleneck-layer-mean-in-neural-networks,neural-networks|image-processing,262044,True,262060,56941,2,28,1636886759,1487157551,32,1487163904,"A bottleneck layer is a layer that contains few nodes compared to the previous layers. It can be used to obtain a representation of the input with reduced dimensionality. An example of this is the use of autoencoders with bottleneck layers for nonlinear dimensionality reduction.
My understanding of the quote is that previous approaches use a deep network to classify faces. They then take the first several layers of this network, from the input up to some intermediate layer (say, the $k$th layer, containing $n_k$ nodes). This subnetwork implements a mapping from the input space to an $n_k$-dimensional vector space. The $k$th layer is a bottleneck layer, so the vector of activations of nodes in the $k$th layer gives a lower dimensional representation of the input. The original network can't be used to classify new identities, on which it wasn't trained. But, the $k$th layer may provide a good representation of faces in general. So, to learn new identities, new classifier layers can be stacked on top of the $k$th layer and trained. Or, the new training data can be fed through the subnetwork to obtain representations from the $k$th layer, and these representations can be fed to some other classifier.
","**Summary:**

A bottleneck layer in a neural network is a layer with fewer nodes than the preceding layers. It compresses the dimensionality of the input representation. Autoencoders with bottleneck layers are used for nonlinear dimensionality reduction.

In a typical application, a pre-trained deep network for face classification is utilized. Its early layers, up to an intermediate bottleneck layer, form a subnetwork that maps input faces to lower-dimensional feature vectors. This bottleneck layer representation allows for efficient representation and classification of new faces.

Classifier layers can be added to the bottleneck layer to extend the network's classification capabilities to new identities. Alternatively, the bottleneck layer representation can be used as input to other classification models for improved performance."
"Difference between samples, time steps and features in neural network",https://stats.stackexchange.com/questions/264546/difference-between-samples-time-steps-and-features-in-neural-network,neural-networks|lstm|recurrent-neural-network|tensorflow|tensor,264546,True,264550,54277,3,28,1579815758,1488288351,25,1488288833,"I found this just below the [samples, time_steps, features] you are concerned with. 
X = numpy.reshape(dataX, (len(dataX), seq_length, 1))

Samples - This is the len(dataX), or the amount of data points you have.
Time steps - This is equivalent to the amount of time steps you run your recurrent neural network. If you want your network to have memory of 60 characters, this number should be 60.
Features - this is the amount of features in every time step. If you are processing pictures, this is the amount of pixels. In this case you seem to have 1 feature per time step.
","**Summary:**

The provided code reshapes a data array into a format suitable for a recurrent neural network (RNN). The reshaped array has three dimensions:

* **Samples:** The number of data points or observations in the dataset.
* **Time Steps:** The number of time steps (or memory units) the RNN will have. This determines the length of the sequence that the RNN can process at once.
* **Features:** The number of features or variables included in each time step. For example, in image processing, this would be the number of pixels in each image. In the given example, each time step has a single feature.

This reshaping is necessary because RNNs require input data to be presented in a sequential manner. The time steps dimension represents the sequence of inputs, while the samples dimension represents multiple sequences (if any) being processed by the RNN."
What&#39;s the effect of scaling a loss function in deep learning?,https://stats.stackexchange.com/questions/346299/whats-the-effect-of-scaling-a-loss-function-in-deep-learning,deep-learning|optimization|loss-functions,346299,True,395443,18094,1,28,1567894751,1526375400,30,1551666460,"Short answer:
It depends on the optimizer and the regularization term:

Without regularization, using SGD optimizer: scaling loss by $\alpha$ is equivalent to scaling SGD's learning rate by $\alpha$.
Without regularization, using Nadam: scaling loss by $\alpha$ has no effect.
With regularization, using either SGD or Nadam optimizer: changing the scale of prediction loss will affect the trade-off between prediction loss and regularization.


Full answer:

No regularization + SGD: Assuming your total loss consists of a prediction loss (e.g. mean-squared error) and no regularization loss (such as L2 weight decay), then scaling the output value of the loss function by $\alpha$ would be equivalent to scaling the learning rate ($\eta$) by $\alpha$ when using SGD:
$$L_{\text{new}} = \alpha L_{\text{old}}\\
\Rightarrow \nabla_{W_{t}}L_{\text{new}} = \alpha\nabla_{W_{t}}L_{\text{old}}\\
W_{t+1} = W_{t} - \eta \nabla_{W_{t}}L_{\text{new}}\\
= W_{t} - \eta \alpha\nabla_{W_{t}}L_{\text{old}}\\
= W_{t} - \eta'\nabla_{W_{t}}L_{\text{old}} \quad \text{where } \eta'=\eta\alpha$$
No regularization + Nadam: Using Nadam, assuming you do not have a regularization term, I do not believe there is going to be a difference in the training procedure if you just multiply the loss output by $\alpha$ all the way through training. In this case, in the update rule, $\hat{m_t}$ will be multiplied by $\alpha$ and $\hat{n_t}$ will be multiplied by $\alpha^2$, causing the scales to cancel each other out:
$$ W_{t+1} = W_{t} - \frac{\eta} {\sqrt{\hat{n_t}+\epsilon}}\hat{m_t} $$
With regularization: This is most likely the case you are facing. If you have an explicit regularization term such as L2 weight decay in your loss, then scaling the output of your prediction loss changes the trade-off between your prediction loss and the regularization loss:
$$L_\text{old} = \text{MSE} + \lambda*\text{weight_decay}\\
L_\text{new} = \alpha\text{MSE} + \lambda*\text{weight_decay}\\
= \alpha\left(\text{MSE} + \frac{\lambda}{\alpha}*\text{weight_decay}\right)$$
Using SGD, this would be equivalent to using $\alpha\eta$ as your new learning rate instead of $\eta$, and using $\frac{\lambda}{\alpha}$ as your new L2 regularization scale.
Using Nadam, the $\alpha$ scale in the back would have no effect, and in the end you just end up with $\frac{\lambda}{\alpha}$ as your new L2 regularization scale.

","The effect of scaling loss depends on the optimizer and regularization term.

**Without regularization:**

* SGD: Scaling loss by $\alpha$ is equivalent to scaling the learning rate by $\alpha$.
* Nadam: Scaling loss has no effect.

**With regularization:**

* SGD or Nadam: Scaling loss changes the trade-off between prediction loss and regularization.
* SGD: Equivalent to changing the learning rate and regularization scale.
* Nadam: Only affects the regularization scale.

**No regularization:**

* SGD: Scaling loss affects the learning rate.
* Nadam: Scaling loss has no effect.

Overall, scaling loss can impact the training procedure and optimization effectiveness, especially when regularization is used."
What is a feasible sequence length for an RNN to model?,https://stats.stackexchange.com/questions/158834/what-is-a-feasible-sequence-length-for-an-rnn-to-model,neural-networks|deep-learning|lstm,158834,True,159048,35298,1,28,1526987051,1435338247,15,1435506277,"It totally depends on the nature of your data and the inner correlations, there is no rule of thumb. However, given that you have a large amount of data a 2-layer LSTM can model a large body of time series problems / benchmarks.
Furthermore, you don't backpropagate-through-time to the whole series but usually to (200-300) last steps. To find the optimal value you can cross-validate using grid search or bayesian optimisation. Furthermore, you can have a look at the parameters here: https://github.com/wojzaremba/lstm/blob/master/main.lua (https://github.com/wojzaremba/lstm/blob/master/main.lua).
So, the sequence length doesn't really affect your model training but it's like having more training examples, that you just keep the previous state instead of resetting it.
","**Summary:**

The optimal configuration for a Long Short-Term Memory (LSTM) model depends on the specific data and its correlations. However, for large datasets, a 2-layer LSTM with a sequence length of 200-300 time steps is often sufficient to model time series problems.

Instead of backpropagating through the entire sequence, it's common to only backpropagate through the most recent time steps. The optimal sequence length can be determined through grid search or Bayesian optimization.

The sequence length does not directly impact the model's ability to learn, but it increases the effective training data size by preserving state information from previous time steps."
What does it mean to take the expectation with respect to a probability distribution?,https://stats.stackexchange.com/questions/487095/what-does-it-mean-to-take-the-expectation-with-respect-to-a-probability-distribu,neural-networks|mathematical-statistics|expected-value|notation,487095,True,585522,9452,1,28,1660522255,1599843993,8,1660510411,"The expression
$$\mathbb E[g(x;y;\theta;h(x,z),...)]$$
always means ""the expected value with respect to the joint distribution of all things having a non-degenerate distribution inside the brackets.""
Once you start putting subscripts in $\mathbb E$ then you specify perhaps a ""narrower"" joint distribution for which you want (for your reasons), to average over. For example, if you wrote
$$\mathbb E_{\theta, z}[g(x;y;\theta;h(x,z),...)]$$
I would be inclined to believe that  you mean only
$$\mathbb E_{\theta, z} = \int_{S_z}\int_{S_\theta}f_{\theta,z}(\theta, z)g(x;y;\theta;h(x,z),...) d\theta dz$$
and not
$$\int_{S_z}\int_{S_\theta}\int_{S_x}\int_{S_y}f_{\theta,z,x,y}(\theta, z,x,y)g(x;y;\theta;h(x,z),...) d\theta\, dz\,dx \,dy$$
But it could also mean something else, see on the matter also https://stats.stackexchange.com/a/72614/28746 (https://stats.stackexchange.com/a/72614/28746)
","**Summary:**

The expected value notation, $\mathbb E[g(\cdot)]$, denotes the mean under the joint distribution of all non-degenerate random variables within the brackets.

Subscripts within the expected value notation specify a more specific joint distribution over which the mean is calculated. For instance, $\mathbb E_{\theta, z}[g(\cdot)]$ typically represents the mean under the joint distribution of only the random variables $\theta$ and $z$. However, this notation could also have alternative interpretations depending on the context."
Loss function autoencoder vs variational-autoencoder or MSE-loss vs binary-cross-entropy-loss,https://stats.stackexchange.com/questions/350211/loss-function-autoencoder-vs-variational-autoencoder-or-mse-loss-vs-binary-cross,neural-networks|loss-functions|tensorflow|autoencoders|variational-bayes,350211,True,350687,25730,3,27,1630889863,1528360069,24,1528636930,"I don't believe there's some kind of deep, meaningful rationale at play here - it's a showcase example running on MNIST, it's pretty error-tolerant.

Optimizing for MSE means your generated output intensities are symmetrically close to the input intensities. A higher-than-training intensity is penalized by the same amount as an equally valued lower intensity. 

Cross-entropy loss is assymetrical. 
If your true intensity is high, e.g. 0.8, generating a pixel with the intensity of 0.9 is penalized more than generating a pixel with intensity of 0.7.
Conversely if it's low, e.g. 0.3, predicting an intensity of 0.4 is penalized less than a predicted intensity of 0.2. 
You might have guessed by now - cross-entropy loss is biased towards 0.5 whenever the ground truth is not binary. For a ground truth of 0.5, the per-pixel zero-normalized loss is equal to 2*MSE.
This is quite obviously wrong! The end result is that you're training the network to always generate images that are blurrier than the inputs. You're actively penalizing any result that would enhance the output sharpness more than those that make it worse!

MSE is not immune to the this behavior either, but at least it's just unbiased and not biased in the completely wrong direction. 
However, before you run off to write a loss function with the opposite bias - just keep in mind pushing outputs away from 0.5 will in turn mean the decoded images will have very hard, pixellized edges.
That is - or at least I very strongly suspect is - why adversarial methods yield better results - the adversarial component is essentially a trainable, 'smart' loss function for the (possibly variational) autoencoder.
","**Summary:**

Optimizing neural networks for Mean Squared Error (MSE) loss keeps generated image intensities close to the originals, regardless of direction (higher or lower). Conversely, Cross-entropy loss penalizes deviations from target intensities asymmetrically, favoring values closer to 0.5 when targets are non-binary. This bias towards blurriness is problematic.

MSE, while unbiased, can also result in pixellized edges if modified to counteract the blurriness bias. Adversarial methods mitigate these issues by using a trainable loss function that adapts to the data and prevents both blurry and pixellized outputs."
What can we learn about the human brain from artificial neural networks?,https://stats.stackexchange.com/questions/159093/what-can-we-learn-about-the-human-brain-from-artificial-neural-networks,machine-learning|neural-networks|bioinformatics|artificial-intelligence|neuroscience,159093,True,159104,5819,4,27,1592164658,1435531279,13,1435541765,"As you mentioned, most neural networks are based on general simple abstractions of the brain.  Not only are they lacking in mimicking characteristics like plasticity, but they do not take into account signals and timing as real neurons do.
There's a fairly recent interview, that I felt was appropriate for your specific question, Machine-Learning Maestro Michael Jordan on the Delusions of Big Data and Other Huge Engineering Efforts (http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts), and I quote:

But itâs true that with neuroscience, itâs going to require decades or even hundreds of years to understand the deep principles. There is progress at the very lowest levels of neuroscience. But for issues of higher cognitionâhow we perceive, how we remember, how we actâwe have no idea how neurons are storing information, how they are computing, what the rules are, what the algorithms are, what the representations are, and the like. So we are not yet in an era in which we can be using an understanding of the brain to guide us in the construction of intelligent systems.

","Most neural networks used in machine learning do not accurately represent the brain's capabilities, as they lack plasticity and the incorporation of signals and timing. According to AI expert Michael Jordan, understanding the brain's principles sufficiently to guide the development of intelligent systems will take decades or centuries, as current neuroscience research is limited to basic levels and lacks insights into higher cognitive functions. Therefore, neural networks currently cannot fully leverage the brain's complexity for inspiration."
"Keras, how does SGD learning rate decay work?",https://stats.stackexchange.com/questions/211334/keras-how-does-sgd-learning-rate-decay-work,neural-networks|python,211334,True,211340,30459,1,27,1499183192,1462638132,28,1462641191,"The documentation that you're referring to includes a reference to the Python source (just click on the [Source] link in the appropriate place), that can be used to answer your questions. Here's the most relevant line, showing how decay modifies the learning rate:
lr = self.lr * (1. / (1. + self.decay * self.iterations))

The nesterov option does not have to be set to True for momentum to be used; it results in momentum being used in a different way, as again can be seen from the source:
v = self.momentum * m - lr * g  # velocity

if self.nesterov:
    new_p = p + self.momentum * v - lr * g
else:
    new_p = p + v

","**Summary:**

The provided documentation includes Python source code that explains how learning rate decay and momentum are applied.

**Learning Rate Decay:**

* Decay gradually reduces the learning rate using the formula `lr = self.lr * (1. / (1. + self.decay * self.iterations))`.
* The learning rate is inversely proportional to the number of iterations with decay applied.

**Momentum:**

* Momentum can be used without setting `nesterov` to `True`.
* With `nesterov` set to `True`, momentum is applied differently: the parameter update factor is modified by considering both the gradient and the momentum-driven velocity.
* Without `nesterov` set to `True`, the parameter update factor only considers the momentum-driven velocity."
Rules for selecting convolutional neural network hyperparameters,https://stats.stackexchange.com/questions/148139/rules-for-selecting-convolutional-neural-network-hyperparameters,neural-networks|deep-learning|convolutional-neural-network,148139,True,189075,21567,2,27,1519751142,1429886130,15,1451834535,"To some degree yes, a recent paper came out by Google researchers on how to choose good Inception architectures. Inception nets achieve very high performance on a constrained parameter budget, so this is as good of a place to start as any, and it's recent. Here's the link: Rethinking the Inception Architecture for Computer Vision (http://arxiv.org/abs/1512.00567). 
They do not offer any hard quantitative rules but rather guidelines that they used and believe have helped them achieve good performance in the recent ImageNet competitions. 
For example, some of the principles they discuss are:

Use stacks of smaller receptive field convolutional layers instead of using a single large receptive field convolutional layers, i.e. 2 stacks of 3x3 conv layers vs a single 7x7 conv layer. This idea isn't new, it was also discussed in Return of the Devil in the Details:
Delving Deep into Convolutional Networks (http://www.robots.ox.ac.uk/~vgg/research/deep_eval/) by the Oxford VGG team. This is motivated by the need to be parameter efficient. It also has the dual effect of more representational capacity as we introduce more nonlinearity with more layers. 
Something that I haven't seen in the literature that this article mentioned is factorizing convolutional layers into deep layers. So instead of having a single 7x7 conv layer, we would have a 1x7 conv layer and then a 7x1 conv layer. Adds more depth, I believe it also parameter efficient as well.
Balance the depth and width of your net. Use high dimensional representations. This is one of the principles behind their Inception modules, which concatenate multiple convolutinal layers together. So even if you have a small spatial size in your conv net, using Inception modules we can use a high dimensional representation via multi-scale convolutional concatenation: 1x1, 3x3, 3x3-3x3, max pool all put together. These Inception modules have a ""width"" since they can be interpreted as performing multiple operations in parallel. They go even further with new Inception modules which have factorized convolutional sizes, 1x3, 3x1, etc. 
Use 1x1 conv layers (Network in Network style) to reduce dimensionality. They use a lot of dimensionality reduction techniques to achieve parameter efficiency. They believe that this is effective because adjacent feature maps have highly correlated outputs. Which makes sense as natural images are known to exhibit some local statistical properties consistent with this. So reducing dimensionality via 1x1 NIN layers does not have a disastrous effect on representational power.

There's more in the article. I think it's an article that can offer some insight as to what you are asking about. They are talking about some very core concepts of conv net architectural design. 
","In a recent paper, Google researchers provide guidelines for designing effective Inception network architectures for computer vision tasks. To maximize performance within a limited budget, they recommend:

* Using multiple smaller convolutional layers instead of one large layer.
* Factorizing convolutional layers into deep structures, increasing depth and parameter efficiency.
* Balancing depth and width by employing Inception modules, which concatenate multiple convolutional layers with varying sizes.
* Reducing dimensionality with 1x1 convolutional layers to improve parameter efficiency and preserve representational power.

These principles aim to enhance the network's representational capacity, parameter efficiency, and ability to process multi-scale features. The researchers emphasize that these guidelines are not strict rules but rather principles that have contributed to their success in ImageNet competitions."
How do I make my neural network better at predicting sine waves?,https://stats.stackexchange.com/questions/307210/how-do-i-make-my-neural-network-better-at-predicting-sine-waves,regression|neural-networks|python|keras,307210,True,307224,18354,6,26,1620144624,1507646500,23,1507649421,"You're using a feed-forward network; the other answers are correct that FFNNs are not great at extrapolation beyond the range of the training data.
However, since the data has a periodic quality, the problem may be amenable to modeling with an LSTM. LSTMs are a variety of neural network cell that operate on sequences, and have a ""memory"" about what they have ""seen"" before. The abstract of this book chapter (https://link.springer.com/chapter/10.1007/978-3-319-47054-2_10/fulltext.html) suggests an LSTM approach is a qualified success on periodic problems.
In this case, the training data would be a sequence of tuples $(x_i, \sin(x_i))$, and the task to make accurate predictions for new inputs $x_{i+1} \dots x_{i+n}$ for some $n$ and $i$ indexes some increasing sequence. The length of each input sequence, the width of the interval which they cover, and their spacing, are up to you. Intuitively, I'd expect a regular grid covering 1 period to be a good place to start, with training sequences covering a wide range of values, rather than restricted to some interval.
(Jimenez-Guarneros, Magdiel
and Gomez-Gil, Pilar
and Fonseca-Delgado, Rigoberto
and Ramirez-Cortes, Manuel
and Alarcon-Aquino, Vicente, ""Long-Term Prediction of a Sine Function Using a LSTM Neural Network"", in Nature-Inspired Design of Hybrid Intelligent Systems)
","**Summary:**

To address the limitations of feed-forward neural networks (FFNNs) in extrapolating beyond training data, a Long Short-Term Memory (LSTM) neural network is proposed for predicting a sine function.

LSTMs have a ""memory"" that enables them to model sequences effectively. The training data would consist of tuples (x_i, sin(x_i)), where x_i represents a sequence of input values.

The LSTM would then predict future values x_{i+1} to x_{i+n} for some n. The length of the input sequences, interval width, and spacing are design choices.

Intuitively, a regular grid covering one period of the sine function, with training sequences covering a wide range of values, is a good starting point. This approach could yield more accurate predictions than FFNNs for extrapolating beyond the training data range."
Reason for not shrinking the bias (intercept) term in regression,https://stats.stackexchange.com/questions/86991/reason-for-not-shrinking-the-bias-intercept-term-in-regression,regression|neural-networks|ridge-regression|intercept|regularization,86991,True,161689,19071,6,26,1674070289,1392717045,46,1437003038,"The Elements of Statistical Learning by Hastie et al. define ridge regression as follows (Section 3.4.1, equation 3.41): $$\hat \beta{}^\mathrm{ridge} = \underset{\beta}{\mathrm{argmin}}\left\{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2\right\},$$ i.e. explicitly exclude the intercept term $\beta_0$ from the ridge penalty.
Then they write:

[...] notice that the intercept $\beta_0$ has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin
chosen for $Y$; that is, adding a constant $c$ to each of the targets $y_i$ would
not simply result in a shift of the predictions by the same amount $c$.

Indeed, in the presence of the intercept term, adding $c$ to all $y_i$ will simply lead to $\beta_0$ increasing by $c$ as well and correspondingly all predicted values $\hat y_i$ will also increase by $c$. This is not true if the intercept is penalized: $\beta_0$ will have to increase by less than $c$.
In fact, there are several nice and convenient properties of linear regression that depend on there being a proper (unpenalized) intercept term. E.g. the average value of $y_i$ and the average value of $\hat y_i$ are equal, and (consequently) the squared multiple correlation coefficient $R$ is equal to the coefficient of determination $R^2$: $$(R)^2 = \text{cor}^2(\hat {\mathbf y}, \mathbf y) = \frac{\|\hat{\mathbf y}\|^2}{\|\mathbf y\|^2} = R^2,$$ see e.g. this thread for an explanation: Geometric interpretation of multiple correlation coefficient $R$ and coefficient of determination $R^2$ (https://stats.stackexchange.com/questions/123651).
Penalizing the intercept would lead to all of that not being true anymore.
","**Summary:**

Ridge regression minimizes the sum of squared errors between predictions and actual values while penalizing regression coefficients. The penalty term does not include the intercept term ($\beta_0$). This exclusion ensures that the regression procedure is invariant to shifts in the response variable.

Penalizing the intercept would make the regression dependent on the origin of the response variable. Adding a constant to the response values would not result in a corresponding shift in the predicted values. This conflicts with desirable properties of linear regression, such as:

* Equality of the mean response and mean predicted value
* Equality of the squared multiple correlation coefficient ($R^2$) and the coefficient of determination ($R$)

These properties depend on an unpenalized intercept term. Penalizing the intercept would compromise these properties and introduce unwanted variability in the model."
What is Connectionist Temporal Classification (CTC)?,https://stats.stackexchange.com/questions/320868/what-is-connectionist-temporal-classification-ctc,machine-learning|deep-learning|convolutional-neural-network|recurrent-neural-network,320868,True,331376,13469,1,26,1587059328,1514605990,39,1519985279,"You have a dataset containing:

images I1, I2, ...
ground truth texts T1, T2, ... for the images I1, I2, ...

So your dataset could look something like that:
 (https://i.sstatic.net/IstXI.png)
A Neural Network (NN) outputs a score for each possible horizontal position (often called time-step t in the literature) of the image.
This looks something like this for a image with width 2 (t0, t1) and 2 possible characters (""a"", ""b""):

  | t0  | t1
--+-----+----
a | 0.1 | 0.6
b | 0.9 | 0.4

To train such a NN, you must specify for each image where a character of the ground truth text is positioned in the image.
As an example, think of an image containing the text ""Hello"". 
You must now specify where the ""H"" starts and ends (e.g. ""H"" starts at the 10th pixel and goes until the 25th pixel).
The same for ""e"", ""l, ...
That sounds boring and is a hard work for large datasets.
Even if you managed to annotate a complete dataset in this way, there is another problem.
The NN outputs the scores for each character at each time-step, see the table I've shown above for a toy example.
We could now take the most likely character per time-step, this is ""b"" and ""a"" in the toy example.
Now think of a larger text, e.g. ""Hello"". 
If the writer has a writing style which uses much space in horizontal position, each character would occupy multiple time-steps.
Taking the most probable character per time-step, this could give us a text like ""HHHHHHHHeeeellllllllloooo"".
How should we transform this text into the correct output? Remove each duplicate character? This yields ""Helo"", which is not correct. 
So, we would need some clever postprocessing. 
CTC solves both problems:

you can train the network from pairs (I, T) without having to specify at which position a character occurs using the CTC loss
you don't have to postprocess the output, as a CTC decoder transforms the NN output into the final text

How is this achieved? 

introduce a special character (CTC-blank, denoted as ""-"" in this text) to indicate that no character is seen at a given time-step
modify the ground truth text T to T' by inserting CTC-blanks and by repeating characters in in all possible ways
we know the image, we know the text, but we don't know where the text is positioned. So, let's just try all possible positions of the text ""Hi----"", ""-Hi---"", ""--Hi--"", ...
we also don't know how much space each character occupies in the image. So let's also try all possible alignments by allowing characters to repeat like ""HHi----"", ""HHHi---"", ""HHHHi--"", ...
do you see a problem here? Of course, if we allow a character to repeat multiple times, how do we handle real duplicate characters like the ""l"" in ""Hello""? Well, just always insert a blank in between in these situations, that is e.g. ""Hel-lo"" or ""Heeellll-------llo""
calculate score for each possible T' (that is for each transformation and each combination of these), sum over all scores which yields the loss for the pair (I, T)
decoding is easy: pick character with highest score for each time step, e.g. ""HHHHHH-eeeellll-lll--oo---"", throw away duplicate characters ""H-el-l-o"", throw away blanks ""Hello"", and we are done.

To illustrate this, have a look at the following image. It is in the context of speech recognition, however, text recognition is just the same. Decoding yields the same text for both speakers, even though alignment and position of the character differs.
 (https://i.sstatic.net/wbHRk.png)
Further reading:

an intuitive introduction: https://medium.com/@harald_scheidl/intuitively-understanding-connectionist-temporal-classification-3797e43a86c (https://medium.com/@harald_scheidl/intuitively-understanding-connectionist-temporal-classification-3797e43a86c) (mirror (https://web.archive.org/web/20180625203739/https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c?gi=f615bb019923))
a more in-depth introduction: https://distill.pub/2017/ctc (https://distill.pub/2017/ctc) (mirror (https://web.archive.org/web/20180625203826/https://distill.pub/2017/ctc/))
Python implementation which you can use to ""play"" around with CTC decoders to get a better understanding of how it works: https://github.com/githubharald/CTCDecoder (https://github.com/githubharald/CTCDecoder)
and, of course, the paper Graves, Alex, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen Schmidhuber. ""Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks (https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Connectionist+Temporal+Classification%3A+Labelling+Unsegmented+Sequence+Data+with+Recurrent+Neural+Networks&btnG=)."" In Proceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.

","**CTC (Connectionist Temporal Classification)** is a technique for training neural networks to recognize text or speech without requiring manual annotation of character positions.

**Training:**
- CTC introduces a ""blank"" character to represent no character.
- The ground truth text is modified to include CTC-blanks and repeat characters.
- The network outputs scores for each character (including blanks) at each time step.
- Loss is calculated by summing scores for all possible alignments and repetitions of the ground truth text.

**Decoding:**
- The network's output is decoded by selecting the highest-scoring character at each time step.
- Duplicate characters and blanks are removed to produce the final text.

**Benefits:**
- Eliminate manual annotation of character positions.
- Avoids post-processing to correct for text alignment and spacing.
- Handles variable-length inputs and outputs.

**Example:**
For an image of the text ""Hello"", CTC allows for multiple possible alignments: ""Hi---"", ""-Hi--"", ""--Hi-"", etc. The network outputs scores for all these alignments, and the decoder selects the highest-scoring character sequence: ""H-el-l-o"", which is the correct output."
What are the practical uses of Neural ODEs?,https://stats.stackexchange.com/questions/445562/what-are-the-practical-uses-of-neural-odes,machine-learning|neural-networks|backpropagation|differential-equations|neural-odes,445562,True,445994,14425,1,26,1602709080,1579490835,37,1579719403,"TL;DR: For time series and density modeling, neural ODEs offer some benefits that we don't know how to get otherwise.  For plain supervised learning, there are potential computational benefits, but for practical purposes they probably aren't worth using yet in that setting.
To answer your first question:

Is there something NeuralODEs do that ""conventional"" Neural Networks
  cannot?

Neural ODEs differ in two ways from standard nets:

They represent a different set of functions, which can be good or bad depending on what you're modeling.
We have to approximate their exact solution, which gives more freedom in how to compute the answer, but adds complexity.

I'd say the clearest setting where neural ODEs help is building continuous-time time series models, which can easily handle data coming at irregular intervals (https://arxiv.org/abs/1907.03907).  However, ODEs can only model deterministic dynamics, so I'm more excited by generalization of these time-series models to stochastic differential equations (https://arxiv.org/abs/2001.01328).
If you're modeling data sampled at regular time intervals (like video or audio), I think there's not much advantage, and standard approaches will probably be simpler and faster.
Another setting where they have an advantage is in building normalizing flows for density modeling.  The bottleneck in normalizing flows is keeping track of the change in density, which is slow (O(D^3)) for standard nets.  That's why discrete-time normalizing flow models like Glow or Real-NVP have to restrict the architectures of their layers, for example only updating half the units depending on the other half.  In continuous time, it's easier to track the change in density, even for unrestricted architectures.  That's what the FFJORD (https://arxiv.org/abs/1810.01367) paper is about.  Since then, Residual Flows (https://arxiv.org/abs/1906.02735) were developed, which are discrete time flows that can also handle unrestricted architectures, with some caveats.
For standard deep learning, there are two potential big advantages:

Constant memory cost at training time.  Before neural ODEs there was already some work (https://arxiv.org/abs/1707.04585) showing we can reduce the memory cost of computing reverse-mode  gradients of neural networks if we could 'run them backwards' from the output, but this required restricting the architecture of the network.  The nice thing about neural ODEs that you you can simply run their dynamics backwards to reconstruct the original trajectory.  In both cases, compounding numerical error could be a problem in some cases, but we didn't find this to be a practical concern.
Adaptive time cost.  The idea is that since we're only approximating an exact answer, sometimes we might only need a few iterations of our approximate solver to get an acceptably good answer, and so could save time.

Both of these potential advantages are shared by Deep Equilibrium Models (https://arxiv.org/abs/1909.01377), and they've already been scaled up to transformers.  But in both cases, these models so far in practice have tended to be slower overall than standard nets, because we don't yet know how to regularize these models to be easy to approximate.
To answer your second question:

Is there something ""conventional"" Neural Networks do that NeuralODEs
  cannot do?


Conventional nets can fit non-homeomorphic (https://en.wikipedia.org/wiki/Homeomorphism) functions, for example functions whose output has a smaller dimension that their input, or that change the topology of the input space.  There was a nice paper from Oxford (https://arxiv.org/abs/1904.01681) pointing out these issues, and showing that you can also fix it by adding extra dimensions. 
Of course, you could handle this by composing ODE nets with standard network layers.  
Conventional nets can be evaluated exactly with a fixed amount of computation, and are typically faster to train.  Plus, with standard nets you don't have to choose an error tolerance for a solver.

","**Main Ideas of Neural Ordinary Differential Equations (ODEs)**

**Advantages of Neural ODEs:**

* **Time series modeling:** Easily handle irregular data intervals and deterministic dynamics.
* **Density modeling:** Enable efficient tracking of density changes, facilitating unrestricted architectures in normalizing flows.
* **Potential computational benefits:** Constant memory cost at training time and adaptive time cost through approximate solvers.

**Limitations of Neural ODEs:**

* **Homeomorphic function approximation:** Cannot handle functions that reduce output dimensions or change input topology.
* **Approximate solution:** Requires numerical solvers, which may introduce compounding errors.
* **Regularization and training time:** Practical implementation often lags behind standard neural networks in terms of speed and regularization.

**Comparison to Standard Neural Networks:**

* **Function representation:** Neural ODEs represent a different set of functions, suitable for specific modeling tasks.
* **Computation:** Neural ODEs offer potential computational advantages, but may face practical limitations.
* **Flexibility:** Neural ODEs provide more freedom in calculating solutions but require consideration of approximation accuracy.

**Overall:**

Neural ODEs offer unique capabilities for time series and density modeling. For plain supervised learning, their potential computational benefits may not yet outweigh the challenges of practical implementation. However, they present exciting possibilities for advancing these specialized modeling domains."
How does minibatch gradient descent update the weights for each example in a batch?,https://stats.stackexchange.com/questions/266968/how-does-minibatch-gradient-descent-update-the-weights-for-each-example-in-a-bat,neural-networks|gradient-descent|backpropagation|tensorflow,266968,True,266977,24304,2,26,1489357331,1489313895,36,1489318406,"Gradient descent doesn't quite work the way you suggested but a similar problem can occur.
We don't calculate the average loss from the batch, we calculate the average gradients of the loss function. The gradients are the derivative of the loss with respect to the weight and in a neural network the gradient for one weight depends on the inputs of that specific example and it also depends on many other weights in the model.
If your model has 5 weights and you have a mini-batch size of 2 then you might get this:
Example 1. Loss=2, $\text{gradients}=(1.5,-2.0,1.1,0.4,-0.9)$
Example 2. Loss=3, $\text{gradients}=(1.2,2.3,-1.1,-0.8,-0.7)$
The average of the gradients in this mini-batch are calculated, they are $(1.35,0.15,0,-0.2,-0.8)$
The benefit of averaging over several examples is that the variation in the gradient is lower so the learning is more consistent and less dependent on the specifics of one example. Notice how the average gradient for the third weight is $0$, this weight won't change this weight update but it will likely be non-zero for the next examples chosen which get computed with different weights.
edit in response to comments:
In my example above the average of the gradients is computed. For a mini-batch size of $k$ where we calculate the loss $L_i$ for each example we and aim to get the average gradient of the loss with respect to a weight $w_j$.
The way I wrote it in my example I averaged each gradient like: $\frac{\partial L}{\partial w_j} = \frac{1}{k} \sum_{i=1}^{k} \frac{\partial L_i}{\partial w_j}$
The tutorial code you linked to in the comments uses Tensorflow to minimize the average loss.
Tensorflow aims to minimize $\frac{1}{k} \sum_{i=1}^{k} L_i$
To minimize this it computes the gradients of the average loss with respect to each weight and uses gradient-descent to update the weights:
$\frac{\partial L}{\partial w_j} = \frac{\partial }{\partial w_j}  \frac{1}{k} \sum_{i=1}^{k} L_i$
The differentiation can be brought inside the sum so it's the same as the expression from the approach in my example.
$\frac{\partial }{\partial w_j}  \frac{1}{k} \sum_{i=1}^{k} L_i = \frac{1}{k} \sum_{i=1}^{k} \frac{\partial L_i}{\partial w_j}$
","**Gradient Descent with Mini-Batch Averaging**

Gradient descent is an optimization algorithm used in machine learning to train neural networks. In standard gradient descent, the loss (error) for the entire batch is calculated and used to update the model's weights. However, in gradient descent with mini-batch averaging, the average gradients of the loss function are calculated instead.

**Averaging Gradients**

The gradients used in gradient descent are the derivatives of the loss function with respect to the model's weights. By averaging the gradients over a mini-batch (a subset of the training data), the variation in the gradients is reduced, making the learning process more consistent and less dependent on individual examples.

**Example**

Consider a model with 5 weights and a mini-batch size of 2. For two examples, the loss and gradients may be:

Example 1: Loss = 2, Gradients = (1.5, -2.0, 1.1, 0.4, -0.9)
Example 2: Loss = 3, Gradients = (1.2, 2.3, -1.1, -0.8, -0.7)

Averaging these gradients results in: (1.35, 0.15, 0, -0.2, -0.8)

This means that weight 3 will not change during this weight update, but may change in subsequent updates as the inputs and weights change.

**TensorFlow Implementation**

TensorFlow aims to minimize the average loss over the mini-batch. It computes the gradients of the average loss with respect to each weight and uses gradient-descent to update the weights. This is equivalent to averaging the gradients over the mini-batch."
When to &quot;add&quot; layers and when to &quot;concatenate&quot; in neural networks?,https://stats.stackexchange.com/questions/361018/when-to-add-layers-and-when-to-concatenate-in-neural-networks,neural-networks|keras,361018,True,361041,33380,2,26,1579860105,1533588751,32,1533602529,"Adding is nice if you want to interpret one of the inputs as a residual ""correction"" or ""delta"" to the other input. For example, the residual connections in ResNet are often interpreted as successively refining the feature maps. Concatenating may be more natural if the two inputs aren't very closely related. However, the difference is smaller than you may think.
Note that $W[x,y] = W_1x + W_2y$ where $[\ ]$ denotes concat and $W$ is split horizontally into $W_1$ and $W_2$. Compare this to $W(x+y) = Wx + Wy$. So you can interpret adding as a form of concatenation where the two halves of the weight matrix are constrained to $W_1 = W_2$. 
","**Summary:**

The choice between adding and concatenating inputs in neural networks depends on the relationship between the inputs. Adding is suitable when one input can be seen as a refinement or ""delta"" to the other. For example, in ResNet, residual connections refine feature maps incrementally.

Concatenation is more appropriate when the inputs are unrelated. However, the difference between adding and concatenating is subtle. Mathematically, adding can be viewed as a type of concatenation where the two halves of the weight matrix for the inputs are constrained to be equal.

In practice, the decision between adding and concatenating is less substantial than it may seem. The choice may depend on the interpretation of the inputs and the specific task at hand."
Does Dimensionality curse effect some models more than others?,https://stats.stackexchange.com/questions/186184/does-dimensionality-curse-effect-some-models-more-than-others,neural-networks|svm|k-means|k-nearest-neighbour|high-dimensional,186184,True,186912,17310,1,26,1563170021,1449797390,31,1450206618,"In general, the curse of dimensionality makes the problem of searching through a space much more difficult, and effects the majority of algorithms that ""learn"" through partitioning their vector space. The higher the dimensionality of our optimization problem the more data we need to fill the space that we are optimizing over. 
Generalized Linear Models 
Linear models suffer immensely from the curse of dimensionality. Linear models partition the space in to a single linear plane. Even if we are not looking to directly compute $$\hat{\beta} = (X^{'}X)^{-1}X^{'}y$$ the problem posed is still very sensitive to collinearity, and can be considered ""ill conditioned"" without some type of regularization. In very high dimensional spaces, there is more than one plane that can be fitted to your data, and without proper type of regularization can cause the model to behave very poorly. Specifically what regularization does is try to force one unique solution to exist. Both L1 and squared L2 regularization try to minimize the weights, and can be interpreted selecting the model with the smallest weights to be the most ""correct"" model. This can be thought of as a mathematical formulation of Occams Razor.
Decision Trees
Decision trees also suffer from the curse of dimensionality. Decision trees directly partition the sample space at each node. As the sample space increases, the distances between data points increases, which makes it much harder to find a ""good"" split. 
Random Forests
Random Forests use a collection of decision trees to make their predictions. But instead of using all the features of your problem, individual trees only use a subset of the features. This minimizes the space that each tree is optimizing over and can help combat the problem of the curse of dimensionality.     
Boosted Tree's
Boosting algorithms such as AdaBoost suffer from the curse of dimensionality and tend to overffit if regularization is not utilized. I won't go in depth, because the post Is AdaBoost less or more prone to overfitting? (https://stats.stackexchange.com/questions/20622/is-adaboost-less-or-more-prone-to-overfitting)
explains the reason why better than I could.
Neural Networks
Neural networks are weird in the sense that they both are and are not impacted by the curse of dimensionality dependent on the architecture, activations, depth etc.  So to reiterate the curse of dimensionality is the problem that a huge amount of
points are necessary in high dimensions to cover an input space. One way to interpret deep neural networks is to think of all layers expect the very last layer as doing a complicated projection of a high dimensional manifold into a lower dimensional manifold, where then the last layer classifies on top of. So for example in a convolutional network for classification where the last layer is a softmax layer, we can interpret the architecture as doing a non-linear projection onto a smaller dimension and then doing a multinomial logistic regression (the softmax layer) on that projection. So in a sense the compressed representation of our data allows us to circumvent the curse of dimensionality. Again this is one interpretation, in reality the curse of dimensionality does in fact impact neural networks, but not at the same level as the models outlined above.
SVM
SVM tend to not overffit as much as generalized linear models due to the excessive regularization that occurs. Check out this post SVM, Overfitting, curse of dimensionality (https://stats.stackexchange.com/questions/35276/svm-overfitting-curse-of-dimensionality) for more detail.
K-NN, K-Means 
Both K-mean and K-NN are greatly impacted by the curse of dimensionality, since both of them use the L2 squared distance measure. As the amount of dimensions increases the distance between various data-points increases as well. This is why you need a greater amount of points to cover more space in hopes the distance will be more descriptive.
Feel free to ask specifics about the models, since my answers are pretty general. Hope this helps.
","**Curse of Dimensionality**

The curse of dimensionality significantly complicates searching through high-dimensional spaces, particularly for algorithms that learn by partitioning this space. As dimensionality increases, more data is required to adequately cover the optimization space. This problem affects a range of machine learning models:

**Generalized Linear Models**
- Linear models are especially prone to the curse of dimensionality due to their single-plane partitioning.
- Collinearity can make these models ill-conditioned and unstable, requiring regularization techniques to force a unique solution.

**Decision Trees**
- Decision trees directly partition the sample space, making it harder to find good splits as the dimensionality increases.
- Random forests use multiple decision trees with subsets of features, mitigating the impact of dimensionality.

**Boosted Trees**
- Boosting algorithms like AdaBoost can overfit in high dimensions without regularization.

**Neural Networks**
- Deep neural networks can be interpreted as projecting high-dimensional data into lower dimensions, potentially circumventing the curse of dimensionality.
- However, it still affects neural networks, albeit to a lesser extent than other models.

**Support Vector Machines (SVMs)**
- SVMs use excessive regularization, reducing overfitting in high dimensions.

**K-NN and K-Means**
- These models use the L2 squared distance measure, which becomes less informative as dimensionality increases, requiring more data to cover the space effectively."
Optimal construction of day feature in neural networks,https://stats.stackexchange.com/questions/126230/optimal-construction-of-day-feature-in-neural-networks,machine-learning|neural-networks|feature-engineering,126230,True,126234,19764,1,26,1563506169,1417477960,30,1417479823,"Your second representation is more traditional for categorical variables like day of week.
This is also known as creating dummy variables (http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29) and is a widely used method for encoding categorical variables.  If you used 1-7 encoding you're telling the model that days 4 and 5 are very similar, while days 1 and 7 are very dissimilar.  In fact, days 1 and 7 are just as similar as days 4 and 5.  The same logic holds up for 0-30 encoding for days of the month.
Day of the month is a little trickier, because while every week has the same 7 days, not every month has the same 30 days: some months have 31 days, and some months have 28 days.  Since both weeks and months are cyclical, you could use fourier transformations (http://en.wikipedia.org/wiki/Fourier_transform) to convert them to smooth linear variables.
For example (using R, my programming language of choice (http://cran.r-project.org/)):
day_of_month = c(1:31, 1:28, 1:30)
day_of_year <- 1:length(day_of_month)
s = sin((2*pi)/30*day_of_month)
c = cos((2*pi)/30*day_of_month)
plot(day_of_month ~ day_of_year)
lines(15*s+15 ~ day_of_year, col='blue')
lines(15*c+15 ~ day_of_year, col='red')
legend(10, 30, c('raw', 'sin', 'cos'), c('black', 'blue', 'red'))


(I scaled the sine/cosine variables to be 0/30, rather than -1/1 so the graph looks better)
As you can see, while the raw ""day of month variable"" jumps back to zero at the end of each month, the sine and cosine transformations make a smooth transition that lets the model know days at the end of one month are be similar to days at the beginning of the next month.
You can add the rest of the fourier terms as follows:
for(i in 1:3){
  s = sin((2*pi)/30*day_of_month + 30 * i/4)
  c = cos((2*pi)/30*day_of_month + 30 * i/4)
  lines(15*s+15 ~ day_of_year, col='blue')
  lines(15*c+15 ~ day_of_year, col='red')
}
legend(10, 30, c('raw', 'sin', 'cos'), c('black', 'blue', 'red'))


Each pair of sine/cosine waves makes a circle:
m <- lapply(1:4, function(i){
  as.matrix(
    data.frame(
    s = sin((2*pi)/30*day_of_month + 30 * i/4),
    c = cos((2*pi)/30*day_of_month + 30 * i/4)
    )
  )
})
m <- do.call(cbind, m)
pairs(m)


This page has a really handy explanation (http://www.purplemath.com/modules/grphtrig.htm) of how to manipulate sine and cosine waves.
","Encoding categorical variables is essential for machine learning models to understand their impact. One method is using dummy variables, which represent each category with a binary indicator. However, this approach assumes equal similarity between all categories, which is not always accurate.

For variables with a cyclical nature like day of month, using a simple numerical encoding (e.g., 1-30) can mislead the model by suggesting that adjacent days are very dissimilar. Instead, Fourier transformations can be used to convert the cyclical variable into smooth linear variables that maintain the similarity between adjacent values.

This technique is demonstrated by transforming days of the month into sine and cosine waves. The resulting waves form circles, where each pair represents a month. By using multiple pairs of sine/cosine waves, a smoother transition between months can be achieved, allowing the model to capture the cyclical nature of the variable more accurately."
number of feature maps in convolutional neural networks,https://stats.stackexchange.com/questions/128880/number-of-feature-maps-in-convolutional-neural-networks,machine-learning|neural-networks|deep-learning|pattern-recognition|convolutional-neural-network,128880,True,168444,33993,1,26,1479752709,1418405172,20,1440351253,"1) C1 in the layer 1 has 6 feature maps, does that mean there are six convolutional kernels? Each convolutional kernel is used to generate a feature map based on input. 
There are 6 convolutional kernels and each is used to generate a feature map based on input. Another way to say this is that there are 6 filters or 3D sets of weights which I will just call weights. What this image doesn't show, that it probably should, to make it clearer is that typically images have 3 channels, say red, green, and blue. So the weights that map you from the input to C1 are of shape/dimension 3x5x5 not just 5x5. The same 3 dimensional weights, or kernel, are applied across the entire 3x32x32 image to generate a 2 dimensional feature map in C1. There are 6 kernels (each 3x5x5) in this example so that makes 6 feature maps ( each 28x28 since the stride is 1 and padding is zero) in this example, each of which is the result of applying a 3x5x5 kernel across the input. 
2) S1 in layer 1 has 6 feature maps, C2 in layer 2 has 16 feature maps. What is the process look like to get these 16 feature maps based on 6 feature maps in S1?
Now do the same thing we did in layer one, but do it for layer 2, except this time the number of channels is not 3 (RGB) but 6, six for the number of feature maps/filters in S1. There are now 16 unique kernels each of shape/dimension 6x5x5. each layer 2 kernel is applied across all of S1 to generate a 2D feature map in C2. This is done 16 times for each unique kernel in layer 2, all 16, to generate the 16 feature maps in layer 2 (each 10x10 since stride is 1 and padding is zero)
source: http://cs231n.github.io/convolutional-networks/ (http://cs231n.github.io/convolutional-networks/)
","Summary:

In convolutional neural networks (CNNs), a layer's feature maps result from applying convolutional kernels to the previous layer's feature maps or the input image. Each kernel is a set of weights that generates one feature map.

In Layer 1, C1 has 6 feature maps because there are 6 convolutional kernels, each of size 3x5x5 and applied to the input image with 3 channels (RGB).

To generate Layer 2's 16 feature maps (C2), 16 kernels of size 6x5x5 are applied to Layer 1's 6 feature maps (S1). Each kernel produces a 2D feature map, resulting in 16 feature maps in C2.

This process continues through subsequent layers, with kernels of increasing depth matching the number of input channels. The resulting feature maps represent increasingly complex features learned from the input data."
Can $\sin(x)$ be used as activation in deep learning?,https://stats.stackexchange.com/questions/402618/can-sinx-be-used-as-activation-in-deep-learning,neural-networks|deep-learning|backpropagation,402618,True,402645,12933,4,25,1723562353,1555058109,28,1555070199,"Here's a paper dedicated to this very question:

Parascandolo and Virtanen (2016) (https://openreview.net/forum?id=Sks3zF9eg). Taming the waves: sine as activation function in deep neural networks.

Some key points from the paper:

Sinusoidal activation functions have been largely ignored, and are considered difficult to train.
They review past work that has used sinusoidal activation functions. Most of this is earlier work (before the modern 'deep learning' boom). But, there are a couple more recent papers.
The periodic nature of sinusoidal activation functions can give rise to a 'rippling' cost function with bad local minima, which may make training difficult.
The problem may not be so bad when the data is dominated by low-frequency components (which is expected for many real-world datasets). Learning is easier in this regime, but is sensitive to how network parameters are initialized.
They show that networks with sinusoidal activation functions can perform reasonably well on a couple real-world datasets. But, after training, the networks don't really use the periodic nature of the activation function. Rather, they only use the central region near zero, where the sinusoid is nearly equivalent to the more traditional $\tanh$ activation function.
They trained recurrent networks on a synthetic task where periodic structure is expected to be helpful. Networks learn faster and are more accurate using $\sin$ compared to $\tanh$ activation functions. But, the difference is bigger for vanilla RNNs than LSTMs.

Here's another relevant paper:

Ramachandran, Zoph, Le (2017) (https://arxiv.org/abs/1710.05941). Searching for Activation Functions.

They performed a large-scale, automatic search over activation functions to find new variants that perform well. Some of the activation functions they discovered use sinusoidal components (but they're not pure sinusoids--they also tend to have a monotonic component). The paper doesn't discuss these variants much, except to say that they're an interesting future research direction.
","**Summary:**

Sinusoidal activation functions (sinusoids) have been underutilized in neural networks. Past studies have faced challenges due to their periodic nature, which can lead to problematic cost functions. However, recent research suggests that sinusoids can be effective in certain scenarios.

One study (Parascandolo and Virtanen, 2016) found that sinusoids are suitable for training networks when data predominantly consists of low-frequency components. In practice, however, the networks did not fully leverage the periodic nature of sinusoids, instead utilizing only their central region.

Another study (Ramachandran, Zoph, Le, 2017) used automated search to discover new activation functions, some of which incorporate sinusoidal components. However, these variants were not extensively explored in the paper.

In a specific task involving recurrent networks where periodic structure was beneficial, sinusoids outperformed traditional hyperbolic tangent (tanh) activation functions, particularly in vanilla RNNs.

Overall, sinusoids have potential in neural networks, but further research is needed to optimize their usage and explore their capabilities in diverse applications."
What do the fully connected layers do in CNNs?,https://stats.stackexchange.com/questions/182102/what-do-the-fully-connected-layers-do-in-cnns,neural-networks|deep-learning|convolutional-neural-network,182102,True,182122,67272,2,25,1523075158,1447721599,37,1447729877,"The output from the convolutional layers represents high-level features in the data.  While that output could be flattened and connected to the output layer, adding a fully-connected layer is a (usually) cheap way of learning non-linear combinations of these features.
Essentially the convolutional layers are providing a meaningful, low-dimensional, and somewhat invariant feature space, and the fully-connected layer is learning a (possibly non-linear) function in that space.
NOTE:
It is trivial to convert from FC layers to Conv layers. Converting these top FC layers to Conv layers can be helpful as this (http://cs231n.github.io/convolutional-networks/) page describes.
","Summary:

Convolutional layers in machine learning models extract high-level features from data. To further process these features, a fully-connected (FC) layer is typically added. This layer combines the features in a non-linear manner, creating a low-dimensional feature space.

The FC layer's function is to learn specific patterns or relationships within the features extracted by the convolutional layers. By combining these features in non-linear ways, the FC layer allows the model to make predictions based on complex combinations of the input data.

It is important to note that converting FC layers to convolutional layers is possible. This technique can sometimes improve model performance."
Sum or average of gradients in (mini) batch gradient decent?,https://stats.stackexchange.com/questions/183840/sum-or-average-of-gradients-in-mini-batch-gradient-decent,neural-networks|gradient-descent|backpropagation,183840,True,183990,23222,1,25,1467798723,1448615003,34,1448713399,"Average.
Examples: Notes to Andrew Ng's Machine Learning Course on Coursera (http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html) compiled by Alex Holehouse.
Summing the gradients due to individual samples you get a much smoother gradient. The larger the batch the smoother the resulting gradient used in updating the weight.
Dividing the sum by the batch size and taking the average gradient has the effect of:

The magnitude of the weight does not grow out of proportion. Adding L2 regularization to the weight update penalizes large weight values. This often leads to improved generalization performance. Taking the average, especially if the gradients happen to point in the same direction, keep the weights from getting too large.
The magnitude of the gradient is independent of the batch size. This allows comparison of weights from other experiments using different batch sizes.
Countering the effect of the batch size with the learning rate can be numerically equivalent but you end up with a learning rate that is implementation specific. It makes it difficult to communicate your results and experimental setup if people cannot relate to the scale of parameters you're using and they'll have trouble reproducing your experiment.

Averaging enables clearer comparability and keeping gradient magnitudes independent of batch size. Choosing a batch size is sometimes constrained by the computational resources you have and you want to mitigate the effect of this when evaluating your model.
","**Summary:**

Batch averaging in machine learning involves summing gradients from multiple samples and dividing by the batch size. This smoothing process offers several benefits:

**1. Gradient Smoothing:**
Averaging reduces fluctuations in the gradient, leading to a smoother update direction for weights.

**2. Weight Control:**
By dividing by the batch size, the average gradient scales down the weight updates, preventing excessive growth. L2 regularization, which penalizes large weights, further promotes weight moderation.

**3. Batch Size Independence:**
Averaging ensures that the gradient magnitude does not vary with batch size, enabling fair weight comparisons across experiments with different batch sizes.

**4. Comparability and Communication:**
By making gradient magnitudes independent of batch size, averaging facilitates clear communication and reproducibility of experimental results.

In resource-constrained environments, batch size may be limited. Averaging mitigates the impact of a smaller batch size on model evaluation, ensuring a more accurate assessment of its performance."
Restricted Boltzmann machines vs multilayer neural networks,https://stats.stackexchange.com/questions/40598/restricted-boltzmann-machines-vs-multilayer-neural-networks,r|machine-learning|classification|neural-networks,40598,True,40601,10815,2,25,1419724709,1350493754,21,1350494834,"First of all RBM's are certainly different from normal Neural Nets, and when used properly they achieve much better performance. Also, training a few layers of a RBM, and then using the found weights as a starting point for a Mulitlayer NN often yields better results than simply using a Multilayer NN.
The best pointer I can think of is this course on Coursera, taught by Geoffrey Hinton, who is one of the people responsible for RBMs: 
https://class.coursera.org/neuralnets-2012-001/class/index (https://class.coursera.org/neuralnets-2012-001/class/index)
the videos on RBMs and Denoising Autoencoders are a valuable learning resource for anyone interested in the topic.
As to implementation in R, I don't know any either, but if you want to implement it, better not use pure R (unless your data is not to big). The training of an RBM takes quite a long time, and if you use pure R instead of R with C it can grow significantly.
","Restricted Boltzmann Machines (RBMs) differ from traditional neural networks and offer superior performance. Pretraining an RBM and using the resulting weights in a multilayer neural network often enhances performance.

Geoffrey Hinton's Coursera course provides valuable insights into RBMs and denoising autoencoders.

Implementing RBMs in R is challenging due to lengthy training times. Using R with C instead of pure R is recommended to improve efficiency."
What is the essential difference between a neural network and nonlinear regression?,https://stats.stackexchange.com/questions/344658/what-is-the-essential-difference-between-a-neural-network-and-nonlinear-regressi,regression|neural-networks|nonlinear-regression,344658,True,345065,17005,1,25,1687204109,1525538735,23,1525773738,"In theory, yes. In practice, things are more subtle.
First of all, let's clear the field from a doubt raised in the comments: neural networks can handle multiple outputs in a seamless fashion, so it doesn't really matter whether we consider multiple regression or not (see The Elements of Statistical Learning (https://web.stanford.edu/~hastie/Papers/ESLII.pdf), paragraph 11.4). 
Having said that, a neural network of fixed architecture and loss function would indeed just be a parametric nonlinear regression model. So it would even less flexible than nonparametric models such as Gaussian Processes. To be precise, a single hidden layer neural network with a sigmoid or tanh activation function would be less flexible than a Gaussian Process: see http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf (http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf). For deep networks this is not true, but it becomes true again when you consider Deep Gaussian Processes.
So, why are Deep Neural Networks such a big deal? For very good reasons:

They allow fitting models of a complexity that you wouldn't even begin to dream of, when you fit Nonlinear Least Squares models with the Levenberg-Marquard algorithm. See for example https://arxiv.org/pdf/1611.05431.pdf (https://arxiv.org/pdf/1611.05431.pdf), https://arxiv.org/pdf/1706.02677.pdf (https://arxiv.org/pdf/1706.02677.pdf) and https://arxiv.org/pdf/1805.00932.pdf (https://arxiv.org/pdf/1805.00932.pdf) where the number of parameters $p$ goes from 25 to 829 millions. Of course DNNs are overparametrized, non-identifiable, etc. etc. so the number of parameters is very different from the ""degrees of freedom"" of the model (see https://arxiv.org/abs/1804.08838 (https://arxiv.org/abs/1804.08838) for some intuition). Still, it's undeniably amazing that models with $N <<p$ ($N=$ sample size) are able to generalize so well.
They scale to huge data sets. A vanilla Gaussian Process is a very flexible model, but inference has a $O(N^3)$ cost which is completely unacceptable for data sets as big as ImageNet or bigger  such as Open Image V4 (https://storage.googleapis.com/openimages/web/index.html). There are approximations to inference with GPs which scale as well as NNs (https://arxiv.org/pdf/1803.06058.pdf), but I don't know why they don't enjoy the same fame (well, I have my ideas about that, but let's not digress).
For some tasks, they're impressively accurate, much better than many other statistical learning models. You can try to match ResNeXt accuracy on ImageNet, with a 65536 inputs kernel SVM, or with a random forest for classification. Good luck with that.

However, the real difference between theory: 

all neural networks are parametric nonlinear regression or classification models

and practice in my opinion, is that in practice nothing about a deep neural network is really fixed in advance, so you end up fitting a model from a much bigger class than you would expect. In real-world applications, none of these aspects are really fixed:

architecture (suppose I do sequence modeling: shall I use an RNN? A dilated CNN? Attention-based model?)
details of the architecture (how many layers? how many units in layer 1, how many in layer 2, which activation function(s), etc.)
how do I preprocess the data? Standardization? Minmax normalization? RobustScaler?
kind of regularization ($l_1$? $l_2$? batch-norm? Before or after ReLU? Dropout? Between which layers?)
optimizer (SGD? Path-SGD? Entropy-SGD? Adam? etc.)
other hyperparameters such as the learning rate, early stopping, etc. etc.
even the loss function is often not fixed in advance! We use NNs for mostly two applications (regression and classification), but people use a swath of different loss functions (https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications).

Look how many choices are performed even in a relatively simple case where there is a strong seasonal signal, and the number of features is small, as far as DNNs go:
https://stackoverflow.com/questions/48929272/non-linear-multivariate-time-series-response-prediction-using-rnn (https://stackoverflow.com/questions/48929272/non-linear-multivariate-time-series-response-prediction-using-rnn)
Thus in practice, even though ideally fitting a DNN would mean to just fit a model of the type 
$y=f(\mathbf{x}\vert\boldsymbol{\theta})+\epsilon$
where $f$ has a certain hierarchical structure, in practice very little (if anything at all) about the function and the fitting method is defined in advance, and thus the model is much more flexible than a ""classic"" parametric nonlinear model.
","Theoretically, neural networks can be viewed as parametric nonlinear regression models due to their fixed architecture and loss function. However, in practice, deep neural networks (DNNs) are highly flexible because various aspects are not fixed in advance.

DNNs offer several advantages:

* They can fit models of unprecedented complexity, significantly outperforming traditional nonlinear regression methods.
* They scale well to massive datasets due to their efficient training algorithms.
* They achieve impressive accuracy in certain tasks, surpassing other statistical learning models.

Real-world applications involve numerous choices, including architecture, preprocessing, regularization, optimization, and loss functions. These choices effectively expand the class of models beyond what would be expected in theory. Therefore, in practice, DNNs are not constrained to a specific model type but rather adapt to the task at hand."
Why can&#39;t a single ReLU learn a ReLU?,https://stats.stackexchange.com/questions/379884/why-cant-a-single-relu-learn-a-relu,machine-learning|neural-networks|optimization|keras,379884,True,379957,4418,1,25,1546016814,1543737558,22,1543778600,"There's a hint in your plots of the loss as a function of $w$. These plots have a ""kink"" near $w=0$: that's because on the left of 0, the gradient of the loss is vanishing to 0 (however, $w=0$ is a suboptimal solution because the loss is higher there than it is for $w=1$). Moreover, this plot shows that the loss function is non-convex (you can draw a line that crosses the loss curve in 3 or more locations), so that signals that we should be cautious when using local optimizers such as SGD. Indeed, the following analysis shows that when $w$ is initialized to be negative, it is possible to converge to a suboptimal solution. 
The optimization problem is 
$$
\begin{align}
\min_{w,b} &\|f(x)-y\|_2^2 \\
f(x) &= \max(0, wx+b)
\end{align}
$$
and you're using first-order optimization to do so. A problem with this approach is that $f$ has gradient
$$
f^\prime(x)=
\begin{cases}
w, & \text{if $x>0$} \\
0, & \text{if $x<0$}
\end{cases}
$$
When you start with $w<0$, you'll have to move to the other side of $0$ to come closer to the correct answer, which is $w=1$. This is hard to do, because when you have $|w|$ very, very small, the gradient will likewise become vanishingly small. Moreover, the closer you get to 0 from the left, the slower your progress will be!
This is why in your plots for initializations that are negative $w^{(0)} <0 $, your trajectories all stall out near $w^{(i)}=0$. This is also what your second animation is showing.
This is related to the dying relu phenomenon; for some discussion, see My ReLU network fails to launch (https://stats.stackexchange.com/questions/188040/my-relu-network-fails-to-launch/366651#366651)
An approach which might be more successful would be to use a different nonlinearity such as the leaky relu, which does not have the so-called ""vanishing gradient"" issue. The leaky relu function is 
$$
g(x)=
\begin{cases}
x, & \text{if $x>0$} \\
cx, & \text{otherwise}
\end{cases}
$$
where $c$ is a constant so that $|c|$ is small and positive. The reason that this works is the derivative isn't 0 ""on the left.""
$$
g^\prime(x)=
\begin{cases}
1, & \text{if $x>0$} \\
c, & \text{if $x < 0$}
\end{cases}
$$
Setting $c=0$ is the ordinary relu. Most people choose $c$ to be something like $0.1$ or $0.3$. I haven't seen $c<0$ used, though I'd be interested to see a study of what effect, if any, it has on such networks. (Note that for $c=1,$ this reduces to the identity function; for $|c|>1$, compositions of many such layers may cause exploding gradients because the gradients become larger in successive layers.)
Slightly modifying OP's code provides a demonstration that the issue lies with the choice of activation function. This code initializes $w$ to be negative and uses the LeakyReLU in place of the ordinary ReLU. The loss quickly decreases to a small value, and the weight correctly moves to $w=1$, which is optimal.
 (https://i.sstatic.net/rs7YI.png)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, ReLU
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

batch = 1000


def tests():
    while True:
        test = np.random.randn(batch)

        # Generate ReLU test case
        X = test
        Y = test.copy()
        Y[Y < 0] = 0

        yield X, Y


model = Sequential(
    [Dense(1, 
           input_dim=1, 
           activation=None, 
           use_bias=False)
    ])
model.add(keras.layers.LeakyReLU(alpha=0.3))
model.set_weights([[[-10]]])

model.compile(loss='mean_squared_error', optimizer='sgd')


class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.weights = []
        self.n = 0
        self.n += 1

    def on_epoch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        w = model.get_weights()
        self.weights.append([x.flatten()[0] for x in w])
        self.n += 1


history = LossHistory()

model.fit_generator(tests(), steps_per_epoch=100, epochs=20,
                    callbacks=[history])

fig, (ax1, ax2) = plt.subplots(2, 1, True, num='Learning')

ax1.set_title('LeakyReLU learning ReLU')
ax1.semilogy(history.losses)
ax1.set_ylabel('Loss')
ax1.grid(True, which=""both"")
ax1.margins(0, 0.05)

ax2.plot(history.weights)
ax2.set_ylabel('Weight')
ax2.set_xlabel('Epoch')
ax2.grid(True, which=""both"")
ax2.margins(0, 0.05)

plt.tight_layout()
plt.show()

Another layer of complexity arises from the fact that we're not moving infinitesimally, but instead in finitely many ""jumps,"" and these jumps take us from one iteration to the next. This means that there are some circumstances where negative initial vales of $w$ won't get stuck; these cases arise for particular combinations of $w^{(0)}$ and gradient descent step sizes large enough to ""jump"" over the vanishing gradient.
I've played around with this code some and I've found that leaving the initialization at $w^{(0)}=-10$ and changing the optimizer from SGD to Adam, Adam + AMSGrad or SGD + momentum does nothing to help. Moreover, changing from SGD to Adam actually slows the progress in addition to not helping to overcome the vanishing gradient on this problem.
On the other hand, if you change the initialization to $w^{(0)}=-1$ and change the optimizer to Adam (step size 0.01), then you can actually overcome the vanishing gradient. It also works if you use $w^{(0)}=-1$ and SGD with momentum (step size 0.01). It even works if you use vanilla SGD (step size 0.01) and $w^{(0)}=-1$.
The relevant code is below; use opt_sgd or opt_adam.
opt_sgd = keras.optimizers.SGD(lr=1e-2, momentum=0.9)
opt_adam = keras.optimizers.Adam(lr=1e-2, amsgrad=True)
model.compile(loss='mean_squared_error', optimizer=opt_sgd)

","**Summary:**

The paragraph analyzes the optimization of a loss function with a non-convex kink at $w=0$. The optimization goal is to find the optimal $w$ that minimizes the loss.

**Issues with Negative Initializations:**

* When $w$ is initialized negatively, it can get stuck near $w=0$ due to the vanishing gradient.
* This is because the gradient of the loss function approaches zero as $w$ approaches zero from the left.

**Vanishing Gradient Phenomenon:**

* The derivative of the ReLU activation function is zero for negative inputs.
* This leads to a vanishing gradient when $w$ is negative, making it difficult to move towards the optimal solution.

**Alternative Activation Function (Leaky ReLU):**

* The leaky ReLU activation function is a modified version of ReLU that has a non-zero gradient for negative inputs.
* Using leaky ReLU prevents the vanishing gradient issue and allows the optimization to converge to the optimal solution.

**Effect of Optimizer:**

* While SGD cannot overcome the vanishing gradient, Adam and momentum-based optimizers can do so under certain conditions.
* If $w$ is initialized at $-1$, Adam, momentum-based SGD, and even vanilla SGD can overcome the issue with a small step size."
Autoencoders can&#39;t learn meaningful features,https://stats.stackexchange.com/questions/130809/autoencoders-cant-learn-meaningful-features,machine-learning|neural-networks|feature-engineering|restricted-boltzmann-machine|autoencoders,130809,True,130848,26608,2,25,1552959059,1420009695,15,1420058610,"Debugging neural networks usually involves tweaking hyperparameters, visualizing the learned filters, and plotting important metrics. Could you share what hyperparameters you've been using?

What's your batch size?
What's your learning rate?
What type of autoencoder are you're using?
Have you tried using a Denoising Autoencoder? (What corruption values have you tried?)
How many hidden layers and of what size?
What are the dimensions of your input images?

Analyzing the training logs is also useful. Plot a graph of your reconstruction loss (Y-axis) as a function of epoch (X-axis). Is your reconstruction loss converging or diverging?
Here's an example of an autoencoder for human gender classification that was diverging, was stopped after 1500 epochs, had hyperparameters tuned (in this case a reduction in the learning rate), and restarted with the same weights that were diverging and eventually converged.

Here's one that's converging: (we want this)

Vanilla ""unconstrained""can run into a problem where they simply learn the identity mapping. That's one of the reasons why the community has created the Denoising, Sparse, and Contractive flavors.
Could you post a small subset of your data here? I'd be more than willing to show you the results from one of my autoencoders. 
On a side note: you may want to ask yourself why you're using images of graphs in the first place when those graphs could easily be represented as a vector of data. I.e.,
[0, 13, 15, 11, 2, 9, 6, 5]

If you're able to reformulate the problem like above, you're essentially making the life of your auto-encoder easier. It doesn't first need to learn how to see images before it can try to learn the generating distribution.
Follow up answer (given the data.)
Here are the filters from a 1000 hidden unit, single layer Denoising Autoencoder. Note that some of the filters are seemingly random. That's because I stopped training so early and the network didn't have time to learn those filters.

Here are the hyperparameters that I trained it with:
batch_size = 4
epochs = 100
pretrain_learning_rate = 0.01
finetune_learning_rate = 0.01
corruption_level = 0.2

I stopped pre-training after the 58th epoch because the filters were sufficiently good to post here. If I were you, I would train a full 3-layer Stacked Denoising Autoencoder with a 1000x1000x1000 architecture to start off.
Here are the results from the fine-tuning step:
validation error 24.15 percent
test error 24.15 percent

So at first look, it seems better than chance, however, when we look at the data breakdown between the two labels we see that it has the exact same percent (75.85% profitable and 24.15% unprofitable). So that means the network has learned to simply respond ""profitable"", regardless of the signal. I would probably train this for a longer time with a larger net to see what happens. Also, it looks like this data is generated from some kind of underlying financial dataset. I would recommend that you look into Recurrent Neural Networks after reformulating your problem into the vectors as described above. RNNs can help capture some of the temporal dependencies that is found in timeseries data like this. Hope this helps. 
","**Debugging Neural Networks (Autoencoders)**

Debugging neural networks involves adjusting **hyperparameters**, such as batch size, learning rate, and hidden layer configuration. To analyze training, plot **reconstruction loss** over epochs to monitor convergence.

**Hyperparameter Optimization**

To optimize **autoencoder** hyperparameters, consider:

* Batch size: 4
* Learning rate (pretraining): 0.01
* Learning rate (finetuning): 0.01
* Corruption level (Denoising Autoencoders): 0.2

**Example**

For a human gender classification autoencoder, adjusting the learning rate (reduction) helped to **converge** the training process.

**Additional Tips**

* Consider **Denoising Autoencoders** to prevent identity mappings.
* **Reformulate image data** as vectors to simplify the learning task.
* For time-series data, explore **Recurrent Neural Networks** to capture temporal dependencies."
What is the architecture of a stacked convolutional autoencoder?,https://stats.stackexchange.com/questions/137537/what-is-the-architecture-of-a-stacked-convolutional-autoencoder,neural-networks|deep-learning|autoencoders|deep-belief-networks,137537,True,147867,12301,3,25,1505556429,1423816119,12,1429764356,"I am currently exploring stacked-convolutional autoencoders.
I will try and answer some of your questions to the best of my knowledge. Mind you, I might be wrong so take it with a grain of salt.

Yes, you have to ""reverse"" pool and then convolve with a set of filters to recover your output image. A standard neural network (considering MNIST data as input, i.e. 28x28 input dimensions) would be:
    28x28(input) -- convolve with 5 filters, each filter 5x5 -->  5 @ 28 x 28 maps -- maxPooling --> 5 @ 14 x 14 (Hidden layer) -- reverse-maxPool --> 5 @ 28 x 28 -- convolve with 5 filters, each filter 5x5 --> 28x28 (output)

My understanding is that conventionally that is what one should do, i.e. train each layer separately. After that you stack the layers and train the entire network once more using the pre-trained weights. However, Yohsua Bengio has some research (the reference escapes my memory) showcasing that one could construct a fully-stacked network and train from scratch.
My understanding is that ""noise layer"" is there to introduce robustness/variability in the input so that the training does not overfit. 
As long as you are still ""training"" pre-training or fine-tuning, I think the  reconstruction part (i.e. reversePooling, de-convolution etc) is necesary. Otherwise how should one perform error-back-propagation to tune weights?
I have tried browsing through numerous papers, but the architecture is never explained in full. If you find any please do let me know.  

","**Summary:**

Stacked-convolutional autoencoders involve using a sequence of convolutional layers to encode and decode data. To recover output from the encoder, pooling operations must be reversed through reverse-pooling and convolution.

Traditionally, each layer in the autoencoder is trained separately, followed by stacking and retraining the entire network. However, research by Yoshua Bengio suggests training a fully-stacked network from scratch.

A ""noise layer"" can inject variability into the input to prevent overfitting. To fine-tune weights through error back-propagation, the reconstruction phase (reverse-pooling, deconvolution) is necessary.

Despite extensive research, comprehensive architectural explanations of stacked-convolutional autoencoders remain scarce."
Why is the cost function of neural networks non-convex?,https://stats.stackexchange.com/questions/281240/why-is-the-cost-function-of-neural-networks-non-convex,machine-learning|neural-networks|optimization|loss-functions|convex,281240,True,281256,7539,1,24,1503434537,1495554233,47,1495559623,"$\sum_i (y_i- \hat y_i)^2$ is indeed convex in $\hat y_i$. But if $\hat y_i = f(x_i ; \theta)$ it may not be convex in $\theta$, which is the situation with most non-linear models, and we actually care about convexity in $\theta$ because that's what we're optimizing the cost function over.
For example, let's consider a network with 1 hidden layer of $N$ units and a linear output layer: our cost function is
$$
g(\alpha, W) = \sum_i \left(y_i - \alpha_i\sigma(Wx_i)\right)^2
$$
where $x_i \in \mathbb R^p$ and $W \in \mathbb R^{N \times p}$ (and I'm omitting bias terms for simplicity). This is not necessarily convex when viewed as a function of $(\alpha, W)$ (depending on $\sigma$: if a linear activation function is used then this still can be convex). And the deeper our network gets, the less convex things are.
Now define a function $h : \mathbb R \times \mathbb R \to \mathbb R$ by $h(u, v) = g(\alpha, W(u, v))$ where $W(u,v)$ is $W$ with $W_{11}$ set to $u$ and $W_{12}$ set to $v$. This allows us to visualize the cost function as these two weights vary.
The figure below shows this for the sigmoid activation function with $n=50$, $p=3$, and $N=1$ (so an extremely simple architecture). All data (both $x$ and $y$) are iid $\mathcal N(0,1)$, as are any weights not being varied in the plotting function. You can see the lack of convexity here.
 (https://i.sstatic.net/A2feE.png)
Here's the R code that I used to make this figure (although some of the parameters are at slightly different values now than when I made it so they won't be identical):
costfunc <- function(u, v, W, a, x, y, afunc) {
  W[1,1] <- u; W[1,2] <- v
  preds <- t(a) %*% afunc(W %*% t(x))
  sum((y - preds)^2)
}

set.seed(1)
n <- 75  # number of observations
p <- 3   # number of predictors
N <- 1   # number of hidden units


x <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)  # all noise
a <- matrix(rnorm(N), N)
W <- matrix(rnorm(N * p), N, p)

afunc <- function(z) 1 / (1 + exp(-z))  # sigmoid

l = 400  # dim of matrix of cost evaluations
wvals <- seq(-50, 50, length = l)  # where we evaluate costfunc
fmtx <- matrix(0, l, l)
for(i in 1:l) {
  for(j in 1:l) {
    fmtx[i,j] = costfunc(wvals[i], wvals[j], W, a, x, y, afunc)
  }
}

filled.contour(wvals, wvals, fmtx,plot.axes = { contour(wvals, wvals, fmtx, nlevels = 25, 
                                           drawlabels = F, axes = FALSE, 
                                           frame.plot = FALSE, add = TRUE); axis(1); axis(2) },
               main = 'NN loss surface', xlab = expression(paste('W'[11])), ylab = expression(paste('W'[12])))

","**Main Ideas:**

* Summing squared residuals is convex in predicted values ($\hat y_i$) but may not be convex in model parameters ($\theta$) for nonlinear models.

* When optimizing a cost function over parameters ($\theta$), convexity in $\theta$ is crucial.

* To illustrate this, consider a network with a hidden layer and a loss function that is not necessarily convex in the weights ($W$) and bias ($\alpha$).

* A visualization shows that the loss function is non-convex as the weights $W_{11}$ and $W_{12}$ vary.

* This implies that optimizing the loss function over model parameters can be challenging due to the presence of local minima."
Why do neural networks need feature selection / engineering?,https://stats.stackexchange.com/questions/349155/why-do-neural-networks-need-feature-selection-engineering,neural-networks|deep-learning|feature-selection|feature-engineering,349155,True,349202,13280,3,24,1528489772,1527760258,28,1527778556,"
What if the ""sufficiently deep"" network is intractably huge, either making model training too expensive (AWS fees add up!) or because you need to deploy the network in a resource-constrained environment?
How can you know, a priori that the network is well-parameterized? It can take a lot of experimentation to find a network that works well.
What if the data you're working with is not ""friendly"" to standard analysis methods, such as a binary string comprising thousands or millions of bits, where each sequence has a different length? 
What if you're interested in user-level data, but you're forced to work with a database that only collects transaction-level data?
Suppose your data are the form of integers such as $12, 32, 486, 7$, and your task is to predict the sum of the digits, so the target in this example is $3, 5, 18, 7$. It's dirt simple to parse each digit into an array and then sum the array (""feature engineering"") but challenging otherwise.

We would like to live in a world where data analysis is ""turnkey,"" but these kinds of solutions usually only exist in special instances. Lots of work went into developing deep CNNs for image classification - prior work had a step that transformed each image into a fixed-length vector.
Feature engineering lets the practitioner directly transform knowledge about the problem into a fixed-length vector amenable to feed-forward networks. Feature selection can solve the problem of including so many irrelevant features that any signal is lost, as well as dramatically reducing the number of parameters to the model.
","Training deep neural networks can pose challenges due to extensive computational costs, resource constraints, and non-standard data formats. Moreover, feature engineering and selection techniques are often necessary to convert raw data into a format compatible with feed-forward networks.

Feature engineering allows experts to incorporate domain knowledge into fixed-length vectors, simplifying network training. Feature selection helps reduce dimensionality and eliminate redundant features, improving model efficiency and reducing overfitting.

While turnkey solutions for data analysis are desirable, they are often limited to specific applications. Specialized techniques, such as image conversion for CNNs, demonstrate the need for customized approaches when dealing with unique data formats.

In summary, addressing the challenges of deep network training and non-standard data requires a combination of feature engineering, feature selection, and tailored solutions to optimize model performance and efficiency."
Fine Tuning vs. Transferlearning vs. Learning from scratch,https://stats.stackexchange.com/questions/343763/fine-tuning-vs-transferlearning-vs-learning-from-scratch,deep-learning|computer-vision|object-detection|transfer-learning,343763,True,387095,60524,3,24,1726094370,1525165929,31,1547464260,"Transfer learning is when a model developed for one task is reused to work on a second task. Fine-tuning is one approach to transfer learning where you change the model output to fit the new task and train only the output model.
In Transfer Learning or Domain Adaptation, we train the model with a dataset. Then, we train the same model with another dataset that has a different distribution of classes, or even with other classes than in the first training dataset).
In Fine-tuning, an approach of Transfer Learning, we have a dataset, and we use let's say 90% of it in training. Then, we train the same model with the remaining 10%. Usually, we change the learning rate to a smaller one, so it does not have a significant impact on the already adjusted weights. You can also have a base model working for a similar task and then freezing some of the layers to keep the old knowledge when performing the new training session with the new data. The output layer can also be different and have some of it frozen regarding the training.
In my experience learning from scratch leads to better results, but it is much costly than the others especially regarding time and resources consumption.
Using Transfer Learning you should freeze some layers, mainly the pre-trained ones and only train in the added ones, and decrease the learning rate to adjust the weights without mixing their meaning for the network. If you speed up the learning rate you normally face yourself with poor results due to the big steps in the gradient descent optimisation. This can lead to a state where the neural network cannot find the global minimum but only a local one.
Using a pre-trained model in a similar task, usually have great results when we use Fine-tuning. However, if you do not have enough data in the new dataset or even your hyperparameters are not the best ones, you can get unsatisfactory results. Machine learning always depends on its dataset and network's parameters. And in that case, you should only use the ""standard"" Transfer Learning.
So, we need to evaluate the trade-off between the resources and time consumption with the accuracy we desire, to choose the best approach.
","**Summary:**

Transfer learning is a technique where a model trained for one task is reused for a different task. One approach to transfer learning is fine-tuning, where the model output is modified to fit the new task and only the output model is retrained.

Transfer learning can also involve training a model on different datasets with varying class distributions.

In fine-tuning, a model is pre-trained on a dataset and then further trained on a new dataset with a different task. To preserve knowledge from the pre-training, layers in the model may be frozen.

While training from scratch can yield better results, it is more resource-intensive. Transfer learning, and specifically fine-tuning, offer a less expensive alternative.

To optimize results, it's important to freeze layers in the pre-trained model and only train in newly added layers while lowering the learning rate. Inadequate data or hyperparameters can hinder the effectiveness of fine-tuning.

The choice between training from scratch and transfer learning depends on the trade-off between accuracy and resource consumption."
"In neural nets, why use gradient methods rather than other metaheuristics?",https://stats.stackexchange.com/questions/207450/in-neural-nets-why-use-gradient-methods-rather-than-other-metaheuristics,neural-networks|optimization|deep-learning|gradient-descent|backpropagation,207450,True,237030,5387,2,24,1555254229,1460704467,15,1474912357,"Extending @Dikran Marsupial's answer....
Anna Choromanska and her colleagues in Yan LeCunn's group at NYU, address this in their 2014 AISTATS paper ""The Loss Surface of Multilayer Nets"" (https://arxiv.org/pdf/1412.0233v3.pdf). Using random matrix theory, along with some experiments, they argue that:


For large-size networks, most local minima are equivalent and yield similar performance on a test set.
The probability of finding a ""bad"" (high value) local minimum is non-zero for small-size networks and decreases quickly with networks size.
Struggling to find the global minimum on the training set (as opposed to one of the many good local ones) is not useful in practice and may lead to overfitting.

[From page 2 of the paper]

In this view, there's not a great reason to deploy heavy-weight approaches for finding the global minimum. That time would be better spent trying out new network topologies, features, data sets, etc.  
That said, lots of people have thought about augmenting or replacing SGD. For fairly small networks (by contemporary standards), these improved metahuristics do seem to do something Mavrovouniotis and Yang (2016) (http://link.springer.com/article/10.1007/s00500-014-1334-5) show that ant colony optimization + backprop beats unmodified backprop on several benchmark data sets (albeit not by much). Rere el al. (2015) (http://www.sciencedirect.com/science/article/pii/S1877050915035759) use simulated annealing to train a CNN and find it initially performs better on the validation set. After 10 epochs, however, only a a very small (and not-tested-for-significance) difference in performance remains. The faster convergence-per-epoch advantage is also offset by a dramatically larger amount of computation time per epoch, so this is not a obvious win for simulated annealing.
It is possible that these heuristics do a better job of initializing the network and once it has been pointed down the right path, any optimizer will do. Sutskever et al. (2013) from Geoff Hinton's group argue something like this in their 2013 ICML paper (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf). 
","**Summary:**

- **Most local minima in large neural networks are equivalent** and yield similar performance.
- **The risk of finding a poor local minimum is low** for large networks but higher for small networks.
- **Finding the global minimum is not crucial** and may lead to overfitting.
- **Heavy-weight approaches to find the global minimum are not recommended.**
- **Improved metaheuristics for optimization do not significantly outperform** standard SGD in most cases.
- **Metaheuristics may be useful for initializing** networks, but any optimizer can then be used."
Train a Neural Network to distinguish between even and odd numbers,https://stats.stackexchange.com/questions/161189/train-a-neural-network-to-distinguish-between-even-and-odd-numbers,machine-learning|classification|categorical-data|neural-networks|genetic-algorithms,161189,True,161195,15753,8,24,1647780536,1436791936,9,1436793014,"As with any machine learning task, the representation of your input plays a crucial role in how well you learn and generalise.
I think, the problem with the representation is that the function (modulo) is highly non-linear and not smooth in the input representation you've chosen for this problem.
I would try the following:

Try a better learning algorithm (back-propagation/gradient descent and its variants).
Try representing the numbers in binary using a fixed length precision.
If your input representation is a b-bit number, I would ensure your training set isn't biased towards small or large numbers.  Have numbers that are uniformly, and independently chosen at random from the range $[0, 2^b-1]$.
As you've done, use a multi-layer network (try 2 layers first: i.e., hidden+output, before using more layers).
Use a separate training+test set.  Don't evaluate your performance on the training set.

","**Summary:**

The performance of machine learning models depends heavily on the representation of their input data. In the context of a specific problem involving the modulus function, the input representation is non-linear and non-smooth.

To improve the model's performance, several recommendations are made:

* Use a more suitable learning algorithm, such as back-propagation or gradient descent.
* Represent the input numbers in binary using a fixed-length precision.
* Ensure that the training data contains a uniform distribution of numbers across the entire range of possible values.
* Use a multi-layer network with at least a hidden layer and an output layer.
* Establish separate training and test sets to avoid overfitting and ensure accurate evaluation."
What is the origin of the autoencoder neural networks?,https://stats.stackexchange.com/questions/238381/what-is-the-origin-of-the-autoencoder-neural-networks,neural-networks|autoencoders|history,238381,True,238783,19549,4,24,1677155987,1475589164,10,1475765920,"According to the history provided in Schmidhuber, ""Deep learning in neural networks: an overview (https://arxiv.org/abs/1404.7828),"" Neural Networks (2015), auto-encoders were proposed as a method for unsupervised pre-training in Ballard, ""Modular learning in neural networks,"" Proceedings AAAI (1987). It's not clear if that's the first time auto-encoders were used, however; it's just the first time that they were used for the purpose of pre-training ANNs.
As the introduction to the Schmidhuber article makes clear, it's somewhat difficult to attribute all of the ideas used in ANNs because the literature is diverse and terminology has evolved over time.
","**Summary:**

Auto-encoders, a type of neural network, were introduced in 1987 by Ballard as a method for unsupervised pre-training of Artificial Neural Networks (ANNs). However, it is uncertain if they were first introduced at that time.

Tracing the origins of ideas in ANNs is challenging due to the extensive and evolving literature, making it difficult to accurately attribute specific concepts."
"In a convolutional neural network (CNN), when convolving the image, is the operation used the dot product or the sum of element-wise multiplication?",https://stats.stackexchange.com/questions/335321/in-a-convolutional-neural-network-cnn-when-convolving-the-image-is-the-opera,deep-learning|convolutional-neural-network|matrix,335321,True,335500,32475,5,23,1598714098,1521420345,12,1521486008,"I believe the key is that when the filter is convolving some part of the image (the ""receptive field"") each number in the filter (i.e. each weight) is first flattened into vector format.  Likewise, the pixels of the image are also flattened into vector format.  THEN, the dot product is calculated.  Which is the exact same thing as finding the sum of the element-by-element (element-wise) multiplication. 
Of course, these  flattened vectors can also be combined in a matrix format, as the below image shows.  In this case then true matrix multiplication can be used, but it's important to note that the flattening the image pixels from each convolution and also the weights filter is the precursor.  
 (https://i.sstatic.net/IQWdM.png)
image credit: TensorFlow and Deep Learning without a PhD, Part 1 (Google Cloud Next '17) (https://www.youtube.com/watch?v=u4alGiomYP4&t=641s)
","Convolution in neural networks involves a dot product calculation between flattened vectors of filter weights and image pixels in a ""receptive field."" The filter weights are first flattened into a vector, and pixels in the receptive field are also flattened. Then, the dot product is calculated, which is equivalent to the sum of element-wise multiplications.

Alternatively, a matrix-multiplication approach can be used, where flattened vectors are arranged in a matrix format. However, both approaches require the initial flattening of filter weights and image pixels. The flattening process ensures that the convolution operation can be performed efficiently, allowing the network to learn features and patterns within the input data."
Importance of the bias node in neural networks,https://stats.stackexchange.com/questions/153933/importance-of-the-bias-node-in-neural-networks,neural-networks|deep-learning|bias-node,153933,True,153945,17018,3,23,1490743856,1432577121,19,1432584031,"Removing the bias will definitely affect the performance and here is why... 
Each neuron is like a simple logistic regression and you have $y=\sigma(W x + b)$. The input values are multiplied with the weights and the bias affects the initial level of squashing in the sigmoid function (tanh etc.), which results the desired the non-linearity.
For example, assume that you want a neuron to fire $y\approx1$ when all the input pixels are black $x\approx0$. If there is no bias no matter what weights $W$ you have, given the equation $y=\sigma(W x)$ the neuron will always fire $y\approx0.5$.

Therefore, by removing the bias terms you would substantially decrease your neural network's performance.
","**Summary:**

Removing bias terms from neurons negatively impacts neural network performance due to the following reasons:

* Neurons are like logistic regressions, and bias affects the squashing function's initial level.
* Without bias, neurons cannot output desired values, such as close to 1 for dark input pixels.
* Removing bias prevents neurons from capturing non-linear relationships in data.
* Consequently, reducing bias terms significantly diminishes the neural network's overall performance and accuracy."
How are weights updated in the batch learning method in neural networks?,https://stats.stackexchange.com/questions/174708/how-are-weights-updated-in-the-batch-learning-method-in-neural-networks,machine-learning|neural-networks,174708,True,189251,30089,3,23,1603031257,1443547581,16,1451943413,"Using average or sum are equivalent, in the sense that there exist pairs of learning rates for which they produce the same update.
To confirm this, first recall the update rule:
$$\Delta w_{ij} = -\alpha \frac{\partial E}{\partial w_{ij}}$$
Then, let $\mu_E$ be the average error for a dataset of size $n$ over an epoch. The sum of error is then $n\mu_E$, and because $n$ doesn't depend on $w$, this holds:
$$\Delta w_{ij} = -\alpha \frac{\partial (n\mu)}{\partial w_{ij}}= -\alpha n\frac{\partial \mu}{\partial w_{ij}}$$
To your second question, the phrase ""accumulating the delta weights"" would imply that one of these methods retains weight updates. That isn't the case: Batch learning accumulates error. There's only one, single $\Delta w$ vector in a given epoch. (Your pseudocode code omits the step of updating the weights (https://en.wikipedia.org/wiki/Backpropagation#Algorithm), after which one can discard $\Delta w$.)
","**Summary:**

* Using the average or sum of errors in weight updates results in identical outcomes for certain learning rates.

* This is because the sum of errors can be expressed as the product of the number of data points and the average error. When applying the update rule, the number of data points becomes a constant factor, leaving the average error as the relevant measure.

* The term ""accumulating the delta weights"" refers to accumulating errors, not weight updates. In both batch and stochastic gradient descent, only a single update vector is computed per epoch, which is then used to update the weights. After the weights are updated, the update vector is discarded."
Does machine learning really need data-efficient algorithms?,https://stats.stackexchange.com/questions/529926/does-machine-learning-really-need-data-efficient-algorithms,machine-learning|neural-networks|sample-size|small-sample|efficiency,529926,True,529938,5133,11,22,1628681339,1623190095,37,1623195503,"You are not entirely wrong, often it will be a lot easier to collect more/better data to improve an algorithm than to squeeze minor improvements out of the algorithm.
However, in practice, there are many settings, where it is difficult to get really large dataset.
Sure, it's easy to get really large datasets when you use (self-/un-)supervised approaches or if your labels are automatically created (e.g. if you are Google whether a user clicks on a link or not). However, many practical problems rely on human experts (whose time may be expensive) to label the examples. When any human can do the job (e.g. labeling dog or cat or something else for ImageNet), this can be scaled to millions of images, but when you pay physicians to classify medical images, tens of thousands (or perhaps 100,000ish) labelled images is a pretty large dataset. Or, if you need to run a chemical experiment for each label.
Additionally, there may be cases, where the or the number of possible real-world examples is naturally limited (e.g. training data for forecasting winners of US presidential elections, predicting eruptions of a volcanoes from seismic data etc., which are just things for which we so far can only have so much data).
","**Summary:**

While collecting more data is often more feasible than improving algorithms, some practical settings face challenges in acquiring large datasets.

* Human-annotated data can be expensive, limiting dataset size.
* Certain problems inherently have limited real-world examples, such as forecasting rare events.

Therefore, in scenarios where large datasets are difficult to obtain, it may be necessary to explore alternative approaches, such as:

* Leveraging self-/un-supervised methods or automated labeling.
* Scaling labeling tasks to millions of images when possible.
* Exploring techniques to extract more information from limited data."
Minimum number of layers in a deep neural network,https://stats.stackexchange.com/questions/229619/minimum-number-of-layers-in-a-deep-neural-network,machine-learning|neural-networks|deep-learning|terminology,229619,True,229659,17565,3,22,1553961847,1471045482,26,1471093915,"""Deep"" is a marketing term: you can therefore use it whenever you need to market your multi-layered neural network.
","The term ""deep"" has become a marketing buzzword associated with multi-layered neural networks, a type of artificial intelligence architecture. This terminology is employed to enhance the marketability of such networks."
What does &quot;permutation invariant&quot; mean in the context of neural networks doing image recognition?,https://stats.stackexchange.com/questions/120089/what-does-permutation-invariant-mean-in-the-context-of-neural-networks-doing-i,machine-learning|neural-networks|terminology|convolutional-neural-network|definition,120089,True,120095,27016,2,22,1501766433,1413360975,32,1413363174,"In this context this refers to the fact that the model does not assume any spatial relationships between the features. E.g. for multilayer perceptron, you can permute the pixels and the performance would be the same. This is not the case for convolutional networks, which assume neighbourhood relations.
","**Summary:**

Traditional machine learning models, such as multilayer perceptrons, treat data points independently. Permuting (rearranging) the data features does not affect the model's performance.

In contrast, convolutional networks assume relationships between neighboring data points. Permuting the features would disrupt these relationships and degrade the model's performance. This assumption of spatial relationships is crucial for convolutional networks to effectively process data with spatial structure, such as images."
Keras: why does loss decrease while val_loss increase?,https://stats.stackexchange.com/questions/260294/keras-why-does-loss-decrease-while-val-loss-increase,machine-learning|cross-validation|deep-learning|tensorflow|theano,260294,True,260346,64722,2,22,1546394212,1486401083,30,1486416357,"(this may be a duplicate) It looks like your model is over fitting, that is just memorizing the training data. In general a model that over fits can be improved by adding more dropout, or training and validating on a larger data set. Explain more about the data/features and the model for further ideas.  
","**Summary:**

Overfitting occurs when a model memorizes the training data instead of learning generalizable patterns. This can be addressed by:

* **Adding Dropout:** Dropout randomly drops out neurons during training, preventing the model from relying too heavily on specific features.
* **Using a Larger Dataset:** Training and validating on a larger dataset provides the model with more examples to learn from, reducing overfitting.

Further improvement may require examining the data and features used. If the data is noisy or contains irrelevant features, cleaning and feature selection can help. Complex models with many parameters are more prone to overfitting, so it may be necessary to use simpler models or reduce the number of parameters."
Why is step function not used in activation functions in machine learning?,https://stats.stackexchange.com/questions/271701/why-is-step-function-not-used-in-activation-functions-in-machine-learning,machine-learning|neural-networks|activation-function,271701,True,318772,20722,5,22,1723006161,1491266354,18,1513228902,"There are two main reasons why we cannot use the Heaviside step function in (deep) Neural Net:

At the moment, one of the most efficient ways to train a multi-layer
neural network is by using gradient descent with backpropagation. A
requirement for backpropagation algorithm is a differentiable
activation function. However, the Heaviside step function is
non-differentiable at x = 0 and it has 0 derivative elsewhere. This
means that gradient descent wonât be able to make a progress in
updating the weights.
Recall that the main objective of the neural network is to learn the
values of the weights and biases so that the model could produce a
prediction as close as possible to the real value. In order to do
this, as in many optimisation problems, weâd like a small change in
the weight or bias to cause only a small corresponding change in the
output from the network. By doing this, we can continuously tweaked
the values of weights and bias towards resulting the best
approximation. Having a function that can only generate either 0 or
1 (or yes and no), won't help us to achieve this objective.

","**Summary:**

The Heaviside step function is not suitable for neural networks due to the following reasons:

**Non-differentiability:** The Heaviside step function is non-differentiable at x = 0, which prevents its use in backpropagation, a key training algorithm for neural networks. Without differentiability, gradient descent cannot update the weights of the network effectively.

**Binary Output:** The Heaviside step function only generates binary outputs (0 or 1), limiting its ability to produce continuous approximations of real-world data. Neural networks aim to learn weights and biases that result in small changes in output corresponding to small changes in weights and biases. However, the binary nature of the Heaviside step function makes this impossible."
Why do we use masking for padding in the Transformer&#39;s encoder?,https://stats.stackexchange.com/questions/422890/why-do-we-use-masking-for-padding-in-the-transformers-encoder,neural-networks|natural-language,422890,True,446571,16566,6,22,1697537765,1566277784,15,1580095587,"I hadn't realized this question was unanswered. If I were to attempt to answer my own question, we apply masks to the source data because after the data passes through the Encoder sublayer, there are values for the padding sequences. We don't need nor want the model to attend to these padding sequences, and so we mask them out.
It's slightly different from masking in the decoder in the sense that masking in the decoder takes an additional step of having a ""no peeking"" mechanism so that our model can't look at future tokens.
","**Summary:**

Masking is used in neural models to prevent attention mechanisms from considering padded data sequences. In the Encoder sublayer, padding values are masked out because they represent empty data and should not influence the model's predictions.

In contrast, masking in the Decoder sublayer serves an additional purpose: it incorporates a ""no peeking"" mechanism to prevent the model from prematurely attending to future tokens in the sequence. This ensures that the model focuses on the current token and makes predictions based on context information up to that point."
Why in Variational Auto Encoder (Gaussian variational family) we model $\log\sigma^2$ and not $\sigma^2$ (or $\sigma$) itself?,https://stats.stackexchange.com/questions/353220/why-in-variational-auto-encoder-gaussian-variational-family-we-model-log-sig,neural-networks|variational-bayes|generative-models,353220,True,353222,5702,1,22,1530006514,1530005452,23,1530006514,"it brings stability and ease of training. 
by definition sigma has to be a positive real number. one way to enforce this would be to use a ReLU funtion to obtain its value, but the gradient is not well defined around zero. in addition, the standard deviation values are usually very small 1>>sigma>0. the optimization has to work with very small numbers, where the floating point arithmetic and the poorly defined gradient bring numerical instabilities.
if you use the log transform, you map the numerically unstable very small numbers in [1,0] interval to [log(1), -inf], where you have a lot more space to work with. calculating log and exp are numerically stable and easy, so you basically gain space where your optimization variable can move within. 
please do not confuse: people do not use the log(sigma) value as the sigma value, but always transform it back to the original space. also in VAEs, you need the log(sigma) value in the Kullback-Leibler divergence term, so you need to calculate it anyways...
","In machine learning, optimizing sigma (a positive real number representing stability and ease of training) can be numerically unstable. Using a ReLU function to obtain sigma's value introduces a poorly defined gradient at zero, while the standard deviation values are typically very small.

To address this, a log transform is used, mapping the small numbers in [1,0] to [log(1), -inf], providing more space for optimization and numerical stability. However, the log(sigma) value is not used directly but is transformed back to the original space. Additionally, the log(sigma) value is calculated for use in the Kullback-Leibler divergence term in Variational Autoencoders (VAEs)."
How exactly to compute Deep Q-Learning Loss Function?,https://stats.stackexchange.com/questions/249355/how-exactly-to-compute-deep-q-learning-loss-function,least-squares|deep-learning|loss-functions|reinforcement-learning|q-learning,249355,True,250005,17287,2,22,1590548524,1480719106,15,1481042933,"After reviewing the equations a few more times. I think the correct loss is the following:
$$\mathcal{L}  = (11.1 - 4.3)^2$$
My reasoning is that the q-learning update rule for the general case is only updating the q-value for a specific $state,action$ pair.
$$Q(s,a) = r + \gamma \max_{a*}Q(s',a*)$$
This equation means that the update happens only for one specific $state,action$ pair and for the neural q-network that means the loss is calculated only for one specific output unit which corresponds to a specific $action$.
In the example provided $Q(s,a) = 4.3$ and the $target$ is $r + \gamma \max_{a*}Q(s',a*) = 11.1$.
","The loss equation provided, $\mathcal{L} = (11.1 - 4.3)^2$, aims to minimize the difference between the predicted value of a specific action, given a state ($4.3$), and the desired target value ($11.1$).

This equation aligns with the q-learning update rule, which focuses on updating the q-value for a particular state and action pair. In neural network terms, this implies that the loss is calculated only for a single output unit corresponding to that specific action.

Therefore, the loss equation effectively captures the error between the current q-value prediction and the desired target for the specific action under consideration."
Classification with noisy labels?,https://stats.stackexchange.com/questions/218656/classification-with-noisy-labels,machine-learning|neural-networks|loss-functions|noise,218656,True,218696,14126,2,22,1667595283,1465818821,12,1465833434,"The right thing to do here is to change the model, not the loss. Your goal is still to correctly classify as many data points as possible (which determines the loss), but your assumptions about the data have changed (which are encoded in a statistical model (https://en.wikipedia.org/wiki/Statistical_model), the neural network in this case).
Let $\mathbf{p}_t$ be a vector of class probabilities produced by the neural network and $\ell(y_t, \mathbf{p}_t)$ be the cross-entropy loss for label $y_t$. To explicitly take into account the assumption that 30% of the labels are noise (assumed to be uniformly random), we could change our model to produce 
$$\mathbf{\tilde p}_t = 0.3/N + 0.7 \mathbf{p}_t$$
instead and optimize
$$\sum_t \ell(y_t, 0.3/N + 0.7 \mathbf{p}_t),$$
where $N$ is the number of classes. This will actually behave somewhat according to your intuition, limiting the loss to be finite.
","**Summary:**

When encountering noisy data in machine learning models, it is suggested to modify the model rather than the loss function to account for the noise. Instead of assuming that all data points are reliable, a modified model can incorporate the assumption that a certain percentage of labels are random noise.

This adjustment is made by incorporating a new vector of class probabilities, $\mathbf{\tilde p}_t$, which represents the probability distribution of the data points with the assumption of noise. The new loss function, $\ell^*$, is then optimized to minimize the cross-entropy loss of the modified class probabilities.

The modified model effectively limits the loss function to a finite value, even in the presence of noise, by adjusting the class probability distribution to account for the random noise. This ensures that the model can still correctly classify as many data points as possible while considering the uncertainty introduced by the noise."
Why is this prediction of time series &quot;pretty poor&quot;?,https://stats.stackexchange.com/questions/306276/why-is-this-prediction-of-time-series-pretty-poor,time-series|neural-networks|predictive-models|deep-learning|prediction,306276,True,306283,4579,3,21,1698973588,1507134857,32,1507136903,"It's sort of an optical illusion: the eye looks at the graph, and sees that the red and blue graphs are right next to each. The problem is that they are right next to each other horizontally, but what matters is the vertical distance. The eye most easily see the distance between the curves in the two-dimensional space of the Cartesian graph, but what matters is the one-dimensional distance within a particular t value. For example, suppose we had points A1= (10,100), A2 = (10.1, 90), A3 = (9.8,85), P1 = (10.1,100.1), and P2 = (9.8, 88). The eye is naturally going to compare P1 to A1, because that is the closest point, while P2 is going to be compared to A2. Since P1 is closer to A1 than P2 is to A3, P1 is going to look like a better prediction. But when you compare P1 to A1, you're just looking at how well A1 is able to just repeat what it saw earlier; with respect to A1, P1 isn't a prediction. The proper comparison is between P1 v. A2, and P2 v. A3, and in this comparison P2 is better than P1. It would have been clearer if, in addition to plotting y_actual and y_pred against t, there had been graphs of (y_pred-y_actual) against t.
","The perception of distance between curves in a graph can be misleading. The eye naturally focuses on the horizontal separation, but the vertical distance is more important. To accurately compare predictions, the distance should be measured vertically within the same time point (t value). This means comparing P1 with A2 and P2 with A3, rather than P1 with A1 and P2 with A2. When this is done, it may become clear that P2 is a better prediction than P1, even though P1 appears closer to A1 horizontally. To clarify, a graph of (y_pred - y_actual) could be included to show the vertical distance between the actual and predicted values directly."
The reason of superiority of Limited-memory BFGS over ADAM solver,https://stats.stackexchange.com/questions/315626/the-reason-of-superiority-of-limited-memory-bfgs-over-adam-solver,machine-learning|neural-networks|optimization|scikit-learn|adam,315626,True,315632,15518,2,21,1718709159,1511630122,37,1511633755,"There are a lot of reasons that this could be the case. Off the top of my head I can think of one plausible cause, but without knowing more about the problem it is difficult to suggest that it is the one. 
An L-BFGS solver is a true quasi-Newton method in that it estimates the curvature of the parameter space via an approximation of the Hessian. So if your parameter space has plenty of long, nearly-flat valleys then L-BFGS would likely perform well. It has the downside of additional costs in performing a rank-two update to the (inverse) Hessian approximation at every step. While this is reasonably fast, it does begin to add up, particularly as the input space grows. This may account for the fact that ADAM outperforms L-BFGS for you as you get more data.
ADAM is a first order method that attempts to compensate for the fact that it doesn't estimate the curvature by adapting the step-size in every dimension. In some sense, this is similar to constructing a diagonal Hessian at every step, but they do it cleverly by simply using past gradients. In this way it is still a first order method, though it has the benefit of acting as though it is second order. The estimate is cruder than that of the L-BFGS in that it is only along each dimension and doesn't account for what would be the off-diagonals in the Hessian. If your Hessian is nearly singular then these off-diagonals may play an important role in the curvature and ADAM is likely to underperform relative the BFGS.
","L-BFGS and ADAM are optimization algorithms used to minimize functions. L-BFGS estimates the curvature of the parameter space using an approximated Hessian. It works well when the parameter space has long, flat valleys but is computationally expensive as it updates the Hessian approximation at each step.

ADAM, a first-order method, adapts the step size in each dimension to compensate for its lack of curvature estimation. It's less computationally expensive than L-BFGS but provides a cruder estimate of the curvature.

In situations where the Hessian is nearly singular (i.e., has small off-diagonal values), L-BFGS may perform better than ADAM due to its ability to capture the off-diagonal curvature. However, for larger datasets with more data, ADAM may outperform L-BFGS due to its lower computational cost."
Is there a way to incorporate new data into an already trained neural network without retraining on all my data in Keras?,https://stats.stackexchange.com/questions/352750/is-there-a-way-to-incorporate-new-data-into-an-already-trained-neural-network-wi,neural-networks|train|keras,352750,True,352771,25503,2,21,1529702302,1529694031,28,1529702302,"In keras, you can save your model using model.save and then load that model using model.load. If you call .fit again on the model that you've loaded, it will continue training from the save point and will not restart from scratch. Each time you call .fit, keras will continue training on the model. .fit does not reset model weights. 
I would like to point out one issue that might arise from training your model this way though, and that issue is catastrophic forgetting (https://en.wikipedia.org/wiki/Catastrophic_interference). If you feed your model examples that differ significantly from previous training examples, it might be prone to catastrophic forgetting. This is basically when the neural network learns your new examples well and forgets all the previous examples because you are no longer feeding those examples to it. It arises because as optimizers get more efficient, the neural network will be get more efficient at fitting new data quickly - and the best way to fit the new data quickly might be to forget old data. If your future data is very similar to your current data, then this won't be a problem. But imagine you trained a named entity recognition system to recognize organizations. If in the future you feed it a bunch of data that teach it how to recognize people names, it might catastrophically forget how to recognize organizations.
","**Summary:**

In Keras, calling `.fit()` on a loaded model continues training from the saved point rather than resetting it. This allows for incremental training.

However, this approach may lead to ""catastrophic forgetting,"" where the model forgets previously learned information when introduced to significantly different data. This occurs because optimizers prioritize fitting new data swiftly, which can result in discarding old knowledge.

Catastrophic forgetting is less likely when future data aligns closely with past data. However, it can become an issue when the model encounters drastically different information from its initial training set. For instance, a model trained for organization recognition might struggle to recall this knowledge if it's subsequently trained on person name recognition."
What *is* an Artificial Neural Network?,https://stats.stackexchange.com/questions/362425/what-is-an-artificial-neural-network,machine-learning|neural-networks|deep-learning|unsupervised-learning|supervised-learning,362425,True,362427,1989,4,21,1675253171,1534380113,10,1534380622,"JÃ¼rgen Schmidhuber, ""Deep Learning in Neural Networks: An Overview (https://arxiv.org/abs/1404.7828)"" traces the history of key concepts in neural networks and deep learning. In his view, neural networks would appear to encompass essentially any model which can be characterized as a directed graph where each node represents some computational unit. Schmidhuber is a prominent neural networks researcher, and wrote the original paper on LSTM networks with Sepp Hochreiter.

Which modifiable components of a learning system are responsible for its success or failure? What changes to them improve performance? This has been called the fundamental credit assignment problem (Minsky, 1963). There are general credit assignment methods for universal problem solvers that are time-optimal in various theoretical senses (Sec. 6.8). The present survey, however, will focus on the narrower, but now commercially important, subfield of Deep Learning (DL) in Artificial Neural Networks (NNs).
A standard neural network (NN) consists of many simple, connected processors called neurons, each producing a sequence of real-valued activations. Input neurons get activated through sensors perceiving the environment, other neurons get activated through weighted connections from previously active neurons (details in Sec. 2). Some neurons may influence the environment by triggering actions. Learning or credit assignment is about finding weights that make the NN exhibit desired behavior, such as driving a car. Depending on the problem and how the neurons are connected, such behavior may require long causal chains of computational stages (Sec. 3), where each stage transforms (often in a non-linear way) the aggregate activation of the network. Deep Learning is about accurately assigning credit across many such stages.
Shallow NN-like models with few such stages have been around for many decades if not centuries (Sec. 5.1). Models with several successive nonlinear layers of neurons date back at least to the 1960s (Sec. 5.3) and 1970s (Sec. 5.5). An efficient gradient descent method for teacher-based Supervised Learning (SL) in discrete, differentiable networks of arbitrary depth called backpropagation (BP) was developed in the 1960s and 1970s, and applied to NNs in 1981 (Sec. 5.5). BP-based training of deep NNs with many layers, however, had been found to be difficult in practice by the late 1980s (Sec. 5.6), and had become an explicit research subject by the early 1990s (Sec. 5.9). DL became practically feasible to some extent through the help of Unsupervised Learning (UL), e.g., Sec. 5.10 (1991), Sec. 5.15 (2006). The 1990s and 2000s also saw many improvements of purely supervised DL (Sec. 5). In the new millennium, deep NNs have finally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines (Vapnik, 1995; Scholkopf et al., 1998) in numerous important applications. In fact, since 2009, supervised deep NNs have won many official international pattern recognition competitions (e.g., Sec. 5.17, 5.19, 5.21, 5.22), achieving the first superhuman visual pattern recognition results in limited domains (Sec. 5.19, 2011). Deep NNs also have become relevant for the more general field of Reinforcement Learning (RL) where there is no supervising teacher (Sec. 6).

On the other hand, I'm not sure that it's necessarily profitable to try and construct a taxonomy of mutually-exclusive buckets for machine learning strategies. I think we can say that there are perspectives from which models can be viewed as neural networks. I don't think that perspective is necessarily the best or useful in all contexts. For example, I'm still planning to refer to random forests and gradient boosted trees as ""tree ensembles"" instead of abstracting away their distinctions and calling them ""neural network trees"". Moreover, Schmidhuber distinguishes NNs from kernel machines -- even though kernel machines have some connections to NNs -- when he writes ""In the new millennium, deep NNs have finally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines ... in numerous important applications. ""
","JÃ¼rgen Schmidhuber's paper traces the history of key concepts in neural networks and deep learning. He defines neural networks as models that can be represented as directed graphs where each node is a computational unit.

**Deep learning** focuses on assigning credit for success or failure across multiple stages of computation in neural networks. Despite the existence of shallow neural networks for decades, deep neural networks with many layers have only recently become feasible due to advances in unsupervised learning and improvements in supervised learning.

Schmidhuber notes that it may not be beneficial to create strict categories for machine learning strategies. He distinguishes neural networks from kernel machines, even though they have some similarities."
Data augmentation on training set only?,https://stats.stackexchange.com/questions/320800/data-augmentation-on-training-set-only,machine-learning|deep-learning|regularization|data-augmentation,320800,True,320967,16171,3,21,1549122085,1514563045,17,1514689252,"In terms of the concept of augmentation, ie making the data set bigger for some reason, we'd tend to only augment the training set. We'd evaluate the result of different augmentation approaches on a validation set.
However, as @Åukasz Grad points out, we might need to perform a similar procedure to the test set as was done on the training set. This is typically so that the input data from the test set resembles as much as possible that of the training set. For example, @Åukasz Grad points out the example of image cropping, where we'd need to crop the test images too, so they are the same size as the training images. However, in the case of the training images, we might use each training image multiple times, with crops in different locations/offsets. At test time we'd likely either do a single centred crop, or do random crops and take an average.
Running the augmentation procedure against test data is not to make the test data bigger/more accurate, but just to make the input data from the test set resemble that of the input data from the training set, so we can feed it into the same net (eg same dimensions). We'd never consider that the test set is 'better' in some way, by applying an augmentation procedure. At least, that's not something I've ever seen.
On the other hand, for the training set, the point of the augmentation is to reduce overfitting during training. And we evaluate the quality of the augmentation by then running the trained model against our more-or-less fixed test/validation set.
","**Summary:**

Data augmentation involves increasing the size of the training set by modifying existing data. While it's common to augment the training set only, it may be necessary to apply similar transformations to the test set for consistency.

Augmentation aims to make the test set data resemble the training set data as closely as possible. This ensures compatibility with the trained model and prevents misinterpretations about improving the test set's accuracy through augmentation.

In contrast, augmentation on the training set reduces overfitting by exposing the model to a wider range of data. The effectiveness of augmentation is evaluated by observing model performance on a separate test set."
How do CNN&#39;s avoid the vanishing gradient problem,https://stats.stackexchange.com/questions/130596/how-do-cnns-avoid-the-vanishing-gradient-problem,machine-learning|optimization|deep-learning|gradient-descent,130596,True,130603,25620,1,21,1518351289,1419852331,17,1419856397,"The vanishing gradient problem requires us to use small learning rates with gradient descent which then needs many small steps to converge. This is a problem if you have a slow computer which takes a long time for each step. If you have a fast GPU which can perform many more steps in a day, this is less of a problem.
There are several ways to tackle the vanishing gradient problem. I would guess that the largest effect for CNNs came from switching from sigmoid nonlinear units to rectified linear units. If you consider a simple neural network whose error $E$ depends on weight $w_{ij}$ only through $y_j$, where
$$y_j = f\left( \sum_iw_{ij}x_i \right),$$
its gradient is
\begin{align}
\frac{\partial}{\partial w_{ij}} E
&= \frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial w_{ij}} \\
&= \frac{\partial E}{\partial y_j} \cdot f'\left(\sum_i w_{ij} x_i\right) x_i.
\end{align}
If $f$ is the logistic sigmoid function, $f'$ will be close to zero for large inputs as well as small inputs. If $f$ is a rectified linear unit,
\begin{align}
f(u) = \max\left(0, u\right),
\end{align}
the derivative is zero only for negative inputs and 1 for positive inputs. Another important contribution comes from properly initializing the weights. This paper looks like a good source for understanding the challenges in more details (although I haven't read it yet):
http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
","**Summary**

The vanishing gradient problem arises due to small learning rates and the slow convergence of gradient descent. This problem is less severe with powerful GPUs that can execute numerous iterations quickly.

To address this problem, rectified linear units (ReLUs) have been found to alleviate the issue. ReLUs have a non-zero derivative for positive inputs, unlike sigmoid units which have near-zero derivatives for both large and small inputs.

Proper weight initialization is another key technique. By properly initializing weights, the problem can be mitigated. For more detailed insights, the recommended paper discusses the challenges and provides solutions."
Using RNN (LSTM) for predicting the timeseries vectors (Theano),https://stats.stackexchange.com/questions/159148/using-rnn-lstm-for-predicting-the-timeseries-vectors-theano,neural-networks|python|lstm,159148,True,159747,35064,3,21,1644500400,1435574312,10,1435910632,"I finally found a way and documented it on my blog here (http://www.danielhnyk.cz/predicting-sequences-vectors-keras-using-rnn-lstm).
There is comparison of several frameworks and then also one implementation in Keras.
","The author announces the publication of a blog post detailing a method for predicting sequences through vectorization using RNN-LSTM models. The blog post presents a comparison of different frameworks for this purpose, followed by an implementation guide using Keras. By providing this resource, the author aims to simplify the process of sequence prediction for readers interested in using deep learning techniques."
Which comes first - domain expertise or an experimental approach?,https://stats.stackexchange.com/questions/558754/which-comes-first-domain-expertise-or-an-experimental-approach,machine-learning|hypothesis-testing|neural-networks|classification,558754,True,558757,1502,4,20,1641133691,1640866583,27,1640870534,"This will probably be closed quickly as opinion-based, but here is a point you may want to consider.
200 features is a lot, and 30k rows is less than it sounds like. A ""fishing expedition"" to find relevant features is quite likely to overfit and select spurious features (https://www.tylervigen.com/spurious-correlations). The danger is that when you go to your domain experts with these features you ""found"" to be relevant, they may not push back. Instead, it's a very common human reaction to start telling stories about how these features are indeed useful, because we humans are very good at explaining stuff, even stuff that is simply noise.
Talking to your domain experts first will not completely avoid this problem, but it may reduce the number of wild goose chases.
You may be interested in my answer to ""How to know that your machine learning problem is hopeless?"" (https://stats.stackexchange.com/a/222189/1352).
","**Summary:**

It's risky to blindly search for relevant features in large datasets using ""fishing expeditions."" This approach can lead to overfitting and the selection of spurious features.

Consulting domain experts before feature selection can mitigate this issue. Domain experts can provide valuable insights and prevent the selection of irrelevant or misleading features.

While talking to experts may not completely eliminate false positives, it can reduce the likelihood of embarking on fruitless ""wild goose chases."""
Why do we use Gaussian distributions in Variational Autoencoder?,https://stats.stackexchange.com/questions/402569/why-do-we-use-gaussian-distributions-in-variational-autoencoder,neural-networks|normal-distribution|gaussian-mixture-distribution|weights|variational-bayes,402569,True,402610,13958,2,20,1555081527,1555021159,22,1555055706,"Normal distribution is not the only distribution used for latent variables in VAEs. There are also works using von Mises-Fisher distribution (Hypershperical VAEs [1]), and there are VAEs using Gaussian mixtures, which is useful for unsupervised [2] and semi-supervised [3] tasks.
Normal distribution has many nice properties, such as analytical evaluation of the KL divergence in the variational loss, and also we can use the reparametrization trick for efficient gradient computation (however, the original VAE paper [4] names many other distributions for which that works). Moreover, one of the apparent advantages of VAEs is that they allow generation of new samples by sampling in the latent spaceâwhich is quite easy if it follows Gaussian distribution. Finally, as @shimao remarked, it does not matter so much what distribution latent variables follow since using the non-linear decoder it can mimic arbitrarily complicated distribution of observations. It is simply convenient.
As for the second question, I agree with @shimao's answer.

[1]: Davidson, T.R., Falorsi, L., De Cao, N., Kipf, T. and Tomczak, J.M., 2018. Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891. (https://arxiv.org/abs/1804.00891)
[2]: Dilokthanakul, N., Mediano, P.A., Garnelo, M., Lee, M.C., Salimbeni, H., Arulkumaran, K. and Shanahan, M., 2016. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648. (https://arxiv.org/pdf/1611.02648)
[3]: Kingma, D.P., Mohamed, S., Rezende, D.J. and Welling, M., 2014. Semi-supervised learning with deep generative models. In Advances in neural information processing systems (pp. 3581-3589). (http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf)
[4]: Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. (https://arxiv.org/pdf/1312.6114)
","**Summary:**

Variational Autoencoders (VAEs) typically use normal distributions for latent variables due to their analytical properties and ease of sample generation. However, alternative distributions can also be employed.

Von Mises-Fisher distribution is used in Hyperspherical VAEs, while Gaussian mixtures are valuable for unsupervised and semi-supervised learning tasks.

The choice of latent variable distribution is not crucial as the nonlinear decoder can model complex observation distributions.

Despite the convenience of normal distribution, other distributions may be more suitable for specific tasks."
Why pure exponent is not used as activation function for neural networks?,https://stats.stackexchange.com/questions/532309/why-pure-exponent-is-not-used-as-activation-function-for-neural-networks,machine-learning|neural-networks|perceptron,532309,True,532313,6073,1,20,1624744285,1624724312,34,1624726048,"I think the most prominent reason is stability. Think about having consequent layers with exponential activation, and what happens to the output when you input a small number to the NN (e.g. $x=1$), the forward calculation will look like:
$$o=\exp(\exp(\exp(\exp(1))))\approx e^{3814279}$$
It can go crazy very quickly and I don't think you can train deep networks with this activation function unless you add other mechanisms like clipping.
","**Summary:**

The stability of a neural network can be compromised by using exponential activation functions, which can amplify small input values into excessively large outputs. This instability becomes more pronounced with deeper networks.

Specifically, in a network with consecutive exponential layers, even small inputs (e.g., 1) can result in exponentially increasing outputs that reach astronomical values (e.g., e^3,814,279). This extreme output amplification makes it challenging to train deep networks with exponential activations.

To alleviate this instability, mechanisms like clipping are often used to limit the output values within a reasonable range. However, such workarounds introduce additional complexity and may impact the network's ability to learn complex patterns effectively."
MNIST digit recognition: what is the best we can get with a fully connected NN only? (no CNN),https://stats.stackexchange.com/questions/376312/mnist-digit-recognition-what-is-the-best-we-can-get-with-a-fully-connected-nn-o,machine-learning|neural-networks|deep-learning|image-processing|backpropagation,376312,True,378614,27250,3,20,1714771330,1541847830,15,1543110729,"Yann LeCun has compiled a big list of results (http://yann.lecun.com/exdb/mnist/) (and the associated papers) on MNIST, which may be of interest.
The best non-convolutional neural net result is by CireÅan, Meier, Gambardella and Schmidhuber (2010) (https://www.mitpressjournals.org/doi/pdf/10.1162/NECO_a_00052) (arXiv (https://arxiv.org/abs/1003.0358)), who reported an accuracy of 99.65%. As their abstract describes, their approach was essentially brute force:

Good old on-line back-propagation for plain multi-layer perceptrons
yields a very low 0.35% error rate on the famous MNIST handwritten
digits benchmark. All we need to achieve this best result so far are
many hidden layers, many neurons per layer, numerous deformed training
images, and graphics cards to greatly speed up learning.

The network itself was a six layer MLP with 2500, 2000, 1500, 1000, 500, and 10 neurons per layer, and the training set was augmented with affine and elastic deformations. The only other secret ingredient was a lot of compute--the last few pages describe how they parallelized it.
A year later, the same group (Meier et al., 2011) (http://people.idsia.ch/%7Ejuergen/icdar2011b.pdf) reported similar results using an ensemble of 25 one-layer neural networks (0.39% test error*). These were individually smaller (800 hidden units), but the training strategy is a bit fancier.
Similar strategies with convnets do a little bit better (~0.23% test error*). However, MLPs are are universal approximators, so I can't see why a suitable MLP wouldn't be able to match that though it might be very large and difficult to train directly.
* Annoyingly very few of these papers report confidence intervals, standard errors, or anything like that, making it difficult to directly compare these results.
","Yann LeCun has compiled a comprehensive list of MNIST results, including a non-convolutional neural network result by CireÅan et al. (2010) that achieved 99.65% accuracy on the MNIST handwritten digit dataset. Their approach used brute force: deep multi-layer perceptrons (MLPs) with numerous hidden layers, neurons, deformed training images, and graphics cards for faster training. The network consisted of six layers with varying neuron counts and used affine and elastic deformations to augment the training set.

A year later, Meier et al. (2011) reported similar results using an ensemble of 25 one-layer neural networks, achieving a 0.39% test error. Though slightly smaller than the MLP, the training strategy was more sophisticated.

Convolutional neural networks (convnets) perform slightly better on MNIST, achieving around 0.23% test error. Despite the differences in performance, MLPs remain universal approximators, suggesting that with sufficient size and training, they could theoretically match the accuracy of convnets. However, the lack of reported confidence intervals and standard errors in many studies makes direct result comparison challenging."
what makes neural networks a nonlinear classification model?,https://stats.stackexchange.com/questions/222639/what-makes-neural-networks-a-nonlinear-classification-model,neural-networks|nonlinear-regression|nonlinear,222639,True,222642,23834,3,20,1562777444,1467916545,20,1467917415,"I think you forget the activation function (https://en.wikipedia.org/wiki/Activation_function) in nodes in neural network, which is non-linear and will make the whole model non-linear.
In your formula is not totally correct, where, 
$$
h_1 \neq w_1x_1+w_2x_2
$$
but 
$$
h_1 = \text{sigmoid}(w_1x_1+w_2x_2)
$$
where sigmoid function like this, $\text{sigmoid}(x)=\frac 1 {1+e^{-x}}$
 (https://i.sstatic.net/hcuIK.png)
Let's use a numerical example to explain the impact of the sigmoid function, suppose you have $w_1x_1+w_2x_2=4$ then $\text{sigmoid}(4)=0.99$. On the other hand, suppose you have $w_1x_1+w_2x_2=4000$, $\text{sigmoid}(4000)=1$ and it is almost as same as $\text{sigmoid}(4)$, which is non-linear.

In addition, I think the slide 14 in this tutorial (http://www2.econ.iastate.edu/tesfatsi/NeuralNetworks.CheungCannonNotes.pdf) can show where you did wrong exactly. For $H_1$ please not the otuput is not -7.65, but $\text{sigmoid}(-7.65)$
 (https://i.sstatic.net/0eBCo.png)
","**Summary:**

The paragraph highlights the importance of activation functions in neural networks, which introduce nonlinearity and enhance model accuracy.

The formula provided in the original text incorrectly states that the hidden unit output ($h_1$) is a linear combination of input values ($x_1$ and $x_2$) multiplied by weights ($w_1$ and $w_2$). However, the correct formula includes a sigmoid activation function, which is a nonlinear function:

$$
h_1 = \text{sigmoid}(w_1x_1 + w_2x_2)
$$

The sigmoid function maps values to a range of 0 to 1, effectively squashing large values and amplifying small values. This nonlinearity allows the model to capture complex patterns in the data.

A numerical example illustrates the impact of the sigmoid function. Even for very large inputs (e.g., 4000), the sigmoid function produces values close to 1. This nonlinear behavior makes the model more expressive and capable of fitting a wider range of datasets.

The provided tutorial slide also points out an error in the calculation of $H_1$, which should be the result of applying the sigmoid function to the linear combination of inputs, not the linear combination itself."
Where and why does deep learning shine?,https://stats.stackexchange.com/questions/86856/where-and-why-does-deep-learning-shine,machine-learning|data-mining|deep-learning|deep-belief-networks,86856,True,86892,2298,2,20,1463438597,1392644410,22,1392657796,"The main purported benefits:
(1) Don't need to hand engineer features for non-linear learning problems (save time and scalable to the future, since hand engineering is seen by some as a short-term band-aid) 
(2) The learnt features are sometimes better than the best hand-engineered features, and can be so complex (computer vision - e.g. face-like features) that it would take way too much human time to engineer.
(3) Can use unlabeled data to pre-train the network. Suppose we have 1000000 unlabeled images and 1000 labeled images. We can now drastically improve a supervised learning algorithm by pre-training on the 1000000 unlabeled images with deep learning. In addition, in some domains we have so much unlabeled data but labeled data is hard to find. An algorithm that can use this unlabeled data to improve classification is valuable.
(4) Empirically, smashed many benchmarks that were only seeing incremental improvements until the introduction of deep learning methods.
(5) Same algorithm works in multiple areas with raw (perhaps with minor pre-processing) inputs.
(6) Keeps improving as more data is fed to the network (assuming stationary distributions etc).
","**Summary:**

Deep learning offers several advantages in machine learning:

* **Automated Feature Engineering:** It eliminates the need for manual feature engineering, saving time and effort, especially for non-linear problems.

* **Superior Learned Features:** The learned features often surpass hand-engineered ones, particularly in complex domains (e.g., computer vision).

* **Utilization of Unlabeled Data:** Deep learning algorithms can leverage vast amounts of unlabeled data to enhance supervised learning, making them especially valuable in domains with scarce labeled data.

* **Benchmark Breakthroughs:** Deep learning methods have achieved significant performance improvements across various benchmarks, surpassing prior approaches.

* **Versatility:** Deep learning algorithms can be applied to a wide range of domains with minimal input preprocessing.

* **Scalability:** Their performance typically improves with increasing data availability, enabling continuous model enhancements."
How few training examples is too few when training a neural network?,https://stats.stackexchange.com/questions/226672/how-few-training-examples-is-too-few-when-training-a-neural-network,neural-networks,226672,True,226693,11896,1,20,1475514450,1470057481,22,1470063533,"It really depends on your dataset, and network architecture. One rule of thumb I have read (2) was a few thousand samples per class for the neural network to start to perform very well. 
In practice, people try and see. It's not rare to find studies showing decent results with a training set smaller than 1000 samples.

A good way to roughly assess to what extent it could be beneficial to have more training samples is to plot the performance of the neural network based against the size of the training set, e.g. from (1):
 (https://i.sstatic.net/N2RHk.png)


(1) Dernoncourt, Franck, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits. ""De-identification of Patient Notes with Recurrent Neural Networks (https://arxiv.org/abs/1606.03475)"" arXiv preprint arXiv:1606.03475 (2016).
(2) CireÅan, Dan C., Ueli Meier, and JÃ¼rgen Schmidhuber. ""Transfer learning for Latin and Chinese characters with deep neural networks."" In The 2012 International Joint Conference on Neural Networks (IJCNN), pp. 1-6. IEEE, 2012. https://scholar.google.com/scholar?cluster=7452424507909578812&hl=en&as_sdt=0,22 (https://scholar.google.com/scholar?cluster=7452424507909578812&hl=en&as_sdt=0,22) ; http://people.idsia.ch/~ciresan/data/ijcnn2012_v9.pdf (http://people.idsia.ch/~ciresan/data/ijcnn2012_v9.pdf):

For classification tasks with a few thousand samples per
  class, the benefit of (unsupervised or supervised) pretraining is not easy to demonstrate. 


","The number of training samples required for optimal neural network performance varies based on the dataset and architecture. However, a rule of thumb suggests a few thousand samples per class.

In practice, the optimal sample size is typically determined empirically. Studies have shown that acceptable results can be achieved with training sets under 1,000 samples.

To estimate the potential benefit of increasing the training sample size, plot the network's performance against the size of the training set.

For classification tasks with several thousand samples per class, the benefit of unsupervised or supervised pretraining is less pronounced."
How to construct a cross-entropy loss for general regression targets?,https://stats.stackexchange.com/questions/378274/how-to-construct-a-cross-entropy-loss-for-general-regression-targets,neural-networks|maximum-likelihood|loss-functions|cross-entropy,378274,True,445761,5887,2,20,1579851314,1542894789,16,1579603686,"Suppose that we are trying to infer the parametric distribution $p(y|\Theta(X))$, where $\Theta(X)$ is a vector output inverse link function (https://en.wikipedia.org/wiki/Generalized_linear_model) with $[\theta_1,\theta_2,...,\theta_M]$.
We have a neural network at hand with some topology we decided. The number of outputs at the output layer matches the number of parameters we would like to infer (it may be less if we don't care about all the parameters, as we will see in the examples below).
 (https://i.sstatic.net/jynZU.jpg)
In the hidden layers we may use whatever activation function we like. What's crucial are the output activation functions for each parameter as they have to be compatible with the support of the parameters.
 (https://i.sstatic.net/bXQcA.png)
Some example correspondence:

Linear activation: $\mu$, mean of Gaussian distribution
Logistic activation: $\mu$, mean of Bernoulli distribution
Softplus activation: $\sigma$, standard deviation of Gaussian distribution, shape parameters of Gamma distribution

Definition of cross entropy:
$$H(p,q) = -E_p[\log q(y)] = -\int p(y) \log q(y) dy$$
where $p$ is ideal truth, and $q$ is our model.
Empirical estimate:
$$H(p,q) \approx -\frac{1}{N}\sum_{i=1}^N \log q(y_i)$$
where $N$ is number of independent data points coming from $p$. 
Version for conditional distribution:
$$H(p,q) \approx -\frac{1}{N}\sum_{i=1}^N \log q(y_i|\Theta(X_i))$$
Now suppose that the network output is $\Theta(W,X_i)$ for a given input vector $X_i$ and all network weights $W$, then the training procedure for expected cross entropy is:
$$W_{opt} = \arg \min_W -\frac{1}{N}\sum_{i=1}^N \log q(y_i|\Theta(W,X_i))$$
which is equivalent to Maximum Likelihood Estimation (https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood) of the network parameters.
Some examples:

Regression: Gaussian distribution (https://en.wikipedia.org/wiki/Normal_distribution) with heteroscedasticity

$$\mu = \theta_1 : \text{linear activation}$$
$$\sigma = \theta_2: \text{softplus activation*}$$
$$\text{loss} = -\frac{1}{N}\sum_{i=1}^N \log [\frac{1} {\theta_2(W,X_i)\sqrt{2\pi}}e^{-\frac{(y_i-\theta_1(W,X_i))^2}{2\theta_2(W,X_i)^2}}]$$
under homoscedasticity we don't need $\theta_2$ as it doesn't affect the optimization and the expression simplifies to (after we throw away irrelevant constants):
$$\text{loss} = \frac{1}{N}\sum_{i=1}^N (y_i-\theta_1(W,X_i))^2$$

Binary classification: Bernoulli distribution (https://en.wikipedia.org/wiki/Bernoulli_distribution)

$$\mu = \theta_1 : \text{logistic activation}$$
$$\text{loss} = -\frac{1}{N}\sum_{i=1}^N \log [\theta_1(W,X_i)^{y_i}(1-\theta_1(W,X_i))^{(1-y_i)}]$$
$$= -\frac{1}{N}\sum_{i=1}^N y_i\log [\theta_1(W,X_i)] + (1-y_i)\log [1-\theta_1(W,X_i)]$$
with $y_i \in \{0,1\}$.

Regression: Gamma (https://en.wikipedia.org/wiki/Gamma_distribution) response

$$\alpha \text{(shape)} = \theta_1 : \text{softplus activation*}$$
$$\beta \text{(rate)} = \theta_2: \text{softplus activation*}$$
$$\text{loss} = -\frac{1}{N}\sum_{i=1}^N \log [\frac{\theta_2(W,X_i)^{\theta_1(W,X_i)}}{\Gamma(\theta_1(W,X_i))} y_i^{\theta_1(W,X_i)-1}e^{-\theta_2(W,X_i)y_i}]$$

Multiclass classification: Categorical distribution (https://en.wikipedia.org/wiki/Categorical_distribution)

Some constraints cannot be handled directly by plain vanilla neural network toolboxes (but these days they seem to do very advanced tricks). This is one of those cases:
$$\mu_1 = \theta_1 : \text{logistic activation}$$
$$\mu_2 = \theta_2 : \text{logistic activation}$$
...
$$\mu_K = \theta_K : \text{logistic activation}$$
We have a constraint $\sum \theta_i = 1$. So we fix it before we plug them into the distribution:
$$\theta_i' = \frac{\theta_i}{\sum_{j=1}^K \theta_j}$$
$$\text{loss} = -\frac{1}{N}\sum_{i=1}^N \log [\Pi_{j=1}^K\theta_i'(W,X_i)^{y_{i,j}}]$$
Note that $y$ is a vector quantity in this case. Another approach is the Softmax (https://en.wikipedia.org/wiki/Softmax_function).
*ReLU is unfortunately not a particularly good activation function for $(0,\infty)$ due to two reasons. First of all it has a dead derivative zone on the left quadrant which causes optimization algorithms to get trapped. Secondly at exactly 0 value, many distributions would go singular for the value of the parameter. For this reason it is usually common practice to add a small value $\epsilon$ to assist off-the shelf optimizers and for numerical stability.
As suggested by @Sycorax Softplus activation is a much better replacement as it doesn't have a dead derivative zone.
 (https://i.sstatic.net/O043I.png)
Summary:

Plug the network output to the parameters of the distribution and
take the -log then minimize the network weights.
This is equivalent to Maximum Likelihood Estimation of the
parameters.

","**Summary:**

* **Goal:** Infer a parametric distribution $p(y|\Theta(X))$, where $\Theta(X)$ is an inverse link function representing the parameters.

* **Approach:** Use a neural network with the following architectural considerations:

    * Number of outputs matches the number of parameters to infer.
    * Appropriate output activation functions based on the support of the parameters (e.g., linear for mean, logistic for probability).

* **Training:** Minimize the cross entropy between the model distribution $q(y|\Theta(X))$ and the ground truth distribution $p(y)$, which is equivalent to Maximum Likelihood Estimation.

**Examples:**

* **Regression:** Gaussian distribution with heteroscedasticity (separate mean and standard deviation parameters)
* **Binary classification:** Bernoulli distribution (probability parameter)
* **Multiclass classification:** Categorical distribution (multiple probability parameters summed to 1)

**Advantages:**

* Simplifies training by using a neural network to automatically learn the inverse link function.
* Can handle complex distributions with heteroscedasticity or non-linear relationships between parameters and input data."
What does the convolution step in a Convolutional Neural Network do?,https://stats.stackexchange.com/questions/116362/what-does-the-convolution-step-in-a-convolutional-neural-network-do,neural-networks|deep-learning|convolutional-neural-network|convolution,116362,True,116390,13086,2,20,1513220349,1411415262,16,1411427067,"I'll first try to share some intuition behind CNN and then comment the particular topics you listed.
The convolution and sub-sampling layers in a CNN are not different from the hidden layers in a common MLP, i. e. their function is to extract features from their input. These features are then given to the next hidden layer to extract still more complex features, or are directly given to a standard classifier to output the final prediction (usually a Softmax, but also SVM or any other can be used). In the context of image recognition, these features are images treats, like stroke patterns in the lower layers and object parts in the upper layers.
In natural images these features tend to be the same at all locations. Recognizing a certain stroke pattern in the middle of the images will be as useful as recognizing it close to the borders. So why don't we replicate the hidden layers and connect multiple copies of it in all regions of the input image, so the same features can be detected anywhere? It's exactly what a CNN does, but in a efficient way. After the replication (the ""convolution"" step) we add a sub-sample step, which can be implemented in many ways, but is nothing more than a sub-sample. In theory this step could be even removed, but in practice it's essential in order to allow the problem remain tractable.
Thus:

Correct.
As explained above, hidden layers of a CNN are feature extractors as in a regular MLP. The alternated convolution and sub-sampling steps are done during the training and classification, so they are not something done ""before"" the actual processing. I wouldn't call them ""pre-processing"", the same way the hidden layers of a MLP is not called so.
Correct.

A good image which helps to understand the convolution is CNN page in the ULFDL tutorial. Think of a hidden layer with a single neuron which is trained to extract features from $3 \times 3$ patches. If we convolve this single learned feature over a $5 \times 5$ image, this process can be represented by the following gif:

In this example we were using a single neuron in our feature extraction layer, and we generated $9$ convolved features. If we had a larger number of units in the hidden layer, it would be clear why the sub-sampling step after this is required.
The subsequent convolution and sub-sampling steps are based in the same principle, but computed over features extracted in the previous layer, instead of the raw pixels of the original image.
","Convolutional Neural Networks (CNNs) consist of layers that extract increasingly complex features from input data, such as images. These layers work similarly to hidden layers in Multilayer Perceptrons (MLPs).

CNNs utilize convolution and sub-sampling steps to efficiently detect features that occur anywhere in the input image. Convolution involves replicating and applying feature extractors to different regions of the image. Sub-sampling reduces the spatial dimensions of the convolved features, helping to manage computational complexity.

The convolution step can be visualized as applying a filter with learned weights to a small patch of the input. The resulting convolved feature represents the presence or absence of the target feature within that patch.

Multiple convolution and sub-sampling layers allow CNNs to extract increasingly complex features, such as stroke patterns or object parts. These features can then be classified using conventional methods, such as Softmax or SVM, to make predictions or perform image recognition tasks."
What are the differences between autoencoders and t-SNE?,https://stats.stackexchange.com/questions/153276/what-are-the-differences-between-autoencoders-and-t-sne,neural-networks|deep-learning|dimensionality-reduction|autoencoders|tsne,153276,True,153743,9254,4,20,1561969384,1432165156,13,1432412048,"Both of them try to find a lower dimensionality embedding of your data. However, there are different minimization problems. More specifically, an autoencoder tries to minimize the reconstruction error, while t-SNE tries to find a lower dimensional space and at the same time it tries to preserve the neighborhood distances. As a result of this attribute, t-SNE is usually preferred for plots and visualizations.
","Autoencoders and t-SNE are dimensionality reduction techniques used to embed high-dimensional data into lower-dimensional spaces.

**Autoencoders** minimize reconstruction error, aiming to generate a representation that can accurately reproduce the original data. This approach is suitable for tasks such as denoising, compression, and feature extraction.

**t-SNE** (t-Distributed Stochastic Neighbor Embedding) differs from autoencoders by prioritizing the preservation of neighborhood distances in the lower-dimensional embedding. It aims to create a representation where nearby points in the original high-dimensional space remain close together in the reduced space. This property makes t-SNE particularly valuable for visualization and exploratory data analysis, as it effectively reveals local structures and relationships within the data.

While both techniques offer advantages, autoencoders excel at reconstruction tasks and are less susceptible to local minima, while t-SNE excels at preserving local neighborhood relationships and is preferred for visualization purposes."
What is the significance of the number of convolution filters in a convolutional network?,https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional,deep-learning|convolutional-neural-network,196646,True,249480,25677,1,20,1553289704,1455534653,11,1480798722,"What does the number of filters in a convolution layer convey?
- I usually like to think of filters as feature detectors. Although it depends on the problem domain, the significance # of feature detectors intuitively is the number of features (like edges, lines, object parts etc...) that the network can potentially learn. Also note that each filter generates a feature map. Feature maps allow you to learn the explanatory factors within the image,  so the more # filters means the more the network learns (not necessarily good all the time - saturation and convergence matter the most)
How does this number affect the performance or quality of the architecture?
- I don't think you will find a good answer for these types of question since we are still trying to formalize what is going on inside DL black box. Intuitively once again you will learn a more robust non-linear function the more filter banks you have however the performance is going to depend on the type of task and the data characteristics. You typically want to know what kind of data you are dealing with to  determine the # parameters in your architecture (including filter). How many filters do I need? is more like asking how complex (specially) are the images in my dataset. There isn't any formal notion that relates # filters to performance. Its all experimental and iterative. Lots of trail and error forsure. 
","**Summary:**

- **Number of filters:**
   - Represents the number of feature detectors in a convolution layer, which detect specific features (e.g., edges, lines, object parts) in an image.
   - Each filter generates a feature map, allowing the network to learn explanatory factors within the image.
   - A higher number of filters generally indicates a network's ability to learn more features.

- **Effect on performance:**
   - There is no formal connection between the number of filters and network performance.
   - Intuitively, more filters can lead to a more robust function; however, performance depends on the task and data characteristics.
   - Determining the optimal number of filters requires experimentation and iteration based on the complexity of the images in the dataset."
Differences between logistic regression and perceptrons,https://stats.stackexchange.com/questions/102759/differences-between-logistic-regression-and-perceptrons,neural-networks|logistic,102759,True,139849,12884,3,20,1612262513,1402359938,0,1425226965,"You mentioned already the important differences. So the results should not differ that much. 
","The provided paragraph is brief and does not contain much information to summarize. The only statement it makes is that the results of unspecified actions or comparisons should not differ significantly because important differences have already been addressed. Without additional context or information, it is not possible to provide a detailed or meaningful summary."
Neural networks vs everything else,https://stats.stackexchange.com/questions/344258/neural-networks-vs-everything-else,regression|machine-learning|classification|neural-networks|deep-learning,344258,True,344269,7459,2,19,1525457492,1525363999,33,1525365699,"Each machine learning algorithm has a different inductive bias, so it's not always appropriate to use neural networks. A linear trend will always be learned best by simple linear regression rather than a ensemble of nonlinear networks.
If you take a look at the winners of past Kaggle competitions (https://www.kaggle.com/competitions), excepting any challenges with image/video data, you will quickly find that neural networks are not the solution to everything. Some past solutions here. (http://ndres.me/kaggle-past-solutions/)

apply regularization till you see no over-fitting and then train them to the end

There is no guarantee that you can apply enough regularization to prevent overfitting without completely destroying the capacity of the network to learn anything. In real life, it is rarely feasible to eliminate the train-test gap, and that's why papers still report train and test performance.

they are universal estimators

This is only true in the limit of having an unbounded number of units, which isn't realistic.

you can give me the link to the problem and i would train the best neural network that i can and we can see if 2 layered or 3 layered neural networks falls short of any other benchmark machine learning algorithm

An example problem which I expect a neural network would never be able to solve: Given an integer, classify as prime or not-prime.
I believe this could be solved perfectly with a simple algorithm that iterates over all valid programs in ascending length and finds the shortest program which correctly identifies the prime numbers. Indeed, this 13 character regex string (https://iluxonchik.github.io/regular-expression-check-if-number-is-prime/) can match prime numbers, which wouldn't be computationally intractable to search.


Can regularization take a model from one that overfits to the one that has its representational power severely hamstrung by regularization? Won't there always be that sweet spot in between?

Yes, there is a sweet spot, but it is usually way before you stop overfitting. See this figure:
 (https://i.sstatic.net/bZkJc.png)
If you flip the horizontal axis and relabel it as ""amount of regularization"", it's pretty accurate -- if you regularize until there is no overfitting at all, your error will be huge. The ""sweet spot"" occurs when there is a bit of overfitting, but not too much.

How is a 'simple algorithm that iterates over all valid programs in
  ascending length and finds the shortest program which correctly
  identifies the prime numbers.' an algorithm that learns?

It finds the parameters $\theta$ such that we have a hypothesis $H(\theta)$ which explains the data, just like backpropagation finds the parameters $\theta$ which minimize the loss (and by proxy, explains the data). Only in this case, the parameter is a string instead of many floating point values.


so if i get you correctly you are making the argument that if the data is not substantial the deep network will never hit the validation accuracy of the best shallow network given the best hyperparameters for both?

Yes. Here is an ugly but hopefully effective figure to illustrate my point.
 (https://i.sstatic.net/zrxnh.png)

but that doesnt make sense. a deep network can just learn a 1-1
  mapping above the shallow

The question is not ""can it"", but ""will it"", and if you are training backpropagation, the answer is probably not.

We discussed the fact that larger networks will always work better than smaller networks

Without further qualification, that claim is just wrong.
","Neural networks, despite their popularity, have limitations. They may not be the optimal choice for every machine learning problem. For linear trends, simpler algorithms like linear regression are preferred. Past Kaggle competition winners indicate that neural networks are not always the solution.

Regularization cannot guarantee the prevention of overfitting without compromising learning capacity. Real-world scenarios often show a performance gap between training and testing data.

The claim that neural networks are universal estimators is valid only with an unrealistic number of units.

An example problem where neural networks may struggle is prime number classification. A simple algorithm that searches for the shortest program that identifies prime numbers may surpass neural networks.

Regularization can lead to a ""sweet spot"" where there is slight overfitting, but not excessive, resulting in the model's optimal performance.

The argument presented is that if the data is limited, a deep network may not achieve the validation accuracy of a well-optimized shallow network, even with ideal hyperparameters.

While larger networks generally perform better, this statement requires qualification. In some cases, smaller networks may be more suitable."
Showing machine learning results are statistically irrelevant,https://stats.stackexchange.com/questions/581630/showing-machine-learning-results-are-statistically-irrelevant,time-series|neural-networks|random-forest|r-squared|validation,581630,True,581637,2642,2,19,1657700752,1657557847,32,1657560352,"You answered yourself:

I made two additional models (mean and last sample) which often match or beat the RMSE of the RF and ANN models published in the paper. The mean model just takes the mean of training and uses that in all predictions. The dataset is a timeseries (time-varying, usually 1-2 samples per week), so the last sample model just uses the previous sample's value.

You benchmarked the result with trivial models and they outperform the model. This is enough to discard the model. What you did is a pretty standard procedure for validating time-series models.
Negative $R^2$ are consistent with your benchmarks. In fact, $R^2$ already compares the model to the mean model because it is defined as
$$
R^2 = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y_i)^2}
$$
so the numerator is the sum of squared errors of the model and the denominator is sum of squared errors of the mean model. Your model should have a smaller squared error than the mean model for it to be positive.
Maybe the authors of the published paper didn't run the sanity check? Many crappy results somehow get published.
I'm afraid that if reasonable arguments like comparing the results to the benchmarks don't convince your colleagues, I doubt âa statistical testâ will. They already are willing to ignore the results they don't like, so it seems rather hopeless.
","**Summary:**

The author created two simple models (mean and last sample) for a time-series dataset and found that they performed as well as or better than more complex models reported in a published paper. This suggests that the published models may not be reliable.

The author notes that a negative $R^2$ score is consistent with the underperformance of the published models, as $R^2$ compares a model's accuracy to that of the mean model. Since the published models had higher squared errors than the mean model, their $R^2$ scores were negative.

The author suggests that the authors of the published paper may not have performed basic sanity checks, such as comparing their models to trivial benchmarks. The author expresses skepticism that statistical tests will convince colleagues, who seem willing to disregard inconvenient results."
dropout: forward prop VS back prop in machine learning Neural Network,https://stats.stackexchange.com/questions/219236/dropout-forward-prop-vs-back-prop-in-machine-learning-neural-network,neural-networks|backpropagation|dropout,219236,True,219240,17654,1,19,1466098824,1466097915,32,1466098824,"Yes, the neurons are considered zero during backpropagation as well. Otherwise dropout wouldn't do anything! Remember that forward propagation during training is only used to set up the network for backpropagation, where the network is actually modified (as well as for tracking training error and such).
In general, it's important to account for anything that you're doing in the forward step in the backward step as well â otherwise you're computing a gradient of a different function than you're evaluating.
The way it's implemented in Caffe, for example, is (as can be verified from the source (https://github.com/BVLC/caffe/blob/df412ac0da3e2e7eb194f0c16842fd126496d90d/src/caffe/layers/dropout_layer.cpp)):

In forward propagation, inputs are set to zero with probability $p$, and otherwise scaled up by $\frac{1}{1 - p}$.
In backward propagation, gradients for the same dropped units are zeroed out; other gradients are scaled up by the same $\frac{1}{1-p}$.

","During backpropagation, neurons are treated as inactive (zero) when implementing dropout. This ensures that the gradient computed during backpropagation aligns with the changes made during forward propagation.

To account for actions taken during forward propagation, corresponding adjustments must be made during backpropagation. In the case of dropout, inputs are set to zero with probability p during forward propagation and scaled by 1/(1-p) otherwise. Consequently, during backpropagation, gradients for the inactive neurons are set to zero, while gradients for the active neurons are multiplied by 1/(1-p).

This alignment between forward and backward propagation ensures that the network's weights are modified based on the effective training data that accounts for dropout."
Why are residual connections needed in transformer architectures?,https://stats.stackexchange.com/questions/565196/why-are-residual-connections-needed-in-transformer-architectures,neural-networks|transformers|attention|residual-networks,565196,True,565203,16134,1,19,1645430691,1645421826,31,1645430691,"The reason for having the residual connection in Transformer is more technical than motivated by the architecture design.
Residual connections mainly help mitigate the vanishing gradient problem. During the back-propagation, the signal gets multiplied by the derivative of the activation function. In the case of ReLU, it means that in approximately half of the cases, the gradient is zero. Without the residual connections, a large part of the training signal would get lost during back-propagation. Residual connections reduce effect because summation is linear with respect to derivative, so each residual block also gets a signal that is not affected by the vanishing gradient. The summation operations of residual connections form a path in the computation graphs where the gradient does not get lost.
Another effect of residual connections is that the information stays local in the Transformer layer stack. The self-attention mechanism allows an arbitrary information flow in the network and thus arbitrary permuting the input tokens. The residual connections, however, always ""remind"" the representation of what the original state was. To some extent, the residual connections give a guarantee that contextual representations of the input tokens really represent the tokens.
","**Summary:**

Residual connections in Transformers are primarily introduced to address the vanishing gradient problem during back-propagation. When using ReLU activations, gradients can become zero in half of the cases, potentially losing training signals.

Residual connections mitigate this issue by summing gradients linearly. This creates a path in the computation graph where gradients are not lost, allowing for effective learning.

Additionally, residual connections preserve local information in the Transformer layer stack. While self-attention permits arbitrary information flow, residual connections ensure that representations of input tokens remain contextually relevant, preventing arbitrary token permutations."
What is maxnorm constraint? How is it useful in Convolutional Neural Networks?,https://stats.stackexchange.com/questions/257996/what-is-maxnorm-constraint-how-is-it-useful-in-convolutional-neural-networks,neural-networks|regularization|convolutional-neural-network|optimization,257996,True,258001,24578,2,19,1525853860,1485309101,20,1485310390,"From http://cs231n.github.io/neural-networks-2/#reg (http://cs231n.github.io/neural-networks-2/#reg):

Max norm constraints. Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $\vec{w}$ of every neuron to satisfy $\Vert \vec{w} \Vert_2 < c.$ Typical values of $c$ are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot âexplodeâ even when the learning rates are set too high because the updates are always bounded.

","**Summary:**

Max norm constraints regularize neural networks by imposing an upper bound on neuron weight magnitudes. After performing parameter updates, weight vectors are ""clamped"" to satisfy this constraint.

This approach helps prevent network ""explosion"" caused by high learning rates, as updates are always bounded. It also improves network performance in certain cases.

Typical values for the upper bound (c) are around 3 or 4, and larger values may further enhance regularization effectiveness."
Using neural network for trading in stock exchange,https://stats.stackexchange.com/questions/44840/using-neural-network-for-trading-in-stock-exchange,machine-learning|neural-networks,44840,True,44844,19766,3,19,1578609544,1354357705,29,1354366524,"There are severe flaws with this approach.
First, there are many gambles which usually win, but which are bad gambles. Suppose you have the chance to win \$1 $90\%$ of the time and lose \$100 $10\%$ of the time. This has a negative expected value, but the way you are training the neural network would teach it to recommend such reverse lottery tickets.
Second, you are missing a big point of the stock exchange, which is to manage risk. What determines the price of an investment is not just its return, it is the return versus the risk which can't be hedged away. Investments with high returns and high risks are not necessarily better than investments with low returns and low risk. If you can invest risk-free at $6\%$ and borrow money at $5\%$, this is more valuable than finding a very risky investment with a return of $60\%$. An investment with a negative rate of return may still be valuable if it is strongly negatively correlated with a risky investment with a high rate of return. So, the rate of return is insufficient for evaluating investments.
Third, you should realize that you are competing with other people who also have access to neural networks. There are a lot of commercial programs aimed at day traders based on neural networks. (These are made by people who find it more profitable to sell software to confused day traders than to use their own systems.) There are many proprietary systems, some of which may involve neural networks. To find value they overlook, you need to have some advantage, and you haven't mentioned any.
I'm a big fan of neural networks, but I think typical users of neural networks in the stock market do not understand the basics and burn money.
","**Summary:**

Using neural networks to predict stock returns has several drawbacks.

Firstly, relying solely on return rates can result in recommending poor investments with negative expected values. The stock market considers both return and risk, meaning high returns paired with high risks are not always preferable to low returns with low risks.

Secondly, the rate of return alone is insufficient for evaluating investments. Their value can be influenced by their correlation with other investments, allowing negative-return investments to be valuable if they offset risks of other investments.

Thirdly, using neural networks for stock trading faces competition from commercial programs and proprietary systems. To succeed, traders must have a distinct advantage, which is not provided by typical neural network users.

Overall, neural networks can aid in stock market analysis, but users must understand the limitations of return-based predictions and consider risk management and competitive factors."
Universal approximation theorem for convolutional networks,https://stats.stackexchange.com/questions/306300/universal-approximation-theorem-for-convolutional-networks,neural-networks|convolutional-neural-network|approximation,306300,True,381113,3721,3,19,1589893215,1507146402,10,1544371635,"It seems this question has been answered in the affirmative in this recent article by Dmitry Yarotsky: Universal approximations of invariant maps by neural networks (https://arxiv.org/abs/1804.10306).
The article shows that any translation equivariant function can be approximated arbitrarily well by a convolutional neural network given that it is sufficiently wide, in direct analogy to the classical universal approximation theorem.
","**Summary:**

Dmitry Yarotsky's research (2018) proves that convolutional neural networks (CNNs) can universally approximate any function that remains invariant under translations. This means that CNNs, when sufficiently wide, can accurately model any transformation that does not change the object's location.

This finding parallels the classical universal approximation theorem, which states that feedforward neural networks can approximate any continuous function. Yarotsky's theorem extends this concept to CNNs, highlighting their ability to represent invariant relationships in data.

The result establishes the theoretical foundation for using CNNs in tasks where translation invariance is important, such as image recognition and processing. By approximating invariant functions, CNNs can effectively capture the spatial features of objects regardless of their position within an image."
CNN xavier weight initialization,https://stats.stackexchange.com/questions/198840/cnn-xavier-weight-initialization,normal-distribution|variance|neural-networks|convolutional-neural-network,198840,True,198845,24039,2,19,1580754464,1456534840,16,1456544758,"In this case the amount of neurons should be 5*5*3.  
I found it especially useful for convolutional layers. Often a uniform distribution over the interval $[-c/(in+out), c/(in+out)]$ works as well.  
It is implemented as an option in almost all neural network libraries. Here (https://github.com/fchollet/keras/blob/998efc04eefa0c14057c1fa87cab71df5b24bf7e/keras/initializations.py) you can find the source code of Keras's implementation of Xavier Glorot's initialization.
","**Summary:**

Xavier Glorot's initialization is recommended for convolutional layers and uses a uniform distribution within a specific range to initialize neuron weights. The range is determined by the number of neurons in the input and output layers. The recommended number of neurons for a convolutional layer is 5*5*3. Many neural network libraries, such as Keras, provide this initialization as an option.

Xavier Glorot's initialization is designed to prevent vanishing or exploding gradients, which can hinder neural network training. By initializing weights within a specific range, it ensures that gradients are distributed evenly and do not grow too large or too small during backpropagation.

The Keras implementation of Xavier Glorot's initialization is available for reference in the provided GitHub link. This initialization technique is commonly used in deep learning to improve network performance and stability."
Tensors in neural network literature: what&#39;s the simplest definition out there?,https://stats.stackexchange.com/questions/233253/tensors-in-neural-network-literature-whats-the-simplest-definition-out-there,neural-networks|terminology|definition|tensor,233253,True,233276,2370,2,19,1617374208,1472958295,14,1472975440,"For the purposes of data analysis, you can effectively consider them as arrays, possibly multidimensional. Thus they include scalars, vectors, matrices, and all higher order arrays. 
The precise mathematical definition is more complicated. Basically the idea is that tensors transform multilinear functions to linear functions. See (1) (https://www.youtube.com/watch?v=qHuUazkUcnU) or (2) (https://en.wikipedia.org/wiki/Tensor_product#Universal_property). (Multilinear functions are functions which are linear in each of their components, an example being the determinant considered as a function of column vectors.) 
One consequence of this mathematical property defining tensors is that tensors transform nicely with respect to Jacobians, which encode transformations from one system of coordinates to another. This is why one often sees the definition of tensor as ""an object which transforms in a certain way under changes of coordinates"" in physics. See this video (https://www.youtube.com/watch?v=f5liqUk0ZTw) for example, or this one (https://youtu.be/TC98KfiGAOk?t=16m).
If we are dealing with sufficiently ""nice"" objects (all of the derivatives we would like to exist and well-defined are), then all of these ways of thinking about tensors are essentially equivalent. Note that the first way to think of tensors which I mentioned (multidimensional arrays) ignores the distinction between covariant and contravariant tensors. (The distinction is with regards to how their coefficients change under a change of basis of the underlying vector space, i.e. between row and column vectors essentially.) See these other StackExchange questions: (1) (https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor) (2) (https://math.stackexchange.com/questions/657494/what-exactly-is-a-tensor) (3) (https://math.stackexchange.com/questions/10282/an-introduction-to-tensors) (4) (https://physics.stackexchange.com/questions/32011/what-is-a-tensor)
For a book used by researchers studying applications of tensors to neural networks (for example at Technion in Israel), there is Wolfgang Hackbusch's Tensor Spaces and Numerical Calculus (http://link.springer.com/book/10.1007%2F978-3-642-28027-6). I have not read it yet myself, although some of the later chapters seem to use advanced mathematics.
","Tensors are mathematical objects that can be represented as arrays, including scalars, vectors, matrices, and higher-order arrays. They are defined as objects that transform multilinear functions into linear functions. This property allows tensors to transform consistently with Jacobians, which represent coordinate system transformations.

Colloquially, tensors are often described as objects that transform in a specific manner under coordinate changes. In the context of data analysis, tensors can be viewed as multidimensional arrays. However, this simplified view does not distinguish between covariant and contravariant tensors, which differ in how their coefficients change under basis changes.

Tensors provide a versatile framework for studying and manipulating data, particularly in fields like physics and neural networks. They facilitate the transformation and analysis of data in various coordinate systems. For more detailed understanding, external resources and references are provided, including books, articles, and videos."
How to estimate training time prior to training? (Machine Learning &amp; Neural Networks),https://stats.stackexchange.com/questions/81985/how-to-estimate-training-time-prior-to-training-machine-learning-neural-netw,machine-learning|neural-networks,81985,True,82003,28475,3,19,1600379417,1389514542,8,1389531129,"This question does not really depend on what type of an algorithm you run, it deals with computational complexity of algorithms and as such, it would be better suited for StackOverflow. The computer science guys live for these questions and they are very good at them... 
In either case, the complexity of an algorithm is reported using the big-O notation (http://en.wikipedia.org/wiki/Big_O_notation). Usually, if you look at the wikipedia description of the algo, you can find the information if the bound is known. Alternatively, it is usually reported by the authors of the algorithm, when they publish it.
For example for SVM, the complexity bound is between $\mathit{O}(dn^2)$ and $\mathit{O}(dn^3)$, where n is the number of your data points and d is the number of your features, or dimensionality of the data. (see libSVM implementation in Python Scikit (http://scikit-learn.org/stable/modules/svm.html))
The scenario you describe above would occur if an algorithm has $O(n)$ time complexity. (Complexity of algorithms is measured separately for both time and storage). It means that the run-time scales with the number of examples $\textit{n}$.
Example (starting with $\textit{n}$ inputs for your algorithm):
Algorithm A time complexity $O(n)$:

old input size $\textit{n}$
old run time $\textit{t}$
new input size $\textit{3n}$, 
new run time will be $\textit{3t}$

Algorithm B time complexity $O(n^2)$:

old input size $\textit{n}$
old run time $\textit{t}$
new input size $\textit{3n}$, 
new run time will be $\mathit{9t}$

You can apply the same rule for $\mathit{O}(n^3)$, $\mathit{O}(n\log(n))$, or $\mathit{O}(n!)$. Using these rules, you can make a rough (worst-case) estimation of your run-time.
Now, things are a bit more tricky than that, as the $\mathit{O}$ is an upper bound, but not necessarily a tight upper bound (see this StackOverflow thread (https://stackoverflow.com/questions/464078/difference-between-lower-bound-and-tight-bound)). That means that the $\mathit{O}$ will tell you the worst case run-time, which, depending on your application might prove useless to you, as it will be too large for any sensible planning and you will notice that your average run-times are in fact much lower. 
In that case you might want to look whether there is a ${\Theta}$ for your algorithm (see Bachman-Landau notation (http://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann.E2.80.93Landau_notations)), which is the asymptotically tight upper bound. For many algorithms, the best, worst and average time complexity is reported (http://en.wikipedia.org/wiki/Sorting_algorithm). For many others, we have only a very loose upper bound. Many machine learning algorithms involve a costly operation (http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations) such as matrix inversion, or the SVD at some point, which will effectively determine their complexity.
The other issue is that complexity ignores constant factors, so complexity $\mathit{O}(kn)$ is in fact $\mathit{O}(n)$ as long as $\mathit{k}$ doesn't depend on $\mathit{n}$. But obviously, in practice it can make a difference whether $k=2$ or $k=1e6$.
EDIT 1: Fixed a mistake in runtime based on comment from @H_R
EDIT 2: Re-reading this reply after some years of working with neural networks, it seems quite educational, but unfortunately, utterly useless given the question.
Yes, we can quantify the complexity of an algorithm. In the case of a neural networks it is the number of operations required for a forward and backward pass. However, the question asks about the total training time and not how much longer a forward pass will take if we increase the input.
The training time depends on how long it takes to approximate the relationship between your inputs and outputs sufficiently, i.e. the number of gradient descent iterations you need to make to achieve a sufficiently small loss. This depends completely on how complex is the function you are trying to approximate and how useful and noisy is the sample data you have.
The long and short of it is:
a. I don't believe it is possible to tell in advance how long it will take to train a neural network, particularly a deep neural network (though I'm sure there is existing research trying to do this).
b. It is not even possible to extrapolate reliably the training duration after some training steps. 
c. It is usually not possible to use a reduced set of inputs to train the network and extrapolate from this reduced training time to the full dataset as the network will typically fail to perform well when trained with few data.
This is why people either manually oversee their training loss when training neural networks (tensorflow-board (https://www.tensorflow.org/tensorboard)), or use different heuristics to detect when the loss minimization starts to plateau out (early stopping (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)).
","**Computational Complexity of Algorithms**

The complexity of an algorithm describes its time and storage requirements. It is measured using the big-O notation, which provides an upper bound on the worst-case runtime. For example, an algorithm with O(n) time complexity means its runtime increases linearly with the number of inputs (n).

**Time Complexity**

The time complexity of an algorithm refers to how long it takes to execute. For instance, an algorithm with O(n) time complexity will take three times longer to run when the input size triples.

**Big-O Notation**

Big-O notation captures the asymptotic behavior of an algorithm's runtime. It provides a worst-case estimate that may not be exact in practice. For a tighter estimate, the asymptotically tight upper bound (Î) can be used.

**Additional Considerations**

Constant factors that do not depend on input size are ignored in complexity analysis. However, in practice, these factors can significantly impact runtime performance.

**Training Time Estimation**

For neural networks, the total training time depends on the complexity of the function being approximated and the quality of the input data. It is challenging to predict or extrapolate training time even with partial training data. Instead, manual monitoring or heuristics are typically used to determine when training should end."
Can a neural network with only $1$ hidden layer solve any problem?,https://stats.stackexchange.com/questions/563604/can-a-neural-network-with-only-1-hidden-layer-solve-any-problem,machine-learning|neural-networks,563604,True,563692,6238,5,18,1666707463,1644343319,10,1644408274,"The question asks about ""arbitrary functions"" and ""any problem""; the accepted answer talks only about continuous functions.
The answer to the question as stated now, in both versions, is clearly ""no"". Some fun counterexamples:

""Any problem"" includes Turing's Entscheidungsproblem (https://en.wikipedia.org/wiki/Entscheidungsproblem), which is famously unsolvable.
""Arbitrary functions"", if you wish to go to a more ""mathematical"" class of problems, can be very weird. The Ackermann Function (https://en.wikipedia.org/wiki/Ackermann_function) is a nice example for a function with a relatively benign definition which can be calculated readily by any kid with basic maths skills (and a lot of time) but which grows at a huge rate (much faster than exponential). It is not primitive recursive, that is, it cannot be computed by a program which consists only of for loops, i.e. where the number of iteration for every loops is known at the beginning of the loop. A neural net with its very simple structure of linear summing neurons and some multiplications is restricted to primitive-recursive functions (at least over an unbounded domain) and cannot approximate that.
There are discontinous functions which certainly work well as  a counter-example too, for example the Dirichlet function (https://en.wikipedia.org/wiki/Dirichlet_function).
As per the comments, and to come down to earth a bit, a simple sin function will do the job of providing a counterexample regarding UATs, as well.

","The paragraph discusses the limitations of neural networks in approximating arbitrary functions and solving unsolvable problems. It provides several counterexamples to illustrate these limitations:

* **Entscheidungsproblem (unsolvable problem):** Neural networks cannot solve problems like determining whether a given mathematical statement is provable.
* **Ackermann Function (weird function):** Neural networks cannot approximate functions with extremely high growth rates.
* **Dirichlet Function (discontinuous function):** Neural networks cannot approximate functions with discontinuities.
* **Sin Function (common function):** Neural networks cannot perfectly approximate functions like the sine function, which can be used to construct counterexamples for universal approximation theorems (UATs).

These counterexamples demonstrate that neural networks have limitations in their ability to approximate certain functions and solve certain problems. Their computational power is restricted to primitive-recursive functions, which cannot handle the complexities of functions like the Ackermann Function."
Can I use a tiny Validation set?,https://stats.stackexchange.com/questions/304977/can-i-use-a-tiny-validation-set,machine-learning|neural-networks|validation,304977,True,304996,8672,3,18,1506442792,1506399600,23,1506414607,"Larger validation sets give more accurate estimates of out-of-sample performance. But as you've noticed, at some point that estimate might be as accurate as you need it to be, and you can make some rough predictions as to the validation sample size you need to reach that point.
For simple correct/incorrect classification accuracy, you can calculate the standard error of the estimate as $\sqrt{p(1âp)/n}$ (standard deviation of a Bernouilli variable), where $p$ is the probability of a correct classification, and $n$ is the size of the validation set. Of course you don't know $p$, but you might have some idea of its range. E.g. let's say you expect an accuracy between 60-80%, and you want your estimates to have a standard error smaller than 0.1%:
$$
\sqrt{p(1âp)/n}<0.001
$$
How large should $n$ (the size of the validation set) be? For $p=0.6$ we get:
$$
n > \frac{0.6-0.6^2}{0.001^2}=240,000
$$
For $p=0.8$ we get:
$$
n > \frac{0.8-0.8^2}{0.001^2}=160,000
$$
So this tells us you could get away with using less than 5% of your 5 million data samples, for validation. This percentage goes down if you expect higher performance, or especially if you are satisfied with a lower standard error of your out-of-sample performance estimate (e.g. with $p=0.7$ and for a s.e. < 1%, you need only 2100 validation samples, or less than a twentieth of a percent of your data).
These calculations also showcase the point made by Tim in his answer, that the accuracy of your estimates depends on the absolute size of your validation set (i.e. on $n$), rather than its size relative to the training set. 
(Also I might add that I'm assuming representative sampling here. If your data are very heterogeneous you might need to use larger validation sets just to make sure that the validation data includes all the same conditions etc. as your train & test data.)
","**Summary:**

The accuracy of performance estimates for machine learning models improves with larger validation sets. However, the necessary validation set size depends on the desired accuracy and expected model performance.

To calculate the required validation set size for a given error tolerance, the standard error of the estimate formula is used. For example, with an expected accuracy range of 60-80% and a desired standard error of 0.1%, the required validation set size ranges from 160,000 to 240,000 samples.

However, it's important to note that the absolute size of the validation set matters, not its size relative to the training set. Representative sampling is also crucial, as heterogeneous data may require larger validation sets to capture all possible conditions."
Simulated annealing for deep learning: Why is gradient free statistical learning not in the main stream?,https://stats.stackexchange.com/questions/559251/simulated-annealing-for-deep-learning-why-is-gradient-free-statistical-learning,machine-learning|neural-networks|optimization|backpropagation,559251,True,559262,5577,4,18,1714515497,1641340809,19,1641350881,"Gradient-free learning is in the mainstream very heavily, but not used heavily in deep learning.  Methods used for training neural networks that don't involve derivatives are typically called ""metaheuristics.""  In computer science and pattern recognition (which largely originated in electrical engineering), metaheuristics are the go-to for NP-hard problems, such as airline flight scheduling, traffic route planning to optimize fuel consumption by delivery trucks, or the traveling salesman problem (annealing).  As an example see swarm-based learning for neural networks (https://www.igi-global.com/chapter/swarm-based-nature-inspired-metaheuristics-for-neural-network-optimization/187679) or genetic algorithms for training neural networks (https://www.sciencedirect.com/science/article/abs/pii/S0957417415006570) or use of a metaheuristic for training a convolutional neural network (https://www.hindawi.com/journals/cin/2016/3263612/).  These are all neural networks which use metaheuristics for learning, and not derivatives.
While metaheuristics encompasses a wide swath of the literature, they're just not strongly associated with deep-learning, as these are different areas of optimization. Look up ""solving NP-hard problems with metaheuristics.""  Last, recall that gradients used for neural networks don't have anything to do with the derivatives of a function that a neural network can be used to minimize (maximize). (This would be called function approximation using a neural network as opposed to classification analysis via neural network.) They're merely derivatives of the error or cross-entropy with respect to connection weight change within the network.
In addition, the derivatives of a function may not be known, or the problem can be too complex for using derivatives.   Some of the newer optimization methods involve finite differencing as a replacement for derivatives, since compute times are getting faster, and derivative-free methods are becoming less computationally expensive in the time complexity.
","**Summary:**

Gradient-free learning, often termed ""metaheuristics,"" optimizes complex problems without using derivatives. It is commonly employed in computer science for NP-hard problems like scheduling and route planning. Despite its prevalence in these areas, metaheuristics are not widely used in deep learning.

In deep learning, derivatives are usually employed to adjust connection weights and minimize errors. However, derivatives may be unknown or computationally expensive. Some modern optimization methods use finite differencing as an alternative to derivatives, making gradient-free methods more feasible.

Overall, metaheuristics are a valuable tool for optimizing complex problems, but their use in deep learning is limited compared to gradient-based methods."
Google Inception model:why there is multiple softmax?,https://stats.stackexchange.com/questions/274286/google-inception-modelwhy-there-is-multiple-softmax,deep-learning|convolutional-neural-network,274286,True,274623,10676,2,18,1551860814,1492501739,34,1492623043,"Short answer: Deep architectures, and specifically GoogLeNet (22 layers) are in danger of the vanishing gradients problem during training (back-propagation algorithm). The engineers of GoogLeNet addressed this issue by adding classifiers in the intermediate layers as well, such that the final loss is a combination of the intermediate loss and the final loss. This is why you see a total of three loss layers, unlike the usual single layer as the last layer of the network.
Longer answer: In classic Machine Learning, there is usually a distinction between feature engineering and classification. Neural networks are most famous for their ability to solve problems ""end to end"", i.e, they combine the stages of learning a representation for the data, and training a classifier. Therefore, you can think of a neural network with a standard architecture (for example, AlexNet) as being composed of a ""representation learning"" phase (the layers up until previous to last) and a ""classification"" phase, which as expected, includes a loss function. 
When creating deeper networks, there arises a problem coined as the ""vanishing gradients"" problem. It's actually not specific to neural networks; rather to any gradient based learning methods. It not that trivial and therefore deserves a proper explanation for itself; see here (http://neuralnetworksanddeeplearning.com/chap5.html) for a good reference. Intuitively, you can think about the gradients carrying less and less information the deeper we go inside the network, which is of course a major concern, since we tune the network's parameters (weights) based solely on the gradients, using the ""back-prop"" algorithm. 
How did the developers of GoogLeNet handle this problem? They recognized the fact that it's not only the features of the final layers that carry all the discriminatory information: intermediate features are also capable of discriminating different labels; and, most importantly, their values are more ""reliable"" since they are extracted from earlier layers in which the gradient carry more information. Building on this intuition, they added ""auxiliary classifiers"" in two intermediate layers. This is the reason for the ""early escape"" loss layers in the middle of the network which you referenced to in your question. 
The total loss is then a combination of these three loss layers. I quote from the original article: 

These classifiers take the form
  of smaller convolutional networks put on top of the output
  of the Inception (4a) and (4d) modules. During training,
  their loss gets added to the total loss of the network
  with a discount weight (the losses of the auxiliary classi-
  fiers were weighted by 0.3). At inference time, these auxiliary
  networks are discarded. 

Visually:
 (https://i.sstatic.net/U6O7Wl.png)
","**Summary:**

Deep neural networks, including GoogLeNet, face the ""vanishing gradients"" problem during training. To mitigate this, GoogLeNet's developers introduced ""auxiliary classifiers"" in intermediate layers, which provide more reliable gradients.

Neural networks typically combine feature engineering and classification, with the final layers specializing in classification and using a loss function. However, in deep networks, gradients weaken with depth, making it difficult to train deeper layers.

GoogLeNet's auxiliary classifiers overcome this issue by extracting discriminatory features from intermediate layers where gradients are stronger. The total loss is then a weighted combination of losses from the final classifier and the auxiliary classifiers. This approach ensures that all layers contribute to the network's optimization, reducing the impact of vanishing gradients."
What&#39;s the relation between game theory and reinforcement learning?,https://stats.stackexchange.com/questions/208661/whats-the-relation-between-game-theory-and-reinforcement-learning,deep-learning|reinforcement-learning|game-theory,208661,True,208700,13266,5,18,1651650798,1461262949,19,1461280737,"In Reinforcement Learning (RL) it is common to imagine an underlying Markov Decision Process (MDP). Then the goal of RL is 
to learn a good policy for the MDP, which is often only partially specified. MDPs can have different objectives such as total, average, or discounted reward, where discounted reward is the most common assumption for RL. There are well-studied extensions of MDPs to two-player (i.e., game) settings; see, e.g.,
Filar, Jerzy, and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business Media, 2012.
There is an underlying theory shared by MDPs and their extensions to two-player (zero-sum) games, including, e.g., the Banach fixed point theorem, Value Iteration, Bellman Optimality, Policy Iteration/Strategy Improvement etc. However, while there are these close connections between MDPs (and thus RL) and these specific type of games:

you can learn about RL (and MDPs) directly, without GT as a prerequisite;
anyway, you would not learn about this stuff in the majority of GT courses (which would normally be focused on, e.g., strategic-form, extensive-form, and repeated games, but not the state-based infinite games that generalize MDPs).

","Reinforcement Learning (RL) often uses Markov Decision Processes (MDPs) to model decision-making scenarios. The goal of RL is to find an optimal policy for the MDP. MDPs can have various objectives, but discounted reward is common in RL.

MDPs and two-player (zero-sum) games share an underlying theory, including concepts like the Banach fixed point theorem and Bellman Optimality. However, learning about RL and MDPs does not require prior knowledge of game theory (GT). Most GT courses focus on different types of games, such as strategic-form and extensive-form games."
Why isn&#39;t (symmetric) log(1+x) used as neural network activation function?,https://stats.stackexchange.com/questions/605641/why-isnt-symmetric-log1x-used-as-neural-network-activation-function,neural-networks|backpropagation|activation-function,605641,True,605646,1538,1,18,1679663795,1676559201,23,1676561152,"For a long time, neural network researchers believed that sigmoid (https://stats.stackexchange.com/questions/544711/is-wikipedias-page-on-the-sigmoid-function-incorrect) activations like the inverse logit (https://stats.stackexchange.com/questions/371402/does-the-function-ex-1ex-have-a-standard-name/371403#371403) and $\tanh$ were the only activations that were necessary. This is because the Cybenko (1989) Universal Approximation Theorem (loosely) states that, under certain conditions, a neural network can approximate certain functions to a desired level of precision with 1 hidden layer & a finite number of units. One of the conditions is that the activation function is bounded. (For full details, consult the paper.)
The function $f(x)=\log(1+x)$ is not bounded, so it does not satisfy the boundedness condition.
However, in the time since Cybenko published his UAT, many other UAT variations have been proven in different settings & allowing more flexibility in the choice of activation functions, number of layers, and so on.
From the perspective of modern neural network theory, you would need to show that the proposed activation has some desirable property that is not found in alternative choices. One problem that I anticipate with this activation is that its derivative is $f^\prime(x)=\frac{1}{1+|x|}$, and goes to 0 as $x$ gets large or small. This is undesirable because of the vanishing gradient (https://stats.stackexchange.com/search?q=vanishing%20gradient%20answers%3A1%20score%3A3) phenomenon.
By contrast, an activation function with derivative exactly 1 for a ""large"" portion of inputs is preferable because it ameliorates the vanishing gradient. An example of this type of function is the ReLU and related functions.

Cybenko, G. Approximation by superpositions of a sigmoidal function. Math. Control Signal Systems 2, 303â314 (1989). https://doi.org/10.1007/BF02551274 (https://doi.org/10.1007/BF02551274)
","Neural network researchers once believed sigmoid activations (e.g., inverse logit and tanh) were essential due to the Cybenko Universal Approximation Theorem (UAT). This theorem states that, with certain conditions (including bounded activation functions), a neural network can approximate functions efficiently.

However, un bounded activation functions are not allowed by the Cybenko UAT. The function f(x) = log(1+x) is an example of an unbounded activation function.

Since Cybenko's UAT, several variations have been introduced, providing more flexibility in activation function choices. Modern neural network theory requires demonstrating desirable properties not found in alternative activations.

One challenge with the unbounded f(x) activation is its diminishing derivative, which can lead to the vanishing gradient problem. Activations with a derivative of 1 for a significant range of inputs, such as ReLU, are preferred to alleviate this issue."
Deep learning vs. Decision trees and boosting methods,https://stats.stackexchange.com/questions/12687/deep-learning-vs-decision-trees-and-boosting-methods,machine-learning|deep-learning|cart|restricted-boltzmann-machine|adaboost,12687,True,12732,5551,2,18,1509209539,1309900313,1,1309983412,"Can you be more specific about the types of data you are looking at? This will in part determine what type of algorithm will converge the fastest.
I'm also not sure how to compare methods like boosting and DL, as boosting is really just a collection of methods. What other algorithms are you using with the boosting?
In general, DL techniques can be described as layers of encoder/decoders. Unsupervised pre-training works by first pre-training each layer by encoding the signal, decoding the signal, then measuring the reconstruction error. Tuning can then be used to get better performance (e.g. if you use denoising stacked-autoencoders you can use back-propagation). 
One good starting point for DL theory is:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.795&rep=rep1&type=pdf (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.795&rep=rep1&type=pdf)
as well as these:
http://portal.acm.org/citation.cfm?id=1756025 (http://portal.acm.org/citation.cfm?id=1756025)
(sorry, had to delete last link due to SPAM filtration system)
I didn't include any information on RBMs, but they are closely related (though personally a little more difficult to understand at first).
","**Summary:**

The type of data and algorithms used determines the convergence speed of machine learning models. Boosting is a collection of methods, so comparing it to Deep Learning (DL) requires specifying the specific boosting algorithms being used.

DL involves layers of encoder/decoders. Unsupervised pre-training involves pre-training each layer by encoding and decoding the signal and measuring reconstruction error. Tuning improves performance.

For DL theory, the following resources are recommended:

* [PDF link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.795&rep=rep1&type=pdf)
* [ACM link](http://portal.acm.org/citation.cfm?id=1756025)

Restricted Boltzmann Machines (RBMs) are related to DL but can be more challenging to understand initially."
When was the ReLU function first used in a neural network?,https://stats.stackexchange.com/questions/447674/when-was-the-relu-function-first-used-in-a-neural-network,neural-networks|history,447674,True,598746,11867,3,17,1702624476,1580749087,9,1670797505,"Fukushima first used ReLU in a paper published in 1969, 6 years before the Cognitron paper, in a so-called analog threshold element (see Equation 2 and Figure 3):
 (https://i.sstatic.net/tJSVV.png)
K. Fukushima, ""Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements (https://ieeexplore.ieee.org/document/4082265),"" in IEEE Transactions on Systems Science and Cybernetics, vol. 5, no. 4, pp. 322-333, Oct. 1969, doi: 10.1109/TSSC.1969.300225.
","In 1969, prior to the Cognitron paper, K. Fukushima introduced the ReLU (Rectified Linear Unit) activation function in a paper titled ""Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements.""

Fukushima's work involved the use of ReLU in an analog threshold element, as depicted in Equation 2 and Figure 3 of the paper. This predated the use of ReLU in artificial neural networks by six years.

In essence, Fukushima's paper demonstrated the application of ReLU in a layered network for visual feature extraction, paving the way for its subsequent adoption in deep learning architectures."
How does the Rectified Linear Unit (ReLU) activation function produce non-linear interaction of its inputs?,https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear,neural-networks,299915,True,299933,9394,2,17,1566788133,1503752378,27,1503766012,"Suppose you want to approximate $f(x)=x^2$ using ReLUs $g(ax+b)$. One approximation might look like $h_1(x)=g(x)+g(-x)=|x|$.
 (https://i.sstatic.net/R14EO.png)
But this isn't a very good approximation. But you can add more terms with different choices of $a$ and $b$ to improve the approximation. One such improvement, in the sense that the error is ""small"" across a larger interval, is we have $h_2(x)=g(x)+g(-x)+g(2x-2)+g(-2x+2)$, and it gets better.
 (https://i.sstatic.net/GUDKV.png)
You can continue this procedure of adding terms to as much complexity as you like.
Notice that, in the first case, the approximation is best for $x\in[-1,1]$, while in the second case, the approximation is best for $x\in[-2,2]$. 
 (https://i.sstatic.net/5xKok.png)
x <- seq(-3,3,length.out=1000)
y_true <- x^2
relu <- function(x,a=1,b=0) sapply(x, function(t) max(a*t+b,0))

h1 <- function(x) relu(x)+relu(-x)
png(""fig1.png"")
    plot(x, h1(x), type=""l"")
    lines(x, y_true, col=""red"")
dev.off()

h2 <- function(x) h1(x) + relu(2*(x-1)) + relu(-2*(x+1))
png(""fig2.png"")
    plot(x, h2(x), type=""l"")
    lines(x, y_true, col=""red"")
dev.off()

l2 <- function(y_true,y_hat) 0.5 * (y_true - y_hat)^2

png(""fig3.png"")
    plot(x, l2(y_true,h1(x)), type=""l"")
    lines(x, l2(y_true,h2(x)), col=""red"")
dev.off()

","Approximating functions using ReLU (Rectified Linear Unit) networks is described. ReLU networks use functions of the form $g(ax+b) = \max(ax+b,0)$, and multiple ReLUs can be combined to create complex approximations.

The simplest approximation, $h_1(x)=g(x)+g(-x)=|x|$, is not effective. However, by adding more terms with different coefficients, the approximation improves, as in $h_2(x)=g(x)+g(-x)+g(2x-2)+g(-2x+2)$.

The more terms added, the better the approximation becomes. The approximation range also improves, as seen with $h_2(x)$ performing better over $x\in[-2,2]$ compared to $h_1(x)$ which approximates well over $x\in[-1,1]$.

The example showcases the approximation of $f(x)=x^2$ using ReLU networks and demonstrates how adding complexity can significantly improve the approximation."
Text Mining: how to cluster texts (e.g. news articles) with artificial intelligence?,https://stats.stackexchange.com/questions/155880/text-mining-how-to-cluster-texts-e-g-news-articles-with-artificial-intellige,clustering|neural-networks|feature-selection|text-mining|self-organizing-maps,155880,True,155903,27009,4,17,1519571841,1433690094,15,1433706968,"Latent Dirichlet Allocation (LDA) is great, but if you want something better that uses neural networks I would strongly suggest doc2vec (https://radimrehurek.com/gensim/models/doc2vec.html (https://radimrehurek.com/gensim/models/doc2vec.html)).
What it does? It works similarly to Google's word2vec but instead of a single word feature vector you get a feature vector for a paragraph. The method is based on a skip-gram model and neural networks and is considered one of the best methods to extract a feature vector for documents.
Now given that you have this vector you can run k-means clustering (or any other preferable algorithm) and cluster the results.
Finally, to extract the feature vectors you can do it as easy as that: 
from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence

class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(self.filename)):
            yield LabeledSentence(words=line.split(), labels=['TXT_%s' % uid])


sentences = LabeledLineSentence('your_text.txt')

model = Doc2Vec(alpha=0.025, min_alpha=0.025, size=50, window=5, min_count=5,
                dm=1, workers=8, sample=1e-5)

model.build_vocab(sentences)

for epoch in range(500):
    try:
        print 'epoch %d' % (epoch)
        model.train(sentences)
        model.alpha *= 0.99
        model.min_alpha = model.alpha
    except (KeyboardInterrupt, SystemExit):
        break

","**Summary:**

Latent Dirichlet Allocation (LDA) is a powerful method for extracting feature vectors from text documents. However, doc2vec is recommended as a more advanced approach. Doc2vec uses neural networks to create a feature vector for an entire paragraph, making it particularly useful for document clustering.

To use doc2vec, you can label sentences within a text file and use the LabeledLineSentence class to iterate over them. The Doc2Vec model can then be trained on these labeled sentences.

By feeding the model a labeled sentence, you can retrieve its feature vector. These vectors can then be clustered using k-means or other algorithms to identify patterns within the text.

Doc2vec's advantages include its ability to capture semantic relationships between words and its efficiency in handling large datasets. It is widely used in tasks such as natural language processing, document similarity analysis, and topic modeling."
Non-linearity before final Softmax layer in a convolutional neural network,https://stats.stackexchange.com/questions/163695/non-linearity-before-final-softmax-layer-in-a-convolutional-neural-network,neural-networks|deep-learning|convolutional-neural-network|nonlinear|softmax,163695,True,177551,24696,3,17,1540213693,1438159936,16,1445211266,"You should not use a non-linearity for the last layer before the softmax classification. The ReLU non-linearity (used now almost exclusively) will in this case simply throw away information without adding any additional benefit. You can look at the caffe implementation (https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/deploy.prototxt) of the well-known AlexNet for a reference of what's done in practice.
","**Summary:**

In a neural network, it is not advisable to use a non-linearity, such as ReLU, as the last layer before the softmax classification. This is because non-linearities discard information during the training process, offering no additional benefits for the classification task.

As an example, the widely-used AlexNet model demonstrates this practice by not employing a non-linearity before the softmax layer. Instead, a linear activation function is utilized to preserve all the learned features and ensure that they are represented in the final classification."
tanh vs. sigmoid in neural net,https://stats.stackexchange.com/questions/142348/tanh-vs-sigmoid-in-neural-net,neural-networks,142348,True,142357,22278,3,17,1512497449,1426703719,23,1426705821,"In Symon Haykin's ""Neural Networks: A Comprehensive Foundation"" book there is the following explanation from which I quote:

For the learning time to be minimized, the use of non-zero mean inputs should be avoided. Now, insofar as the signal vector $\bf x$ applied to a neuron in the first hidden layer of a multilayer perceptron is concerned, it is easy to remove the mean from each element of $\bf x$ before its application to the network. But what about the signals applied to the neurons in the remaining hidden and output layers of the network? The answer to this question lies in the type of activation function used in the network. If the activation function is non-symmetric, as in the case of the sigmoid function, the output of each neuron is restricted to the interval $[0,1]$. Such a choice introduces a source of systematic bias for those neurons located beyond the first layer of the network. To overcome this problem we need to use an antisymmetric activation function such as the hyperbolic tangent function. With this latter choice, the output of each neuron is permitted to assume both positive and negative values in the interval $[-1,1]$, in which case it is likely for its mean to be zero. If the network connectivity is large, back-propagation learning with antisymmetric activation functions can yield faster convergence than a similar process with non-symmetric activation functions, for which there is also empirical evidence (LeCun et al. 1991).

The cited reference is:

Y. LeCun, I. Kanter, and S.A.Solla: ""Second-order properties of error surfaces: learning time and generalization"", Advances in Neural Information Processing Systems, vol. 3, pp. 918-924, 1991.

Another interesting reference is the following:

Y. LeCun, L. Bottou, G. Orr and K. Muller: ""Efficient BackProp (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)"", in Orr, G. and Muller K. (Eds), Neural Networks: Tricks of the trade, Springer, 1998

","**Summary:**

Minimizing learning time in neural networks requires avoiding non-zero mean inputs. For the first hidden layer, this can be achieved by subtracting the mean from each input element. However, for subsequent hidden and output layers, the type of activation function used is crucial.

Non-symmetric activation functions like the sigmoid function restrict neuron outputs to [0,1], introducing a bias and slowing down learning. To overcome this, antisymmetric functions like the hyperbolic tangent are recommended, allowing outputs in [-1,1] with a zero mean.

With symmetric (antisymmetric) activation functions, back-propagation learning can converge faster than with non-symmetric functions. This finding is supported by the work of LeCun et al. (1991). The reference provided by LeCun et al. discusses the second-order properties of error surfaces and their impact on learning time and generalization."
Does Neural Networks based classification need a dimension reduction?,https://stats.stackexchange.com/questions/67986/does-neural-networks-based-classification-need-a-dimension-reduction,pca|neural-networks,67986,True,67988,20553,3,17,1647538555,1377097496,24,1377098807,"In principle, the linear transformation performed by PCA can be performed just as well by the input layer weights of the neural network, so it isn't strictly speaking necessary. However, as the number of weights in the network increases, the amount of data needed to be able to reliably determine the weights of the network also increases (often quite rapidly), and over-fitting becomes more of an issue (so using regularisation is also a good idea). The benefit of dimensionality reduction is that it reduces the size of the network, and hence the amount of data needed to train it. The disadvantage of using PCA is that the discriminative information that distinguishes one class from another might be in the low variance components, so using PCA can make performance worse.
Like most things in statistical pattern recognition, there is no single recipe that works reliably for all problems, and really the best thing to do is to try both approaches and see which works best.
","PCA, a linear transformation, can be performed by neural network input layer weights. However, as network complexity increases, data requirements increase, and over-fitting becomes more likely.

Dimensionality reduction via PCA reduces network size and data requirements for training. Yet, PCA may discard discriminative information crucial for classification.

Using PCA versus direct neural network transformations depends on the specific problem. Experimenting with both approaches is recommended to determine the optimal solution. No universal approach works reliably for all problems."
Why Can&#39;t I use Test set as Validation Set,https://stats.stackexchange.com/questions/237790/why-cant-i-use-test-set-as-validation-set,machine-learning|neural-networks|cross-validation|deep-learning,237790,True,237802,9896,2,17,1583015672,1475254027,15,1475260997,"I presume you're already understand why performance on the training set isn't representative of the actual performance of the trained model: overfitting. The parameters you learn during training are optimized to the training set. If you're not careful, you can over-optimize the parameters, leading to a model that's really, really good on the training set, but doesn't generalize to completely unseen real-world data.
The thing is, in practice the ""parameters"" of the training method aren't the only thing you need to specify for a learning example. You also have hyperparameters. Now, those hyperparameters might be an explicit part of the model fitting (like learning rate), but you can also view other choices as ""hyperparameters"": do you choose an SVM or a neural network? If you implement early stopping, at what point do you stop?
Just like overfitting of the parameters on the training set, you can overfit the hyperparameters to the validation set. As soon as you use the results of the method on the validation set to inform how you do modeling, you now have the chance of overfitting to the training+validation set combo. Perhaps this particular validation set does better with an SVM than the general case.
That's the main reason people separate out the validation and test sets. If you use a set during your model fitting - even at the ""hmm, that method doesn't do so well, maybe I should try ..."" level - the results you get on that set will not be fully indicative of the general results you'll obtain on completely new data. That's why you hold out a fraction of the data till the very end, past the point where you're making any decisions on what to do.
","**Summary:**

Overfitting occurs when a trained model's performance on the training data is not representative of its performance on new data. This can happen due to overfitting of model parameters or hyperparameters.

Hyperparameters are additional settings that control the training process, such as learning rate or the choice of model type. Overfitting of hyperparameters occurs when these settings are optimized using the validation set, leading to a model that performs well on the training and validation data but not on unseen data.

To mitigate this issue, the data is typically split into three sets: training, validation, and test. The training set is used to train the model, the validation set is used to tune hyperparameters and stop training early, and the test set is used to evaluate the final model performance without influencing the tuning process. By holding out a separate test set, researchers can avoid overfitting and obtain a more accurate assessment of the model's generalization capabilities."
Is teacher forcing more accurate than using actual model output or just faster?,https://stats.stackexchange.com/questions/259333/is-teacher-forcing-more-accurate-than-using-actual-model-output-or-just-faster,machine-learning|neural-networks|recurrent-neural-network,259333,True,374826,12250,2,17,1678059020,1485950496,17,1541079565,"I'll begin by saying I'm no expert but was thinking about this same question. A little googling led me to this page:
https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/ (https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)
and, in turn, this paper:
https://arxiv.org/abs/1610.09038 (https://arxiv.org/abs/1610.09038)
which as a paragraph addressing this to some degree in the introduction:

Unfortunately, [teacher forcing] can result in
problems in generation as small prediction error compound in the conditioning context. This can
lead to poor prediction performance as the RNNâs conditioning context (the sequence of previously
generated samples) diverge from sequences seen during training

In addition, from the deeplearning.org book,Â 10,Â p.378 (https://www.deeplearningbook.org/contents/rnn.html#pfc):

The disadvantage of strict teacher forcing arises if the network is going to be later used in a closed-loop mode, with the network outputs (or samples from the output distribution) fed back as input. In this case, the fed-back inputs that the network sees during training could be quite different from the kind of inputs that it will see at test time.

I would imagine (again, not an expert) that it is fairly problem-dependent, but that the main gain of teacher forcing is in the computational training and simplifying the loss landscape (i.e. since the whole sequence will contribute to the gradient of the parameters, for long sequences backpropagation through time may make it difficult for the optimiser to converge even if it has a lot of computational time.)
","**Summary:**

Teacher forcing, a technique used in training recurrent neural networks (RNNs), involves using ground truth values as inputs for subsequent steps instead of predicted values. While beneficial for computational training and simplifying the loss landscape, it can also lead to issues in generation.

The main disadvantage of teacher forcing arises when using an RNN in closed-loop mode, where network outputs are fed back as inputs. In these cases, the inputs during training may differ significantly from those encountered during testing. This discrepancy can result in poor prediction performance since the network's conditioning context (previously generated samples) diverges from those seen during training.

The applicability of teacher forcing depends on the specific problem and the network's intended use. If the network will be used in open-loop mode, where predicted values are not fed back, teacher forcing may be beneficial. However, if the network will operate in closed-loop mode, other training methods may be more suitable."
Fine Tuning vs Joint Training vs Feature Extraction,https://stats.stackexchange.com/questions/255364/fine-tuning-vs-joint-training-vs-feature-extraction,machine-learning|deep-learning|terminology|computer-vision|transfer-learning,255364,True,255387,18426,2,17,1608543152,1483988602,20,1483996777,"As shown in figure 2 of {1}, in the fine-tuning strategy all weights are changed when training on the new task (except for the weights of the last layers for the original task), whereas in the feature extraction strategy only the weights of the newly added last layers change during the training phase:
 (https://i.sstatic.net/BiClc.png)

References:

{1} Li, Zhizhong, and Derek Hoiem. ""Learning without forgetting."" In European Conference on Computer Vision, pp. 614-629. Springer International Publishing, 2016. https://arxiv.org/abs/1606.09282 (https://arxiv.org/abs/1606.09282) ;  https://doi.org/10.1007/978-3-319-46493-0_37 (https://doi.org/10.1007/978-3-319-46493-0_37)

","**Summary:**

Two approaches are used to adapt pre-trained neural networks to new tasks without forgetting previous knowledge: fine-tuning and feature extraction.

**Fine-Tuning:**
In fine-tuning, all network weights are updated during training, except for the weights of the final layers related to the original task. This allows the network to adapt to the new task while retaining the knowledge learned from the previous one.

**Feature Extraction:**
In feature extraction, only the weights of the newly added final layers are modified during training. The pre-trained layers are frozen, retaining their learned features. This approach allows the network to leverage existing features for the new task, reducing training time and mitigating forgetting.

Both approaches seek to strike a balance between adaptation to the new task and preservation of prior knowledge, and the choice between them depends on the specific transfer learning scenario."
Wouldn&#39;t multiple filters in a convolutional layer learn the same parameter during training?,https://stats.stackexchange.com/questions/351661/wouldnt-multiple-filters-in-a-convolutional-layer-learn-the-same-parameter-duri,neural-networks|convolutional-neural-network|convolution|filter,351661,True,351758,6272,3,17,1595920334,1529139418,9,1529218521,"I have found the answer to this question:
https://www.quora.com/Why-does-each-filter-learn-different-features-in-a-convolutional-neural-network (https://www.quora.com/Why-does-each-filter-learn-different-features-in-a-convolutional-neural-network)
It says here: ""... (optimization) algorithm finds that loss does not decrease if two filters have similar weights and biases, so itâll eventually change one of the filter(âs weights and biases) in order to reduce loss thereby learning a new feature.""
Thank you for the answers. Appreciate it :)
","**Summary:**

In a convolutional neural network (CNN), each filter learns different features to enhance the model's ability to recognize patterns. This is because the optimization algorithm used in training the network ensures that filters do not have similar weights and biases. If two filters have similar characteristics, the algorithm modifies one of them to minimize loss and learn a unique feature.

This process results in each filter specializing in detecting a specific aspect of the input data. For example, in image recognition networks, early filters may learn basic shapes like edges or lines, while later filters combine these features to recognize more complex objects. By leveraging this specialization, CNNs can effectively extract and identify patterns in data."
State of the art in general learning from data in &#39;69,https://stats.stackexchange.com/questions/193492/state-of-the-art-in-general-learning-from-data-in-69,classification|neural-networks|history,193492,True,221125,262,2,17,1477409610,1454340669,13,1467151688,"I was curious about this, so I did some digging. I was surprised to find that recognizable versions of many common classification algorithms were already available in 1969 or thereabouts.  Links and citations are given below.
It is worth noting that AI research was not always so focused on classification. There was a lot of interest in planning and symbolic reasoning, which are no longer in vogue, and labelled data was much harder to find. Not all of these articles may have been widely available then either: for example, the proto-SVM work was mostly published in Russian. Thus, this might over-estimate how much an average scientist knew about classification in 1969. 

Discriminant Analysis
In a 1936 article (http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf) in the Annals of Eugenics, Fisher described a procedure for finding a linear function which discriminates between three species of iris flowers, on the basis of their petal and sepal dimensions. That paper mentions that Fisher had already applied a similar technique to predict the sex of human mandibles (jaw bones) excavated in Egypt, in a collaboration with E. S Martin and Karl Pearson (jstor (http://www.jstor.org/stable/2334128?seq=1#page_scan_tab_contents)), as well as in a separate cranial measurement project with a Miss Mildred Barnard (which I couldn't track down).
Logistic Regression
The logistic function itself has been known since the 19th century, but mostly as a model for saturating processes, such as population growth or  biochemical reactions. Tim links to JS Cramer's article above, which is a nice history of its early days. By 1969, however, Cox had published the first edition of Analysis of Binary Data (http://rads.stackoverflow.com/amzn/click/0412306204). I could not find the original, but a later edition contains an entire chapter on using logistic regression to perform classification. For example:

In discriminant analysis, the primary notion is that there are two distinct populations, defined by $y=0,1$, usually two intrinsically different groups, like two species of bacteria or plants, two different kinds of product, two distinct but rather similar drugs, and so on....Esentially the focus in discriminant analysis is on the question: how do the two distributions differ most sharply? Often, this is put into a more specific form as follows. There is given a new vector $x'$ from an individual of unknown $y$. What can we say about that $y$....

$k$-Nearest Neighbors
Cover and Hart are often credited with inventing/discovering the $k$-nearest neighbor rule. Their 1967 paper (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1053964) contains a proof that $k$-NN's error rate is at most twice the Bayes error rate. However, they actually credit Fix and Hodge with inventing it in 1951, citing a technical report they prepared for the USAF School of Aviation Medicine (reprint via jstor (https://www.jstor.org/stable/1403797?seq=1#page_scan_tab_contents)).
Neural networks
Rosenblatt published a technical report (http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf) describing the perceptron in 1957 and followed it up with a book (https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566;view=1up;seq=9), Principles of Neurodynamics in 1962. Continuous versions of backpropagation have been around since the early 1960s, including work by Kelley (http://arc.aiaa.org/doi/abs/10.2514/8.5282?journalCode=arsj), Bryson, and Bryson & Ho (revised in 1975, but the original is from 1969 (http://e.guigon.free.fr/rsc/book/BrysonHo75a.pdf). However, it wasn't applied to neural networks until a bit later, and methods for training very deep networks are much more recent. This scholarpedia article on deep learning (http://www.scholarpedia.org/article/Deep_Learning#Backpropagation) has more information.
Statistical Methods
I suspect using Bayes' Rule for classification has been discovered and rediscovered many times--it is a pretty natural consequence of the rule itself. Signal detection theory developed a quantitative framework for deciding whether a given input was a ""signal"" or noise. Some of it came out of radar research after WWII, but it was rapidly adapted for perceptual experiments (e.g., by Green and Swets (http://rads.stackoverflow.com/amzn/click/0932146236)). I do not know who discovered that assuming independence between predictors works well, but work from the early 1970s seems to have exploited this idea, as summarized in this article (https://www.jstor.org/stable/1403452?seq=2#page_scan_tab_contents). Incidentally, that article also points out that Naive Bayes was once called ""idiot Bayes""!
Support Vector Machines
In 1962, Vapnik and Chervonenkis described the ""Generalised Portrait Algorithm"" (terrible scan, sorry (http://web.cs.iastate.edu/~cs573x/vapnik-portraits1963.pdf)), which looks like a special case of a support vector machine (or actually, a one-class SVM). Chervonenkis wrote an article entitled ""Early History of Support Vector Machines"" (http://download.springer.com/static/pdf/305/chp%253A10.1007%252F978-3-642-41136-6_3.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-41136-6_3&token2=exp=1467091194~acl=%2Fstatic%2Fpdf%2F305%2Fchp%25253A10.1007%25252F978-3-642-41136-6_3.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fchapter%252F10.1007%252F978-3-642-41136-6_3*~hmac=ff5e6783052bd54920a0aae574f7f86a18f4ca11f0b53f0587d3a24ea6a01907) which describes this and their follow-up work in more detail. The kernel trick (kernels as inner products) was described by Aizerman, Braverman and Rozonoer in 1964. svms.org has a bit more about the history of support vector machines here (http://www.svms.org/history.html).
","In the 1960s, many fundamental classification algorithms were already developed, predating their widespread use in AI. Discriminant analysis, introduced in 1936, used linear functions to distinguish species. Logistic regression, published in 1969, modeled binary data classification using logistic functions. K-nearest neighbor (k-NN) emerged in 1951 and 1967, with proofs of its efficiency. Neural networks, including perceptrons and backpropagation, were introduced in the 1950s-1960s, but their application to classification came later. Statistical methods like Bayes' Rule and signal detection theory provided frameworks for classification, while support vector machines (SVMs) originated in the 1960s, with the kernel trick introduced in 1964. However, the accessibility of these methods in 1969 may have been limited, especially due to language barriers with Russian publications on SVMs."
Are there mathematical reasons for convolution in neural networks beyond expediency?,https://stats.stackexchange.com/questions/269854/are-there-mathematical-reasons-for-convolution-in-neural-networks-beyond-expedie,machine-learning|neural-networks|convolutional-neural-network|convolution,269854,True,269883,4818,2,17,1635946474,1490499794,10,1490521888,"There are no differences in what neural networks can do when they use convolution or correlation. This is because the filters are learned and if a CNN can learn to do a particular task using convolution operation, it can also learn to do the same task using correlation operation (It would learn the rotated version of each filter).
To find more details about the reasons that people sometimes find it more intuitive to think about convolution than correlation, this post (http://timdettmers.com/2015/03/26/convolution-deep-learning/) may be useful.
There remains this question that if there is no difference between convolution and cross-correlation, what is the point of flipping the weights into the kernel matrix? I would like to include some sentences from the Deep learning book by Ian Goodfellow et al. (http://deeplearningbook.org) to answer this question:

The only reason to flip the kernel is to obtain the commutative property. While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation... Many machine learning libraries implement cross-correlation but call it convolution.

The takeaway is that although convolution is a favorite operation in classic machine vision applications, it is replaced by correlation in many of the implementations of the convolutional neural networks.
","Convolution and correlation operations in neural networks are essentially equivalent, as filters are learned and can be adapted to perform the same tasks with either operation.

While convolution is commonly used in classic machine vision applications, correlation is often employed in convolutional neural network implementations. The choice between the two is primarily for mathematical convenience (commutative property), rather than functional differences.

Therefore, the flipping of weights in the kernel matrix is not necessarily required and is primarily done for mathematical consistency."
Sensitivity Analysis in Deep Neural Networks,https://stats.stackexchange.com/questions/263657/sensitivity-analysis-in-deep-neural-networks,neural-networks|python|feature-selection|sensitivity-analysis,263657,True,263894,10605,2,17,1683740294,1487867177,12,1487972950,"The sensitivity analysis you suggest corresponds to examining the partial derivatives of the outputs with respect to the inputs. Say the output vector $y \in \mathbb{R}^m$ is given by $y= f(x)$ , where $x \in \mathbb{R}^d$ is the input vector and $f$ is the function the network implements. The Jacobian (https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the outputs w.r.t. the inputs is:
$$J_{ij}(x) = \frac{\partial}{\partial x_j} f_i(x)$$
The Jacobian gives the local rate of change of each output w.r.t. each input, so it tells us how $f$ will behave in response to infinitesimal perturbations. If we start with input $x$ and add an infinitesimal value $\Delta$ to the $j$th input, we expect the $i$th output to increase by $\Delta J_{ij}(x)$.
If $J_{ij}(x)$ has large magnitude, it means that output $i$ is sensitive to input $j$ in the vicinity of $x$. Because $f$ is, in general, nonlinear, this notion of sensitivity depends on the input; it may be large in some regions and near zero in others. If you want some kind of summary measure of how strongly the outputs depend on the inputs, you'd have to aggregate over multiple input values. For example, you could take the absolute value of the Jacobian, averaged over all inputs in the training set (which acts as a surrogate for the expected value w.r.t. the underlying distribution of inputs). Of course, this kind of summary will end up discarding information, so could be misleading in some circumstances.
You can use the chain rule to derive an expression for the Jacobian, similarly to how you'd derive the gradient of the loss function w.r.t. the parameters for use with backprop. You can also compute it using automatic differentiation, using a library like Theano, TensorFlow, etc. There's not much reason to perform finite differencing (i.e. actually simulate the perturbation and measure the change in output), unless the function your network implements is nondifferentiable (in which case the Jacobian doesn't exist).
A couple caveats: If the inputs have different units/scales than each other, the sensitivities will also have different units/scales, and can't be directly compared. Standardizing/scaling the inputs is one possible solution. It's also important to keep in mind is that this type of analysis tells us about the model itself, but not necessarily the underlying distribution that generated the data. For example, if two inputs are correlated, the model might end up using the first but not the second. In this case, we'd find that the sensitivity is high for the first input and low for the second, but should not conclude that the first input is inherently more important for predicting the output in general.
This article (ftp://ftp.sas.com/pub/neural/importance.html) should be of interest.
","**Summary: Sensitivity Analysis in Neural Networks**

Sensitivity analysis examines how the outputs of a neural network change in response to changes in its inputs. The Jacobian matrix represents this sensitivity by containing the partial derivatives of outputs with respect to inputs. Each element in the Jacobian indicates the local rate of change for an output-input pair.

For a non-linear network, sensitivity depends on the input value. An aggregated measure, such as the average absolute Jacobian, provides a summary of input-output sensitivity. This metric can be computed using the chain rule or automatic differentiation methods.

Caveats:
* Inputs with different units or scales complicate direct comparison of sensitivities.
* Sensitivity analysis reflects the model itself, rather than the underlying data distribution.
* Correlation between inputs can influence individual input sensitivities.

This analysis helps evaluate the impact of input changes on network outputs. However, it should be interpreted with caution, considering the non-linearity, input value dependencies, and potential effects of data distribution."
Timeseries analysis procedure and methods using R,https://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r,r|time-series|forecasting|neural-networks|arima,140163,True,140192,12016,2,16,1475572020,1425405234,22,1425413893,"You should use the forecast package (http://cran.r-project.org/web/packages/forecast/index.html), which supports all of these models (and more) and makes fitting them a snap:
library(forecast)
x <- AirPassengers
mod_arima <- auto.arima(x, ic='aicc', stepwise=FALSE)
mod_exponential <- ets(x, ic='aicc', restrict=FALSE)
mod_neural <- nnetar(x, p=12, size=25)
mod_tbats <- tbats(x, ic='aicc', seasonal.periods=12)
par(mfrow=c(4, 1))
plot(forecast(mod_arima, 12), include=36)
plot(forecast(mod_exponential, 12), include=36)
plot(forecast(mod_neural, 12), include=36)
plot(forecast(mod_tbats, 12), include=36)

I would advise against smoothing the data prior to fitting your model.  Your model is inherently going to try to smooth the data, so pre-smoothing just complicates things.

Edit based on new data: 
It actually looks like arima is one of the worst models you could chose for this training and test set.
I saved your data to a file call coil.csv, loaded it into R, and split it into a training and test set:
library(forecast)
dat <- read.csv('~/coil.csv')
x <- ts(dat$Coil, start=c(dat$Year[1], dat$Month[1]), frequency=12)
test_x <- window(x, start=c(2012, 3))
x <- window(x, end=c(2012, 2))

Next I fit a bunch of time series models: arima, exponential smoothing, neural network, tbats, bats, seasonal decomposition, and structural time series:
models <- list(
  mod_arima = auto.arima(x, ic='aicc', stepwise=FALSE),
  mod_exp = ets(x, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(x, p=12, size=25),
  mod_tbats = tbats(x, ic='aicc', seasonal.periods=12),
  mod_bats = bats(x, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(x, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(x)
  )

Then I made some forecasts and compared to the test set.  I included a naive forecast that always predicts a flat, horizontal line:
forecasts <- lapply(models, forecast, 12)
forecasts$naive <- naive(x, 12)
par(mfrow=c(4, 2))
for(f in forecasts){
  plot(f)
  lines(test_x, col='red')
}


As you can see, the arima model gets the trend wrong, but I kind of like the look of the ""Basic Structural Model""
Finally, I measured each model's accuracy on the test set:
acc <- lapply(forecasts, function(f){
  accuracy(f, test_x)[2,,drop=FALSE]
})
acc <- Reduce(rbind, acc)
row.names(acc) <- names(forecasts)
acc <- acc[order(acc[,'MASE']),]
round(acc, 2)
                ME    RMSE     MAE   MPE MAPE MASE ACF1 Theil's U
mod_sts     283.15  609.04  514.46  0.69 1.27 0.10 0.77      1.65
mod_bats     65.36  706.93  638.31  0.13 1.59 0.12 0.85      1.96
mod_tbats    65.22  706.92  638.32  0.13 1.59 0.12 0.85      1.96
mod_exp      25.00  706.52  641.67  0.03 1.60 0.12 0.85      1.96
naive        25.00  706.52  641.67  0.03 1.60 0.12 0.85      1.96
mod_neural   81.14  853.86  754.61  0.18 1.89 0.14 0.14      2.39
mod_arima   766.51  904.06  766.51  1.90 1.90 0.14 0.73      2.48
mod_stl    -208.74 1166.84 1005.81 -0.52 2.50 0.19 0.32      3.02

The metrics used are described in Hyndman, R.J. and Athanasopoulos, G. (2014) ""Forecasting: principles and practice"" (https://www.otexts.org/fpp/2/5), who also happen to be the authors of the forecast package.  I highly recommend you read their text: it's available for free online.  The structural time series is the best model by several metrics, including MASE, which is the metric I tend to prefer for model selection.
One final question is: did the structural model get lucky on this test set?  One way to assess this is looking at training set errors.  Training set errors are less reliable than test set errors (because they can be over-fit), but in this case the structural model still comes out on top:
acc <- lapply(forecasts, function(f){
  accuracy(f, test_x)[1,,drop=FALSE]
})
acc <- Reduce(rbind, acc)
row.names(acc) <- names(forecasts)
acc <- acc[order(acc[,'MASE']),]
round(acc, 2)
                ME    RMSE     MAE   MPE MAPE MASE  ACF1 Theil's U
mod_sts      -0.03    0.99    0.71  0.00 0.00 0.00  0.08        NA
mod_neural    3.00 1145.91  839.15 -0.09 2.25 0.16  0.00        NA
mod_exp     -82.74 1915.75 1359.87 -0.33 3.68 0.25  0.06        NA
naive       -86.96 1936.38 1386.96 -0.34 3.75 0.26  0.06        NA
mod_arima  -180.32 1889.56 1393.94 -0.74 3.79 0.26  0.09        NA
mod_stl     -38.12 2158.25 1471.63 -0.22 4.00 0.28 -0.09        NA
mod_bats     57.07 2184.16 1525.28  0.00 4.07 0.29 -0.03        NA
mod_tbats    62.30 2203.54 1531.48  0.01 4.08 0.29 -0.03        NA

(Note that the neural network overfit, performing excellent on the training set and poorly on the test set)
Finally, it would be a good idea to cross-validate all of these models, perhaps by training on 2008-2009/testing on 2010, training on 2008-2010/testing on 2011, training on 2008-2011/testing on 2012, training on 2008-2012/testing on 2013, and averaging errors across all of these  time periods.  If you wish to go down that route, I have a partially complete package for cross-validating time series models on github (https://github.com/zachmayer/cv.ts) that I'd love you to try out and give me feedback/pull requests on:
devtools::install_github('zachmayer/cv.ts')
library(cv.ts)

Edit 2: Lets see if I remember how to use my own package! 
First of all, install and load the package from github (see above).  Then cross-validate some models (using the full dataset):
library(cv.ts)
x <- ts(dat$Coil, start=c(dat$Year[1], dat$Month[1]), frequency=12)
ctrl <- tseriesControl(stepSize=1, maxHorizon=12, minObs=36, fixedWindow=TRUE)
models <- list()

models$arima = cv.ts(
  x, auto.arimaForecast, tsControl=ctrl,
  ic='aicc', stepwise=FALSE)

models$exp = cv.ts(
  x, etsForecast, tsControl=ctrl,
  ic='aicc', restrict=FALSE)

models$neural = cv.ts(
  x, nnetarForecast, tsControl=ctrl,
  nn_p=6, size=5)

models$tbats = cv.ts(
  x, tbatsForecast, tsControl=ctrl,
  seasonal.periods=12)

models$bats = cv.ts(
  x, batsForecast, tsControl=ctrl,
  seasonal.periods=12)

models$stl = cv.ts(
  x, stl.Forecast, tsControl=ctrl,
  s.window=12, ic='aicc', robust=TRUE, method='ets')

models$sts = cv.ts(x, stsForecast, tsControl=ctrl)

models$naive = cv.ts(x, naiveForecast, tsControl=ctrl)

models$theta = cv.ts(x, thetaForecast, tsControl=ctrl)

(Note that I reduced the flexibility of the neural network model, to try to help prevent it from overfitting)
Once we've fit the models, we can compare them by MAPE (cv.ts doesn't yet support MASE):
res_overall <- lapply(models, function(x) x$results[13,-1])
res_overall <- Reduce(rbind, res_overall)
row.names(res_overall) <- names(models)
res_overall <- res_overall[order(res_overall[,'MAPE']),]
round(res_overall, 2)
                 ME    RMSE     MAE   MPE MAPE
naive     91.40 1126.83  961.18  0.19 2.40
ets       91.56 1127.09  961.35  0.19 2.40
stl     -114.59 1661.73 1332.73 -0.29 3.36
neural     5.26 1979.83 1521.83  0.00 3.83
bats     294.01 2087.99 1725.14  0.70 4.32
sts     -698.90 3680.71 1901.78 -1.81 4.77
arima  -1687.27 2750.49 2199.53 -4.23 5.53
tbats   -476.67 2761.44 2428.34 -1.23 6.10

Ouch.  It would appear that our structural forecast got lucky.  Over the long term, the naive forecast makes the best forecasts, averaged across a 12-month horizon (the arima model is still one of the worst models).  Let's compare the models at each of the 12 forecast horizons, and see if any of them ever beat the naive model:
library(reshape2)
library(ggplot2)
res <- lapply(models, function(x) x$results$MAPE[1:12])
res <- data.frame(do.call(cbind, res))
res$horizon <- 1:nrow(res)
res <- melt(res, id.var='horizon', variable.name='model', value.name='MAPE')
res$model <- factor(res$model, levels=row.names(res_overall))
ggplot(res, aes(x=horizon, y=MAPE, col=model)) +
  geom_line(size=2) + theme_bw() +
  theme(legend.position=""top"") +
  scale_color_manual(values=c(
    ""#1f78b4"", ""#ff7f00"", ""#33a02c"", ""#6a3d9a"",
    ""#e31a1c"", ""#b15928"", ""#a6cee3"", ""#fdbf6f"",
    ""#b2df8a"")
    )


Tellingly, the exponential smoothing model is always picking the naive model (the orange line and blue line overlap 100%).  In other words, the naive forecast of ""next month's coil prices will be the same as this month's coil prices"" is more accurate (at almost every forecast horizon) than 7 extremely sophisticated time series models.  Unless you have some secret information the coil market doesn't already know, beating the naive coil price forecast is going to be extremely difficult.
It's never the answer anyone wants to hear, but if forecast accuracy is your goal, you should use the most accurate model.  Use the naive model.
","**Summary:**

The `forecast` package provides various time series models for forecasting, including ARIMA, exponential smoothing, neural networks, and structural time series. Pre-smoothing the data before modeling is discouraged.

After evaluating seven models (ARIMA, exponential smoothing, neural network, TBATS, BATS, seasonal decomposition, and structural time series) on a training and test set, the structural time series model performed best on several metrics, including MASE. However, cross-validation revealed that the naive forecast (predicting flat, horizontal lines) consistently outperformed the structural time series model and all others.

This result highlights the difficulty of beating the naive forecast for the coil price dataset, suggesting that extraneous information not considered in the time series models may be highly influential. Therefore, the naive model is recommended for forecasting coil prices for optimal accuracy."
Cost function turning into nan after a certain number of iterations,https://stats.stackexchange.com/questions/325451/cost-function-turning-into-nan-after-a-certain-number-of-iterations,machine-learning|neural-networks|deep-learning|gradient-descent,325451,True,326029,45163,3,16,1593498875,1517139284,16,1517393727,"Well, if you get NaN values in your cost function, it means that the input is outside of the function domain. E.g. the logarithm of 0. Or it could be in the domain analytically, but due to numerical errors we get the same problem (e.g. a small value gets rounded to 0).
It has nothing to do with an inability to ""settle"".
So, you have to determine what the non-allowed function input values for your given cost function are. Then, you have to determine why you are getting that input to your cost function. You may have to change the scaling of the input data and the weight initialization. Or you just have to have an adaptive learning rate as suggested by Avis, as the cost function landscape may be quiet chaotic. Or it could be because of something else, like numerical issues with some layer in your architecture. 
It is very difficult to say with deep networks, but I suggest you start looking at the progression of the input values to your cost function (the output of your activation layer), and try to determine a cause.
","NaN values in a cost function indicate that the input is outside the function's domain or numerical errors have occurred. The issue is unrelated to the network's ability to ""settle.""

To resolve the issue, determine the disallowed input values for the cost function and identify why they are being received. Possible solutions include:

* Adjusting the scaling of input data and weight initialization
* Using an adaptive learning rate
* Resolving numerical issues within the network architecture

It is recommended to examine the progression of input values to the cost function to determine the cause. Deep networks present challenges, but this approach can assist in identifying the source of NaN values and implementing appropriate remedies."
What is the difference between a neural network and a perceptron?,https://stats.stackexchange.com/questions/134401/what-is-the-difference-between-a-neural-network-and-a-perceptron,machine-learning|neural-networks|terminology|perceptron,134401,True,134406,27452,4,16,1496314244,1421876653,10,1421878337,"Yes, there is - ""perceptron"" refers to a particular supervised learning model, which was outlined by Rosenblatt in 1957. The perceptron is a particular type of neural network, and is in fact historically important as one of the types of neural network developed. There are other types of neural network which were developed after the perceptron, and the diversity of neural networks continues to grow (especially given how cutting-edge and fashionable deep learning is these days).
","The term ""perceptron"" refers to a specific type of machine learning model, introduced by Rosenblatt in 1957. It is a type of neural network, and was among the first such networks developed. Perceptrons are historically significant, but various types of neural networks have been developed since, diversifying the field. Deep learning, a modern and popular approach, further contributes to this diversity."
"For neural networks, is mini-batching done purely because of memory constraints?",https://stats.stackexchange.com/questions/589298/for-neural-networks-is-mini-batching-done-purely-because-of-memory-constraints,neural-networks,589298,True,589302,3208,5,16,1663716902,1663601381,12,1663602545,"There is evidence that supports the proposition that it is best to use the biggest batch size your machine can handle. See e.g. Goyal et al. (2018) (https://arxiv.org/pdf/1706.02677.pdf).
However, that paper (and another (https://arxiv.org/pdf/1609.04836.pdf)) reveal optimization difficulties with extremely large batch sizes. There is concern that large-batch optimization converges to ""sharp"" minima that generalize less well. Goyal et al. present some strategies to avoid those optimization difficulties (e.g. low-training-rate warmup phase, how to adjust learning rate dependent on batch size, training data shuffling each epoch).
","**Summary:**

Large batch sizes are generally recommended for training machine learning models. However, extremely large batch sizes can lead to optimization difficulties and convergence to ineffective minima.

Researchers have identified potential solutions to these difficulties, including:

* Low-training-rate warmup phase
* Adjustments to the learning rate based on batch size
* Shuffling training data each epoch

By employing these strategies, it is possible to mitigate the risks associated with using extremely large batch sizes and harness their benefits for improved model training and generalization."
Different definitions of the cross entropy loss function,https://stats.stackexchange.com/questions/223799/different-definitions-of-the-cross-entropy-loss-function,neural-networks|loss-functions|softmax|cross-entropy,223799,True,224491,8592,2,16,1536166703,1468512027,27,1468923038,"These three definitions are essentially the same.
1) The Tensorflow introduction (https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html#training),
$$C = -\frac{1}{n} \sum\limits_x\sum\limits_{j} (y_j \ln a_j).$$
2) For binary classifications $j=2$, it becomes
$$C = -\frac{1}{n} \sum\limits_x (y_1 \ln a_1 + y_2 \ln a_2)$$
and because of the constraints $\sum_ja_j=1$ and $\sum_jy_j=1$, it can be rewritten as
$$C = -\frac{1}{n} \sum\limits_x (y_1 \ln a_1 + (1-y_1) \ln (1-a_1))$$
which is the same as in the 3rd chapter (http://neuralnetworksanddeeplearning.com/chap3.html).
3) Moreover, if $y$ is a one-hot vector (https://en.wikipedia.org/wiki/One-hot) (which is commonly the case for classification labels) with $y_k$ being the only non-zero element, then the cross entropy loss of the corresponding sample is
$$C_x=-\sum\limits_{j} (y_j \ln a_j)=-(0+0+...+y_k\ln a_k)=-\ln a_k.$$
In the cs231 notes (https://cs231n.github.io/linear-classify/#softmax-classifier), the cross entropy loss of one sample is given together with softmax normalization as
$$C_x=-\ln(a_k)=-\ln\left(\frac{e^{f_k}}{\sum_je^{f_j}}\right).$$
","**Summary:**

Cross-entropy loss is a measure of the difference between predicted and actual outcomes in machine learning models. It is used in classification tasks where the model predicts a probability distribution over multiple classes.

Three equivalent definitions of cross-entropy loss are provided:

1. A general definition using summations over data points and classes.
2. A simplified definition for binary classification, using the probability of one class and the constraint that class probabilities sum to one.
3. A specialized definition for cases where actual outcomes are one-hot vectors, where only one element is non-zero, resulting in a simplified expression.

Additionally, the definition of cross-entropy loss with softmax normalization is given, which is used in conjunction with the softmax function to normalize class probabilities."
Why is a 0-1 loss function intractable?,https://stats.stackexchange.com/questions/365444/why-is-a-0-1-loss-function-intractable,neural-networks|deep-learning|loss-functions,365444,True,365446,9806,2,16,1546440420,1536118218,21,1536120499,"The 0-1 loss function is non-convex and discontinuous, so (sub)gradient methods cannot be applied. For binary classification with a linear separator, this loss function can be formulated as finding the $\beta$ that minimizes the average value of the indicator function $\mathbf{1}(y_{i}\beta\mathbf{x}_{i} \leq 0)$ over all $i$ samples. This is exponential in the inputs, as since there are two possible values for each pair, there are $2^{n}$ possible configurations to check for $n$ total sample points. This is known to be NP-hard. Knowing the current value of your loss function doesnât provide any clue as to how you should possibly modify your current solution to improve, as you could derive if gradient methods for convex or continuous functions were available.
","**Summary:**

The 0-1 loss function, used in binary classification with linear separators, is non-convex and discontinuous. This makes it challenging to optimize using gradient-based methods. To minimize the loss, one must find the $\beta$ that minimizes the average indicator function $\mathbf{1}(y_{i}\beta\mathbf{x}_{i} \leq 0)$ over all samples. This problem is exponentially complex, requiring the evaluation of $2^n$ possible configurations for $n$ samples.

Furthermore, the lack of continuity and convexity hinders the use of optimization techniques that rely on knowing the current loss function value to guide improvements. As a result, finding the optimal solution for the 0-1 loss function remains a significant computational challenge."
What is the benefit of the truncated normal distribution in initializing weights in a neural network?,https://stats.stackexchange.com/questions/228670/what-is-the-benefit-of-the-truncated-normal-distribution-in-initializing-weights,neural-networks|backpropagation|weights|truncated-normal-distribution,228670,True,269780,7566,2,16,1631803219,1470591699,14,1490462283,"I think its about saturation of the neurons. Think about you have an activation function like sigmoid.
 (https://i.sstatic.net/5VmI7.png)
If your weight val gets value >= 2 or <=-2 your neuron will not learn. So, if you truncate your normal distribution you will not have this issue(at least from the initialization) based on your variance. I think thats why, its better to use truncated normal in general.
","**Summary:**

The saturation of neuron activations is a potential problem in neural networks. When neuron weights become too large, the activation function (such as the sigmoid function) becomes saturated, preventing the neuron from learning further.

Truncated normal distribution assigns probability zero to weights outside a specified range. By initializing neuron weights using a truncated normal distribution, the risk of saturation is reduced. This ensures that neuron weights are less likely to fall into the saturated region, allowing the neuron to continue learning and adapting.

Therefore, using a truncated normal distribution for weight initialization in neural networks is generally preferred to prevent saturation and enhance learning capabilities."
Epoch Vs Iteration in CNN training,https://stats.stackexchange.com/questions/360157/epoch-vs-iteration-in-cnn-training,machine-learning|deep-learning|tensorflow|caffe,360157,True,360160,23965,1,16,1533118026,1533116830,20,1533118026,"One iteration means one batch processed.
One epoch means all data processed one times.
So one epoch is counted when (batch_size * number_iteration) >= number_data
","**Summary:**

An iteration represents a single batch of data being processed through a machine learning model. An epoch is completed when all data points have been processed through the model at least once. The completion of an epoch is measured by comparing the product of the batch size and the number of iterations to the total number of data points. When this product is greater than or equal to the number of data points, an epoch has been completed.

In simpler terms, an epoch represents a full pass through the entire dataset, while an iteration represents a single step in that pass."
How many parameters are in a gated recurrent unit (GRU) recurrent neural network (RNN) layer?,https://stats.stackexchange.com/questions/328926/how-many-parameters-are-in-a-gated-recurrent-unit-gru-recurrent-neural-network,neural-networks|recurrent-neural-network|gru,328926,True,328927,17824,2,16,1676995520,1518756206,18,1518756206,"The original GRU paper ""Learning Phrase Representations using RNN EncoderâDecoder for Statistical Machine Translation (https://arxiv.org/pdf/1406.1078.pdf)"" by Kyunghyum Cho et al. does not include bias parameters in their equations. Instead, the authors write

To make the equations uncluttered, we omit biases.

which does not help a reader understand how the authors envisioned using bias neurons; nor does it allow readers to easily count the number of bias neurons.
So we have to look elsewhere. According to Rahul Dey and Fathi M. Salem, ""Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks (https://arxiv.org/pdf/1701.05923.pdf)"":

... the total number of parameters in the GRU RNN equals $\color{blue}{3 (n^2 + nm + n)}$.

where $m$ is the input dimension and $n$ is the output dimension. This is due to the fact that there are three sets of operations requiring weight matrices of these sizes.
Dey and Salem outline the GRU in this manner:

The GRU RNN reduce the gating signals to two from the
LSTM RNN model. The two gates are called an update gate $z_t$
and a reset gate $r_t$. The GRU RNN model is presented in the
form:
$$\begin{align}
h_t &= (1 - z_t)\odot h_{t-1} + z_t \odot \tilde{h}_t \\
\tilde{h}_t &= g(W_h x_t + U_h(r_t \odot h_{t-1}) + b_h)
\end{align}$$
with the two gates presented as:
$$\begin{align}
z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r)
\end{align}$$

and in the beginning of the paper, they lay out the notation used as

$W$ is an $n \times m$ matrix, $U$ is an $n \times n$ matrix and $b$ is an $n \times 1$ matrix (or vector)
for a hidden state of size $n$ and an input of size $m$.

These parameters counts might differ from what you find in software. It seems that some software (e.g. PyTorch, Keras) has made the decision to over-parameterize the model, by including additional bias units. In these software implementations, the total parameter count is given as
$$
\color{blue}{3 (n^2 + nm + }\color{red}{2}\color{blue}{n)}.
$$
This appears to change three of the GRU equations:
$$\begin{align}
\tilde{h}_t &= g(W_h x_t + b_{hW} + U_h(r_t \odot h_{t-1}) + b_{hU}) \\
z_t &= \sigma(W_z x_t + b_{zW} + U_z h_{t-1} + b_{zU}) \\
r_t &= \sigma(W_r x_t + b_{rW} + U_r h_{t-1} + b_{rU})
\end{align}$$
which we can see is algebraically the same, using the substitution $b_{iW} + b_{iU} = b_{i}$. I'm not sure why software would do this. Perhaps the intention is to create the GRU using compositions of existing linear layer classes, and biases are included in both linear layers. Perhaps this parameterization works better with CUDA devices for some reason.
Here's a toy script to verify this behavior, using torch==1.12.1.
import torch.nn as nn

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

if __name__ == ""__main__"":
    n = 7
    m = 5
    gru = nn.GRU(input_size=m, hidden_size=n, num_layers=1)
    print(count_parameters(gru)==3 * (n ** 2 + n * m + 2 * n)) # True

","The original GRU paper omitted bias parameters from its equations, leaving readers unsure how to use or count them. Later research by Dey and Salem revealed that the total number of parameters in a GRU RNN is 3(nÂ² + nm + n), where n is the output dimension and m is the input dimension.

The GRU has two gates: an update gate and a reset gate. The update gate controls the flow of information from the previous hidden state to the current hidden state, while the reset gate determines how much of the previous hidden state is forgotten.

Software implementations of the GRU (e.g., PyTorch, Keras) differ by including additional bias units. This increases the parameter count to 3(nÂ² + nm + 2n). The additional bias units do not alter the mathematical equivalence of the equations but may improve performance with CUDA devices or simplify implementation."
Variational Autoencoder â Dimension of the latent space,https://stats.stackexchange.com/questions/327966/variational-autoencoder-%e2%88%92-dimension-of-the-latent-space,machine-learning|neural-networks|normal-distribution|autoencoders|generative-models,327966,True,350978,18336,1,16,1528804272,1518306894,20,1528804272,"You seem to have misunderstood your architecture and are, quite simply, overfitting your data.
It looks like your interpretation of the latent space is that it represents a manifold of realistic-looking images. That is unlikely in the best case, and if your decoder performs any transformation (except perhaps an affine transformation) on the sampling outputs - impossible.
Autoencoders (or rather the encoder component of them) in general are compression algorithms. This means that they approximate 'real' data with a smaller set of more abstract features. 
For example, a string '33333333000000000669111222222' could be losslessly compressed by a very simplistic algorithm to '8:3/9:0/2:6/1:9/3:1/6:2' - occurences:number, maintaining position. If your criterion was length of text, the encoding is six characters shorter - not a huge improvement, but an improvement nonetheless.
What happened was we've introduced an abstract, higher-dimensional feature - 'number of repetitions' - that helps us express the original data more tersely. You could compress the output further; for example, noticing that even positions are just separators, you could encode them as a single-bit padding rather than an ASCII code.
Autoencoders do exactly that, except they get to pick the features themselves, and variational autoencoders enforce that the final level of coding (at least) is fuzzy in a way that can be manipulated.
So what you do in your model is that you're describing your input image using over sixty-five thousand features. And in a variational autoencoder, each feature is actually a sliding scale between two distinct versions of a feature, e.g. male/female for faces, or wide/thin brushstroke for MNIST digits.
Can you think of just a hundred ways to describe the differences between two realistic pictures in a meaningful way? Possible, I suppose, but they'll get increasingly forced as you try to go on.
With so much room to spare, the optimizer can comfortably encode each distinct training image's features in a non-overlapping slice of the latent space rather than learning the features of the training data taken globally.
So, when you feed it a validation picture, its encoding lands somewhere between islands of locally applicable feature encodings and so the result is entirely incoherent.
","**Summary:**

Autoencoders (specifically the encoder component) compress data into a smaller set of abstract features. The latent space in autoencoders does not represent a manifold of realistic images but rather a compressed representation of the input data.

In the context of the given model, the overfitting occurs because the encoder has too many features (over 65,000), allowing it to encode each training image with unique features. This results in the model memorizing the training data rather than learning general features applicable to unseen data.

When a validation image is presented, its encoding falls between the non-overlapping ""islands"" of feature encodings learned from the training data, leading to incoherent results. Therefore, to prevent overfitting, the model should be designed with fewer features to encourage the learning of general features common to all images."
Feature selection using deep learning?,https://stats.stackexchange.com/questions/250381/feature-selection-using-deep-learning,feature-selection|deep-learning|deep-belief-networks|restricted-boltzmann-machine,250381,True,250390,12268,3,16,1555135186,1481191686,13,1481193243,"One approach you can take for almost any prediction model is to first train your model and find its accuracy, then for one input add some noise to it and check the accuracy again. Repeat this for each input and observe how the noise worsens the predictions. If an input is important then the extra uncertainty due to the noise will be detrimental.
Remember set the variance of the noise to be proportional to the variance of the input in question.
Of course noise is random and you don't want one input to appear unimportant due to random effects. If you have few training examples then consider repeatedly calculating the change in accuracy for each training example with a new noise added each time.
In response to the comments:
This analysis can also be done by removing a variable entirely but this has some downsides compared to adding noise. 

Suppose that one of your inputs is constant, it acts like a bias term so it has some role to play in the prediction but it adds no information. If you removed this input entirely then the prediction would become less accurate because the perceptrons are getting the wrong bias. This makes the input look like it is important for prediction even though it adds no information. Adding noise won't cause this problem. This first point isn't a problem if you have standardized all inputs to have zero mean.
If two inputs are correlated then the the information about one input gives information about the other. A model could be trained well if you used only one of the correlated inputs so you want the analysis to find that one input isn't helpful. If you just removed one of the inputs then, like the first point made, the prediction accuracy would decrease a lot which indicates that it is important. However, adding noise won't cause this problem.

","**Summary:**

To assess the importance of input variables in a prediction model, a noise analysis can be performed. This involves adding noise to the inputs and observing how it affects accuracy. If an input is important, the noise will significantly worsen the model's predictions.

It's important to set the noise variance proportional to that of the input. To minimize randomness, repeat the analysis multiple times with different noise instances.

Benefits of using noise analysis over removing variables:

* **Constant inputs:** Noise analysis does not decrease accuracy when an input is constant, unlike removing it, which can falsely suggest importance.
* **Correlated inputs:** Noise analysis can identify the importance of one correlated input without affecting the prediction accuracy, while removing it may indicate importance based on the presence of its correlated partner."
Why don&#39;t we use non-constant learning rates for gradient decent for things other then neural networks?,https://stats.stackexchange.com/questions/331411/why-dont-we-use-non-constant-learning-rates-for-gradient-decent-for-things-othe,machine-learning|deep-learning|optimization|gradient-descent,331411,True,331428,1162,1,16,1520007825,1520002020,18,1520007825,"Disclaimer: I don't have so much experience with optimization outside of neural networks, so my answer will be clearly biased, but there are several things that play role:

(Deep) neural networks have a lot of parameters. This has several implications:
Firstly, it kind-of rules out higher order methods simply because computing Hessian and higher derivatives becomes infeasible. In other domains, this may be a valid approach better than any tweaks to SGD.
Secondly, although SGD is wonderful (https://arxiv.org/abs/1509.01240), it tends to be impractically slow. These improved SGD variants mainly enable faster training, while potentially losing some of the nice properties of SGD (https://arxiv.org/abs/1705.08292). In other domains, the SGD training time may not be the bottleneck, so improvements gained by speeding it up may be simply negligible.
Training (deep) neural networks is non-convex optimization and I am not aware of significant convex relaxation results in the field. Unlike other fields, neural networks are not focused on provably globally optimal solutions, which leads to investing more efforts into improving the properties of loss surface and its traversal during the optimization.
In other fields, employing convex relaxation and obtaining globally optimal solutions may be in the center of the interest instead of the optimization algorithm, because once the problem is defined as a convex problem, the choice of the optimization algorithm cannot improve the quality of the solution.

I suppose this answer does not cover all possible aspects and I am myself curious about other opinions.
","Training deep neural networks requires a large number of parameters, presenting challenges for optimization. Higher-order methods are impractical due to the high computational cost of computing derivatives.

While Stochastic Gradient Descent (SGD) remains an effective optimizer, its slow convergence can hinder training efficiency. Variants of SGD have been developed to accelerate training, but may compromise its desirable properties.

Unlike other optimization domains, training neural networks involves non-convex optimization, where finding globally optimal solutions is not a primary focus. Instead, efforts are dedicated to improving the loss surface and optimizing the traversal during training.

In contrast, other fields often prioritize using convex relaxation and obtaining guaranteed globally optimal solutions, emphasizing the problem formulation rather than the choice of optimization algorithm."
Neural network - binary vs discrete / continuous input,https://stats.stackexchange.com/questions/157985/neural-network-binary-vs-discrete-continuous-input,neural-networks,157985,True,203440,28799,3,16,1547986948,1434888023,13,1458815034,"Whether to convert input variables to binary depends on the input variable. You could think of neural network inputs as representing a kind of ""intensity"": i.e., larger values of the input variable represent greater intensity of that input variable. After all, assuming the network has only one input, a given hidden node of the network is going to learn some function $f(wx + b)$. where $f$ is the transfer function (e.g. the sigmoid) and $x$ the input variable.
This setup does not make sense for categorical variables. If categories are represented by numbers, it makes no sense to apply the function $f(wx + b)$ to them. E.g. imagine your input variable represents an animal, and sheep=1 and cow=2. It makes no sense to multiply sheep by $w$ and add $b$ to it, nor does it make sense for cow to be always greater in magnitude than sheep. In this case, you should convert the discrete encoding to a binary, 1-of-$k$ encoding.
For real-valued variables, just leave them real-valued (but normalize inputs). E.g. say you have two input variables, one the animal and one the animal's temperature. You'd convert animal to 1-of-$k$, where $k$=number of animals, and you'd leave temperature as-is.
","Input variables in neural networks can be binary or real-valued depending on their nature.

**Categorical Variables (Binary):**
- Represent distinct categories with numbers.
- Convert to binary 1-of-k encoding to avoid meaningless arithmetic operations.

**Real-Valued Variables:**
- Represent intensity or continuous values.
- Can be left as-is, but should be normalized.

Normalization for real-valued variables helps ensure equal influence of different input features and improves network performance."
Getting started with neural networks for forecasting,https://stats.stackexchange.com/questions/9842/getting-started-with-neural-networks-for-forecasting,time-series|neural-networks|forecasting|references,9842,True,9869,8441,2,16,1559121856,1303414615,11,1303477082,"Here's a good quick introduction:
intro to neural networks. (http://arxiv.org/pdf/cs/0308031)
Note that R has neural-network functionality, so no need to spend any time implementing NN yourself until you've given it a spin and decided it looks promising for your application.
Neural networks are not obsolete, but they have gone through a couple of hype cycles, and then after realizing they don't do everything as was claimed, their reputation goes into a trough for a while (we're currently in one of those).  Neural networks are good at certain tasks, and generally are better for tasks in which a human can do a similar task, but cannot explain exactly how they do it.
Neural networks do not give you much insight into the system you're using them to analyze, even after they are trained and operating well.  That is, they can predict what will happen (for some systems), but not tell you why.  In some cases, that is fine.  In others, that is not fine.  Generally, if you want or especially if you already have an understanding of the rules of how something works, you can use other techniques.
But, for certain tasks, they work well.
For time-series in particular, see this question's discussion:
Proper way of using recurrent neural network for time series analysis (https://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis)
","**Summary:**

Neural networks are powerful tools for solving certain tasks, but they come with limitations. They excel at tasks where humans can perform well but cannot fully explain their process. In such cases, neural networks can learn and predict, but they provide limited insight into the underlying mechanisms.

However, neural networks are not obsolete. They have proven effective in specific domains. For instance, they are well-suited for time-series analysis, where they can capture complex patterns and make accurate predictions.

It's important to note that neural networks are not a panacea. They are best suited for tasks where other techniques, such as traditional statistical methods or rule-based systems, may be insufficient or less effective. Understanding the strengths and limitations of neural networks is crucial for making informed decisions about their use."
State-of-the-art ensemble learning algorithm in pattern recognition tasks?,https://stats.stackexchange.com/questions/232841/state-of-the-art-ensemble-learning-algorithm-in-pattern-recognition-tasks,machine-learning|neural-networks|pattern-recognition|ensemble-learning|optical-character-recognition,232841,True,233210,2287,3,16,1484234187,1472732416,11,1472919489,"State-of-the-art algorithms may differ from what is used in production in the industry. Also,  the latter can invest in fine-tuning more basic (and often more interpretable) approaches to make them work better than what academics would. 
Example 1: According to TechCrunch (https://techcrunch.com/2016/08/16/dragon-15/), Nuance will start using ""deep learning tech"" in its Dragon speech recognition products this september.
Example 2: Chiticariu, Laura, Yunyao Li, and Frederick R. Reiss. ""Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!."" In EMNLP, no. October, pp. 827-832. 2013. https://scholar.google.com/scholar?cluster=12856773132046965379&hl=en&as_sdt=0,22 (https://scholar.google.com/scholar?cluster=12856773132046965379&hl=en&as_sdt=0,22) ; http://www.aclweb.org/website/old_anthology/D/D13/D13-1079.pdf (http://www.aclweb.org/website/old_anthology/D/D13/D13-1079.pdf)
 (https://i.sstatic.net/wrrM0.png)
With that being said:

Which of the ensemble learning algorithms is considered to be state-of-the-art nowadays

One of the state-of-the-art systems for image classification gets some nice gain with ensemble (just like most other systems I far as I know): He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep residual learning for image recognition."" arXiv preprint arXiv:1512.03385 (2015). https://scholar.google.com/scholar?cluster=17704431389020559554&hl=en&as_sdt=0,22 (https://scholar.google.com/scholar?cluster=17704431389020559554&hl=en&as_sdt=0,22) ; https://arxiv.org/pdf/1512.03385v1.pdf (https://arxiv.org/pdf/1512.03385v1.pdf)
 (https://i.sstatic.net/9yGuZ.png)
","**Summary:**

State-of-the-art algorithms in research may differ from those used in industry. Industry may prioritize refining simpler algorithms for better performance, while academics focus on cutting-edge techniques.

**Examples:**

- Nuance is implementing deep learning in its Dragon speech recognition products.
- Researchers argue that rule-based information extraction systems remain valuable despite the emergence of deep learning.

**Ensemble Learning:**

Ensemble learning algorithms are considered state-of-the-art and have been successfully applied in image classification systems, such as Deep Residual Learning."
How are filters and activation maps connected in Convolutional Neural Networks?,https://stats.stackexchange.com/questions/180850/how-are-filters-and-activation-maps-connected-in-convolutional-neural-networks,machine-learning|deep-learning|convolutional-neural-network,180850,True,192769,17776,3,16,1718532749,1447057269,13,1453915099,"The second convolutional neural network (CNN) architecture you posted comes from this paper (http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf). In the paper the authors give a description of what happens between layers S2 and C3. Their explanation is not very clear though. I'd say that this CNN architecture is not 'standard', and it can be quite confusing as a first example for CNNs.

First of all, a clarification is needed on how feature maps are produced and what their relationship with filters is. A feature map is the result of the convolution of a filter with a feature map. Let's take the layers INPUT and C1 as an example. In the most common case, to get 6 feature maps of size $28 \times 28$ in layer C1 you need 6 filters of size $5 \times 5$ (the result of a 'valid' convolution (http://uk.mathworks.com/help/matlab/ref/conv.html) of an image of size $M \times M$ with a filter of size $N \times N$, assuming $M \geq N$, has size $(M-N+1) \times (M-N+1)$. You could, however, produce 6 feature maps by combining feature maps produced by more or less than 6 filters (e.g. by summing them up). In the paper, nothing of the sort is implied though for layer C1.
What happens between layer S2 and layer C3 is the following. There are 16 feature maps in layer C3 produced from 6 feature maps in layer S2. The number of filters in layer C3 is indeed not obvious. In fact, from the architecture diagram only, one cannot judge what the exact number of filters that produce those 16 feature maps is. The authors of the paper provide the following table (page 8): 

With the table they provide the following explanation (bottom of page 7): 

Layer C3 is a convolutional layer with 16 feature maps. Each unit in each feature map is connected to several $5 \times 5$ neighborhoods at identical locations in a subset of S2's feature maps.

In the table the authors show that every feature map in layer C3 is produced by combining 3 or more feature maps (page 8):

The first six C3 feature maps take inputs from every contiguous subsets of three feature maps in S2. The next six take input from every contiguous subset of four. The next three take input from some discontinuous subsets of four. Finally, the last one takes input from all S2 feature maps.

Now, how many filters are there in layer C3? Unfortunately, they do not explain this. The two simplest possibilities would be:

There is one filter per S2 feature map per C3 feature map, i.e. there is no filter sharing between S2 feature maps associated with the same C3 feature map.
There is one filter per C3 feature map, which is shared across the (3 or more) feature maps of layer S2 that are combined.

In both cases, to 'combine' would mean that the results of convolution per S2 feature map group, would need to be combined to produced C3 feature maps. The authors do not specify how this is done, but addition is a common choice (see for example the animated gif near the middle of this page (http://cs231n.github.io/convolutional-networks/).
The authors give some additional information though, which can help us decipher the architecture. They say that 'layer C3 has 1,516 trainable parameters' (page 8). We can use this information to decide between cases (1) and (2) above.
In case (1) we have $(6 \times 3) + (9 \times 4) + (1 \times 6) = 60$ filters. The filter size is $(14-10+1) \times (14-10+1) = 5 \times 5$. The number of trainable parameters in this case would be $5 \times 5 \times 60 = 1,500$ trainable parameters. If we assume one bias unit per C3 feature map, we get $1,500 + 16 = 1,516$ parameters, which is what the authors say. For completeness, in case (2) we would have $(5 \times 5 \times 16) + 16 = 416$ parameters, which is not the case.
Therefore, if we look again at Table I above, there are 10 distinct C3 filters associated with each S2 feature map (thus 60 distinct filters in total).
The authors explain this type of choice:

Different feature maps [in layer C3] are forced to extract different (hopefully complementary) features because they get different sets of inputs.

I hope this clarifies the situation. 
","The second CNN architecture in the paper introduces a more complex layer structure between layers S2 and C3. Unlike typical CNNs, where each layer is produced by applying multiple filters to a single feature map, this architecture combines multiple feature maps in layer S2 to produce feature maps in layer C3.

The number of filters in layer C3 is not explicitly stated in the architecture diagram. However, based on the number of trainable parameters provided by the authors, it can be inferred that there are 60 distinct filters in total, with 10 filters connected to each feature map in layer S2.

To combine the S2 feature maps, each C3 feature map receives input from a subset of S2 feature maps. The first six C3 feature maps combine three consecutive S2 feature maps, the next six combine four consecutive S2 feature maps, and the remaining three combine discontinuous subsets of four S2 feature maps. The last C3 feature map combines all six S2 feature maps.

By using different combinations of S2 feature maps, the authors aim to force different C3 feature maps to extract complementary features from the input."
How does the back-propagation work in a siamese neural network?,https://stats.stackexchange.com/questions/154652/how-does-the-back-propagation-work-in-a-siamese-neural-network,neural-networks,154652,True,154760,12120,1,16,1433164674,1432929516,14,1433013131,"Both networks share the similar architectures and but they are constrained to have the same weights as the publication describes at section 4 [1].
Their goal is to learn features that minimize the cosine similarity between, their output vectors when signatures are genuine, and maximize it when they are forged (this is the backprop goal as well, but the actual loss function is not presented).
The cosine similarity $\cos(A,B) = {A \cdot B \over \|A\| \|B\|}$ of two vectors $A, B$, is a measure of similarity that gives you the cosine of the angle between them (therefore, its output is not binary). If your concern is how you can backprop to a function that outputs either true or false, think of the case of binary classification.
You shouldn't change the output layer, it consists of trained neurons with linear values and its a higher-level abstraction of your input. The whole network should be trained together. Both outputs $O_1$ and $O_2$ are passed through a $cos(O_1,O_2)$ function that outputs their cosine similarity ($1$ if they are similar, and $0$ if they are not). Given that, and that we have two sets of input tuples $X_{Forged}, X_{Genuine}$, an example of the simplest possible loss function you could have to train against could be:
$$\mathcal{L}=\sum_{(x_A,x_B) \in X_{Forged}} cos(x_A,x_B) - \sum_{(x_C,x_D) \in X_{Genuine}} cos(x_C,x_D)$$
After you have trained your network, you just input the two signatures you get the two outputs pass them to the $cos(O_1,O_2)$ function, and check their similarity.
Finally, to keep the network weights identical there are several ways to do that (and they are used in Recurrent Neural Networks too); a common approach is to average the gradients of the two networks before performing the Gradient Descent update step.
[1] http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf (http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf)
","**Network Architecture and Goal**
Two identical neural networks (referred to as Siamese networks) are used, each with the same weights. Their goal is to learn features that minimize the cosine similarity between their output vectors when genuine signatures are input and maximize it when forged signatures are input.

**Cosine Similarity and Backpropagation**
Cosine similarity measures the similarity between two vectors, with a value of 1 indicating perfect similarity and 0 indicating no similarity. For backpropagation in binary classification, the cosine similarity is used to determine the similarity between the output vectors before being passed through a loss function.

**Loss Function**
A simple loss function that could be used is the sum of the cosine similarities between forged signature pairs minus the sum of the cosine similarities between genuine signature pairs.

**Training and Inference**
After training, new signature pairs are input to the networks, and their output vectors are passed through the cosine similarity function to check their similarity.

**Weight Synchronization**
To keep the network weights identical, the gradients of the two networks are averaged before performing the gradient descent update step."
Why is deep reinforcement learning unstable?,https://stats.stackexchange.com/questions/265964/why-is-deep-reinforcement-learning-unstable,machine-learning|neural-networks|deep-learning|reinforcement-learning,265964,True,266244,12321,1,16,1563272831,1488892542,13,1488999029,"The main problem is that, as in many other fields, DNN can be hard to train. Here, one problem is the correlation of input data: if you think about a video game (they actually use those to test their algorithms), you can imagine that screenshots taken one step after another are highly correlated: the game evolves ""continuously"". That, for NNs, can be a problem: doing many iterations of gradient descent on similar and correlated inputs may lead to overfit them and/or fall into a local minimum. This why they use experience replay: they store a series of ""snapshots"" of the game, then shuffle them, and pick them some steps later to do training. In this way, the data is not correlated anymore. 
Then, they notice how during the training the Q values (predicted by the NN) can change the on going policy, so making the agent prefer only a set of actions and causing it to store data that is correlated for the same reasons as before: this is why they delay training and update Q periodically, to ensure that the agent can explore the game, and train on shuffled and uncorrelated data. 
","**Summary:**

DNN training faces challenges due to the high correlation of input data. To address this, experience replay is used to shuffle and decorrelate the input data. This prevents overfitting and improves training efficiency.

Additionally, to avoid training on correlated data caused by changes in Q values during training, Q values are periodically updated. This allows the agent to explore the game and gather uncorrelated data for training. These techniques help enhance the training process of DNNs in fields like video game AI."
What are alternatives to VC-dimension for measuring the complexity of neural networks?,https://stats.stackexchange.com/questions/23484/what-are-alternatives-to-vc-dimension-for-measuring-the-complexity-of-neural-net,neural-networks|mathematical-statistics|vc-dimension|pac-learning,23484,True,23485,1270,2,16,1491665688,1328744311,8,1328797308,"You might want to have a look at the paper ""(Not) Bounding the True Error by John Langford & Rich Caruana (NIPS, 2001)
The abstract states:

We present a new approach to bounding the true error rate of a continuous valued classifier based upon PAC-Bayes bounds. The method first constructs a distribution over classifiers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artificial neural networks with results of a 2 3 order of magnitude improvement vs. the best deterministic neural net bounds.

They show that you can apply PAC-Bayes style bounds to stochastic neural networks. However the analysis only applies to 2-layer feed-forward neural networks with a sigmoidal transfer function. In this case the complexity term only depends on the number of nodes and the variance of the weights. They show that for this setting the bound effectively predicts when over-training will occur. Unfortunately it doesn't really hit any of your ""preferred"" properties though!
","**Summary:**

John Langford and Rich Caruana introduce a new approach to estimating the true error rate of a continuous valued classifier using PAC-Bayes bounds. Their method involves:

* Determining the sensitivity of model parameters to noise.
* Constructing a distribution over classifiers based on this sensitivity analysis.
* Applying a PAC-Bayes bound to the stochastic classifier to obtain a tight bound on the true error rate.

This approach yields significant improvements in bound accuracy compared to deterministic bounds, as demonstrated on artificial neural networks with 2-layer feed-forward architecture and sigmoidal transfer function.

However, the analysis is limited to this specific neural network setting. The bound provides insights into overfitting but does not fulfill all desirable properties."
Why don&#39;t people use deeper RBFs or RBF in combination with MLP?,https://stats.stackexchange.com/questions/151701/why-dont-people-use-deeper-rbfs-or-rbf-in-combination-with-mlp,machine-learning|neural-networks|rbf-network,151701,True,151984,8388,1,16,1510595612,1431293640,5,1431447751,"The fundamental problem is that RBFs are 
a) too nonlinear, 
b) do not do dimension reduction.
because of a) RBFs were always trained by k-means rather than gradient descent.
I would claim that the main success in Deep NNs is conv nets, where one of the key parts is dimension reduction:  although working with say 128x128x3=50,000 inputs, each neuron has a restricted receptive field, and there are much fewer neurons in each layer.In a given layer in an MLP- each neuron represents a feature/dimension) so you are constantly reducing dimensionality (in going from layer to layer).
Although one could make the RBF covariance matrix adaptive and so  do dimension reduction, this makes it even harder to train. 
","Radial Basis Functions (RBFs) face two limitations: excessive nonlinearity and lack of dimension reduction. Nonlinearity hinders gradient descent training, leading to k-means usage. In Deep Neural Networks (DNNs), convolutional networks achieve success through dimension reduction, with neurons having limited receptive fields. In contrast, Multi-Layer Perceptrons (MLPs) gradually reduce dimensionality layer by layer. While adaptive RBF covariance matrices can enable dimension reduction, they further complicate training."
Does the function $e^x/(1+e^x)$ have a standard name?,https://stats.stackexchange.com/questions/371402/does-the-function-ex-1ex-have-a-standard-name,logistic|neural-networks|deep-learning|terminology,371402,True,371403,7534,1,15,1631712296,1539286196,30,1539286589,"It does not have a standard name. In different areas of statistics, it has different names.
In the neural networks and deep learning community, it is called the sigmoid function. This is confusing for everyone else, because sigmoid is just a fancy way of saying ""S-shaped"" and this function is not unique among S-shaped functions; for example, $\tanh$ is also S-shaped and widely used in neural networks, yet it is not commonly termed ""sigmoidal"" in neural network literature.
In the GLM literature, this is called the logistic function (as in logistic regression).
If the logit function is $$\text{logit}(p)=
\log\left(\frac{p}{1-p}\right)=
\log(p)-\log(1-p)=x$$ for $p\in(0,1)$, then $$\text{logit}^{-1}(x)=
\frac{\exp(x)}{1 + \exp(x)}=
\frac{1}{1+\exp(-x)}=
p$$
for $x\in\mathbb{R}$. This is the reason some people call $\text{logit}^{-1}$ the inverse logit or anti-logit function. (Thanks, Glen_b!)
Rarely, I've seen the name expit used; as far as I can tell, this is a back-formation from the word logit but never really caught on. (Thanks, CliffAB!)
","The function without a standard name is referred to differently in various fields:

* **Neural networks and deep learning:** Sigmoid function (although it is not the only S-shaped function used)
* **GLM literature:** Logistic function (used in logistic regression)

The inverse of the logit function, $\text{logit}^{-1}(x)$, is also known as:

* Inverse logit or anti-logit function
* Rarely, expit function (derived from ""logit"")

The main takeaway is that a specific S-shaped function has multiple names depending on the context. The inverse of the logit function is noteworthy and has its own set of names."
Can one (theoretically) train a neural network with fewer training samples than weights?,https://stats.stackexchange.com/questions/292278/can-one-theoretically-train-a-neural-network-with-fewer-training-samples-than,neural-networks|overfitting|underdetermined,292278,True,292284,4917,2,15,1500544561,1500456681,21,1500458215,"People do that all the time with large networks. For example, the famous AlexNet network has about 60 million parameters, while the ImageNet ILSVRC it was originally trained on has only 1.2 million images.
The reason you don't fit a 5-parameter polynomial to 4 data points is that it can always find a function that exactly fits your data points, but does nonsensical things elsewhere. Well, as was noted recently (https://arxiv.org/abs/1611.03530), AlexNet and similar networks can fit arbitrary random labels applied to ImageNet and simply memorize them all, presumably because they have so many more parameters than training points. But something about the priors of the network combined with the stochastic gradient descent optimization process means that, in practice, these models can still generalize to new data points well when you give them real labels. We still don't really understand why that happens.
","**Summary:**

Deep learning networks often have far more parameters than data points in the training set. Despite this, they can still generalize to new data with real labels.

This phenomenon occurs because deep networks can fit arbitrary random labels to training data, effectively memorizing the data points.

However, through a combination of network priors and stochastic gradient descent optimization, deep networks can generalize well despite overfitting to random labels. This behavior is still not fully understood.

In general, large networks with many more parameters than data points can still generalize well if trained with real labels, even though they can potentially overfit to random labels."
Pros and cons of weight normalization vs batch normalization,https://stats.stackexchange.com/questions/304755/pros-and-cons-of-weight-normalization-vs-batch-normalization,machine-learning|neural-networks|deep-learning|convolutional-neural-network|batch-normalization,304755,True,306211,16263,2,15,1539740544,1506283445,17,1507109108,"Batch Norm:

(+) Stable if the batch size is large
(+) Robust (in train) to the scale & shift of input data
(+) Robust to the scale of weight vector
(+) Scale of update decreases while training
(-) Not good for online learning
(-) Not good for RNN, LSTM
(-) Different calculation between train and test

Weight Norm:

(+) Smaller calculation cost on CNN
(+) Well-considered about weight initialization
(+) Implementation is easy
(+) Robust to the scale of weight vector
(-) Compared with the others, might be unstable on training
(-) High dependence to input data

Layer Norm:

(+) Effective to small mini batch RNN
(+) Robust to the scale of input
(+) Robust to the scale and shift of weight matrix
(+) Scale of update decreases while training
(-) Might be not good for CNN (Batch Norm is better in some cases)

","**Summary:**

Batch Norm and Layer Norm are normalization techniques that improve the stability and robustness of neural networks during training.

**Batch Norm:**
* Stable with large batch sizes
* Robust to input and weight scale
* Decreases update scale during training
* Not suitable for online learning, RNNs, or LSTMs
* Different calculations between train and test

**Weight Norm:**
* Lower computational cost for CNNs
* Well-defined weight initialization
* Easy implementation
* Robust to weight scale
* May be unstable during training
* Dependent on input data

**Layer Norm:**
* Effective for small mini-batch RNNs
* Robust to input and weight matrix scale
* Decreases update scale during training
* May not be optimal for CNNs (Batch Norm performs better in some cases)"
What do the terms &quot;dense&quot; and &quot;sparse&quot; mean in the context of neural networks?,https://stats.stackexchange.com/questions/266996/what-do-the-terms-dense-and-sparse-mean-in-the-context-of-neural-networks,neural-networks|terminology|definition,266996,True,267017,31090,2,15,1673571133,1489328954,24,1489336868,"In mathematics, ""sparse"" and ""dense"" often refer to the number of zero vs. non-zero elements in an array (e.g. vector or matrix). A sparse array is one that contains mostly zeros and few non-zero entries. A dense array contains mostly non-zeros. 
There's no hard threshold for what counts as sparse; it's a loose term, but can be made more specific. For example, a vector is $k$-sparse if it contains at most $k$ non-zero entries. Another way of saying this is that the vector's $\ell_0$ norm is $k$.
The usage of these terms in the context of neural networks is similar to their usage in other fields. In the context of NNs, things that may be described as sparse or dense include the activations (https://stats.stackexchange.com/q/307288/82135) of units within a particular layer, the weights, and the data. One could also talk about ""sparse connectivity"", which refers to the situation where only a small subset of units are connected to each other. This is a similar concept to sparse weights, because a connection with zero weight is effectively unconnected. 
""Sparse array"" can also refer to a class of data types that are efficient for representing arrays that are sparse. This is a concept within the domain of programming languages. It's related to, but distinct from the mathematical concept.
","**Summary:**

In mathematics, ""sparse"" arrays have mostly zeros, while ""dense"" arrays have mostly non-zeros. The term ""k-sparse"" is used to describe vectors with at most k non-zero elements.

In the context of neural networks, these terms refer to activations, weights, and data. Sparse activations indicate that only a few units in a layer are active. Sparse weights or connectivity mean that only a small subset of units are connected.

Programming languages use the term ""sparse array"" for data types that efficiently represent sparse mathematical arrays. This is related but distinct from the mathematical concept of sparsity."
"WaveNet is not really a dilated convolution, is it?",https://stats.stackexchange.com/questions/287774/wavenet-is-not-really-a-dilated-convolution-is-it,neural-networks|deep-learning|convolutional-neural-network|tensorflow,287774,True,288420,9593,2,15,1574282753,1498663915,9,1498996700,"The penny just dropped on this one for me. Of those 3 propositions the correct one is 4: I did not understand the WaveNet paper.
My problem was that I was interpreting the WaveNet diagram as covering a single sample, to be run on different samples arranged in a 2D structure with 1 dimension being the sample size and the other being the batch count.
However, WaveNet is just running that whole filter over a 1D time series with a stride of 1. This obviously has a much lower memory footprint but accomplishes the same thing.
If you tried to do the same trick using a strided structure, the output dimension would be wrong.
So to summarize, doing it the strided way with a 2D sample x batch structure gives the same model, but with a much higher memory usage.
","**Summary:**

The author initially misunderstood the WaveNet paper, interpreting its diagram as a 2D structure covering a single sample, with dimensions representing sample size and batch count.

However, WaveNet operates by applying its filters over a 1D time series with a stride of 1. This reduces memory usage compared to using a 2D strided structure with a similar sample x batch arrangement.

In essence, the 1D strided approach achieves the same modeling results with significantly lower memory footprint."
What is the essential difference between neural network and linear regression,https://stats.stackexchange.com/questions/259950/what-is-the-essential-difference-between-neural-network-and-linear-regression,regression|machine-learning|neural-networks,259950,True,259954,4246,1,15,1576421595,1486214295,20,1486216888,"No, a neural network is not several consecutive linear transformations. As you note, that would only result in another linear transformation in the end, so why do many instead of one? Actually, a neural network performs several (at least one, but possibly more, depending on the number of hidden layers) nonlinear (e.g. sigmoid) transformations. 
That is also the difference between a neural network and a linear regression, since the latter uses a linear combination of regressors to approximate the regressand.
","A neural network is distinct from sequential linear transformations because it employs non-linear transformations. While linear transformations merely chain together to produce another linear transformation, neural networks utilize non-linear functions such as sigmoid to achieve their functionality.

This fundamental difference separates neural networks from linear regression models. Unlike neural networks, which utilize non-linear transformations, linear regression relies on linear combinations of variables to approximate the desired output. This distinction highlights the unique capabilities of neural networks in modeling complex and non-linear relationships in data."
Convolution with a non-square kernel,https://stats.stackexchange.com/questions/351115/convolution-with-a-non-square-kernel,neural-networks|convolutional-neural-network|image-processing|computer-vision,351115,True,351126,14623,4,15,1607056680,1528876118,9,1528879231,"Actually the ideal shape would probably be a circle, but that's computationally inconvenient. The point being that you typically have no a priori assumptions of the shapes of the features that your convolutional net should learn. For instance, the lowest layers of a convolutional net trained on images often learn to detect edges. These edges can have any orientation, e.g. vertical, diagonal, horizontal, or something in between. If you examined the weights for a vertical edge detector, you might find that you could actually fit them inside a tall rectangle and crop out some irrelevant (near-zero) weights from the sides of the kernel. Similarly, the horizontal edge detector might fit inside a wide rectangle, not needing the top and bottom bits of the square. But you don't know beforehand which feature will be learnt by which map, so you can't specify these shapes in advance, nor would it probably confer much of an advantage to do so. A circular kernel fits any feature of a given size (e.g. any edge detector with a maximum dimension of 5 pixels can fit inside a circle with a diameter of 5), and the square is the closest approximation to that that is easy to work with computationally.
If you knew in advance that all your features would tend to (for example) be wider than they are tall, then perhaps it might be worth using a (non-square) rectangular kernel. 
","**Summary:**

The ideal shape for a convolutional filter is a circle, as it can detect features of any orientation. However, circles are computationally inefficient. Since the shape of features learned by convolutional nets is unknown beforehand, it is impractical to specify specific shapes for filters.

Instead, square filters are commonly used as the closest computationally convenient approximation to circles. This allows filters to detect features of various orientations, even though they may not perfectly match the filter's shape.

In some cases, if it is known that features tend to have a specific aspect ratio (e.g., wider than they are tall), rectangular filters can be used to improve efficiency. However, this is typically not necessary as square filters are generally effective for detecting a wide range of features."
Motivating sigmoid output units in neural networks starting with unnormalized log probabilities linear in $z=w^Th+b$ and $\phi(z)$,https://stats.stackexchange.com/questions/269575/motivating-sigmoid-output-units-in-neural-networks-starting-with-unnormalized-lo,neural-networks|deep-learning,269575,True,269640,4428,4,15,1642837779,1490361696,13,1490376437,"There are two possible outcomes for $y \in \{0, 1\}$. It's very important, because this property changes meaning of the multiplication. There are two possible cases:
\begin{align}
\log\tilde P(y=1) &= z \\
\log\tilde P(y=0) &= 0 \\
\end{align}
In addition important to notice that unnormalized logarithmic probability for $y=0$ is constant. This property derives from the main assumption. Applying any deterministic function to the constant value will produce constant output. This property will simplify final formula when we will do normalization over all possible probabilities, because we just need to know only unnormalized probability for $y=1$ and for $y=0$ it's always constant. And since output from the network in unnormalized logarithmic probability we will require only one output, because another one assumed to be constant.
Next, we are applying exponentiation to the unnormalized logarithm probability in order to obtain unnormalized probability.
\begin{align}
\tilde P(y=1) &= e ^ z \\
\tilde P(y=0) &= e ^ 0 = 1
\end{align}
Next we just normalize probabilities dividing each unnormalized probability by the sum of all possible unnormalized probabilities.
\begin{align}
P(y=1) = \frac{e ^ z}{1 + e ^ z} \\
P(y=0) = \frac{1}{1 + e ^ z}
\end{align}
We are interested only in $P(y=1)$, because that's what probability from the sigmoid function means. The obtained function doesn't look like sigmoid on the first look, but they are equal and it's easy to show.
\begin{align}
P(y=1) = \frac{e ^ x}{1 + e ^ x} = \frac{1}{\frac{e ^ x + 1}{e ^ x}} = \frac{1}{1 + \frac{1}{e ^ x}} = \frac{1}{1 + e ^ {-x}}
\end{align}
The last statement can be confusing at first, but it just a way to show that that final probability function is a sigmoid. The $(2yâ1)$ value converts $0$ to $-1$ and $1$ to $1$ (or we can say that it would be without change).
$$
P(y) = \sigma((2y - 1)z) = \begin{cases}
\sigma(z) = \frac{1}{1 + e ^ {-z}} = \frac{e ^ z}{1 + e ^ z} & \text{when } y = 1 \\
\sigma(-z) = \frac{1}{1 + e ^ {-(-z)}} = \frac{1}{1 + e ^ z} & \text{when } y = 0 \\
\end{cases}
$$
As we can see, it just the way to show the relation between $\sigma$ and $P(y)$
","For $y \in \{0, 1\}$, logarithmic probabilities are considered. The unnormalized probability for $y=0$ is constant, simplifying the calculations. Exponentiation is applied to obtain unnormalized probabilities, which are then normalized.

The final formula for $P(y=1)$ is derived, which is equivalent to the sigmoid function. This equivalence is demonstrated by converting $0$ to $-1$ and $1$ to $1$ using $(2yâ1)$. The result simplifies to the sigmoid function $\sigma(z) = \frac{1}{1 + e ^ {-z}}$, where $z$ is the input to the sigmoid function.

This derivation shows that the probability function $P(y)$ is essentially a sigmoid function with the form $\sigma((2y - 1)z)$."
Capturing initial patterns when using truncated backpropagation through time (RNN/LSTM),https://stats.stackexchange.com/questions/167482/capturing-initial-patterns-when-using-truncated-backpropagation-through-time-rn,neural-networks|deep-learning|natural-language|backpropagation,167482,True,167800,3812,2,15,1554448347,1439819844,14,1439951806,"It's true that limiting your gradient propagation to 30 time steps will prevent it from learning everything possible in your dataset. However, it depends strongly on your dataset whether that will prevent it from learning important things about the features in your model!
Limiting the gradient during training is more like limiting the window over which your model can assimilate input features and hidden state with high confidence. Because at test time you apply your model to the entire input sequence, it will still be able to incorporate information about all of the input features into its hidden state. It might not know exactly how to preserve that information until it makes its final prediction for the sentence, but there might be some (admittedly weaker) connections that it would still be able to make.
Think first about a contrived example. Suppose your network is to generate a 1 if there is a 1 anywhere in its input, and a 0 otherwise. Say you train the network on sequences of length 20 and limit then gradient to 10 steps. If the training dataset never contains a 1 in the final 10 steps of an input, then the network is going to have a problem with test inputs of any configuration. However, if the training set has some examples like [1 0 0 ... 0 0 0] and others like [0 0 0 ... 1 0 0], then the network will be able to pick up on the ""presence of a 1"" feature anywhere in its input.
Back to sentiment analysis then. Let's say during training your model encounters a long negative sentence like ""I hate this because ... around and around"" with, say, 50 words in the ellipsis. By limiting the gradient propagation to 30 time steps, the model will not connect the ""I hate this because"" to the output label, so it won't pick up on ""I"", ""hate"", or ""this"" from this training example. But it will pick up on the words that are within 30 time steps from the end of the sentence. If your training set contains other examples that contain those same words, possibly along with ""hate"" then it has a chance of picking up on the link between ""hate"" and the negative sentiment label. Also, if you have shorter training examples, say, ""We hate this because it's terrible!"" then your model will be able to connect the ""hate"" and ""this"" features to the target label. If you have enough of these training examples, then the model ought to be able to learn the connection effectively.
At test time, let's say you present the model with another long sentence like ""I hate this because ... on the gecko!"" The model's input will start out with ""I hate this"", which will be passed into the hidden state of the model in some form. This hidden state is used to influence future hidden states of the model, so even though there might be 50 words before the end of the sentence, the hidden state from those initial words has a theoretical chance of influencing the output, even though it was never trained on samples that contained such a large distance between the ""I hate this"" and the end of the sentence.
","Limiting gradient propagation during training restricts the model's ability to learn from features over a specific time window. However, it does not prevent the model from incorporating information from all input features at test time.

In some cases, limited gradient propagation may not significantly hinder learning important features. For instance, if a model is trained to detect a specific pattern in a sequence, it may be able to learn this pattern even if the gradient is limited to a smaller window, provided the pattern occurs within that window in some training examples.

In scenarios where the distance between relevant features and the output is large (e.g., long sentences with sentiment analysis), gradient limitation may prevent the model from directly associating these features with the output during training. However, the model may still have weaker connections to these features, which can be reinforced through training on shorter sequences or other examples that contain the same features closer to the output.

At test time, even though the model was not trained on sequences with large distances between crucial features and the output, the earlier features can still influence the hidden state and potentially contribute to the final prediction, albeit with reduced confidence."
Run-time analysis of common machine learning algorithms,https://stats.stackexchange.com/questions/19409/run-time-analysis-of-common-machine-learning-algorithms,machine-learning|neural-networks|references|algorithms,19409,True,19411,4449,2,15,1627105277,1323147244,11,1323155569,"Here are some superficial tables:

The Computational Mathematics of Statistical Data Mining. PPT (http://www.cercs.gatech.edu/iucrc08/material/agray.ppt)

Table 1 in:

Chu, C. T., Kim, S. K., Lin, Y. A., Yu, Y., Bradski,
G. R., Ng, A. Y., & Olukotun, K. (2006). Mapreduce for machine learning on multicore. Neural
Information Processing Systems (pp. 281â288). PDF (http://www.cs.stanford.edu/people/ang//papers/nips06-mapreducemulticore.pdf)



","**Summary:**

Tables 1 and 2 provide foundational information on statistical data mining.

**Table 1:**
This table is found in the PowerPoint presentation ""Computational Mathematics of Statistical Data Mining."" It offers insights into the mathematical techniques used in statistical data mining.

**Table 2:**
This table is part of a research paper titled ""MapReduce for Machine Learning on Multicore."" It presents the results of an empirical evaluation demonstrating the performance of multicore systems for machine learning tasks."
What is dense prediction in Deep learning?,https://stats.stackexchange.com/questions/281530/what-is-dense-prediction-in-deep-learning,neural-networks|convolutional-neural-network,281530,True,281628,14519,2,15,1607690335,1495659537,10,1495709934,"
In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image https://arxiv.org/abs/1611.09288 (https://arxiv.org/abs/1611.09288)

","Pixelwise dense prediction in computer vision involves assigning a label to every pixel in an image. This task is essential for many applications, such as semantic segmentation, where each pixel is labeled with the category of the object it represents (e.g., car, building, person).

In pixelwise dense prediction, a model is trained on a dataset of images and corresponding labels. The model learns to predict the label for each pixel in a new image, based on the features extracted from the image.

There are various approaches to pixelwise dense prediction, and the choice of approach depends on the specific application and the size and complexity of the dataset. However, a common approach is to use a convolutional neural network (CNN), which is a type of deep learning model that is well-suited for extracting features from images.

Pixelwise dense prediction is a challenging task, as it requires the model to make accurate predictions for each pixel in the image. However, recent advances in deep learning have led to significant improvements in the accuracy of pixelwise dense prediction models. As a result, pixelwise dense prediction is now widely used in a variety of computer vision applications."
Are Residual Networks related to Gradient Boosting?,https://stats.stackexchange.com/questions/214273/are-residual-networks-related-to-gradient-boosting,machine-learning|neural-networks|deep-learning|gradient-descent|residual-networks,214273,True,349987,2831,2,15,1577411351,1464069916,7,1528238860,"Potentially a newer paper which attempts to address more of it from Langford and Shapire team: Learning Deep ResNet Blocks Sequentially using Boosting Theory (https://arxiv.org/abs/1706.04964)
Parts of interest are (See section 3):

The key difference is that boosting is an ensemble of estimated
  hypothesis whereas ResNet is an ensemble of estimated feature
  representations $\sum_{t=0}^T f_t(g_t(x))$. To solve this problem, we
  introduce an auxiliary linear classifier $\mathbf{w}_t$ on top of each
  residual block to construct a hypothesis module. Formally a
  hypothesis module is defined as $$o_t(x) := \mathbf{w}_t^T g_t(x) \in \mathbb{R}$$
...
(where) $o_t(x) = \sum_{{t'} = 0}^{t-1} \mathbf{w}_t^T f_{t'}(g_{t'}(x))$

The paper goes into much more detail around the construction of the weak module classifier $h_t(x)$ and how that integrates with their BoostResNet algorithm.

Adding a bit more detail to this answer, all boosting algorithms can be written in some form of [1](p 5, 180, 185...):
$$F_T(x) := \sum_{t=0}^T \alpha_t h_t(x)$$
Where $h_t$ is the $t^{th}$ weak hypothesis, for some choice of $\alpha_t$. Note that different boosting algorithms will yield $\alpha_t$ and $h_t$ in different ways. 
For example AdaBoost [1](p 5.) uses $h_t$ to minimize the weighted error $\epsilon_t$ with $\alpha_t = \frac{1}{2} \log \frac{1- \epsilon_t}{\epsilon_t}$
On the other hand, in gradient boosting setting [1](p 190.), $h_t$ is selected that maximizes $\nabla\mathcal{L}(F_{t-1}(x)) \cdot h_t$, and $\alpha_t > 0$ is chosen (as learning rate etc)
Where as in [2] under Lemma 3.2, it is shown that the output of depth-$T$ ResNet is $F(x)$ which is equivalent to
$$F(x) \propto \sum_{t=0}^T h_t(x)$$
this completes the relationship between boosting and resnet. The paper [2] proposes adding auxiliary linear layer to get it into the form $F_T(x) := \sum_{t=0}^T \alpha_t h_t(x)$, which leads to their BoostResNet algorithm and some discussion around that
[1] Robert E. Schapire and Yoav Freund. 2012. Boosting: Foundations and Algorithms. The MIT Press. p 5, 180, 189
[2] Furong Huang, Jordan Ash, John Langford, Robert Schapire: Learning Deep ResNet Blocks Sequentially using Boosting Theory, ICML 2018
","The paper ""Learning Deep ResNet Blocks Sequentially using Boosting Theory"" addresses the challenge of understanding the connection between residual networks (ResNets) and boosting algorithms.

The core idea is to view each residual block in ResNet as a weak hypothesis in boosting. To make this connection, the paper introduces auxiliary linear classifiers on top of each residual block, creating hypothesis modules. These modules enable the construction of a final hypothesis using a weighted sum, similar to boosting algorithms.

The paper shows that the output of a ResNet is equivalent to a weighted sum of the weak hypotheses generated by the residual blocks. This implies that ResNets can be interpreted as ensembles of feature representations, akin to boosting ensembles of hypotheses.

The paper proposes the BoostResNet algorithm, which utilizes this connection to train ResNets more effectively. The algorithm incorporates techniques from boosting, such as weak hypothesis selection and weighting, into the ResNet training process.

Overall, the paper bridges the gap between ResNets and boosting, providing insights into their similarities and offering a new perspective on ResNet training."
Identifiability of neural network models,https://stats.stackexchange.com/questions/316523/identifiability-of-neural-network-models,neural-networks|convolutional-neural-network|recurrent-neural-network|identifiability,316523,True,354442,2231,2,15,1685475495,1512068364,4,1530712048,"Linear, single-layer FFNs are non-identified (https://en.wikipedia.org/wiki/Identifiability)
The question as since been edited to exclude this case; I retain it here because understanding the linear case is a simple example of the phenomenon of interest.
Consider a feedforward neural network with 1 hidden layer and all linear activations. The task is a simple OLS regression task.
So we have the model $\hat{y}=X A B$ and the objective is
$$
\min_{A,B} \frac{1}{2}|| y - X A B ||_2^2
$$
for some choice of $A, B$ of appropriate shape. $A$ is the input-to-hidden weights, and $B$ is the hidden-to-output weights.
Clearly the elements of the weight matrices are not identifiable in general, since there are any number of possible configurations for which two pairs of matrices $A,B$ have the same product.
Nonlinear, single-layer FFNs are still non-identified
Building up from the linear, single-layer FFN, we can also observe non-identifiability in the nonlinear, single-layer FFN.
As an example, adding a $\tanh$ nonlinearity to any of the linear activations creates a nonlinear network. This network is still non-identified, because for any loss value, a permutation of the weights of two (or more) neurons at one layer, and their corresponding neurons at the next layer, will likewise result in the same loss value.
In general, neural networks are non-identified
We can use the same reasoning to show that neural networks are non-identified in all but very particular parameterizations.
For example, there is no particular reason that convolutional filters must occur in any particular order. Nor is it required that convolutional filters have any particular sign, since subsequent weights could have the opposite sign to ""reverse"" that choice.
Likewise, the units in an RNN can be permuted to obtain the same loss.
See also: Can we use MLE to estimate Neural Network weights? (https://stats.stackexchange.com/questions/145902/can-we-use-mle-to-estimate-neural-network-weights/145907#145907)
","**Summary**

Neural networks are typically non-identifiable, meaning there are multiple sets of parameters that produce identical outputs. This is because the ordering and signs of weights and units can be permuted without affecting the network's performance.

**Linear, Single-Layer Networks:**

* Linear networks with a single hidden layer are non-identifiable.
* The input-to-hidden and hidden-to-output weight matrices can be permuted or multiplied by scalars without changing the model's output.

**Nonlinear, Single-Layer Networks:**

* Nonlinear networks with a single hidden layer are also non-identifiable.
* Applying a nonlinear activation, such as tanh, does not resolve the non-identifiability issue.
* Permuting the weights of corresponding neurons in different layers maintains the network's loss value.

**Generalization to Multilayer Networks:**

* The concept of non-identifiability applies to all neural networks, regardless of their number of layers.
* The ordering of convolutional filters and the signs of weights can be arbitrarily changed without affecting network performance.
* Similarly, the units in recurrent neural networks can be permuted without altering the loss."
Why are derived features used in neural networks?,https://stats.stackexchange.com/questions/24825/why-are-derived-features-used-in-neural-networks,machine-learning|neural-networks,24825,True,24831,4441,2,15,1467591361,1332039667,10,1332042610,"1): Including derived features is a way to inject expert knowledge into the training process, and so to accelerate it. For example, I work with physicists a lot in my research. When I'm building an optimization model, they'll give me 3 or 4 parameters, but they usually also know certain forms that are supposed to appear in the equation. For example, I might get variables $n$ and $l$, but the expert knows that $n*l$ is important. By including it as a feature, I save the model the extra effort of finding out that $n*l$ is important. Granted, sometimes domain experts are wrong, but in my experience, they usually know what they're talking about.
2): There are two reasons I know of for this. First, if you have thousands of features supplied (as often happens in real world data), and are short on CPU time for training (also a common occurrence), you can use a number of different feature selection algorithms to pare down the feature space in advance. The principled approaches to this often use information-theoretic measures to select the features with the highest predictive power. Second, even if you can afford to train on all the data and all the features you have, neural networks are often criticized for being 'black box' models. Reducing the feature space in advance can help to mitigate this issue. For example, a user looking at the NN cannot easily tell whether a weight of 0.01 means ""0, but the optimization process didn't quite get there"" or ""This feature is important, but has to be reduced in value prior to use"". Using feature selection in advance to remove useless features makes this less of an issue.
","**Summary:**

Including derived features, such as $n*l$ in the example, in machine learning models can accelerate training by incorporating expert knowledge into the process. Domain experts often possess valuable insights about the relationships between variables, which can be captured through derived features.

Feature selection algorithms can be used to reduce the number of features in a dataset, which is beneficial for several reasons. Firstly, it speeds up training when time and computational resources are limited. Secondly, it enhances the interpretability of neural networks, which can be criticized for their opaque nature. By removing irrelevant features, it becomes easier to understand the role of each feature in the model."
Encoding Date/Time (cyclic data) for Neural Networks,https://stats.stackexchange.com/questions/193034/encoding-date-time-cyclic-data-for-neural-networks,neural-networks|deep-learning,193034,True,263809,16383,3,15,1499878492,1454050620,8,1487939094,"I was looking for an answer to a similar problem and stumbled on this thread. The sinusoidal encoding idea is explored in this blog post:
Encoding cyclical continuous features - 24-hour time (https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/)
Ian's answer fully addressed my needs, so I thought about posting it here for future memory.
","**Summary:**

The paragraph introduces a blog post by Ian London (https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/) discussing sinusoidal encoding for cyclical continuous features, specifically 24-hour time. Sinusoidal encoding is a technique for representing cyclical data in a way that captures its periodic nature. The blog post provides a solution for a similar problem the reader was facing, leading to the decision to share Ian's answer for future reference."
"Can neural network (e.g., convolutional neural network) have negative weights?",https://stats.stackexchange.com/questions/185071/can-neural-network-e-g-convolutional-neural-network-have-negative-weights,machine-learning|neural-networks|deep-learning|convolutional-neural-network,185071,True,185982,16183,3,15,1549315261,1449256148,12,1449700500,"Rectified Linear Units (ReLUs) only make the output of the neurons to be non-negative. The parameters of the network, however, can, and will, become positive or negative depending on the training data. 
Here are two reasons I can think of right now that justifies (intuitively) why some parameters would become negative:

the regularization of the parameters (a.k.a. the weight decay); the variation in the parameter values makes prediction possible, and if the parameters are centered around zero (i.e. their mean is close to zero), then their $\ell 2$ norm (which is a standard regularizer) is low.
although the gradients of the output of a layer with respect to the layer parameters depend on the input to the layer (which are always positive assuming that the previous layer passes its outputs through a ReLU), however, the gradient of the error (which comes from the layers closer to the final output layers) may be positive or negative, making it possible for SGD to make some of the parameter values negative after taking the next gradient step. More specifically, let $I$, $O$, and $w$ denote the input, output, and parameters of a layer in a neural network. Also, let $E$ be the final error of the network induced by some training sample. The gradient of the error with respect to $w$ is computed as $\frac{\partial E}{\partial w} = \left( \sum_{k=1}^K\frac{\partial E}{\partial O_k} \right) \cdot \frac{\partial O_k}{\partial w}$; note that $O_k = O, \forall k$ (see picture below):

 (https://i.sstatic.net/82ktn.png)
","**Summary:**

Rectified Linear Units (ReLUs) ensure non-negative neuron outputs, but parameters in the network remain positive or negative. Two reasons for negative parameters include:

* **Regularization (Weight Decay):** Maintaining a low $\ell 2$ norm (regularization) favors parameter values centered around zero, potentially resulting in negative values.
* **Gradient Propagation:** Gradients of the error function, originating from higher layers, can be positive or negative, allowing the stochastic gradient descent algorithm to adjust parameters in either direction, leading to negative values.

In other words, negative parameters arise due to regularization strategies and the propagation of gradients from different layers in the neural network. These negative values contribute to the overall predictive capabilities of the model."
L2 Regularization Constant,https://stats.stackexchange.com/questions/316961/l2-regularization-constant,neural-networks|regularization,316961,True,316966,21517,2,15,1548708208,1512339329,8,1512346029,"You are absolutely right in your observation that the number of parameters will affect the regularization cost. 
I don't think there are any rule-of-thumb values for $\lambda$ (but $\lambda=1$ would be considered large). If cross-validation is too time-consuming, you could hold-out a part of the training data and tune $\lambda$ using early stopping (https://en.wikipedia.org/wiki/Early_stopping). You would still need to try several values for $\lambda$ common practice is to try something like $0.01, 0.02,\ldots,0.4$.
For really large networks, it might be more convenient to use other regularization methods, like dropout (http://jmlr.org/papers/v15/srivastava14a.html), instead of $\ell_2$ regularization.
","**Summary:**

The number of parameters in a neural network model affects the regularization cost. A larger number of parameters leads to a higher regularization cost.

To determine the optimal value for the regularization parameter $\lambda$, one can use cross-validation or hold-out validation. If cross-validation is not feasible, early stopping can be used. Common practice involves trying a range of $\lambda$ values such as 0.01, 0.02, ..., 0.4.

For very large networks, alternative regularization methods like dropout may be more suitable than $\ell_2$ regularization."
Why do CNNs conclude with FC layers?,https://stats.stackexchange.com/questions/367028/why-do-cnns-conclude-with-fc-layers,neural-networks|svm|random-forest|convolutional-neural-network,367028,True,367029,1275,2,15,1582906426,1536967803,5,1536968396,"It's not so simple. First of all, a SVM is, in a way, a type of neural network (you can learn a SVM solution through backpropagation). See What *is* an Artificial Neural Network? (https://stats.stackexchange.com/questions/362425/what-is-an-artificial-neural-network). Second, you can't know beforehand which model will work better, but the thing is with a fully neuromorphic architecture you can learn the weights end-to-end, while attaching a SVM or RF to the last hidden layer activation of a CNN is simply an ad hoc procedure. It may perform better, and it may not, we can't know without testing.
The important part is that a fully convolutional architecture is capable of representation learning, which is useful for a myriad of reasons. For once, it may reduce or eliminate feature engineering altogether in your problem.
About the FC layers, they are mathematically equivalent to 1x1 Convolutional layers. See Yann Lecun's post (https://www.facebook.com/yann.lecun/posts/10152820758292143), which I transcript below:

In Convolutional Nets, there is no such thing as ""fully-connected
  layers"". There are only convolution layers with 1x1 convolution
  kernels and a full connection table.
It's a too-rarely-understood fact that ConvNets don't need to have a
  fixed-size input. You can train them on inputs that happen to produce
  a single output vector (with no spatial extent), and then apply them
  to larger images. Instead of a single output vector, you then get a
  spatial map of output vectors. Each vector sees input windows at
  different locations on the input.
In that scenario, the ""fully connected layers"" really act as 1x1
  convolutions.

","**Summary:**

* Support vector machines (SVMs) and fully neuromorphic architectures are similar in that they involve training weights end-to-end. However, SVMs can be attached to the last hidden layer of a convolutional neural network (CNN) as an ad hoc procedure.

* Fully convolutional architectures enable representation learning, potentially eliminating feature engineering.

* Fully connected (FC) layers in CNNs are mathematically equivalent to 1x1 convolutional layers. This means that CNNs can be trained on variable-sized inputs, producing spatial maps of output vectors."
How to calculate optimal zero padding for convolutional neural networks?,https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks,machine-learning|neural-networks|deep-learning|convolutional-neural-network|computer-vision,297678,True,297701,43268,7,14,1646813516,1502603317,14,1502625901,"The possible values for the padding size, $P$, depends the input size $W$ (following the notation of the blog), the filter size $F$ and the stride $S$. We assume width and height are the same.
What you need to ensure is that the output size, $(W-F+2P)/S+1$, is an integer. When $S = 1$ then you get your first equation $P = (F-1)/2$ as necessary condition. But, in general, you need to consider the three parameters, namely $W$, $F$ and $S$ in order to determine valid values of $P$.
","**Summary:**

The padding size, $P$, in a convolution operation affects the output size of the operation. To ensure a valid output size, it must satisfy the following equation:

```
(W - F + 2P) / S + 1 = Integer
```

where:

* $W$ is the input size
* $F$ is the filter size
* $S$ is the stride

For a stride of $1$, the necessary condition for a valid padding size is:

```
P = (F - 1) / 2
```

However, for strides greater than 1, the padding size must be determined considering all three parameters, $W$, $F$, and $S$. The equation above ensures that the output size is an integer, which is required for a valid convolution operation."
Can someone please explain the back-propagation algorithm?,https://stats.stackexchange.com/questions/58/can-someone-please-explain-the-back-propagation-algorithm,algorithms|optimization|neural-networks,58,True,6988,2875,3,14,1365595771,1279568577,15,1297169277,"The back propagation algorithm is a gradient descent (http://en.wikipedia.org/wiki/Gradient_descent) algorithm for fitting a neural network model. (as mentionned by @Dikran) Let me explain how. 
Formally: Using the calculation of the gradient at the end of this post within equation [1] below (that is a definition of the gradient descent) gives the back propagation algorithm as a particular case of the use of a gradient descent.  
A neural network model
Formally, we fix ideas with a simple single layer model:  
$$ f(x)=g(A^1(s(A^2(x)))) $$
where $g:\mathbb{R} \rightarrow \mathbb{R}$ and $s:\mathbb{R}^M\rightarrow \mathbb{R}^M$ are known with for all $m=1\dots,M$, $s(x)[m]=\sigma(x[m])$,  and $A^1:\mathbb{R}^M\rightarrow \mathbb{R}$, $A^2\mathbb{R}^p\rightarrow \mathbb{R}^M$ are unknown affine functions. The function $\sigma:\mathbb{R}\rightarrow \mathbb{R}$ is called activation function in the framework of classification. 
A quadratic Loss function is taken to fix ideas. 
Hence the input $(x_1,\dots,x_n)$ vectors of $\mathbb{R}^p$ can be fitted to the real output  $(y_1,\dots,y_n)$ of $\mathbb{R}$ (could be vectors) by minimizing the empirical loss: 
 $$\mathcal{R}_n(A^1,A^2)=\sum_{i=1}^n (y_i-f(x_i))^2\;\;\;\;\;\;\; [1]$$
with respect to the choice of $A^1$ and $A^2$. 
Gradient descent (http://en.wikipedia.org/wiki/Gradient_descent)
A grandient descent for minimizing $\mathcal{R}$ is an algorithm that iterate:
    $$\mathbf{a}_{l+1}=\mathbf{a}_l-\gamma_l \nabla \mathcal{R}(\mathbf{a}_l),\ l \ge 0.$$
for well chosen step sizes $(\gamma_l)_l$ (also called learning rate in the framework of back propagation). It requires the calculation of the gradient of $\mathcal{R}$. In the considered case $\mathbf{a}_l=(A^1_{l},A^2_{l})$.
Gradient of $\mathcal{R}$ (for the simple considered neural net model) 
Let us denote,  by $\nabla_1 \mathcal{R}$ the gradient of $\mathcal{R}$ as a function of $A^1$, and $\nabla_2\mathcal{R}$ the gradient of $\mathcal{R}$ as a function of $A^2$. Standard calculation (using the rule for derivation of composition of functions) and the use of the notation $z_i=A^1(s(A^2(x_i)))$ give
$$\nabla_1 \mathcal{R}[1:M] =-2\times \sum_{i=1}^n z_i g'(z_i) (y_i-f(x_i))$$
for all $m=1,\dots,M$
 $$\nabla_2 \mathcal{R}[1:p,m] =-2\times \sum_{i=1}^n x_i g'(z_i) z_i[m]\sigma'(A^2(x_i)[m]) (y_i-f(x_i))$$
Here I used the R notation: $x[a:b]$ is the vector composed of the coordinates of $x$ from index $a$ to index $b$.
","**Main Idea:**
The back propagation algorithm is a technique for optimizing the parameters of a neural network model.

**How it Works:**
The algorithm iteratively updates the network's parameter values to minimize a predefined error function, typically the quadratic loss. It calculates the gradient of the error function with respect to the parameters and uses the gradient descent method to move in the direction that reduces the error.

**Mathematical Details:**
For a simple single-layer neural network model, the error function can be expressed as a sum of squared deviations between predicted and actual outputs. The gradient of this error function is calculated using the chain rule and activation function derivatives.

**Applications:**
Back propagation is widely used in training neural networks for various tasks, such as:

* Image and speech recognition
* Natural language processing
* Machine learning algorithms

**Key Components:**

* **Learning Rate:** Controls the step size in the gradient descent updates.
* **Activation Function:** Non-linear function applied to intermediate layer values, introducing non-linearity.
* **Gradient Descent:** Iterative method for parameter optimization based on gradient calculations.

**Importance:**
Back propagation enables the optimization of complex neural network models, allowing them to learn from data and perform sophisticated tasks."
Why don&#39;t we just learn the hyper parameters?,https://stats.stackexchange.com/questions/365762/why-dont-we-just-learn-the-hyper-parameters,machine-learning|neural-networks|hyperparameter,365762,True,459009,1833,4,14,1586274818,1536305035,2,1586268004,"Lets see how the first order condition would look like if we plug the hyperparameter $\alpha$ and try to learn it the same way as $\theta$ from data:
$$\frac \partial{\partial\alpha} J''(\theta) = \frac \partial{\partial\alpha}\alpha J(\theta) + \frac \partial{\partial\alpha}(1 â \alpha)J'(\theta)\\
 = J(\theta) â J'(\theta) = 0$$
Hence,
$$J(\theta) = J'(\theta)$$
When this hyperparameter is optimiized, then it will cause both J and J' become the same function, i.e. equal weights. You'll end up with a trivial solution.
If you want a more generic philosophizing then consider this: hyperparameters are usually not tangled with data. What do I mean? In a neural network or even a simple regression your model parameters will be in some ways interacting directly with data: 
$$y_L=X_L\beta_L$$
$$a_L=\sigma(y_L)$$
$$X_{L+1}=a_L$$
and so on down the layers. You see how $\beta_L$ get tangled in your data. So, when you take a derivative over any $\beta$ of the objective function you get data points entering the result in non obvious ways in matrix, Hessians, cross products etc. 
However, if you try to estimate the first order conditions over the hyperparameters, you don'y get this effect. The derivatives of hyperparameters often operate the entire chunks of your model, without shuffling its parts like derivatives over parameters. That's why optimizing hyperparameters often leads to trivial solutions like the one I gave you for the specific paper. Optimizing hyperparameters doesn't distress your data set and make it uncomfortable enough to produce something interesting.
","When learning hyperparameters like $\alpha$ along with model parameters like $\theta$, the first order condition for $\alpha$ leads to an uninformative solution: $J(\theta) = J'(\theta)$.

Hyperparameters typically don't interact directly with data, unlike model parameters. Consequently, optimizing hyperparameters through first order conditions often results in trivial solutions. This is because the derivatives of hyperparameters affect the entire model without involving specific data points, unlike the derivatives of model parameters. Thus, optimizing hyperparameters doesn't ""distress"" the data enough to produce meaningful results."
How to perform Neural Network modelling effectively?,https://stats.stackexchange.com/questions/2328/how-to-perform-neural-network-modelling-effectively,neural-networks,2328,True,2332,3204,1,14,1386116639,1283489617,26,1283500995,"The advice I would give is as follows:

Exhaust the possibilities of linear models (e.g. logistic regression) before going on to neural nets, especially if you have many features and not too many observations.  For many problems a Neural Net does not out-perform simple linear classifiers, and the only way to find out if your problem is in this category is to try it and see.
Investigate kernel methods (e.g. Support Vector Machines (SVM), kernel logistic regression), Gaussian process models first.  In both cases over-fitting is effectively controlled by tuning a small number of hyper-parameters.  For kernel methods this is often performed by cross-validation, for Gaussian process models this is performed by maximising the marginal likelihood (also known as the Bayesian ""evidence"" for the model).  I have found it is much easier to get a reasonable model using these methods than with neural networks, as the means of avoiding over-fitting is so much more straightforward.
If you really want to use a neural network, start with a (regularised) radial basis function network, rather than a feedforward Multilayer Perceptron (MLP) type network.
If yo do use an MLP, then use regularisation.  If you do, it will be less sensitive to choices about architecture, such as optimising the number of hidden units.  Instead, all you have to do is choose a good value for the regularisation parameter.  MacKay's Bayesian ""evidence framework"" provides a good method for setting the regularisation parameter.  If you use regularisation, then the number of observations and number of variables becomes much less of an issue.

To detect over-fitting, simply perform cross-validation to test generalisation performance.
As for classes having equal frequencies, the thing to remember is that if you train a model with a balanced training set, but the classes are not balanced in the operational data, then the model is very likely to under-predict the minority class.  If you use a probabilistic classifier such as logistic regression or a neural network, you can always correct the estimated probabilities to account for that after training.  If your dataset is very imbalanced, I would recommend differential weighting of patterns from the positive and negative classes, with the weighting factors selected by cross-validation.
However, when the classes are very unbalanced, it is normally the case that false-negative and false-positive errors have difference costs (e.g. in medical screening tests a false-negative is much worse than a false-positive).  So often all you need to do is include the misclassification costs into the error function used to train the network.
If you are a MATLAB user (like me) I can strongly recommend the NETLAB software (Ian Nabney and Chris Bishop) or the software that goes with the book Gaussian Process for Machine Learning by Rasmussen and Williams.  I can else strongly recommend the book ""Neural networks for pattern recognition"" by Chris Bishop for anyone starting out in neural nets.  It is a brilliant book, and covers the material with great clarity and the minimum level of maths required to really understand what you are doing, and most of it is implemented in the NETLAB software (which may also run under Octave).
HTH
P.S. The best way of modelling with a neural net is probably to use a Bayesian approach based on Hybrid Monte Carlo (HMC), as developed by Radford Neal.  In general problems start in modelling when you try and optimise some parameters and you end up over-fitting.  The best solution is to never optimise anything and marginalise (integrate) over parameters instead.  Sadly this integration can't be performed analytically, so you need to use sampling based approaches instead.  However, this is (a) computationally expensive and (b) a bit of a ""black art"" and required deep understanding and experience.
","**Summary:**

**Choosing Machine Learning Algorithms:**

* Explore linear models (e.g., logistic regression) before neural networks, especially for problems with limited data and many features.
* Consider kernel methods (e.g., SVMs) or Gaussian process models, as they provide effective overfitting control.

**Neural Networks:**

* Use regularized radial basis function networks over feedforward MLPs.
* Implement regularization with MLPs to reduce sensitivity to architecture choices.
* Set regularization parameters using MacKay's Bayesian evidence framework.

**Overfitting Detection and Mitigation:**

* Use cross-validation to gauge model performance and detect overfitting.
* For imbalanced datasets, weight training patterns according to class frequency or incorporate misclassification costs into the error function.

**Recommended Resources:**

* NETLAB software or ""Gaussian Processes for Machine Learning"" software for software implementation.
* ""Neural Networks for Pattern Recognition"" by Bishop for a comprehensive understanding of neural networks.

**Advanced Techniques:**

* Consider Bayesian modeling with Hybrid Monte Carlo to avoid parameter optimization and overfitting.
* Note the computational expense and expertise required for Bayesian modeling."
Does correlated input data lead to overfitting with neural networks?,https://stats.stackexchange.com/questions/232534/does-correlated-input-data-lead-to-overfitting-with-neural-networks,correlation|neural-networks|overfitting,232534,True,236119,15984,3,14,1671274600,1472581422,13,1474454561,"Actually no.
The question as such is a bit general, and mixes two things that are not really related. Overfitting usually is meant as the opposing quality to being a generalized description; in the sense that an overfitted (or overtrained) network will have less generalization power. This quality is primarily determined by the network architecture, the training and the validation procedure. The data and its properties only enter as ""something that the training procedure happens on"". This is more or less ""text book knowledge""; you could try ""An Introduction to Statistical Learning"" by James, Witten, Hastie and Tibshirani. Or ""Pattern Recognition"" by Bishop (my favourite book ever on the general topic). Or ""Pattern Recognition and Machine Learning"", also by Bishop.
For the correlation itself: Consider the input space having a certain dimension. No matter what transformation you use, the dimensionality will remain the same -- linear algebra says so. In one case the given base will be completely uncorrelated -- this is what you get, when you de-correlate the variables, or simply apply PAT (Principle Axis Transformation.) Take any linear algebra book for this.
Since a neural network with an appropriate architecture can model any (!) function, you can safely assume, that it also could first model the PAT and then do whatever it also should do -- e.g. classification, regression, etc.
You could also consider the correlation a feature, which should be part of the neural network description, since it's a property of the data. The nature of the correlation is not really important, unless it is something that should not be a part of the data. This would actually be a different topic -- you should model or quantify something like noise in the input and account for it.
So, in summary no. Correlated data means you should work harder to make the handling of data technically simpler and more effective. Overfitting can occur, but in won't happen because there is correlated data. 
","**Summary:**

Correlated data does not directly cause overfitting in neural networks. Overfitting refers to a network's poor generalization ability and is primarily influenced by factors like network architecture, training, and validation procedures. Correlated data merely complicates data handling, but a suitable network architecture can model the correlations and perform the desired tasks (e.g., classification, regression).

**Additional Considerations:**

* Correlation in data is a feature that can be incorporated into the network's description.
* If the correlation arises from noise or irrelevant information, it should be accounted for separately.
* While correlated data does not inherently lead to overfitting, it may necessitate more robust data handling techniques to enhance network performance."
Is Cross entropy cost function for neural network convex?,https://stats.stackexchange.com/questions/144378/is-cross-entropy-cost-function-for-neural-network-convex,neural-networks|convex,144378,True,193426,18006,3,14,1648891477,1427898775,11,1454310689,"The cross entropy of an exponential family is always convex.  So, for a multilayer neural network having inputs $x$, weights $w$, and output $y$, and loss function $L$
$$\nabla^2_y L$$
is convex.  However,
$$\nabla^2_w L$$
is not going to be convex for the parameters of the middle layer for the reasons described by iamonaboat.
","**Summary:**

The cross entropy of an exponential family is always convex, indicating that the loss function with respect to the output of a multilayer neural network is convex. This implies that the loss function can be minimized efficiently using gradient descent.

However, the loss function with respect to the weights of the middle layer is not necessarily convex. This lack of convexity arises due to the non-linear transformations applied in the middle layer. Consequently, training neural networks with multiple hidden layers can be more challenging."
What is Bayesian Deep Learning?,https://stats.stackexchange.com/questions/318786/what-is-bayesian-deep-learning,bayesian|deep-learning,318786,True,321629,866,2,14,1522898972,1513235399,10,1515101192,"Going off of your NIPS workshop link, Yee Whye Teh had a keynote speech at NIPS on Bayesian Deep Learning (video: https://www.youtube.com/watch?v=LVBvJsTr3rg (https://www.youtube.com/watch?v=LVBvJsTr3rg), slides: http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/ (http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/)). I think at some point in the talk, Teh summarized Bayesian deep learning as applying the Bayesian framework to ideas from deep learning (like learning a posterior over the weights of a neural network), and deep Bayesian learning as applying ideas from deep learning to the Bayesian framework (like deep Gaussian processes or deep exponential families). There are of course ideas that straddle the line between the two concepts, like variational autoencoders. When most people say Bayesian deep learning, they usually mean either of the two, and that's reflected in the accepted papers at the workshop you linked (along with the workshop the previous year). While the ideas go back to Neal's work on Bayesian learning of neural networks in the 90's (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf)), and there's been work over the years since then, probably one of the more important recent papers would be the original variational autoencoder paper (https://arxiv.org/pdf/1312.6114.pdf (https://arxiv.org/pdf/1312.6114.pdf)).
","**Summary:**

Bayesian deep learning and deep Bayesian learning are two complementary approaches that combine Bayesian statistics and deep learning.

* **Bayesian deep learning:** Applies the Bayesian framework to deep learning, such as using Bayesian inference to estimate the posterior distribution of neural network weights.
* **Deep Bayesian learning:** Applies deep learning techniques to Bayesian modeling, such as creating deep Gaussian processes or deep exponential families.

Both approaches aim to improve the performance and interpretability of deep learning models. They have roots in earlier work on Bayesian learning of neural networks and the variational autoencoder paper."
Choice of neural net hidden activation function,https://stats.stackexchange.com/questions/35776/choice-of-neural-net-hidden-activation-function,machine-learning|classification|neural-networks,35776,True,35783,15675,3,14,1516111216,1346929512,12,1346934731,"LeCun discusses this in Efficient Backprop (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) Section 4.4. The motivation is similar to the motivation for normalizing the input to zero mean (Section 4.3). The average outputs of the tanh activation function are more likely to be close to zero than the sigmoid, whose average output must be positive.
","Yann LeCun, in his paper ""Efficient Backprop,"" discusses normalizing the input to zero mean to improve the efficiency of the backpropagation algorithm. A similar approach can be applied to the tanh activation function.

The sigmoid activation function typically produces positive average outputs, while the tanh activation function's average outputs tend to be closer to zero. This difference in average output makes the tanh activation function more suitable for normalization to zero mean. By normalizing the input to zero mean, the backpropagation algorithm can achieve more efficient learning."
Difference between Conv and FC layers?,https://stats.stackexchange.com/questions/312838/difference-between-conv-and-fc-layers,neural-networks|convolutional-neural-network|convolution,312838,True,313847,30032,3,14,1618256819,1510234121,18,1510739665,"A convolutional layer applies the same (usually small) filter repeatedly at different positions in the layer below it. E.g. if the input layer has dimensions 512 x 512, you could have a conv layer that applies the same 8 x 8 filter (specified by 64 filter coefficients), at each point in (e.g.) a 128 x 128 grid overlaid on the input layer. On the other hand, each node in a fully connected layer would learn 512 x 512 weights, one for each of the nodes in the input layer. 
Conv layers therefore are well suited to detect local features that may appear anywhere in the input (e.g. edges in a visual image). The idea is that you don't have to train every node separately to detect the same feature, but rather you learn one filter that is shared among all the nodes.  
(Note that each conv layer usually learns a set of several filters, each of which gets applied repeatedly across the input. E.g. if the conv layer learns 16 different features, it is said to have a 'depth' of 16.)
FC layers are used to detect specific global configurations of the features detected by the lower layers in the net. They usually sit at the top of the network hierarchy, at a point when the input has been reduced (by the previous, usually convolutional layers) to a compact representation of features. Each node in the FC layer learns its own set of weights on all of the nodes in the layer below it. 
So you can (roughly) think of conv layers as breaking the input (e.g. an image) up into common features, and the FC layers as piecing those features together into e.g. objects that you want the network to recognize.
","**Summary:**

Convolutional layers (Conv layers) apply small filters to local regions of an input layer, detecting specific features regardless of their location. Each filter is shared among multiple nodes, allowing for efficient feature extraction. Conversely, fully connected layers (FC layers) have nodes that learn weights for each node in the previous layer.

Conv layers excel at identifying local features (e.g., edges in images), while FC layers combine these features to detect global structures (e.g., objects). In a typical network, Conv layers reduce the input size by detecting common features, while FC layers assemble these features into higher-level representations.

In essence, Conv layers break down inputs into common features, while FC layers piece these features together to recognize complex patterns."
How should I standardize input when fine-tuning a CNN?,https://stats.stackexchange.com/questions/384484/how-should-i-standardize-input-when-fine-tuning-a-cnn,neural-networks|convolutional-neural-network|transfer-learning,384484,True,388461,9258,1,14,1548181515,1545794565,17,1548111645,"First of all, VGG-16 may not be the right architecture for you. It has been superseded by various architectures, of which the most commonly used in application is ResNet. However, if you have very few data, transfer learning on VGG-16 may be more efficient than on ResNet. Bottom line, use both and compare them on the validation test.
Coming to your point about standardization: it's true that Simonyan
& Zisserman didn't standardize the RGB intensities, but it's false that they didn't apply any other preprocessing: they applied significant data augmentation. See section 3.1 of their paper. You would need to apply the same data augmentation described there, to your training set.
If you choose to use a ResNet, you want to get this paper:
Han et al., Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm (https://www.jidonline.org/article/S0022-202X(18)30111-8/fulltext), 2018
The model they use is ResNet-152: you might try with a smaller ResNet, if transfer-training for this one proves to be too much of a challenge. ResNets are so ubiquitous that you can find implementations of this architecture in basically all frameworks, e.g.
https://github.com/tensorflow/models/tree/master/official/resnet (https://github.com/tensorflow/models/tree/master/official/resnet)
https://github.com/pytorch/vision/tree/master/torchvision (https://github.com/pytorch/vision/tree/master/torchvision)
https://github.com/keras-team/keras-applications (https://github.com/keras-team/keras-applications)

Standardization for a ResNet model
The paper above is behind a paywall (but I'm sure the authors will send you a copy, if you send them an email), so I cannot say for sure if they used standardization in this specific application: btw, note that they didn't just classify the skin lesion to be benign or malignant, but, to the best of my understanding, they classified it to one of 12 different classes. In general Best Practices for training ResNets do suggest to perform standardization. For example, among the BPs suggested in https://arxiv.org/pdf/1812.01187.pdf (https://arxiv.org/pdf/1812.01187.pdf), we find:

Scale hue, saturation, and brightness with coefficients uniformly drawn from [0.6, 1.4]
Normalize RGB channels by subtracting 123.68, 116.779, 103.939 and dividing 
by 58.393, 57.12, 57.375, respectively, which should be the sample mean & sample standard deviation of each channel, computed on the training set of the ISLVRC2012 dataset (a subset of ImageNet), which has 1.3 million images for training and 1000
classes.

Of course, if you plan to compare the results from the VGG-16 and the ResNet-152 (or ResNet-50: another commonly used ResNet model, which is less data-hungry than ResNet-152), you need to use the same standardization for both.
Concerning your second question (standardize the input relative to ImageNet and your dataset, only to ImageNet, or only to your dataset), option 3 is crazy, because when you feed new data to your NN, these data must be standardized too (since you standardized the training set, the weights of the NN after training are the ""right ones"" for standardized inputs). Now, to avoid test set leakage, the usual practice is to standardize new data using the sample mean (and sample standard deviation, if you're using ResNet-style normalization) computed on the training set (ISLVRC2012, in this case, because you did most of the training on it). Now, suppose you get a new skin sample image: you have to normalize it before feeding it to your NN. If you normalize it using sample mean & standard deviation based on your new dataset, you'll be doing something completely different from what you did during training, so I wouldn't expect the NN to work very well. When would option 3 make sense? In two cases: either when you train your NN from scratch on your new dataset, or when you unfreeze a lot of layers and retrain them. However, transfer learning is usually performed by unfreezing only the top layer of the NN (the softmax layer).
Choosing between option 1 and 2 depends on how large is your dataset wrt ImageNet (or to be precise, to the ISLVRC2012 dataset, the subset of ImageNet which has been used to train the ResNet), and how ""extreme"" in term of RGB values your images are wrt those of ISLVRC2012. I suggest you compute the sample mean and sample standard deviation for ISLVRC2012, and ISLVRC2012 + your train set. If, as I suppose, the difference is small, then just use the statistics computed on ISLVRC2012.

Standardization is not what you should really worry about
Finally, since you're doing transfer learning, it will be much more important to perform proper data augmentation, rather than to concentrate on the proper normalization. For obvious reasons, skin sample images will be scarce, and the dataset will be unbalanced (i.e., you will likely have much more images of benign lesions, than of malignant lesions). Since your question didn't ask about data augmentation, I won't touch the topic, but I suggest you read also:
https://www.nature.com/articles/nature21056 (https://www.nature.com/articles/nature21056)
https://academic.oup.com/annonc/article/29/8/1836/5004443 (https://academic.oup.com/annonc/article/29/8/1836/5004443)
https://www.jmir.org/2018/10/e11936 (https://www.jmir.org/2018/10/e11936)
https://arxiv.org/pdf/1812.02316.pdf (https://arxiv.org/pdf/1812.02316.pdf)
","**VGG-16 vs. ResNet**

For transfer learning with limited data, VGG-16 may be more efficient than ResNet. However, using both architectures and comparing results on a validation set is recommended.

**Data Preprocessing**

For VGG-16, use data augmentation instead of the original RGB standardization. For ResNets (e.g., ResNet-152), apply the data augmentation described by Simonyan and Zisserman.

**Standardization for ResNets**

ResNets typically benefit from standardization. Best practices suggest scaling hue, saturation, and brightness and normalizing RGB channels using specific mean and standard deviation values from the ISLVRC2012 dataset.

**Test Set Normalization**

To avoid test set leakage, standardize new data using the sample mean and standard deviation computed from the training set (e.g., ISLVRC2012).

**Focus on Data Augmentation**

Data augmentation is crucial for transfer learning with limited data, especially for unbalanced datasets like skin lesion images."
Simple Linear Regression in Keras,https://stats.stackexchange.com/questions/284189/simple-linear-regression-in-keras,regression|machine-learning|neural-networks|linear|keras,284189,True,284205,24373,2,14,1581433915,1496900419,11,1496910072,"This is probably because there was no normalization done.
Neural network are very sensitive to non-normalized data.
Some intuition: when we're trying to find our multi-dimensional global minimum (like in the stochastic gradient descent model), in every iteration each feature ""pulls"" into its dimension (vector direction) with some force (the length of the vector). When the data is not normalized a small step in value for column A can cause a huge change in column B.
Your code coped with that using your very low learning rate, which ""normalized"" the effect on every column, though caused a delayed learning process, requiring much more epochs to finish.
Add this normalization code:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x = sc.fit_transform(x)
y = sc.fit_transform(y)

And simply drop the learning rate param (lr) - letting it choose wisely an automatic value for you. I got the same desired chart as you now :)
","**Summary:**

Neural networks are sensitive to non-normalized data. This means that features with different ranges can have disproportionate influence during training, especially when using stochastic gradient descent (SGD).

To address this, data must be normalized to ensure that all features are on a similar scale. This balances the ""pulling"" force exerted by each feature during SGD, preventing features with larger ranges from dominating the learning process.

By normalizing the training data, it is possible to remove the need for a very low learning rate, which was previously used to mitigate the effects of non-normalized data. This results in a faster and more efficient learning process while still achieving the desired results."
Variable importance in RNN or LSTM,https://stats.stackexchange.com/questions/191855/variable-importance-in-rnn-or-lstm,neural-networks|lstm|recurrent-neural-network|importance,191855,True,425304,13796,1,14,1606733929,1453406428,14,1567819143,"In short, yes, you can get some measure of variable importances for RNN based models. I won't iterate through all of the listed suggestions in the question, but I will walk through an example of sensitivity analysis in depth.
The data
The input data for my RNN will be composed of a time-series with three features, $x_1$, $x_2$, $x_3$. Each feature will be all be drawn from the random uniform distribution. The target variable for my RNN will be a time-series (one prediction for each time-step in my input):
$$
y = \left\{\begin{array}{lr}
        0, & \text{if } x_1 x_2 \geq 0.25\\
        1, & \text{if } x_1 x_2 < 0.25
        \end{array}\right.
$$
As we can see, the target is dependent on only the first two features. Thus, a good variable importance metric should show the first two variables being important, and the third variable being unimportant.
The model
The model is a simple three layer LSTM with a sigmoid activation in the final layer. The model will be trained in 5 epochs with 1000 batches per epoch.
Variable importance
To measure the variable importance, we'll take a large sample (250 time-series) of our data $\hat{x}$ and compute the model's predictions $\hat{y}$. Then, for each variable $x_i$ we'll perturb that variable (and only that variable) by a random normal distribution centered at 0 with scale 0.2 and compute a prediction $\hat{y_i}$. We'll measure the effect this perturbation has by computing the Root Mean Square difference between the original $\hat{y}$ and the perturbed $\hat{y_i}$. A larger Root Mean Square difference means that variable is ""more important"".
Obviously, the exact mechanism you use to perturb your data, and how you measure the difference between perturbed and unperturbed outputs, will be highly dependent on your particular dataset.
Results
After doing all of the above, we see the following importances:
Variable 1, perturbation effect: 0.1162
Variable 2, perturbation effect: 0.1185
Variable 3, perturbation effect: 0.0077

As we expected, variables 1 and 2 are found to be much more important (about 15x more) than variable 3!
Python code to reproduce
from tensorflow import keras  # tensorflow v1.14.0 was used
import numpy as np            # numpy v1.17.1 was used

np.random.seed(2019)

def make_model():
    inp = keras.layers.Input(shape=(10, 3))
    x = keras.layers.LSTM(10, activation='relu', return_sequences=True)(inp)
    x = keras.layers.LSTM(5, activation='relu', return_sequences=True)(x)
    x = keras.layers.LSTM(1, activation='sigmoid', return_sequences=True)(x)
    out = keras.layers.Flatten()(x)
    return keras.models.Model(inp, out)

def data_gen():
    while True:
        x = np.random.rand(5, 10, 3)  # batch x time x features
        yield x, x[:, :, 0] * x[:, :, 1] < 0.25

def var_importance(model):
    g = data_gen()
    x = np.concatenate([next(g)[0] for _ in range(50)]) # Get a sample of data
    orig_out = model.predict(x)
    for i in range(3):  # iterate over the three features
        new_x = x.copy()
        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])
        new_x[:, :, i] = new_x[:, :, i] + perturbation
        perturbed_out = model.predict(new_x)
        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5
        print(f'Variable {i+1}, perturbation effect: {effect:.4f}')

def main():
    model = make_model()
    model.compile('adam', 'binary_crossentropy')
    print(model.summary())
    model.fit_generator(data_gen(), steps_per_epoch=500, epochs=10)
    var_importance(model)

if __name__ == ""__main__"":
    main()

The output of the code:
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 10, 3)]           0
_________________________________________________________________
lstm (LSTM)                  (None, 10, 10)            560
_________________________________________________________________
lstm_1 (LSTM)                (None, 10, 5)             320
_________________________________________________________________
lstm_2 (LSTM)                (None, 10, 1)             28
_________________________________________________________________
flatten (Flatten)            (None, 10)                0
=================================================================
Total params: 908
Trainable params: 908
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
1000/1000 [==============================] - 14s 14ms/step - loss: 0.6261
Epoch 2/5
1000/1000 [==============================] - 12s 12ms/step - loss: 0.4901
Epoch 3/5
1000/1000 [==============================] - 13s 13ms/step - loss: 0.4631
Epoch 4/5
1000/1000 [==============================] - 14s 14ms/step - loss: 0.4480
Epoch 5/5
1000/1000 [==============================] - 14s 14ms/step - loss: 0.4440
Variable 1, perturbation effect: 0.1162
Variable 2, perturbation effect: 0.1185
Variable 3, perturbation effect: 0.0077

","**Summary:**

Yes, it is possible to obtain approximate variable importances for Recurrent Neural Networks (RNNs). The specific method discussed utilizes sensitivity analysis.

**Sensitivity Analysis for Variable Importance:**

1. **Data:** Input data consists of a time-series with three features, where the target variable depends solely on the first two.
2. **Model:** A simple three-layer LSTM is employed.
3. **Perturbation:** Each variable is perturbed individually using a random normal distribution.
4. **Measurement:** The impact of the perturbation is quantified using the Root Mean Square (RMS) difference between the original and perturbed model predictions. A larger RMS difference indicates higher importance.

**Results:**

* Variables 1 and 2 are significantly more important than Variable 3, as expected.
* The importance values are approximately 0.1162 and 0.1185 for Variables 1 and 2, respectively, compared to 0.0077 for Variable 3.

It's important to note that the specific mechanisms for perturbing the data and measuring differences may vary based on the dataset and model."
Do multiple deep descents exist?,https://stats.stackexchange.com/questions/469381/do-multiple-deep-descents-exist,neural-networks|gradient-descent|bias-variance-tradeoff,469381,True,476557,291,1,14,1601566565,1590808978,12,1594399732,"I found two recent works which seem relevant --
Triple descent and the two kinds of overfitting: Where & why do they appear? (https://arxiv.org/abs/2006.03509)
The claim is that there are two (sample-wise) peaks: one when number of inputs N equals the input dimension D, and one when N equals the number of parameters P. For linear models, D=P, so only one peak is observed. For highly non-linear deep models, the N=D peak is not as pronounced (and presumably wasn't previously observed). The authors present theoretical justification and empirical observations of both peaks.
 (https://i.sstatic.net/3oJj4m.png)
On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry of Kernels (https://arxiv.org/abs/1908.10292)
In the setting of kernel ""ridgeless"" regression, they claim both theoretically and with empirical evidence of multiple descents as the dimensionality of the input data increases (with fixed N) -- to be precise, that there should be a peak at each integer root $D = \sqrt[\leftroot{2}\uproot{2}k]{N}$ of the sample size.
 (https://i.sstatic.net/TXiMnm.png)

x-axis: log D, y-axis: variance of the minimum-norm interpolant.
Vertical lines denote theoretically predicted peaks.

","**Summary:**

Recent research in machine learning has identified two types of overfitting:

**Triple Descent:**

* Occurs in non-linear deep models.
* Characterized by two peaks in error as the number of samples (N) increases:
    * Peak at N = input dimension (D)
    * Peak at N = number of model parameters (P)

**Multiple Descent:**

* Occurs in kernel regression with fixed sample size (N).
* Characterized by multiple peaks in error as the input dimension (D) increases:
    * Peaks occur at integer roots of N

Both triple descent and multiple descent are theoretically justified and supported by empirical evidence. These findings highlight the importance of considering both sample size and model complexity to optimize learning performance."
Is the optimization of the Gaussian VAE well-posed?,https://stats.stackexchange.com/questions/373858/is-the-optimization-of-the-gaussian-vae-well-posed,machine-learning|deep-learning|variational-bayes|generative-models,373858,True,446610,2463,3,14,1618469586,1540550252,7,1580132892,"I co-wrote a paper on this exact problem:
https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models (https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models)
We show that, as you thought, maximum-likelihood is ill-posed for Gaussian output VAEs. Things go pretty much like for GMMs. A solution is to constrain the eigenvalues of the covariance network to be bigger than some threshold.
An interesting remark is that, for discrete data, the problem is well-posed. This possibly explains why VAEs are usually benchmarked on discrete data sets (like binary MNIST).
We show all these results in Section 2.1 of our paper.
Similar investigations were also conducted in this paper:
http://www.jmlr.org/papers/volume19/17-704/17-704.pdf (http://www.jmlr.org/papers/volume19/17-704/17-704.pdf)
they show (Theorem 5) that the VAE objective is unbounded. This means that, in general, even having the KL term does not make the objective well-posed.
","Gaussian Variational Autoencoders (VAEs) often utilize maximum-likelihood estimation, which is problematic due to the ill-posed nature of this technique for Gaussian distributed data. This issue arises from the lack of lower bounds on the likelihood function, making it unbounded and difficult to optimize.

A solution to this problem is constraining the eigenvalues of the covariance matrix in the network to exceed a certain threshold. This constraint prevents the eigenvalues from becoming too small and ensures the well-posedness of the likelihood function.

In contrast to Gaussian data, discrete data does not exhibit this ill-posedness, potentially explaining why VAEs are primarily evaluated on discrete datasets such as binary MNIST.

Other research has also highlighted this issue, demonstrating that the VAE objective is unbounded, meaning that the presence of the KL term does not necessarily guarantee the well-posedness of the objective function."
How to train LSTM layer of deep-network,https://stats.stackexchange.com/questions/159944/how-to-train-lstm-layer-of-deep-network,classification|neural-networks|deep-learning|lstm,159944,True,159969,27950,1,14,1446069700,1436035243,11,1436058672,"The best place to start with LSTMs is the blog post of A. Karpathy http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (http://karpathy.github.io/2015/05/21/rnn-effectiveness/). If you are using Torch7 (which I would strongly suggest) the source code is available at github https://github.com/karpathy/char-rnn (https://github.com/karpathy/char-rnn).
I would also try to alter your model a bit. I would use a many-to-one approach so that you input words through a lookup table and add a special word at the end of each sequence, so that only when you input the ""end of the sequence"" sign you will read the classification output and calculate the error based on your training criterion. This way you would train directly under a supervised context.

On the other hand, a simpler approach would be to use paragraph2vec (https://radimrehurek.com/gensim/models/doc2vec.html (https://radimrehurek.com/gensim/models/doc2vec.html)) to extract features for your input text and then run a classifier on top of your features. Paragraph vector feature extraction is very simple and in python it would be:
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for uid, line in enumerate(open(self.filename)):
            yield LabeledSentence(words=line.split(), labels=['TXT_%s' % uid])

sentences = LabeledLineSentence('your_text.txt')

model = Doc2Vec(alpha=0.025, min_alpha=0.025, size=50, window=5, min_count=5, dm=1, workers=8, sample=1e-5)
model.build_vocab(sentences)

for epoch in range(epochs):
    try:
        model.train(sentences)
    except (KeyboardInterrupt, SystemExit):
        break

","**Summary:**

**LSTMs:**

* Begin with Andrej Karpathy's blog post and Github source code for Torch7 implementation.

**Model Alteration:**

* Implement a many-to-one approach with word input through a lookup table.
* Add a special ""end of sequence"" token to trigger classification output and error calculation.
* This approach enables supervised training.

**Simpler Approach:**

* Utilize paragraph2vec (Gensim) for feature extraction from input text.
* Run a classifier on the extracted features.

**Example Python Code for paragraph2vec:**

* Use `LabeledLineSentence` to label paragraphs with unique IDs.
* Call `Doc2Vec` with specified parameters to build the feature extraction model.
* Train the model on the labeled sentences."
How does Word2Vec&#39;s skip-gram model generate the output vectors?,https://stats.stackexchange.com/questions/194011/how-does-word2vecs-skip-gram-model-generate-the-output-vectors,neural-networks|deep-learning|natural-language|word2vec|word-embeddings,194011,True,198826,13204,3,14,1531333776,1454597050,7,1456528620,"I had the same problem understanding it. It seems that the output score vector will be the same for all C terms. But the difference in error with each one-hot represented vectors will be different. Thus the error vectors are used in back-propagation to update the weights.
Please correct me, if I'm wrong.
source : https://iksinc.wordpress.com/tag/skip-gram-model/ (https://iksinc.wordpress.com/tag/skip-gram-model/)
","**Summary:**

The output score vector for a Skip-gram model remains constant across all C terms (context words). However, the error vectors associated with each one-hot vector representation differ. These error vectors are crucial for back-propagation, as they guide the adjustment of weights during model training.

**Additional Explanation:**

In a Skip-gram model, the output layer consists of C units, one for each context word. However, only one output unit is active at a time, corresponding to the context word being predicted.

The score vector, which calculates the similarity between the input and context words, is the same for all C terms. This means that the model treats all context words equally in terms of their potential for co-occurrence.

The error vectors, on the other hand, are specific to each context word. They quantify the discrepancy between the model's prediction and the actual occurrence of that context word.

During back-propagation, the error vectors are used to update the weights of the input and hidden layers. Specifically, the weights are adjusted to reduce the error for the specific context word being considered. By iteratively adjusting the weights, the model learns to better predict the co-occurrence patterns of words in the language."
Can I use ReLU in autoencoder as activation function?,https://stats.stackexchange.com/questions/144733/can-i-use-relu-in-autoencoder-as-activation-function,machine-learning|neural-networks|deep-learning|autoencoders,144733,True,144736,23914,1,14,1492613445,1428096534,7,1428097555,"Here's a discussion thread (https://groups.google.com/forum/#!topic/pylearn-dev/iWqctW9nkAg) (from July 2013) indicating that there might be some issues with it, but it can be done.
ÃaÄlar GÃ¼lÃ§ehre (from Yoshua Bengio's lab) said he successfully used the following technique in Knowledge Matters: Importance of Prior Information for Optimization (http://arxiv.org/abs/1301.4083):

train the first DAE as usual, but with rectifiers in the hidden layer:
  
      a1(x) = W1 x + b1
      h1 = f1(x) = rectifier(a1(x)) 
      g1(h1) = {sigmoid}(V1 h1 + c1)
  
  minimize cross-entropy or MSE loss, comparing g1(f1(corrupt(x))) and x. the sigmoid is optional depending on the data.
train the 2nd DAE with noise added before the f1 rectifier and use softplus reconstruction units with MSE loss:
  
       h2 = f2(h1) = rectifier(W2 h1 + b2)
       g2(h2) = softplus(V2 h2 + c2)
  
  minimize $\lVert f_1(x) - g_2(f_2(\mathrm{rectifier}(\mathrm{corrupt}(a_1(x))))) \rVert^2 + \lambda_1 \lVert W \rVert_1 + \lambda_2 \lVert W \rVert_2$

Xavier Glorot, also from the Bengio lab, said he did the same except for replacing $\lVert W \rVert_1$ with an $L_1$ penalty ""on the activation values"" (presumably $\lVert g_2(\dots) \rVert_1$?) in both Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach (http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Glorot_342.pdf) (ICML 2011) and in Deep sparse rectifier neural networks (http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf) (AISTATS 2011).
","**Summary:**

**Pretraining Deep Autoencoders (DAEs) with Rectifiers and Regularization**

Researchers have developed a technique to pretrain deep autoencoders using rectified linear units (ReLUs) in the hidden layers. The approach involves training the first DAE with ReLUs and minimizing cross-entropy or MSE loss. The second DAE is then trained with noise added before the ReLU activation and softplus reconstruction units, minimizing a combination of mean squared error loss and L1/L2 regularization on the weights and activation values.

**Specific Techniques:**

* First DAE:
    * ReLU activation in the hidden layer
    * Cross-entropy or MSE loss
    * Sigmoid reconstruction unit (optional)
* Second DAE:
    * ReLU activation with added noise
    * Softplus reconstruction units
    * MSE loss combined with L1/L2 regularization on weights and activation values

**Attribution:**

The technique was successfully implemented by ÃaÄlar GÃ¼lÃ§ehre and Xavier Glorot from Yoshua Bengio's lab. GÃ¼lÃ§ehre presented the method in the paper ""Knowledge Matters: Importance of Prior Information for Optimization."" Glorot used variations of the technique in his papers on domain adaptation and deep sparse rectifier neural networks."
Can Tree-based regression perform worse than plain linear regression?,https://stats.stackexchange.com/questions/286463/can-tree-based-regression-perform-worse-than-plain-linear-regression,regression|modeling|deep-learning|model|cart,286463,True,286541,8233,4,13,1549708705,1498018232,14,1498050128,"You should not just throw the data at different algorithms and look at the quality of the predictions.  You need to understand your data better, and the way of going about that is to first, visualize your data (the marginal distributions).   Even if you are only interested finally in the predictions, you will be in a better position to make better models if you understand the data better.  So, first, try to understand the data (and simple models fitted to the data) better, and then you are in a much better position to create more complex, and hopefully better, models.
Then, fit linear regression models, with your 15 variables as precictors (later you can look at possible interactions).  Then, calculate the residuals from that fit, that is, 
$$
    r_i  = Y_i - \hat{Y}_i, \qquad i=1, 2,\dots, n
$$ 
If the model is adecuate, that is, it was able to extract the signal (structure) from the data, then the residuals should show no patterns. Box, Hunter & Hunter: ""Statistics for Experimenters"" (which you should have a look at, its one of the best ever books on statistics) compares this with an analogy from chemistry: The model is a ""filter"" designed to catch impurities from water (the data).  What is left, which passed through the filter, should then be ""clean"" and analysis of it (residuals analysis) can show that, when it does not contain impurities (structure).  See Checking residuals for normality in generalised linear models (https://stats.stackexchange.com/questions/92394/checking-residuals-for-normality-in-generalised-linear-models) 
To know what to check for you need to understand the assumptions behind linear regression, see What is a complete list of the usual assumptions for linear regression?  (https://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression)
One usual assumption is homoskedasticity, that is, constant variance. To check that, plot the residuals $r_i$ against the predicted values, $\hat{Y}_i$. To understand this procedure see:  Why are residual plots constructed using the residuals vs the predicted values? (https://stats.stackexchange.com/questions/71352/why-are-residual-plots-constructed-using-the-residuals-vs-the-predicted-values).  
Other assumptions is linearity. To check those, plot the residuals against each of the predictors in the model. If you see any curvity in those plots, that is evidence against linearity.  If you find non-linearity, either you can try some transformations or (more modern approch) include that non-linear predictor in the model in a non-linear way, maybe using splines (you have 60 million examples so should be quite feasible!). 
Then you need to check for possible interactions.  The above ideas can be used also for variables not in the fitted model.  Since you fit a model without interactions, that include interaction variables, like the product $x_i \cdot z_i$ for two variables $x$, $z$.  So plot the residuals against all this interaction variables.  A blog post with many example plots is http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/ (http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/)
A book-length treatment is R Dennis Cook & Sanford Weisberg: ""Residuals and influence in regression"", Chapman & Hall.  A more modern book-length treatment is Frank Harrell: ""Regression modeling Strategies"". 
And, coming bact to the question in the title: ""Can Tree-based regression perform worse than plain linear regression?""  Yes, of course it can.  Tree-based models has as regression function a very complex step function.  If the data truly comes from (behave as simulated from) a linear model, then step functions can be a bad approximation. And, as shown by examples in the other answer, tree-based models might extrapolate badly outside the range of the observed predictors.  You could also try randomforrest and see how much better that is than a single tree. 
","**Summary:**

Before building complex models, it is crucial to understand your data by visualizing its distribution and fitting simple models like linear regression.

**Residual Analysis:**

Calculate residuals from linear regression models to assess model adequacy. Residuals should show no patterns if the model effectively captures the data's structure.

**Checking Assumptions:**

Verify assumptions of linear regression, including homoskedasticity (constant variance) and linearity. Plot residuals against predicted values and predictors to detect violations.

**Detecting Non-Linearity and Interactions:**

Look for curvity in residual plots to identify non-linear relationships. Include non-linear predictors in the model using transformations or splines. Check for interactions by plotting residuals against interaction variables.

**Conclusion:**

Tree-based regression methods like decision trees can perform worse than linear regression when data follows linear relationships. They may extrapolate poorly outside the observed predictor range. Consider random forests as an alternative to improve performance."
What is the point of having a dense layer in a neural network with no activation function?,https://stats.stackexchange.com/questions/361066/what-is-the-point-of-having-a-dense-layer-in-a-neural-network-with-no-activation,machine-learning|neural-networks,361066,True,361068,13012,5,13,1660893756,1533624011,12,1533625758,"One such scenario is the output layer of a network performing regression, which should be naturally linear. This tutorial (https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/) demonstrates this case.
Another case that comes to my mind are deep linear networks which are often being used in neural networks literature as a toy model for studying some phenomena that would be too complex with usual non-linear networks.
","**Summary:**

Deep learning models often use nonlinear activation functions in hidden layers to capture complex patterns in data. However, there are specific scenarios where linear activation functions (i.e., no activation) may be more appropriate.

One such scenario is the output layer of regression networks, where the predicted values should be exact numerical values. Linear activation ensures that the output is a continuous, unconstrained value that can accurately represent the target variable.

Another instance where linear activation is employed is in deep linear networks. These networks serve as simplified models used in research to study phenomena that may be too intricate for nonlinear networks. By using linear activation throughout the network, researchers can isolate and analyze specific aspects of the model's behavior without the added complexity of nonlinearity."
How to understand Generative Adversarial Networks Discriminative distribution?,https://stats.stackexchange.com/questions/435140/how-to-understand-generative-adversarial-networks-discriminative-distribution,machine-learning|neural-networks|generative-models|gan,435140,True,435159,721,1,13,1573304800,1573191949,20,1573207369,"Let me try to clear things up a bit if I can. First of all, GANs are not made specifically for generating images, but all kinds of data. In fact the first paper, which you get your figure from, isn't referring to images.
In the figure you gave 3 curves:

The black dots. These are your training samples $x$. If you connect the dots you can form a line (I will refer to this as the black line even if it isn't visible in the figures). This is the data-generating distribution $p_x$, which is the theoretical distribution from where your data is sampled.
The green line. This is the distribution that your generator has learned, $p_g$. When training your discriminator you need real and fake samples. The real ones are the black dots, while the fake ones are sampled from the green distribution.
The blue line. This is the output of the discriminator, i.e. the probability that an image will be classified as real or fake.

Also the black $x$ horizontal line shows the range from which we can draw $x$ samples, while the black $z$ horizontal line shows the same thing with the latent variable $z$. When drawn these will follow their respective distributions (black and green lines).  
Now on to what each figure tells us:

The first figure (a) shows how the distributions look before training. The generator doesn't produce realistic samples (i.e. the green line is far away from the black line) and the discriminator doesn't know how to discriminate properly (i.e. the blue line has a lot of fluctuations).
The second figure (b) is at a point where $D$ has learned to discriminate between the two types of samples (i.e. real and fake). The blue line now resembles a sigmoid. This is needed so that $G$ can have accurate feedback on how its samples fair.
The third figure (c) is at a point where $G$ is beginning to learn how to generate realistic samples. Note how the green line is closer to the black line now. Even though $D$ is also good (the blue line aligns with half of the distance between the two distributions), its job is much harder now.
The fourth figure (d) is at the end of training. $G$ can now produce fully-realistic samples (i.e. the green and black lines are one). Because of this $D$ can't discriminate any more, so it predicts randomly if an image is real or fake (i.e. $P(D) = 1/2$ everywhere)

","Generative Adversarial Networks (GANs) generate various data types, not just images. In a GAN, a generator creates fake data, and a discriminator evaluates its authenticity.

The training process involves three lines:

* **Black Line:** Data distribution ($p_x$) sampled to create training data ($x$).
* **Green Line:** Generator's distribution ($p_g$) from which fake data is sampled.
* **Blue Line:** Discriminator's output ($P(D)$), indicating the probability of classifying data as real.

Four figures illustrate the training process:

* **(a)** Initial state: Generator produces unrealistic samples, and discriminator performs poorly.
* **(b)** Discriminator learns to distinguish real from fake data.
* **(c)** Generator improves, producing more realistic samples.
* **(d)** End of training: Generator creates fully-realistic samples, and discriminator can no longer distinguish them."
Restricted Boltzmann Machines for regression?,https://stats.stackexchange.com/questions/41029/restricted-boltzmann-machines-for-regression,regression|machine-learning|classification|neural-networks,41029,True,41201,7400,3,13,1449436708,1351001543,18,1351189121,"You are right about unlabeled data. RBMs are generative models and most commonly used as unsupervised learners. 
When used for constructing a Deep Belief Network the most typical procedure is to simply train each each new RBM one at a time as they are stacked on top of each other. So contrastive divergence isn't going up and down in the sense that I think you mean. It is only working with one RBM at a time, using the hidden layer of the previous topmost RBM as the input for the new topmost RBM. After all this you can either treat the stack of RBM weights as the initial weights for a standard feed forward neural network and train using your labeled data and backpropagation or do something more exotic like use the wake-sleep algorithm. Notice that we haven't used any labeled data up until this last step, that is one of the benefits of these types of models. We can learn a good generative model using lots of unlabeled data and even if our ultimate goal is to have good discriminative performance, it should help.
On the other hand, there are several ways you can use RBMs for classification.

Train an RBM or a stack of several RBMs. Use the topmost hidden layer as input to some other supervised learner. 
Train an RBM for each class and use the unnormalized energies as input to a discriminative classifier.
Train the RBM to be a joint density model of P(X, Y). Then given some input x, just pick the class y which minimizes the energy function (normalization isn't a problem here like in the above since the constant Z is the same for all classes).
Train a discriminative RBM (http://machinelearning.org/archive/icml2008/papers/601.pdf)

I would highly suggest you read through the technical report A Practical Guide to Training Restricted Boltzmann Machines (http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) by Geoff Hinton. It discusses several of these issues in much greater detail, provides invaluable tips, cites lots of relevant papers, and may help to clear up any other confusion you might have.
","**Summary:**

Restricted Boltzmann Machines (RBMs) are unsupervised generative models commonly used in Deep Belief Networks (DBNs). DBNs are constructed by stacking RBMs, where each RBM is trained independently using contrastive divergence on unlabeled data.

After training the stack of RBMs, labeled data can be used for supervised learning by:

* Treating the RBM weights as initial weights for a feedforward neural network and training with backpropagation.
* Using the topmost hidden layer of the stack as input to a supervised learner.
* Training a separate RBM for each class and using unnormalized energies for classification.
* Training a joint density model RBM for P(X, Y) and picking the class with the lowest energy given X.
* Training a discriminative RBM.

RBMs offer the advantage of learning from unlabeled data, which can enhance subsequent supervised learning performance."
Boosting using other &quot;weak learners&quot; than trees,https://stats.stackexchange.com/questions/282413/boosting-using-other-weak-learners-than-trees,neural-networks|random-forest|gaussian-process|boosting,282413,True,298496,2151,4,13,1638319837,1496093228,9,1503000484,"Trees are often a good choice for new data mining problems with unknown structure. From The Elements of Statistical Learning (https://web.stanford.edu/~hastie/Papers/ESLII.pdf) section 10.7: ""Industrial and commercial data mining applications tend to be especially challenging in terms of the requirements placed on learning procedures. Data sets are often very large in terms of number of observations and number of variables measured on each of them. Thus, computational considerations play an important role. Also, the data are usually messy: the
inputs tend to be mixtures of quantitative, binary, and categorical variables, the latter often with many levels. There are generally many missing values, complete observations being rare. Distributions of numeric predictor and response variables are often long-tailed and highly skewed...In addition they usually contain a substantial fraction
of gross mis-measurements (outliers). The predictor variables are generally
measured on very different scales.
In data mining applications, usually only a small fraction of the large
number of predictor variables that have been included in the analysis are
actually relevant to prediction. Also, unlike many applications such as pattern recognition, there is seldom reliable domain knowledge to help create
especially relevant features and/or filter out the irrelevant ones, the inclusion of which dramatically degrades the performance of many methods.
In addition, data mining applications generally require interpretable models. It is not enough to simply produce predictions. It is also desirable to have information providing qualitative understanding of the relationship between joint values of the input variables and the resulting predicted response value. Thus, black box methods such as neural networks, which can be quite useful in purely predictive settings such as pattern recognition, are far less useful for data mining.
These requirements of speed, interpretability and the messy nature of
the data sharply limit the usefulness of most learning procedures as off-
the-shelf methods for data mining. An âoff-the-shelfâ method is one that
can be directly applied to the data without requiring a great deal of time-
consuming data preprocessing or careful tuning of the learning procedure.
Of all the well-known learning methods, decision trees come closest to
meeting the requirements for serving as an off-the-shelf procedure for data
mining. They are relatively fast to construct and they produce interpretable
models (if the trees are small). As discussed in Section 9.2, they naturally
incorporate mixtures of numeric and categorical predictor variables and
missing values. They are invariant under (strictly monotone) transformations of the individual predictors. As a result, scaling and/or more general
transformations are not an issue, and they are immune to the effects of predictor outliers. They perform internal feature selection as an integral part of the procedure. They are thereby resistant, if not completely immune,
to the inclusion of many irrelevant predictor variables. These properties of
decision trees are largely the reason that they have emerged as the most
popular learning method for data mining.
Trees have one aspect that prevents them from being the ideal tool for
predictive learning, namely inaccuracy. They seldom provide predictive accuracy comparable to the best that can be achieved with the data at hand.
As seen in Section 10.1, boosting decision trees improves their accuracy,
often dramatically. At the same time it maintains most of their desirable
properties for data mining. Some advantages of trees that are sacrificed by
boosting are speed, interpretability, and, for AdaBoost, robustness against
overlapping class distributions and especially mislabeling of the training
data. A gradient boosted model (GBM) is a generalization of tree boosting
that attempts to mitigate these problems, so as to produce an accurate and
effective off-the-shelf procedure for data mining.""
Sorry, long quote, but I think it explains their rationale/excuse for only addressing trees. They go on to derive a Boosting algorithm that uses trees, explain what parameters they have to find to make it work, define steepest descent in the context of what I have seen elsewhere called the ""function space"" (f = {f(x1), f(x2), ... f(xN)}), explain how constraining their next classifier in the ensemble to a tree that has similar outputs to the steepest descent ""direction"" generalizes better than just adding the steepest descent function directly (Some kind of regularization going on?), and then define an algorithm.
But they neglect to address the fact that we often do want to use Boosting in the context of pure prediction. And it's great we see such an amazing performance improvement when GradientBoosting with trees, but what about the other weak learners we are used to Boosting being able to handle?
There is no principled reason decision trees could not be replaced with some other low-bias, high-variance learner like small neural nets. It would definitely change steps 2.b. and 2.c. of their algorithm 10.3. It might also change the initialization at step 1. But it could be done! I am not sure there are any common or popular algorithms in the literature, though. When I looked for them I came across your question instead. It may be that trees are easier to train and have a provably low risk as an ensemble such that no one expects  anything else to be able to do better?
","**Summary**

**Data Mining Challenges**

* Large datasets with diverse variable types (quantitative, binary, categorical)
* Incomplete data, outliers, and skewed distributions
* Need for interpretable models to understand relationships between variables

**Why Decision Trees for Data Mining?**

* Fast to construct and interpretable models
* Handle mixed variable types, missing values, and transformations
* Naturally perform feature selection, resisting irrelevant variables

**Drawbacks of Decision Trees**

* Lower predictive accuracy than other methods

**Gradient Boosted Models (GBM)**

* Enhance decision tree accuracy through boosting
* Retain desirable properties of trees, such as interpretability and robustness

**Potential for Other Weak Learners**

* Decision trees are not the only weak learners that can be boosted
* Low-bias, high-variance learners, such as small neural nets, could potentially enhance performance
* However, common algorithms for such ensembles are lacking."
What is the derivative of Leaky ReLU?,https://stats.stackexchange.com/questions/275521/what-is-the-derivative-of-leaky-relu,machine-learning|neural-networks|optimization|computer-vision,275521,True,275533,16943,3,13,1669138238,1493040802,15,1493042685,"The derivative of a ReLU is zero for x < 0 and one for x > 0. If the leaky ReLU has slope, say 0.5, for negative values, the derivative will be 0.5 for x < 0 and 1 for x > 0.
","**Summary:**

The ReLU (Rectified Linear Unit) activation function has a zero derivative for negative input values and a one derivative for positive input values. The leaky ReLU, a variant of ReLU, introduces a non-zero slope for negative input values.

Specifically, the derivative of the leaky ReLU is:
- 0.5 for x < 0 (slope for negative values)
- 1 for x > 0 (same as ReLU)

This means that for negative input values, the leaky ReLU has a constant gradient of 0.5, allowing for more information flow than the ReLU, which has a zero gradient for negative values. For positive input values, both ReLU and leaky ReLU have a gradient of 1."
Why use Binary Cross Entropy for Generator in Adversarial Networks,https://stats.stackexchange.com/questions/242907/why-use-binary-cross-entropy-for-generator-in-adversarial-networks,neural-networks|convolutional-neural-network|generative-models|generator,242907,True,242970,18645,3,13,1677259853,1477638206,10,1477669764,"I found a really good answer from user ajmooch (https://www.reddit.com/user/ajmooch) on reddit and decided to post it here in case someone had the misconceptions I had:

There's several things to keep in mind here.
The first thing is that the BCE objective for the Generator can more
  accurately be stated as ""the images output by the generator should be
  assigned a high probability by the Discriminator."" It's not BCE as you
  might see in a binary reconstruction loss, which would be BCE(G(Z),X)
  where G(Z) is a generated image and X is a sample, it's BCE(D(G(Z)),1)
  where D(G(Z)) is the probability assigned to the generated image by
  the discriminator. Given a ""perfect"" generator which always has
  photorealistic outputs, the D(G(Z)) values should always be close to
  1. Obviously in practice there's difficulties getting this kind of convergence (the training is sort of inherently unstable) but that is
  the goal.
The second is that in the standard GAN algorithm, the latent vector
  (the ""random noise"" which the generator receives as input and has to
  turn into an image) is sampled independently of training data. If you
  were to use the MSE between the outputs of the GAN and a single image,
  you might get some sort of result out, but you'd effectively be saying
  ""given this (random) Z, produce this specific X"" and you'd be
  implicitly forcing the generator to learn a nonsensical embedding of
  the image. If you think of the Z vector as a high-level description of
  the image, it would be like showing it a dog three times and asking it
  to generate the same dog given three different (and uncorrelated)
  descriptions of the dog. Compare this with something like a VAE which
  has an explicit inference mechanism (the encoder network of the VAE
  infers Z values given an image sample) and then attempts to
  reconstruct a given image using those inferred Zs. The GAN does not
  attempt to reconstruct an image, so in its vanilla form it doesn't
  make sense to compare its outputs to a set of samples using MSE or
  MAE.
There's been some work done recently on incorporating similarity
  metrics into GAN training--this openAI paper adds an MSE objective
  between G(Z) and X in the final FC layer of the discriminator (a la
  Discriminative Regularization), which seems to work really well for
  semi-supervised learning (as based on their insanely good SVHN
  results) but doesn't really improve sample quality.
You can also slam VAEs and GANs together (as I have done and as
  several others have done before me) and use the inference mechanism of
  the VAE to provide guidance for the GAN generator, such that it makes
  sense to do some pixel-wise comparisons for reconstructions.

","**Summary:**

In Generative Adversarial Networks (GANs), the Generator aims to output images that the Discriminator assigns a high probability. This is not the same as Binary Cross-Entropy (BCE) loss commonly used in binary reconstruction. It's BCE(D(G(Z)),1), where D(G(Z)) is the probability assigned by the Discriminator to the generated image.

Unlike VAEs (Variational Autoencoders), GANs do not explicitly infer a latent vector (Z) from input data. The latent vector is sampled independently, and the Generator aims to produce an image based on this randomly generated input. Forcing the Generator to match a specific target image using MSE would lead to nonsensical embedding and limit the Generator's ability to capture generalizable patterns.

Recent research has explored adding similarity metrics, such as MSE, to the Discriminator training process. This can improve semi-supervised learning but not necessarily the quality of generated images.

Combining VAEs and GANs allows for incorporating the inference mechanism of the VAE into the GAN Generator, enabling pixel-wise comparisons for reconstructions."
What are the advantages of using a Bayesian neural network,https://stats.stackexchange.com/questions/141879/what-are-the-advantages-of-using-a-bayesian-neural-network,bayesian|neural-networks|bayesian-network,141879,True,141909,9684,2,13,1463168884,1426469238,10,1426499976,"Bayesian neural nets are useful for solving problems in domains where data is scarce, as a way to prevent overfitting. They often beat all other methods in such situations. Example applications are molecular biology (for example this paper (http://bioinformatics.oxfordjournals.org/content/early/2011/07/29/bioinformatics.btr444.full.pdf+html)) and medical diagnosis (areas where data often come from costly and difficult expiremental work). Actually, Bayesian nets are univerally useful and can obtain better results for a vast number of tasks but they are extremely difficult to scale to large problems. 
","Bayesian neural networks (BNNs) excel in situations with limited data, such as molecular biology and medical diagnosis. They prevent overfitting by using Bayesian statistics to quantify uncertainty in predictions. BNNs generally outperform other methods in data-scarce scenarios.

However, BNNs face challenges in scaling to large problems due to their computational complexity. Despite this limitation, BNNs remain a promising approach for tasks where data availability is constrained, offering improved results compared to traditional neural networks."
"When calculating self-attention for Transformer ML architectures, why do we need both a key and a query weight matrix?",https://stats.stackexchange.com/questions/515477/when-calculating-self-attention-for-transformer-ml-architectures-why-do-we-need,machine-learning|neural-networks|attention|transformers,515477,True,515552,2979,1,13,1616646475,1616607662,14,1616646475,"The weight matrices are $n$ by $m$ with $n >> m$. So $W_Q W_K^T$ is not just any matrix, it's $n$ by $n$ but with rank only $m$ -- there are fewer parameters, and computing $QK^T$ is much faster than $X W' X^T$ for some full rank $W'$
","**Summary:**

The weight matrices $W_Q$ and $W_K$ are tall and skinny ($n \times m$) with $n$ significantly larger than $m$. This means that their multiplication ($W_Q W_K^T$) results in an $n \times n$ matrix with a lower rank ($m$) than expected.

Consequently, there are fewer parameters in $W_Q W_K^T$ compared to a full-rank matrix. Additionally, computing $QK^T$ is significantly faster than computing $X W' X^T$ for a full-rank $W'$, as it involves fewer multiplications."
Anchoring Faster RCNN,https://stats.stackexchange.com/questions/265875/anchoring-faster-rcnn,deep-learning|computer-vision,265875,True,296592,7317,2,13,1567145350,1488843050,12,1502102279,"Anchors Explained
Anchors
For the time being, ignore the fancy term of ""pyramids of reference boxes"", anchors are nothing but fixed-size rectangles to be fed to the Region Proposal Network. Anchors are defined over the last convolutional feature map, meaning there are $(H_{featuremap}*W_{featuremap})*(k)$ of them, but they correspond to the image. For each anchor then the RPN predicts the probability of containing an object in general and four correction coordinates to move and resize the anchor to the right position. But how does the geometry of anchors have to do anything with the RPN?
Anchors Actually Appear in the Loss function
When training the RPN, first a binary class label is assigned to each anchor. Anchors with Intersection-over-Union (IoU) overlap with a ground-truth box, higher than a certain threshold, are assigned a positive label (likewise anchors with IoUs less than a given threshold will be labeled Negative). These labels are further used to compute the loss function:
 (https://i.sstatic.net/x4NL5.png)
$p$ is the classification head output of the RPN that determines the probability of the anchor to contain an object. For anchors labeled as Negative, no loss is incurred from regression â $p^*$, the ground-truth label is zero. In other words the network does't care about the outputted coordinates for negative anchors and is happy as long as it classifies them correctly. In case of positive anchors, regression loss is taken into account. $t$ is the regression head output of the RPN, a vector representing the 4 parameterized coordinates of the predicted bounding box. The parameterization depends on the anchor geometry and is as follows:
 (https://i.sstatic.net/14X3P.png)
where $x, y, w,$ and h denote the boxâs center coordinates and its width and height. Variables $x, x_a,$ and $x^*$ are for the predicted box, anchor box, and ground-truth box respectively (likewise for $y, w, h$).
Also notice anchors with no label are neither classified nor reshaped and the RPM simply throws them out of computations. Once the RPN's job is done, and the proposals are generated, the rest is very similar to Fast R-CNNs.
","**Summary:**

Anchors are fixed-size rectangles used in object detection networks (e.g., Region Proposal Network). Each anchor corresponds to a specific area in the input image.

During training, anchors are assigned positive or negative labels based on their overlap with ground-truth bounding boxes. Positive anchors incur both classification and regression loss, while negative anchors only incur classification loss.

The classification loss determines the probability of an anchor containing an object. The regression loss corrects the anchor's location and size to better align with the ground-truth bounding box. The parameterization of the regression loss depends on the anchor's geometry.

Anchors that do not overlap with any ground-truth boxes are ignored during training. After anchor-based proposals are generated, further object detection steps (e.g., classification and refinement) resemble Fast R-CNNs."
Variational autoencoder: Why reconstruction term is same to square loss?,https://stats.stackexchange.com/questions/347378/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss,probability|deep-learning|inference|autoencoders|variational-bayes,347378,True,348058,13651,3,13,1565203194,1526910481,11,1527185954,"For regular Autoencoders, you start from an input, $x$ and encode it to obtain your latent variable (or code), $z$, using some function that satisfy: $z=f(x)$. After getting the latent variable, you aim to reconstruct the input using some other function $\hat{x}=g(f(x))$. The reconstruction loss is yet another function  $L(x,\hat{x})$ that you use to back-propagate and update $f$ and $g$.
For Variational Autoencoders, you still interpret the latent variables, $z$, as your code. Hence, $p(x|z)$ serves as a probabilistic decoder, since given a code $z$, it produces a distribution over the possible values of $x$. It thus ""makes sense"" that the term $\log p_{\theta}(x|z)$ is somehow connected to reconstruction error.
Both encoder and decoder are deterministic functions. Since $p(x|z)$ is such function that maps $z$ into $\hat{x}$ , you can think of this expression as $p(x|\hat{x})$. When you assume (as they assumed in the paper if I understood it correctly) that this distribution have a Gaussian form:
$$
\log P(x|\hat{x}) \sim \log e^{-|x-\hat{x}|^2} \sim (x-\hat{x})^2
$$
The last expression is proportional to the reconstruction error in regular autoencoders.
","**Summary:**

Autoencoders aim to learn latent representations of input data that can be used for reconstruction. In regular Autoencoders, the latent variable is deterministically derived from the input and used to reconstruct the input using another deterministic function. The reconstruction loss measures the error between the original input and the reconstruction.

Variational Autoencoders (VAEs) also involve latent variables but model them as probability distributions. This probabilistic decoder enables VAEs to generate diverse outputs from the same latent code. The reconstruction term in VAEs, denoted by log p(x|z), is related to the reconstruction error through a Gaussian assumption. This Gaussian assumption leads to an expression that is proportional to the reconstruction error in regular Autoencoders.

Overall, VAEs differ from regular Autoencoders by modeling latent variables as probability distributions and using a probabilistic decoder. Both types of Autoencoders utilize a reconstruction loss to optimize their parameters and achieve efficient data representation."
What does &quot;end to end&quot; mean in deep learning methods?,https://stats.stackexchange.com/questions/224118/what-does-end-to-end-mean-in-deep-learning-methods,machine-learning|terminology|deep-learning,224118,True,224120,7338,1,13,1486415092,1468693835,12,1468694301,"
end-to-end = all parameters are trained jointly (vs. step-by-step (http://www.hangli-hl.com/uploads/3/4/4/6/34465961/naacl_tutorial_version2.2.pdf))
ensembling = several classifiers are trained independently, each classifier makes a prediction, and all predictions are combined into one using some strategy (e.g., take the most common prediction across all classifiers).

","**Summary:**

End-to-end training involves simultaneously optimizing all parameters in a model. This differs from step-by-step training, where parameters are trained sequentially.

Ensembling involves training multiple independent classifiers. Each classifier makes a prediction, and these predictions are combined using a strategy like majority voting or averaging. By leveraging the collective knowledge of multiple classifiers, ensembling often improves accuracy compared to using a single classifier."
What is standing in the way of pulsed neural networks being used in applications?,https://stats.stackexchange.com/questions/23395/what-is-standing-in-the-way-of-pulsed-neural-networks-being-used-in-applications,neural-networks,23395,True,23396,229,2,13,1392237502,1329385319,12,1329397399,"The major issue is that nobody knows how the brain works :)
Theory
As far as  I can tell, there are three big steps in neural network research:

Perceptron (or, threshold-gate) model, where any boolean function can be computed by some multi-layer perceptron with a single hidden layer. 
Neuron model - improved version of the previous one, where network elements use sigmoid activation function (continuous set of possible inpits and outputs). They also can compute any boolean function (after applying a threshold) and, additionally, can approximate any 
continuous function.
Spiking neurons model, which uses ""temporal coding"" to pass information between network elements. It can do everything the previous model does, and often can do so with fewer neurons. 

Basically, the evolution here is to get closer to how human brain works, and the last model has the best fidelity. 
Practice
SNNs seem very promising and there's even a commersial product SpikeNET (http://sccn.ucsd.edu/~arno/spikenet/) built on top of it (under ""What SpikeNET can do"" and ""What SpikeNET cannot yet do"" you can see the problems they faced).
I can't tell about specific issues with spiking networks - but in general I have an impression that the issues arise because people want 
SNNs work more and more like a human brain:

they want to choose how to code information - which can be done via delay coding (more highly stimulated neurons tend to spike more frequently and sooner), binary coding (information is represented by the number of spikes within a given time interval), time coding (information is the inverse of the granularity of the distinguishable time interval), rank order coding (first spikes received by a neuron are given influence and later ones are inhibited) and whatnot.
they emulate Hebbian plasticity which increases weights between neurons when both neurons are ""on"" (or both ""off"") at the same.
they apply self-organization where a group of neurons compete with the winning neuron inhibiting the response of other neurons. With a spiking neuron the winner can be computed quickly based only on single firing events.


Wikipedia has a link to the ""Pulsed Neural Networks"" (http://books.google.cz/books?id=jEug7sJXP2MC&pg=PA3&dq=%22Pulsed%20Neural%20Networks%22%20rate-code%20neuroscience&ei=FEo0ScetL4zukgSyldy8Ag&redir_esc=y#v=onepage&q=%22Pulsed%20Neural%20Networks%22%20rate-code%20neuroscience&f=false) book which has ""Implementation Issues of Pulse-Coded Neural Networks"" section but I'm not educated enough to comment on that. 
As for introduction to the topic, I recommend this paper: Pulsed Neural Networks and their Application (pdf (http://www.ccs.neu.edu/home/kunkle/papers/techreports/pnn.pdf))
","**Summary:**

Understanding neural networks, including their structure and function, remains a significant challenge. The field has progressed through three key models:

* **Perceptron Model:** Can compute any boolean function using a multilayer perceptron with a hidden layer.
* **Neuron Model:** An improved version using sigmoid activation, capable of computing any boolean function and approximating continuous functions.
* **Spiking Neurons Model:** Incorporates temporal coding to pass information, allowing for more efficient computation than the previous models and a closer fidelity to the human brain.

In practice, Spiking Neural Networks (SNNs) show promise, as evidenced by commercial products like SpikeNET. However, challenges arise from the inherent complexity of SNNs, including the need to define information coding methods and emulate biological learning mechanisms.

Researchers are exploring various coding techniques to represent information in SNNs, such as delay coding, binary coding, time coding, and rank order coding. Additionally, they are incorporating Hebbian plasticity and self-organization principles to enhance learning and adaptability.

For further exploration, the ""Pulsed Neural Networks"" book provides insights into implementation issues specific to SNNs."
How to apply neural networks on multi-label classification problems?,https://stats.stackexchange.com/questions/66314/how-to-apply-neural-networks-on-multi-label-classification-problems,machine-learning|neural-networks|natural-language|multilabel,66314,True,66332,16396,2,13,1449188442,1375409191,3,1375431787,"Yes, in multi-label learning the label information is often encoded as the binary vector you described. It is also easier so for evaluation.
We may want to check MULAN (http://mulan.sourceforge.net/), an open source Java library for multi-label learning. It is a Weka extension and has implemented many multi-label classifiers, neural networks included. For example, you can find BP-MLL here (http://mulan.sourceforge.net/doc/mulan/classifier/neural/BPMLL.html). 
","**Summary:**

In multi-label learning, labels are often represented as binary vectors for ease of evaluation.

MULAN, an open-source Java library, offers a range of multi-label classifiers, including the BP-MLL neural network. These tools simplify the implementation and evaluation of multi-label learning tasks."
Yolo v3 loss function,https://stats.stackexchange.com/questions/373266/yolo-v3-loss-function,neural-networks|loss-functions|object-detection|yolo,373266,True,373854,20272,1,13,1581029103,1540266937,10,1540547793,"Good questions. For the first question, the score definitions are different between YOLOv1 and YOLOv3. According to the yolov1 paper (https://arxiv.org/pdf/1506.02640.pdf)

These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts.
If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.

This is the same as your second interpenetration. However in the yolov3 paper (https://pjreddie.com/media/files/papers/YOLOv3.pdf)

YOLOv3 predicts an objectness score for each bounding box using
logistic regression. This should be 1 if the bounding box prior
overlaps a ground truth object by more than any other bounding box
prior.

This is the same as your third interpenetration. Actually both options are implemented in the code you referred to, I guess they both work in practice.
Second question, yes, just don't forget to apply the inverse functions to the ground truth coordinates first. Source code (https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L93-L108) for reference.
","**Summary:**

In YOLOv1, the confidence score measures the model's confidence in both the presence of an object within a bounding box and the accuracy of the predicted box. If no object is present, the confidence score should be zero; otherwise, it should equal the overlap between the predicted box and the actual object.

In YOLOv3, the objectness score, calculated using logistic regression, indicates the probability that a bounding box overlaps with a ground truth object more than any other prior box. This is similar to the third interpretation of the confidence score mentioned in the question. Both YOLOv1's confidence score and YOLOv3's objectness score reflect the model's certainty in predicting an object's presence and location.

Regarding the second question, the ground truth coordinates should undergo inverse transformations before being used in the calculation of the target variables for the loss function. This is necessary to align the ground truth data with the predictions made by the model."
Help Understanding Reconstruction Loss In Variational Autoencoder,https://stats.stackexchange.com/questions/323568/help-understanding-reconstruction-loss-in-variational-autoencoder,neural-networks|autoencoders,323568,True,470850,21323,1,13,1594159505,1516205417,10,1591516379,"Typically in VAE implementations, the output of the decoder is actually the mean $\mu_{x|z}$ which I will just call $\mu$, and people assumes a unitary covariance. So in that case we have:
$logP(x|z)=-\frac{1}{2}[log(|\Sigma|)+klog(2\pi)+(\mathbf{x}-\mathbf{\mu})^T(\mathbf{x}-\mathbf{\mu})]$
This comes from taking the log of the pdf of a multivariate Gaussian distribution. Now you can see that since the first two terms are constant with respect to $\mu$, the optimization problem is equivalent to maximize $-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T(\mathbf{x}-\boldsymbol{\mu})$ which is the just the L2 loss between $\mathbf{x}$ and $\boldsymbol{\mu}$.
Finally the expectation is just approximated by averaging.
","In Variational Autoencoders (VAEs), the decoder output is typically the mean ($\mu$) of a multivariate Gaussian distribution. The probability of the input data given the latent code is then calculated as the negative log-likelihood of the Gaussian distribution, which simplifies to minimizing the squared error between the input data and the mean. This optimization problem is equivalent to minimizing the L2 loss between the input data and the mean. Since the expectation in the VAE loss function is approximated by averaging, the decoder is trained to produce the mean of the distribution that best approximates the input data."
Can a convolutional neural network take as input images of different sizes?,https://stats.stackexchange.com/questions/250258/can-a-convolutional-neural-network-take-as-input-images-of-different-sizes,neural-networks|convolutional-neural-network|computer-vision,250258,True,250301,19412,2,13,1499159293,1481141295,2,1481155920,"
Are there any issues in using different sizes ? If there are, how should I approach this problem (since formulas won't all fit in the same image size) ?

It depends on the architecture of the neural network. Some architectures assume that all images have the same dimension, other (such as im2markup) don't make such an assumption. The fact that im2markup allow images of different widths don't bring any issue I believe, since they use an RNN that scans through the output of the convolution layer.
 (https://i.sstatic.net/PUVuU.png)

group images of similar sizes to facilitate batching

That's typically to speed things up by avoid adding too much padding.
","**Summary:**

The usage of images with different sizes depends on the neural network architecture. Some architectures require images of the same size, while others, like im2markup, do not. If a network allows varying image widths, it can be beneficial to group images of similar sizes together for batching purposes. This helps optimize performance by minimizing the need for padding. However, if a network requires images of the same size, different-sized images must be either resized or cropped to fit. It's important to consider the architectural constraints and trade-offs when working with images of different sizes."
Foundation models : Is it a new paradigm for statistics and machine learning?,https://stats.stackexchange.com/questions/544957/foundation-models-is-it-a-new-paradigm-for-statistics-and-machine-learning,machine-learning|neural-networks|artificial-intelligence,544957,True,545221,3636,2,12,1704320880,1631860837,12,1632065258,"The Bitter Lesson (http://www.incompleteideas.net/IncIdeas/BitterLesson.html)  is that in the long term, progress is dependent on leveraging more and more computational power. This is not to say that algorithmic and modeling progress isn't important, but they aren't the limiting factor -- neural networks have been since the 1950s (or earlier), and it's only now that increasing computation resources have let us exploit them fully.
The scaling hypothesis (https://www.gwern.net/Scaling-hypothesis#:%7E:text=The%20scaling%20hypothesis%3A%20neural%20nets,revolution%20has%20begun%20as%20foretold.) is the proposal that current models are only being held back by computation, and if we had several orders of magnitude more, we'd see dramatic improvements in modeling performance. This was explored and borne out by recent explorations into increasingly large language models.
 (https://i.sstatic.net/LMjgRm.png)
(figure from here (https://arxiv.org/abs/2005.14165))
These recent large scale language models also demonstrate impressive few-shot or zero-shot capabilities, which validates the scaling hypothesis, and it sounds like the linked article concludes these ""Foundation models"" will come to replace more bespoke, individually trained models (although of course, no one is arguing that big models are going to replace the t-test).
Personally, I think there is a mountain of evidence for the bitter lesson, and for the scaling hypothesis, and these large language models are definitely very impressive. I don't have any opinion on whether this constitutes a new ""paradigm"" though (ideas like ""the bitter lesson"" have been floating around for many years, although the exploitation of supervised pretraining is relatively new), or whether these models will replace all others in the near future.
","**Summary:**

The ""Bitter Lesson"" states that progress in AI is ultimately limited by computational power. While algorithmic advancements are important, neural networks have been around for decades, and only recent increases in computational resources have allowed us to fully exploit them.

The ""Scaling Hypothesis"" proposes that limitations in current models stem from insufficient computation. Increasing computation by orders of magnitude would lead to significant performance gains. Recent large language models support this hypothesis, demonstrating impressive ""few-shot"" and ""zero-shot"" capabilities.

These models indicate that ""Foundation models"" may replace more specialized models in the future. Supporting evidence exists for the ""Bitter Lesson"" and ""Scaling Hypothesis."" Whether this constitutes a paradigm shift or complete model replacement remains unclear."
"Is it correct to say the Neural Networks are an alternative way of performing Maximum Likelihood Estimation? if not, why?",https://stats.stackexchange.com/questions/402511/is-it-correct-to-say-the-neural-networks-are-an-alternative-way-of-performing-ma,neural-networks|maximum-likelihood,402511,True,402603,9019,3,12,1555311824,1555003027,19,1555051188,"There seems to be a misunderstanding concerning the actual question behind. There are two questions that OP possibly wants to ask:

Given a fixed other parametrized model class that are formulated in a probabilistic way, can we somehow use NNs to very concretely optimize the Likelihood of the parameters? Then as @Cliff AB posted: This seems strange and unnatural for me. NNs are there for approximizing functions. However, I strongly believe that this was not the question.
Given a concrete dataset consisting of 'real' answers $y^{(i)}$ and real $d$-dimensional data vectors $x^{(i)} = (x^{(i)}_1, ..., x^{(i)}_d)$, and given a fixed architecture of a NN, we can use the cross entropy function in order to find the best parameters. Question: Is this the same as maximizing the likelihood of some probabilistic model (this is the question in the post linked in the comments by @Sycorax).

Since the answer in the linked thread is also somewhat missing insight let me try to answer that again. We are going to consider the following very simple neural network with just one node and sigmoid activation function (and no bias term), i.e. the weights $w = (w_1, ..., w_d)$ are the parameters and the function is:
  $$f_w(x) = \sigma\left(\sum_{j=1}^d w_j x_j\right)$$
The cross entropy loss function is
  $$l(\hat{y}, y) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})] $$
So given the dataset $y^{(i)}, x^{(i)}$ as above, we form
  $$\sum_{i=1}^n l(y^{(i)}, f_w(x^{(i)}))$$
and minimize that in order to find the parameters $w$ for the neural network. Let us put that aside for a moment and go for a completely different model.
We assume that there are random variables $(X^{(i)}, Y^{(i)})_{i=1,...,n}$ such that $(X^{(i)}, Y^{(i)})$ are iid. and such that
  $$P[Y^{(i)}=1|X^{(i)}=x^{(i)}] = f_w(x^{(i)})$$
where again, $\theta=w=(w_1,...,w_d)$ are the parameters of the model. Let us setup the likelihood: Put $Y = (Y^{(1)}, ..., Y^{(n)})$ and $X = (X^{(1)}, ..., X^{(n)})$ and $y = (y^{(1)}, ..., y^{(n)})$ and $x = (x^{(1)}, ..., x^{(n)})$. Since the $Z^{(i)} = (X^{(i)}, Y^{(i)})$ are independent,
\begin{align*}
  P[Y=y|X=x] &= \prod_{i=1}^n P[Y^{(i)}=y^{(i)}|X^{(i)}=x^{(i)}] \\
    &= \prod_{\{i : y^{(i)}=1\}} P[Y^{(i)}=1|X^{(i)}=x^{(i)}] \prod_{\{i:y^{(i)}=0\}} (1 - P[Y^{(i)}=1|X^{(i)}=x^{(i)}]) \\
    &= \prod_{\{i : y^{(i)}=1\}} f_w(x^{(i)}) \prod_{\{i:y^{(i)}=0\}} (1 - f_w(x^{(i)})) \\
    &= \prod_{i=1}^n \left(f_w(x^{(i)})\right)^{y^{(i)}} \left(1 - f_w(x^{(i)})\right)^{1 - y^{(i)}}
\end{align*}
So this is the likelihood. We would need to maximize that, i.e. most probably we need to compute some gradients of that expression with respect to $w$. Uuuh, there is an ugly product in front... The rule $(fg)' = f'g + fg'$ does not look very appealing. Hence we do the following (usual) trick: We do not maximize the likelihood but we compute the log of it and maximize this instead. For technical reasons we actually compute $-\log(\text{likelihood})$ and minimize that... So let us compute $-\log(\text{likelihood})$: Using $\log(ab) = \log(a) + \log(b)$ and $\log(a^b) = b\log(a)$ we obtain
\begin{align*}
   -\log(\text{likelihood}) &= -\log \left( \prod_{i=1}^n \left(f_w(x^{(i)})\right)^{y^{(i)}} \left(1 - f_w(x^{(i)})\right)^{1 - y^{(i)}} \right) \\
  &= - \sum_{i=1}^n y^{(i)} \log(f_w(x^{(i)})) + (1-y^{(i)}) \log(1-f_w(x^{(i)}))
\end{align*}
and if you now compare carefully to the NN model above you will see that this is actually nothing else than $\sum_{i=1}^n l(y^{(i)}, f_w(x^{(i)}))$.
So yes, in this case these two concepts (maximizing a likelihood of a probabilistic model and minimizing the loss function w.r.t. a model parameter) actually coincide. This is a more general pattern that occurs with other models as well. The connection is always
$$-\log(\text{likelihood}) = \text{loss function}$$
and
  $$e^{-\text{loss function}} = \text{likelihood}$$
In that sense, statistics and machine learning are the same thing, just reformulated in a quirky way. Another example would be linear regression: There also exists a precise mathematical description of the probabilistic model behind it, see for example Likelihood in Linear Regression (https://stats.stackexchange.com/questions/305908/likelihood-in-linear-regression).
Notice that it may be pretty hard to figure out a natural explanation for the probabilistic version of a model. For example: in case of SVMs, the probabilistic description seems to be Gaussian Processes: see here (http://mlss2011.comp.nus.edu.sg/uploads/Site/lect1gp.pdf).
The case above however was simple and what I have basically shown you is logistic regression (because a NN with one node and sigmoid output function is exactly logistic regression!). It may be a lot harder to interpret complicated architectures (with tweaks like CNNs etc) as a probabilistic model.
","**Summary:**

The question centers around the relationship between optimizing neural network (NN) loss functions and maximizing the likelihood of probabilistic models.

**Two Interpretations of the Question:**

1. **Using NNs to optimize probabilistic model likelihood:** This is considered unnatural, as NNs are designed for function approximation rather than likelihood optimization.

2. **Equivalence of NN loss minimization and probabilistic model likelihood maximization:** Given a fixed NN architecture and a specific probabilistic model, minimizing the NN cross-entropy loss function is equivalent to maximizing the model's likelihood.

**Mathematical Equivalence:**

In the case of a simple NN with a single node and sigmoid activation function (equivalent to logistic regression), maximizing the probability under the model is equivalent to minimizing the cross-entropy loss function.

**Generalization to Other Models:**

This equivalence extends to other models, as the negative log-likelihood of a probabilistic model is equal to the loss function used in machine learning.

**Relationship between Statistics and Machine Learning:**

The equivalence shows that statistics and machine learning are essentially the same discipline, just expressed in different ways.

**Interpretation of Complex Architectures:**

While it's straightforward for simple NNs, interpreting the probabilistic interpretation of complex architectures like CNNs can be challenging."
How many dimensions does a neural network have?,https://stats.stackexchange.com/questions/505636/how-many-dimensions-does-a-neural-network-have,machine-learning|neural-networks,505636,True,505641,5133,2,12,1688534627,1611101657,14,1611103759,"You're talking about the dimension of the loss function, and the answer is that the input dimension of the loss function is how many parameters and biases there are, and the output dimension is $1$.
Let's focus on that first one by looking at an example with linear regression.
$$\hat y
=
\hat\beta_0 + \hat\beta_1x_1+\hat\beta_2x_2+\hat\beta_3x_3
$$
Square loss is thus
$$
L\big(\hat\beta_0, \hat\beta_1, \hat\beta_2, \hat\beta_3\big) = \sum_{i=1}^n \Bigg[\bigg(y_i - \big( \hat\beta_0 + \hat\beta_1x_{i1}+\hat\beta_2x_{i2}+\hat\beta_3x_{i3} \big)\bigg)^2 \Bigg]
$$
That is a function from $\mathbb{R}^4 \rightarrow \mathbb{R}$, agreed? I would call that five dimensions: four for the regression parameters and one for the output loss.
A neural network is surprisingly simular to this. The estimated value is some composition of functions, and then the loss depends on the values of the estimated parameters.
$$
\hat y =\hat b_2 + \hat w_2\text{ReLU}\big(\hat b_0 + \hat w_0x\big) + \hat w_3\text{ReLU}\big(\hat b_1 + \hat w_1x\big)
$$
(I'll post that network later, but I think it would be a useful exercise to draw out what's going on. People appear quite eager to think of neural networks as being webs of circles and lines, yet there is real math going on.)
Then the loss is a function of the weights $w$ and biases $b$. Let's do absolute loss here.
$$
L\big(
\hat b_0, \hat b_1, \hat b_2, \hat w_0, \hat w_1, \hat w_2, \hat w_3
\big) 
=$$
$$
\sum_{i=1}^n \Bigg\vert y_i - \bigg(\hat b_2 + \hat w_2\text{ReLU}\big(\hat b_0 + \hat w_0x_i\big) + \hat w_3\text{ReLU}\big(\hat b_1 + \hat w_1x_i\big)\bigg)\Bigg\vert
$$
This is a function $\mathbb{R}^7 \rightarrow \mathbb{R}$. I'd call that eight dimensions.
","The dimension of a loss function refers to the number of inputs and outputs it handles. The input dimension represents the number of parameters and biases being adjusted, while the output dimension is always 1.

For a linear regression model, the loss function takes four inputs (the regression parameters) and produces one output (the loss value). Similarly, for a neural network, the loss function takes the network's weights and biases as inputs and generates a loss value.

In the given examples:

* The linear regression loss function has an input dimension of 4 and an output dimension of 1.
* The neural network loss function has an input dimension of 7 (for the weights and biases) and an output dimension of 1.

Generally, the dimension of a neural network's loss function is determined by the number of weights and biases being adjusted in the network."
Can weight decay be higher than learning rate,https://stats.stackexchange.com/questions/324616/can-weight-decay-be-higher-than-learning-rate,neural-networks|deep-learning,324616,True,325675,27526,1,12,1681891556,1516711012,22,1517246401,"Training a neural network means minimizing some error function which generally contains 2 parts: a data term (which penalizes when the network gives incorrect predictions) and a regularization term (which ensures the network weights satisfy some other assumptions), in our case the weight decay penalizing weights far from zero. The error function may look like this:
$E=\frac{1}{N}||\mathbf{y}-\mathbf{t}||_2 + \lambda ||w||_2$,
where $\mathbf{y}$ are the network predictions, $\mathbf{t}$ are the desired outputs (ground truth), $N$ is the size of the training set, and $w$ is the vector of the network weights. The parameter $\lambda$ controls the relative importance of the two parts of the error function. Setting a weight decay corresponds to setting this parameter. If you set it to a high value, the network does not care so much about correct predictions on the training set and rather keeps the weights low, hoping for good generalization performance on the unseen data.
How the error function is minimized is an entirely separate thing. You can use a fancy method such as Adam, or you can take a simple stochastic gradient descent: both work on the same iterative principle:

Evaluate derivatives of the error function w.r.t. weights: $\frac{\partial E}{\partial w}$
Update weights in the negative direction of the derivatives by a small step.

It can be written down like this:
$w_{t+1} = w_t - \eta \frac{\partial E}{\partial w}$
Parameter $\eta$ is called learning rate: it controls the size of the step.

Thus, these two parameters are independent of each other and in principle it can make sense to set weight decay larger than learning rate. Practically, it depends entirely on your specific scenario: Which network architecture are you using? How many weights are there? What is the error function? Are you using some other regularizers? etc. It is your job to find the right hyperparameters.
","**Summary**

Training a neural network involves minimizing an error function that comprises two parts: a data term (penalizing incorrect predictions) and a regularization term (penalizing large weights). One type of regularization is weight decay, which sets a parameter Î» that balances the importance of these two terms.

Minimizing the error function involves iteratively updating the network weights in the negative direction of their gradients. The learning rate Î· controls the step size of these updates.

Weight decay and learning rate are independent parameters, and their optimal values depend on factors such as the network architecture, number of weights, error function, and other regularizers. Optimizing hyperparameters, including weight decay and learning rate, is crucial for effective neural network training and generalization."
Mean Absolute Error (MAE) derivative,https://stats.stackexchange.com/questions/312737/mean-absolute-error-mae-derivative,neural-networks|backpropagation|derivative|mae,312737,True,312779,26552,3,12,1672755831,1510184390,20,1510212698,"The mae (/questions/tagged/mae), as a function of $y_{\text{pred}}$, is not differentiable at $y_{\text{pred}}=y_{\text{true}}$. Elsewhere, the derivative is $\pm 1$ by a straightforward application of the chain rule (https://en.wikipedia.org/wiki/Chain_rule):
$$\dfrac{d\text{MAE}}{dy_{\text{pred}}} = 
\begin{cases}
  +1,\quad y_{\text{pred}}>y_{\text{true}}\\
  -1,\quad y_{\text{pred}}<y_{\text{true}}
\end{cases}$$
The interpretation is straightforward: if you are predicting too high ($y_{\text{pred}}>y_{\text{true}}$), then increasing $y_{\text{pred}}$ yet more by one unit will increase the MAE by an equal amount of one unit, so the gradient encourages you to reduce $y_{\text{pred}}$. And vice versa if $y_{\text{pred}}<y_{\text{true}}$.
Skimming the paper you link, it seems like they approximate the MAE by a differentiable function to avoid the ""kink"" at $y_{\text{pred}}=y_{\text{true}}$.
As to what specifically is implemented in TensorFlow and keras, that is off-topic here. Best to consult the documentation, the source code or any specific help community.
","**Summary:**

The Mean Absolute Error (MAE) function is not differentiable at the point where the predicted value ($y_{\text{pred}}$) equals the true value ($y_{\text{true}}$). Outside this point, the derivative of MAE is +1 if $y_{\text{pred}}$ is greater than $y_{\text{true}}$, and -1 if $y_{\text{pred}}$ is less than $y_{\text{true}}$.

This indicates that increasing $y_{\text{pred}}$ too much will increase the MAE, while decreasing it too much will also increase the MAE.

To avoid this non-differentiability issue, some methods approximate the MAE with differentiable functions. The specific implementation of this approximation in TensorFlow and Keras can be found in their documentation or source code."
How to train an SVM via backpropagation?,https://stats.stackexchange.com/questions/158712/how-to-train-an-svm-via-backpropagation,machine-learning|neural-networks|svm|gradient-descent|backpropagation,158712,True,158721,5410,2,12,1484064268,1435271615,16,1435283934,"You are correct that if you try to directly optimize the SVM's accuracy on training cases, also called the 0-1 loss, the gradient vanishes. This is why people don't do that. :)
What you're trying to do, though, isn't really an SVM yet; it's rather just a general linear classifier. An SVM in particular arises when you replace the 0-1 loss function with a convex surrogate known as hinge loss (https://en.m.wikipedia.org/wiki/Hinge_loss); this amounts to the idea of margin maximimization which is core to the idea of an SVM. This loss function is (almost) differentiable; the only issue is if any outputs are exactly at the hinge point, which (a) happens with probability zero under most reasonable assumptions and (b) then you can just use either 0 or 1 as the derivative (or anything in between), in which case you're technically doing subgradient descent.
Since you're talking about backpropagation, I'll assume you're at least a little familiar with optimizing neural networks. The same problem occurs with neural network classifiers as well; this is why people use other loss functions there too.
","**Summary:**

SVMs rely on hinge loss, a convex surrogate for the 0-1 loss function used in traditional linear classifiers. This switch enables margin maximization, the core concept of SVMs. Hinge loss, while differentiable in most cases, experiences non-differentiability when outputs are exactly at the hinge point.

Neural networks face a similar issue. To overcome this, alternative loss functions are employed. The 0-1 loss, when directly optimized for, results in vanishing gradients, rendering SVM optimization ineffective. Therefore, hinge loss is used instead, introducing differentiability for effective optimization."
Mathematical background for neural networks,https://stats.stackexchange.com/questions/64246/mathematical-background-for-neural-networks,machine-learning|neural-networks|mathematical-statistics|references,64246,True,64363,19446,5,12,1492274874,1373746184,10,1373893579,"The second reference you give is, in my opinion, still the best book on NN, even though it might be a bit outdated and does not deal with more recent developments like deep architectures. You will get the basics right, and become familiar with all the basic concepts around machine learning.
If you go through the book, you will need linear algebra, multivariate calculus and basic notions of statistics (conditional probabilities, bayes theorem and be familiar with binomial distributions). At some points it deals with calculus of variations. The appendix on calculus of variations should be enough though.
","Despite being outdated and lacking coverage of recent advancements like deep architectures, the recommended book remains a valuable resource for understanding the fundamentals of Neural Networks (NN). It introduces key machine learning concepts and requires a foundation in linear algebra, multivariate calculus, and basic statistics (conditional probabilities, Bayes' theorem, and binomial distributions). While the appendix provides an overview of calculus of variations, it suffices for readers' comprehension."
Why is resnet faster than vgg,https://stats.stackexchange.com/questions/280179/why-is-resnet-faster-than-vgg,deep-learning|computer-vision,280179,True,280338,28452,1,12,1619965009,1495045682,19,1495120041,"Updated in order to address @mrgloom's comment
In my original answer, I stated that VGG-16 has roughly 138 million parameters and ResNet has 25.5 million parameters and because of this it's faster, which is not true. Number of parameters reduces amount of space required to store the network, but it doesn't mean that it's faster. Resnet is faster than VGG, but for a different reason.
Also, as @mrgloom pointed out that computational speed may depend heavily on the implementation. Below I'll discuss simple computational case. Also, I'll avoid counting FLOPs for activation functions and pooling layers, since they have relatively low cost.
First of all, speed of the convolution will depend on the size of the input. Let's say you have gray colored (1 channel) 100x100 image and you apply one 3x3 convolution filter with stride 1 and 0 padding. This operation will require you ~163k FLOPs. It's easy to calculate. If you know how convolution works you should know that from the 100x100 image you will get 98x98 (using the setup described above). And in order to compute each value from the 98x98 output image you need to do 9 multiplications and 8 additions, which in total corresponds to 17 operations per value. If you combine everything you get 98 * 98 * 17 and it equals to 163,268. Now, imagine you apply the same filter on the larger image, let's say 200x200. Image has 4 times bigger area and therefor you'll get roughly 4 times more FLOPs.

Now, I'll start with the comparison between VGG19 and Resnet 34, since that's the image that they use in the original paper (https://arxiv.org/pdf/1512.03385.pdf).
In the figure 3, they break down architecture into the set of blocks marked with different colors. At the end each block reduces height and width by a factor of two. In the first two layers Resnet manages to reduces hight and width of the image by a factor of 4.

From the VGG19 you can see that first two layers apply convolution on top of the full 224x224 image which is quite expensive. If you apply similar calculations as I did above you will find that first layer does ~170M FLOPs, but it produces 64x224x224 output from the 3x224x224 image. Since layer applies the same conv filter you should see that number of FLOPs should be close to 170M * (64 / 3). In fact, it's almost 3.7B FLOPs. This layer alone has roughly as many FLOPs as whole Resnet-34. In order to avoid this computational problem in the Resnet they address this issue in the first layer. It reduces number of row and columns by a factor of 2 and it uses only 240M FLOPs and next max pooling operation applies another reduction by factor of 2.
In contrast, these four convolutional layers in the VGG19 make around 10B FLOPs.
Next, convolutional filters in the Resnet build up slowly. You can see that they use less kernels compare to the VGG, but they have more of them stacked alternating between convolutional operation and non-linear activation functions. That's another thing that was pointed out by @mrgloom. They exploited idea of using thinner, but deeper networks.

Next Resnet layers follow the same strategy, trying to make it thinner and deeper.
In addition, from Table 1 in the paper you can notice that convolutional blocks for Renet 50, Resnet 101 and Resnet 152 look a bit different. That's how it looks visually.

It was used in order to reduce number of operations even more, while using networks with larger number of filters in the convolutional layers. The idea of 1x1 convolutional layer allow to reduce channel depth before applying 3x3 convolution and upscale it back afterwords.
","VGG-16 and ResNet are two convolutional neural network architectures. VGG-16 has more parameters (138 million) than ResNet (25.5 million), but this does not indicate faster performance. Computational speed is primarily determined by the input size and network architecture.

ResNet is faster than VGG-16 because it addresses the computational cost of applying convolutions to large input images. In the first layer of VGG-16, the full 224x224 image undergoes convolution, resulting in approximately 3.7 billion floating-point operations (FLOPs). In contrast, ResNet first reduces image size and uses fewer kernels in its initial layers, resulting in significantly reduced computational costs.

ResNet also employs a ""thinner, deeper"" approach, using more convolutional layers with fewer filters stacked together. This reduces the number of FLOPs required for each layer while maintaining overall network depth.

In addition, ResNet uses 1x1 convolutional layers to reduce channel depth before applying 3x3 convolutions, further optimizing computational efficiency. This approach allows ResNet to use more filters in its convolutional layers while reducing the number of operations required."
What are the senones in a Deep Neural Network?,https://stats.stackexchange.com/questions/183438/what-are-the-senones-in-a-deep-neural-network,neural-networks|deep-learning|terminology|natural-language|hidden-markov-model,183438,True,188439,10504,4,12,1718551865,1448404605,7,1451303844,"This is what I've finally understood:
In this architecture the DNN is used to transform the noise into phones. 

In phonetics and linguistics, the word phone may refer to any speech sound or gesture considered as a physical event without regard to its place in the phonology of a language. 

 (https://i.sstatic.net/GJrkA.png)
The last layer of the DNN is formed by all the possibles phones, having one output neuron per phone. The activation of these neurons is the probability of that the input noise corresponding to that phone. 
The combination of these activations is the input of the Hidden Markov Model and establishes the senones of the HMM, which obtains a list of candidate texts by means of a dictionary. 
The senones are the states of the HMM, in the following image, the senones would be x1 x2 and x3.
 (https://i.sstatic.net/775Ij.png)
Please correct me if I've said something wrong, hope it helps!
","**Summary:**

A Deep Neural Network (DNN) is utilized to transform noise into phonemes, which are speech sounds or gestures independent of their linguistic context. Each output neuron in the DNN's final layer corresponds to a possible phoneme, and the activation of these neurons represents the probability of the input noise matching that phoneme.

The combination of these activations serves as the input for a Hidden Markov Model (HMM). The HMM generates a list of potential text candidates using a dictionary. The HMM states, known as senones, are represented by the input activations. The senones, like the states in the HMM diagram, model the probable sequences of phonemes in the input noise. The HMM produces a list of text candidates, providing a textual representation of the recognized speech."
Difference between Naive Bayes vs Recurrent Neural Network (LSTM),https://stats.stackexchange.com/questions/285271/difference-between-naive-bayes-vs-recurrent-neural-network-lstm,machine-learning|neural-networks|python|natural-language,285271,True,285536,11394,1,12,1497537552,1497427556,18,1497537552,"On the difference between Naive Bayes and Recurrent Neural Networks
First of all let's start off by saying they're both classifiers, meant to solve a problem called statistical classification. This means that you have lots of data (in your case articles) split into two or more categories (in your case positive/negative sentiment). The classifier's goal is to learn how the articles are split into those two categories and then be able to classify new articles on it's own.
Two models that can solve this task are the Naive Bayes classifier and Recurrent Neural Networks.
Naive Bayes
In order to use this classifier for text analysis, you usually pre-process the text (bag of words (https://en.wikipedia.org/wiki/Bag-of-words_model) + tf-tdf (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) so that you can transform it into vectors containing numerical values. These vectors serve as an input to the NB model.
This classifier assumes that your features (the attributes of the vectors we produced) are independent of one another. When this assumption holds, it is a very strong classifier that requires very little data to work.
Recurrent Neural Networks
These are networks that read your data sequentially, while keeping a ""memory"" of what they have read previously. These are really useful when dealing with text because of the correlation words have between them. 
The two models (NB and RNN) differ greatly in the way they attempt to perform this classification:

NB belongs to a category of models called generative. This means that during training (the procedure where the algorithm learns to classify), NB tries to find out how the data was generated in the first place. It essentially tries to figure out the underlying distribution that produced the examples you input to the model.
On the other hand RNN is a discriminative model. It tries to figure out what the differences are between your positive and negative examples, in order to perform the classification.

I suggest querying ""discriminative vs generative algorithms"" if you want to learn mire
While NB has been popular for decades RNNs are starting to find applications over the past decade because of their need for high computational resources. RNNs most of the time are trained on dedicated GPUs (which compute a lot faster than CPUs).
tl;dr: they are two very different ways of solving the same task
Libraries
Because the two algorithms are very popular they have implementations in many libraries. I'll name a few python libraries since you mentioned it:
For NB:

scikit-learn: is a very easy to use python library containing implementations of several machine learning algorithms, including Naive Bayes.
NaiveBayes: haven't used it but I guess it's relevant judging by the name.

Because RNNs are considered a deep learning algorithm, they have implementations in all major deep learning libraries:

TensorFlow: Most popular DL library at the moment. Published and maintained by google.
theano: Similar library to tf, older, published by the University of Montreal.
keras: Wrapper for tf and theano. Much easier. What I suggest you use if you ever want to implement RNNs.
caffe: DL library published by UC Berkeley. Has python API.

All the above offer GPU support if you have a CUDA enabled NVIDIA GPU.
Python's NLTK is a library mainly for Natural Language Processing (stemming, tokenizing, part-of-speach tagging). While it has a sentiment package, it's not the focus point. I'm pretty sure NLTK uses NB for sentiment analysis.
","**Summary:**

Naive Bayes and Recurrent Neural Networks (RNNs) are classifiers used to categorize data into different classes (e.g., positive/negative sentiment).

**Naive Bayes:**

* Assumes feature independence.
* Generates the underlying distribution of the data.
* Requires less data and computational resources.
* Popular for decades.

**RNNs:**

* Reads data sequentially and retains memory.
* Identifies differences between classes to perform classification.
* Requires high computational resources and is often trained on GPUs.
* Relatively newer than Naive Bayes.

Both algorithms have Python library implementations:

* Naive Bayes: scikit-learn, NaiveBayes
* RNNs: TensorFlow, theano, keras, caffe (all with GPU support)

In Natural Language Processing, NLTK uses Naive Bayes for sentiment analysis."
Training a neural network on chess data,https://stats.stackexchange.com/questions/479051/training-a-neural-network-on-chess-data,machine-learning|neural-networks|python|large-data|adam,479051,True,479052,1916,2,12,1596046778,1595748581,15,1595750487,"I think you need to consider running it on a GPU. Google Colab is free and Amazon AWS is very cheap. You seem to know what you are doing so you can probably get up and running with PyTorch very quickly. Once you compare the performance of the same network implemented on GPU vs your single processor setup, you will be in a better to position to know where to go next.
","**Summary:**

The author suggests using a Graphical Processing Unit (GPU) for faster performance when running a neural network. They recommend using Google Colab, a free service, or Amazon AWS, a low-cost service, to run the network. The author believes that the improved performance will provide insights into the best course of action.

The main points are:

* Consider using a GPU for faster neural network execution.
* Google Colab and Amazon AWS are economical options.
* Comparing performance with and without a GPU will help determine the next steps."
Binary Encoding vs One-hot Encoding,https://stats.stackexchange.com/questions/325263/binary-encoding-vs-one-hot-encoding,machine-learning|neural-networks|classification|categorical-encoding,325263,True,325296,34781,1,12,1666909787,1517003715,17,1517039949,"If you have a system with $n$ different (ordered) states, the binary encoding of a given state is simply it's $\text{rank number} - 1$ in binary format (e.g. for the $k$th state the binary $k - 1$). The one hot encoding of this $k$th state will be a vector/series of length $n$ with a single high bit (1) at the $k$th place, and all the other bits are low (0).
As an example encodings for the next system (levels of education):




Level
""Decimal encoding""
Binary encoding
One hot encoding




No
0
000
000001


Primary
1
001
000010


Secondary
2
010
000100


BSc/BA
3
011
001000


MSc/MA
4
100
010000


PhD
5
101
100000




References: One hot encoding at Wikipedia (https://en.wikipedia.org/wiki/One-hot)
And a 2017 paper on the comparison on the effects of different encodings to neural networks in the International Journal of Computer Applications could be a good starting point:
A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers (https://www.researchgate.net/profile/Kedar_Potdar/publication/320465713_A_Comparative_Study_of_Categorical_Variable_Encoding_Techniques_for_Neural_Network_Classifiers/links/59e6f9554585151e5465859c/A-Comparative-Study-of-Categorical-Variable-Encoding-Techniques-for-Neural-Network-Classifiers.pdf)
","**Summary of Encoding Techniques**

In systems with ordered states, binary encoding represents a state as its rank, expressed in binary. One hot encoding uses a vector where a single `1` indicates the state, while all other elements are `0`.

**Example:**

Consider levels of education:

| Level | Binary Encoding | One Hot Encoding |
|---|---|---|
| No | 000 | 000001 |
| Primary | 001 | 000010 |
| Secondary | 010 | 000100 |
| BSc/BA | 011 | 001000 |
| MSc/MA | 100 | 010000 |
| PhD | 101 | 100000 |

**Advantages of One Hot Encoding:**

* Easy to understand and implement.
* Efficient in space and computation.
* Avoids the need for additional embedding layers."
How does the DepthConcat operation in &#39;Going deeper with convolutions&#39; work?,https://stats.stackexchange.com/questions/184823/how-does-the-depthconcat-operation-in-going-deeper-with-convolutions-work,neural-networks|torch|convolutional-neural-network,184823,True,193454,10216,2,12,1469266056,1449144219,10,1454325374,"I don't think the output of the inception module are of different sizes.  
 (https://i.sstatic.net/OWRNZ.png) 
For convolutional layers people often use padding to retain the spatial resolution.
The bottom-right pooling layer (blue frame) among other convolutional layers might seem awkward. However unlike conventional pooling-subsampling layers (red frame, stride>1), they used a stride of 1 in that pooling layer. Stride-1 pooling layers actually work in the same manner as convolutional layers, but with the convolution operation replaced by the max operation.
So the resolution after the pooling layer also stays unchanged, and we can concatenate the pooling and convolutional layers together in the ""depth"" dimension.
 (https://i.sstatic.net/3oiu4.png)
As shown in the above figure from the paper, the inception module actually keeps the spatial resolution.
","An inception module in a neural network consists of multiple convolutional layers and a pooling layer. The convolutional layers are typically padded to maintain the spatial resolution (size) of the input. The pooling layer used in inception modules has a stride of 1, which means it operates like a convolutional layer but with the convolution operation replaced by a max operation. This ensures that the spatial resolution after the pooling layer remains the same as the input. As a result, the output of an inception module maintains the spatial resolution of the input, allowing for concatenation of the convolutional and pooling layer outputs along the ""depth"" dimension."
How to classify a unbalanced dataset by Convolutional Neural Networks (CNN)?,https://stats.stackexchange.com/questions/134599/how-to-classify-a-unbalanced-dataset-by-convolutional-neural-networks-cnn,neural-networks|classification|computer-vision|convolution,134599,True,168667,17816,3,12,1727194166,1421984290,11,1440499193,"This happens because when you take a mini-batch, it is very very less likely (given the ratio of the proportions here) that a mini batch will contain samples of your positives at all. So it will end up learning the pattern for the negative class and after a couple of epochs, everything just gets classified as negative class.
There are two possible ways to handle such a situation. 

Given the proportions as 0.3% to 99.7%, this is a very highly skewed data set. You hardly have 3 samples of positive classes for every 1000 samples. I would say you should look at balancing the data set by getting more positive classes. Go and get as many positive samples as you can. Then, you can use a more balanced dataset. For example, you could get 1000 positive samples and then pick a random set of 1000 negative samples and build the classifier. Now, it should be able to learn both the classes.
Use a weighted error measure when updating the weights after a mini-batch. The weights are updated in proportions to the number of samples of the positive and negative classes during any mini-batch. Now, in the current situation given the proportions as 3:1000, even this trick may not work. So you may try getting the proportions to something like 300:1000 by getting 297 more positive samples and combining them with 1000 negative samples. Then with 300:1000 ratio you should weight the error during mini-batches based on number of samples in each of the classes. This should work.

","Highly imbalanced datasets, like those with a 0.3% to 99.7% positive-to-negative class ratio, pose challenges for training classifiers. During minibatch processing, it becomes highly unlikely that the batch will contain positive class samples. Consequently, the classifier primarily learns the negative class pattern, leading to inaccurate positive class classification.

To address this, two strategies are recommended:

1. **Data Balancing:** Acquire additional positive class samples to create a more balanced dataset, ensuring a sufficient number of positive samples in each minibatch for effective learning.

2. **Weighted Error Measure:** Implement a weighted error measure during minibatch weight updates. This assigns weights to errors based on the proportion of positive and negative samples in the minibatch. This technique can alleviate the impact of the class imbalance, improving the classifier's ability to learn both classes effectively."
Purpose of L2 normalization for triplet network,https://stats.stackexchange.com/questions/248511/purpose-of-l2-normalization-for-triplet-network,neural-networks|deep-learning|normalization|image-processing,248511,True,263363,8877,2,12,1488188743,1480396196,14,1487751057,"The squared Euclidean distance between normalized vectors is proportional to their cosine similarity (ref: wikipedia (https://en.wikipedia.org/wiki/Cosine_similarity#Properties)), 
$$\|\frac{A}{\|A\|}-\frac{B}{\|B\|}\|^2 = \|\frac{A}{\|A\|}\|^2+\|\frac{B}{\|B\|}\|^2-2\frac{A\cdot B}{\|A\|\|B\|}=2-2\frac{A\cdot B}{\|A\|\|B\|}$$
so the advantage of using normalization is more or less the advantage of cosine similarity over Euclidean distance. As mentioned in Andy Jones's answer, without normalization scaling the margin by a factor would just scale the embedding correspondingly.
Another nice property is, with such normalization the value of squared Euclidean distance is guaranteed to be within range $[0,4]$, which saves us much effort from choosing a proper margin parameter $\alpha$. 
For instance, in another paper (https://arxiv.org/abs/1406.4773) referenced by this paper, it uses  what called the spring model which is based on (unnormalized) squared Euclidean distance, where one of the practical difficulties is in determining a proper margin and split point since the embedding constantly changes as the training proceeds.
If you're looking for implementing the normalization layer yourself, here's a blog (http://freesouls.github.io/2015/08/30/caffe-implement-l2-normlization-layer/) about the derivations and implementation in Caffe (part of the blog is in Chinese but it won't affect reading).
","**Summary:**

Normalization of vectors provides several advantages:

* **Relationship to cosine similarity:** The squared Euclidean distance between normalized vectors is proportional to their cosine similarity, which measures their angular similarity. This makes normalization beneficial for tasks where cosine similarity is useful.

* **Scaling invariance:** Scaling the embedding by a factor does not affect the normalized distance between vectors. This ensures that scaling does not distort the relationships between vectors.

* **Bounded distance range:** Normalization guarantees that the squared Euclidean distance between vectors is within the range [0, 4]. This simplifies the choice of margin parameters in machine learning models.

* **Implementation:** The blog referenced provides guidance for implementing the normalization layer in Caffe, a deep learning framework."
"Second order approximation of the loss function (Deep learning book, 7.33)",https://stats.stackexchange.com/questions/404781/second-order-approximation-of-the-loss-function-deep-learning-book-7-33,neural-networks|deep-learning|loss-functions|derivative,404781,True,404783,1749,1,12,1556105648,1556101817,15,1556102097,"They talk about the weights at optimum:

We can model the cost function $J$ with a quadratic approximation in the neighborhood of the empirically optimal value of the weights $w^â$

At that point, the first derivative is zeroâthe middle term is thus left out.
","**Summary:**

The goal is to model a cost function using a quadratic approximation centered around the optimal weight values ($w^*$). This approximation results in a simplified function where the first derivative is zero at $w^*$. The middle term in the quadratic equation is therefore omitted.

This approximation allows for a simplified understanding and analysis of the cost function's behavior near the optimal weights. It enables researchers to make inferences and predictions regarding the cost function without the need for complex mathematical calculations.

In essence, this approximation provides a more manageable and interpretable representation of the cost function, facilitating further exploration and optimization efforts."
Interpretation of R-squared score of a Neural Network for classification,https://stats.stackexchange.com/questions/273133/interpretation-of-r-squared-score-of-a-neural-network-for-classification,classification|neural-networks|r-squared,273133,True,273934,18119,1,12,1519733201,1491920499,15,1492292277,"$R^2$ is not a good measure to assess goodness of fit for a classification.   
$R^2$ is suitable for predicting continuous variable. When dependent variable is continuous $R^2$ usually takes values between $0$ and $1$ (in linear regression for example it is impossible to have $R^2$ beyond these boundaries), and it is interpreted as share of variance of dependent variable that model is able to correctly reproduce. When $R^2$ equals $1$ it means that the model is able to fully recreate dependent variable, when it equals $0$, it means that the model completely failed at this task.
When the dependent variable is categorical it makes no sense, because $R^2$ uses distances between predicted and actual values, while distances between let say '1' meaning class 'A', '2' meaning class 'B' and '3' meaning class 'C' make no sense.
Use other measures, for example AUC for classification with two classes and Logarithmic Loss for classification with more classes. And make sure that you are using appropriate parameters in your model: in many machine learning models you must declare if problem is of classification or regression nature and it can affect results dramatically.
","**Summary:**

$R^2$ is not a suitable goodness-of-fit metric for classification models. This is because $R^2$ measures the proportion of explained variance in continuous variables, which is not meaningful for categorical variables. When the dependent variable is categorical, distances between predicted and actual values cannot be calculated, making $R^2$ an irrelevant measure.

For classification problems, it is preferable to use alternative metrics such as the Area Under the Receiver Operating Characteristic Curve (AUC) for two-class problems and Logarithmic Loss for multi-class problems. These metrics are specifically designed for evaluating the performance of classification models.

Additionally, when using machine learning models for classification tasks, it is essential to ensure the correct parameters are specified. This includes declaring whether the problem is a classification or regression type, as this can significantly impact the model's results."
What is the information storage capacity of a neural network?,https://stats.stackexchange.com/questions/242063/what-is-the-information-storage-capacity-of-a-neural-network,neural-networks|information-theory,242063,True,242069,3931,2,12,1651070002,1477311122,9,1477313862,"Well, a table is definitely not the right way to look at this. Consider the function $f(x)=x^2$. I can create an infinite table of input-output pairs that are represented by this function. However I can only represent exactly 1 such table with this function. Now consider this function: $g(x)=c\cdot x^2$. For different values of $c$, this function can represent an infinite number of tables (even an uncountable number).
The best way that I can come up with to describe the information storage capacity of a neural network is to quote the universal approximation theorem: https://en.wikipedia.org/wiki/Universal_approximation_theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem). To summarize it, say we have an arbitrary continuous function and we want to approximate the output of this function. Now say that for every input, the output of our approximation shouldn't deviate more than some given $\epsilon>0$. Then we can create a neural network with a single hidden layer that satisfies this constraint, no matter the continuous function, no matter how small the error tolerance. The only requirement is that the amount of nodes in the hidden layer might grow arbitrarily large if we choose the error-rate smaller and smaller.
","**Summary:**

Traditional tables cannot adequately represent functions like $f(x) = x^2$ because they can only capture a single table of input-output pairs. In contrast, functions like $g(x) = c \cdot x^2$ can represent countless tables based on the value of $c$.

The Universal Approximation Theorem describes the remarkable information storage capacity of neural networks. According to this theorem, a single-hidden-layer neural network can approximate any continuous function with arbitrary accuracy, provided it has a sufficient number of hidden nodes.

This means that neural networks have the potential to learn any relationship between inputs and outputs, making them highly versatile tools for modeling and prediction. The only limitation is that as the desired accuracy increases, the number of hidden nodes may need to increase as well."
difference between neural network and deep learning,https://stats.stackexchange.com/questions/234615/difference-between-neural-network-and-deep-learning,machine-learning|neural-networks|deep-learning|convolutional-neural-network|deep-belief-networks,234615,True,234620,12663,4,12,1532520275,1473705363,8,1473707620,"Deep learning = deep artificial neural networks + other kind of deep models (https://en.wikipedia.org/wiki/Deep_learning).
Deep artificial neural networks = artificial neural networks with more than 1 layer. (see minimum number of layers in a deep neural network (https://stats.stackexchange.com/q/229619/12359))
","Deep learning involves complex models, including deep artificial neural networks and other types of deep models. These neural networks possess multiple layers, distinguishing them from simpler artificial neural networks with only one layer. The minimum number of layers required to define a network as ""deep"" is not explicitly specified but is generally recognized to be more than one."
Are graphical models and Boltzmann machines related mathematically?,https://stats.stackexchange.com/questions/233077/are-graphical-models-and-boltzmann-machines-related-mathematically,machine-learning|neural-networks|mathematical-statistics|graphical-model|restricted-boltzmann-machine,233077,True,234954,1210,2,12,1513480871,1472836887,8,1473861316,"Boltzmann machines vs. restricted Boltzmann machines 
AFAIK the Boltzmann machines is a type of graphical model, and the model that's related to neural networks is the restricted Boltzmann machines (RBM).
The difference between Boltzmann machines and restricted Boltzmann machines, from the book Machine Learning A Probabilistic Perspective
 (https://i.sstatic.net/JVEkL.png)
RBMs vs. neural netowrks 
For RBMs (ref: A Practical Guide to Training Restricted Boltzmann Machines by Geoffrey Hinton (https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf))
$$p(\mathbf{v},\mathbf{h})=\frac{1}{Z}\exp(\sum a_iv_i+\sum b_jh_j + \sum v_ih_jw_{ij})$$
$$p(h_j=1|\mathbf{v})=\sigma(b_j+\sum v_iw_{ij})$$
$$p(v_i=1|\mathbf{h})=\sigma(a_i+\sum h_jw_{ij})$$
where $\mathbf{v}$ and $\mathbf{h}$ correspond to the visible and hidden units in the above figure, and $\sigma()$ is the Sigmoid function.
The conditional probabilities are computed in the same form of network layers, so the trained weights of RBMs can be used directly as the weights of neural networks or as a starting point of training.
I think the RBM itself is more of a graphical model than a type of neural network, since it is undirected, it has well defined conditional independencies, and it uses its own training algorithms (e.g. contrastive divergence).
","**Summary:**

**Boltzmann Machines (BMs):**
- Graphical models used in machine learning.

**Restricted Boltzmann Machines (RBMs):**
- A specific type of BMs with restricted connections between visible and hidden units.
- Used in neural networks as building blocks.

**Key Differences between BMs and RBMs:**
- RBMs have restricted connections between layers.
- RBMs have simpler conditional probability calculations.
- RBMs can be trained using specific algorithms like contrastive divergence.

**RBMs and Neural Networks:**
- RBMs are not strictly neural networks but can be used as building blocks for them.
- Trained weights from RBMs can be directly used in neural networks or as training starting points."
Applying machine learning for DDoS filtering,https://stats.stackexchange.com/questions/23488/applying-machine-learning-for-ddos-filtering,classification|neural-networks|unsupervised-learning,23488,True,23489,6358,2,12,1330452038,1328735261,11,1328782497,"How about anomaly detection algorithms? As you mention Andrew Ng's class you'd probably seen the ""XV. ANOMALY DETECTION"" section  on ml-class.org (http://www.ml-class.org/course/video/list), but anyway. 
Anomaly detection will be superior to a supervised classification in scenarios similar to yours because:

normally you have very few anomalies (ie., too little ""positive"" examples)
normally you have very different types of anomalies
future anomalies may look nothing like the ones you've had so far

Important point in anomaly detection is, which features to choose. Two common advices here are to choose features with 

Gaussian distribution (or distort them to be like that)
probability p(anomaly) be incomparable to p(normal) - say, anomalous values being very large while normal ones being very small (or vice versa).

I'm not sure if geolocation would help for your scenario, but client behavior would definitely matter - although it would probably differ from application to application. You may find that a ratio of GETs/POSTs matters. Or a ratio of response size to request count. Or number of single page hits. If you have such info in logs - definietly you can use the data for retrospective analysis, followed by IP blacklisting :)
","**Summary:**

Anomaly detection algorithms are more suitable than supervised classification for scenarios with limited or diverse anomalies. Unlike classification, anomaly detection does not require labeled training data.

Choosing appropriate features is crucial for effective anomaly detection. Gaussian distribution or extreme values (e.g., very large or small) can make features more discriminative.

For web application security, client behavior can provide valuable insights. Features to consider include ratios of HTTP requests (e.g., GETs/POSTs), response sizes, and single-page hits. Retrospective analysis of this data can identify anomalies and lead to IP blacklisting for suspicious activity."
"Why does $[0,1]$ scaling dramatically increase training time for feed forward ANN (1 hidden layer)?",https://stats.stackexchange.com/questions/364735/why-does-0-1-scaling-dramatically-increase-training-time-for-feed-forward-an,classification|neural-networks|scales,364735,True,364776,5561,1,12,1535728811,1535650366,11,1535669849,"We can find a reasonable explanation for this behavior in the Neural Network FAQ (ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std). TL;DR - try rescaling your data to lie in $[-1,1]$.

But standardizing input variables can have far more important effects on initialization of the weights than simply avoiding saturation. Assume we have an MLP with one hidden layer applied to a classification problem and are therefore interested in the hyperplanes defined by each hidden unit. Each hyperplane is the locus of points where the net-input to the hidden unit is zero and is thus the classification boundary generated by that hidden unit considered in isolation. The connection weights from the inputs to a hidden unit determine the orientation of the hyperplane. The bias determines the distance of the hyperplane from the origin. If the bias terms are all small random numbers, then all the hyperplanes will pass close to the origin. Hence, if the data are not centered at the origin, the hyperplane may fail to pass through the data cloud. If all the inputs have a small coefficient of variation, it is quite possible that all the initial hyperplanes will miss the data entirely. With such a poor initialization, local minima are very likely to occur. It is therefore important to center the inputs to get good random initializations. In particular, scaling the inputs to [-1,1] will work better than [0,1], although any scaling that sets to zero the mean or median or other measure of central tendency is likely to be as good, and robust estimators of location and scale (Iglewicz, 1983) will be even better for input variables with extreme outliers.

The key detail that makes me think that this is the answer is because you do not observe that it takes a long time to train the network when you use $z$-scores, which have negative and positive input values due to the mean-centering.
","**Summary:**

For optimal neural network initialization, it is crucial to standardize input variables. This prevents saturation and ensures that the initial hyperplanes generated by hidden units pass through the data cloud.

**Why it matters:**

* If bias terms are small and data is not centered, hyperplanes may miss the data, leading to poor initializations.
* Local minima are more likely with poor initializations.
* Scaling inputs to [-1,1] is effective for standardization, as it sets the mean to zero.

**Additional considerations:**

* Scaling to [0,1] is less effective than [-1,1] because it does not center the data.
* Other measures of central tendency (e.g., median) can also be used for standardization.
* Robust estimators of location and scale are recommended for inputs with extreme outliers.

**The key takeaway:**

Using standardized input variables improves neural network initialization, reduces training time, and prevents local minima."
Dynamic graphs versus static graphs in deep learning libraries,https://stats.stackexchange.com/questions/297481/dynamic-graphs-versus-static-graphs-in-deep-learning-libraries,neural-networks|deep-learning|tensorflow|torch,297481,True,308590,11923,1,12,1508331103,1502470429,11,1508331103,"
What is the difference between dynamic graphs and static graphs in
  deep learning libraries?

For the static graphs, you should first draw the graph completely and then inject data to run(define-and-run), while using dynamic graphs the graph structure is defined on-the-fly via the actual forward computation. This is a far more natural style of programming(define-by-run). 

Which one is faster? and when to use each one of them?

When you can distribute the computations over multiple machines it is better to harness static graphs. Being able to do this with a dynamic computation graph would be far more problematic.
Some other comparisons:
Having a static graph enables a lot of convenient operations: storing a fixed graph data structure, shipping models that are independent of code, performing graph transformations, but it is a little bit more complex than dynamic graphs(for example when implementing something like recursive neural networks). When you're working with new architectures, you want the most flexibility possible, and these frameworks allow for that.
This answer is gathered from the reference at the bottom, and I hope my conclusions are correct and can help. 
Reference: https://news.ycombinator.com/item?id=13428098 (https://news.ycombinator.com/item?id=13428098)
","**Summary:**

**Dynamic vs. Static Graphs in Deep Learning:**

**Dynamic Graphs:**

* Graph structure is defined during forward computation (""define-by-run"").
* Offers flexibility, making it suitable for exploring new architectures and implementing complex networks.
* However, it can be challenging to distribute computations.

**Static Graphs:**

* Graph is defined and fixed before data injection (""define-and-run"").
* Allows convenient operations such as storing fixed graph data and shipping code-independent models.
* Facilitates efficient distribution of computations.
* May be more complex to implement than dynamic graphs.

**When to Use:**

* Static graphs are preferred for distributed computations.
* Dynamic graphs are ideal for exploring new architectures and implementing complex networks."
Gradient Based Learning Algorithms vs Global Optimization Learning Algorithms for Neural Networks,https://stats.stackexchange.com/questions/98983/gradient-based-learning-algorithms-vs-global-optimization-learning-algorithms-fo,neural-networks|optimization|gradient-descent|genetic-algorithms,98983,True,98985,4474,3,12,1536771100,1400257890,9,1400259163,"Reading your question, I understand that you think that ""global optimization methods"" always give the global optimum whatever the function you are working on. This idea is very wrong. First of all, some of these algorithm indeed give a global optimum, but in practice for most functions they don't... And don't forget about the no free lunch theorem.
Some classes of functions are easy to optimize (convex functions for example), but most of the time, and especially for neural network training criterion, you have : non linearity - non convexity (even though for simple network this is not the case)... These are pretty nasty functions to optimize (mostly because of the pathological curvatures they have). So Why gradient ? Because you have good properties for the first order methods, especially they are scalable. Why not higher order ? Newton method can't be applied because you have too much parameters and because of this you can't hope to effectively inverse the Hessian matrix.
So there are a lot of variants around which are based on second order method, but only rely on first order computations : hessian free optimization, Nesterov gradient, momentum method etc...
So yes, first order methods and approximate second order are the best we can do for now, because everything else doesn't work very well.
I suggest for further detail : ""Learning deep architectures for AI"" from Y. Bengio. The book : ""Neural networks : tricks of the trade"" and Ilya Sutskever phd thesis.
","**Summary:**

Global optimization methods do not always guarantee the global optimum, particularly for complex functions like neural network criteria. Convex functions are relatively easy to optimize, but most real-world functions are non-convex.

First-order methods, such as gradient descent, are preferred due to their scalability and practicality. Higher-order methods, like Newton's method, are impractical due to the high computational cost of inverting large matrices in high-dimensional problems.

Therefore, approximate second-order methods, which combine the efficiency of first-order methods with the benefits of second-order information, are often the most effective approach for optimizing neural network criteria. Examples include Hessian-free optimization, Nesterov gradient, and momentum methods.

For more information, refer to Y. Bengio's ""Learning Deep Architectures for AI,"" the ""Neural Networks: Tricks of the Trade"" book, and Ilya Sutskever's PhD thesis."
What is the weight decay loss?,https://stats.stackexchange.com/questions/273189/what-is-the-weight-decay-loss,neural-networks|convolutional-neural-network,273189,True,273190,23267,1,12,1492082964,1491928403,10,1491936153,"Weight decay specifies regularization in the neural network.
During training, a regularization term is added to the network's loss to compute the backpropagation gradient. The weight decay value determines how dominant this regularization term will be in the gradient computation.
As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have the higher this term should be.
So, Weight decay is a regularization term that penalizes big weights. When the weight decay coefficient is big, the penalty for big weights is also big, when it is small weights can freely grow.
So, now if you go back to reading the answer (https://stats.stackexchange.com/a/117625/84191) which you linked in your question, it would make complete sense now.
","**Summary:**

Weight decay is a regularization technique used in neural networks. It adds a penalty term to the network's loss function, which discourages large weight values. During training, the weight decay value determines the strength of this penalty.

As a general rule, the number of training examples and the number of parameters in the network influence the optimal weight decay value. A higher number of training examples warrants a weaker penalty, while a larger number of parameters necessitates a stronger penalty.

Essentially, weight decay helps control the size of weights in the network. A strong weight decay value penalizes large weights heavily, while a weak value allows weights to grow more freely. This technique prevents overfitting by encouraging the network to learn more generalizable features."
What does log-likelihood mean in the context of generative models like GANs?,https://stats.stackexchange.com/questions/255877/what-does-log-likelihood-mean-in-the-context-of-generative-models-like-gans,machine-learning|deep-learning|likelihood|generative-models|gan,255877,True,345138,7298,2,12,1525797501,1484236786,6,1525797501,"You're absolutely correct. The log likelihood of most GAN models on most datasets or true distributions is $-\infty$, and the probability density under the trained model for any actual image is almost always zero.
This is easy to see. Suppose a GAN has a 200-dimensional latent variable and is generating 200x200 grayscale images. Then the space of all images is 40,000-dimensional, and the GAN can only ever generate images on a 200-dimensional submanifold of this 40,000-dimensional space. Real images will almost always lie off this submanifold, and so have a probability density of zero under the GAN. This argument holds whenever the output space has higher dimension than the latent space, which is typically the case (e.g. Nvidia's recent progressive GANs used a 512-dimensional latent space to generate 1024x1024x3 images).
Whether this is a problem or not depends on what you want the generative model for; GANs certainly generate visually attractive samples in many cases.
","GANs (Generative Adversarial Networks) often have an extremely low probability of generating realistic images. This occurs because the latent space (where GANs generate data) has a lower dimension than the output space (the actual images). Consequently, most images lie outside the submanifold that the GAN can generate, resulting in a probability density of zero under the GAN's distribution. However, despite this mathematical limitation, GANs often produce visually pleasing images, suggesting their utility in certain applications where realism is not paramount."
Formula to compute approximate memory requirements of Transformer models,https://stats.stackexchange.com/questions/563919/formula-to-compute-approximate-memory-requirements-of-transformer-models,machine-learning|neural-networks|transformers,563919,True,581762,5147,1,12,1677216790,1644526210,9,1657651582,"I was surprised that afaik there are no good answers for this (and similar) questions on the internet. I'm going to derive the following approximate formula for GPT:
$M \approx M_{activatons} \approx \frac{BT^2}{4ND^2}$
M = memory
B = batch size
T = sequence length
N = # of attention heads
D = dimension per head

Let's get started. The GPT transformer block has the following form:
Multi-head Attention -> LayerNorm -> MLP LayerNorm
To simplify the problem, let's exclude the layer norm and bias terms from our parameter count.
Assume we have $N$ heads, a hidden dimension of $D$ per each head, and data of batch size $B$ and sequence length $T$. Let's represent the total dimension as $C = N * D$ and assume the MLP has dimension $C$ also.
We want to express the memory footprint in terms of $C, B, T$.
There are three components that will contribute to the overall footprint:

Storing the model $M_{model}$
Storing the activations $M_{activations}$
Storing the gradients $M_{gradients}$

So the total memory is $M = M_{model} + M_{activations} + M_{gradients}$. Unless you are computing higher order gradients $ M_{model} \geq  M_{gradients}$.
For transformers $M_{activations} >> M_{model}$ so the term we care about most is $M_{activations}$. I'll derive both though to show you why:
The model:
Each transformer block will have query, key, value networks and an MLP. We're ignoring layer norms and biases so the total parameters per block are $3C^2 + C^2 = 4C^2$. If the transformer has $L$ layers this means:
$M_{model} = 4LC^2 = 4 L N^2 D^2$
The activations:
Attention is the following operation $\text{Attention}(Q, K, V) = \text{softmax}(Q K^T / \sqrt{d}) V$. The $Q K^T$ operation has the following shape:
[B, N, T, D] @ [B, N, D, T] = [B, N, T, T]
Then the multiplication by $V$ and the MLP both output [B, N, T, D] activations. So the total memory per block is:
$BNT^2 + 2 BNTD = BNT(T + 2D)$
This happens at each layer so
$M_{activations} = BNLT(T+2D)$
Relative activation-to-model memory ratio is
$M_{activations} / M_{model} = BT(T+2D)/4N D^2$
Now let's assume we're modelling long sequences, then $T >> D$ and we have
$M_{activations} / M_{model} \approx \frac{BT^2}{4ND^2}$
Meaning that $M_{activations} >> M_{model}$$ so the total memory is dominated by activations:
$M \approx M_{activatons} \approx M_{model}\frac{BT^2}{4ND^2}$
","**Summary:**

The author presents an approximate formula to estimate the memory footprint of a Generative Pre-trained Transformer (GPT) model:

**Memory Footprint (M) â Activation Memory (M_activations)**

**Derivation:**

* The primary memory consumption in GPT models is due to storing activation data (M_activations).
* Activation memory is proportional to the batch size (B), sequence length (T), number of attention heads (N), and dimension per head (D).
* Assuming long sequences (T >> D), the formula simplifies to:
**M_activations â (BT^2) / (4ND^2)**

**Comparison to Model Memory:**

* Model memory (M_model) is negligible compared to activation memory (M_activations >> M_model).

**Implications:**

* Memory optimization should focus on reducing activation memory.
* Increasing batch size (B) and sequence length (T) significantly increases memory consumption.
* Increasing the number of attention heads (N) and dimension per head (D) has a smaller impact on memory consumption compared to B and T."
What&#39;s the rationale for not checking residuals when building a ML model?,https://stats.stackexchange.com/questions/470936/whats-the-rationale-for-not-checking-residuals-when-building-a-ml-model,machine-learning|neural-networks|residuals|boosting,470936,True,470951,910,2,12,1591765445,1591567362,7,1591577779,"The main rationale is the (wrongly) perceived low return on investment (https://en.wikipedia.org/wiki/Return_on_investment). 
Lack of time and inappropriate training confound the issue. To a lesser extent, these points are aggravated by laziness and technical difficult respectively.
Especially with more complex models it becomes progressively harder to infer why a model made a particular prediction. Yes, there is a multitude of techniques to explain ML predictions (e.g. LIME, SHAP, Partial Dependency and Accumulated Local Effects plots, etc.) but those are ""extra steps"". And even then, maybe after the effort to get a SHAP force plot or an ALE plot to explain a particular prediction, we are still left with the question as to how affect the model's prediction. We generated some new questions but usually no immediate answers.  
Note that in industrial ML applications ""usually"" we are concerned with prediction as the primary deliverable of our work. As long as the overall RMSE/MAE/Huber loss is ""OK"" we ship the model. Questions about the actual model estimation and/or attribution (significance) are often downgraded to ""nice-to-have"".
Efron recently published an insightful discussion paper titled ""Prediction, Estimation, and Attribution"" (https://amstat.tandfonline.com/doi/full/10.1080/01621459.2020.1762613#.Xt2Frpwo-V4) high-lighting these differences further. I think you will find it enlightening on this matter too.
Just to be clear:
You are absolutely correct to say that ""the residuals offer some information"". Just in many cases, the time to extract, interpreter that information and then to appropriately account for it is not factored in. People should always examine model residuals, perform some model spot-checks, etc. Even the strongest ML methods are far from silver bullets when it comes to prediction. 
","**Main Ideas**

The primary reason for underusing model residuals in practice is the perceived low return on investment (ROI) due to lack of time and training. Additionally, the complexity of models makes it challenging to understand predictions.

While techniques like LIME and SHAP exist for explaining ML predictions, they require extra effort and may not directly translate to actionable insights. In industrial ML applications, the focus is typically on prediction, and deeper analysis of model estimation or attribution is often considered secondary.

Despite the potential benefits of examining model residuals, the time and resources required to extract and interpret this information are often not considered.

**Summary**

The underutilization of model residuals is primarily due to perceived low ROI, lack of time, and difficulty interpreting complex models. While techniques for explaining predictions exist, they add complexity and may not provide actionable insights. In practice, prediction alone is often the primary concern, with detailed analysis of model estimation and attribution being lower priorities."
How do I improve my neural network stability?,https://stats.stackexchange.com/questions/23235/how-do-i-improve-my-neural-network-stability,r|machine-learning|neural-networks,23235,True,23241,11074,2,12,1564910012,1329837987,8,1329842126,"In general you would get more stability by increasing the number of hidden nodes and using an appropriate weight decay (aka ridge penalty).
Specifically, I would recommend using the caret package to get a better understanding of your accuracy (and even the uncertainty in your accuracy.)  Also in caret is the avNNet that makes an ensemble learner out of multiple neural networks to reduce the effect of the initial seeds.  I personally haven't seen huge improvement using avNNet but it could address your original question.
I'd also make sure that your inputs are all properly conditioned.  Have you orthogonalized and then re-scaled them?  Caret can also do this pre-processing for you via it's pcaNNet function.
Lastly you can consider tossing in some skip layer connections.  You need to make sure there are no outliers/leverage points in your data to skew those connections though.
","**Summary:**

To improve model stability in neural networks:

* Increase the number of hidden nodes and apply weight decay to reduce overfitting.
* Use the caret package for advanced analysis and ensemble learning (avNNet) to mitigate the impact of initial random seeds.
* Properly condition input data by orthogonalizing and rescaling using functions like pcaNNet in caret.
* Consider incorporating skip layer connections, but ensure there are no outliers or leverage points in the data that could distort the connections."
Batch normalization and the need for bias in neural networks,https://stats.stackexchange.com/questions/482305/batch-normalization-and-the-need-for-bias-in-neural-networks,machine-learning|neural-networks|bias|batch-normalization,482305,True,518875,6760,3,12,1705662558,1597058165,3,1617962022,"Check your software, but broadly the answer to your question is: yes, using batch normalization obviates the need for a bias in the preceding linear layer. Your question does a good job of laying out where you are confused, so let me speak to it: the shift term in batch normalization is also a vector, for instance the documentation for BatchNorm2d in Pytorch reads: ""The mean and standard-deviation are calculated per-dimension over the mini-batches and $\gamma$ and $\beta$ are learnable parameter vectors of size $C$ (where $C$ is the input size). ""
","**Summary:**

**Batch Normalization and Bias:**

Batch normalization (BN) significantly reduces the need for bias terms in preceding linear layers. This is because the shift term in BN plays the same role as the bias term, but it is learnable and applied across the entire batch.

**Shift Term in BatchNorm:**

The shift term in BN is not a scalar, but rather a vector of the same size as the input data. This means that it applies a different shift to each input feature dimension.

**Benefits of Avoiding Bias:**

Eliminating bias terms in linear layers with BN offers several benefits:

* **Reduced Overfitting:** Bias terms can introduce additional parameters that can potentially lead to overfitting.
* **Faster Training:** Removing bias terms allows for faster training times as gradients are simpler to calculate.
* **Increased Robustness:** BN, along with the elimination of bias terms, improves model robustness against noise and parameter initialization.

**Note:**

While BN typically obviates the need for bias terms in preceding linear layers, there may be exceptions where using a bias term is still beneficial."
How to train an LSTM when the sequence has imbalanced classes,https://stats.stackexchange.com/questions/342170/how-to-train-an-lstm-when-the-sequence-has-imbalanced-classes,neural-networks|unbalanced-classes|lstm,342170,True,425021,12994,1,12,1581242580,1524451973,8,1567655767,"Inversely proportional contributions to cost function
Another way of dealing with imbalanced data is to weight each label's contribution to the cost function inversely proportional to the frequency of the label. In your above example, I count the following frequencies of the classes:
1: 10
2:  7
3: 20
4:  2

So you could multiply the cost on a sample-by-sample basis by $\frac{1}{10}$ when the true label is 1, by $\frac{1}{7}$ for the label 2, $\frac{1}{20}$ for the label 3, and $\frac{1}{2}$ for the label 4. So you'll see 5 times as many 1 labels as 4 labels, but they'll each contribute $\frac{1}{5}^{th}$ as much to your overall cost function. In other words, you can expect each label to have roughly the same impact on your cost function on average.
In practice, I would use the frequencies of the labels across my whole training set, and set the numerator so that the sum of my multipliers is 1. E.g. in the above example, I'd use the fractions $\frac{1.26}{10}$, $\frac{1.26}{7}$, $\frac{1.26}{20}$, $\frac{1.26}{2}$ which add up to ~1. You don't need to scale in this way, but if you don't you are in effect modifying your learning rate.
A danger of using this approach (as with resampling) is the increased chance of overfitting to the rare labels. You'll likely want to regularize your model somehow if you use this kind of approach.
On a practical note, I believe most deep learning libraries offer this functionality. For example, in the python library keras, the keras.models.Model.fit() (https://keras.io/models/model/#fit) method has a sample_weight parameter:

sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape  (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=""temporal"" in compile().

Lastly, I'll encourage you to make sure you have a good performance metric you trust. Using an approach like this may result in your model estimating your rare labels more often than is actually desirable. As Tim said in a comment:

If something is more common, it is reasonable that it gets predicted more commonly.

","To address imbalanced data, the cost function can be adjusted to weight labels inversely proportional to their frequency. This ensures that each label has a similar impact on the cost function, regardless of their occurrence. For instance, in a dataset where label 1 appears 10 times and label 4 appears 2 times, the cost for samples of label 1 can be multiplied by 1/10 and for label 4 by 1/2.

While this approach helps balance label contributions, it may increase the risk of overfitting to rare labels. Thus, regularization is often recommended. Deep learning libraries like Keras provide support for sample weighting through the `sample_weight` parameter in the `fit()` method.

It's crucial to select a reliable performance metric, as this approach may lead to the model predicting rare labels more frequently than desired."
Why are activation functions needed in neural networks?,https://stats.stackexchange.com/questions/158549/why-are-activation-functions-needed-in-neural-networks,neural-networks|deep-learning,158549,True,158565,1713,1,12,1566788143,1435180358,8,1435189997,"In general, non-linearities are used to flexibly squash the input through a function and pave the way recovering a higher-level abstraction structure, but allow me to become more specific.
One of the most informative illustrations I have found about ReLU activations is the following:

The picture is from the work of A. Punjani and P. Abbeel (http://www.cs.toronto.edu/~alipunjani/pdf/icra15-helimodel-cameraready.pdf), and depicts a simple neural network with a ReLU activation unit. Now imagine you have a 2D input space, as it can be seen the ReLU unit $\phi$, actually partitions the input space and regresses towards your desired outcome, where the space partitioning is a non-linear operation. That's the reason it is so powerful as it allows combinations of different input transformations, which are learnt from the dataset itself.
More specifically, the author's description is:

A pictorial
  representation of the flexibility of the ReLU Network Model. Each hidden
  unit can be thought of as defining a hyperplane (line) in the 2D input space
  pictured. The data points (grey) each fall somewhere in input space and each
  has a value we wish to regress (not pictured). Consider the hidden unit $i$,
  drawn in purple. The purple arrow points in the direction of weight vector
  $Wi$
  , and has length according to $Bi$
  . Points on one side of this line do not
  activate unit $i$, while points on the other side (shaded) cause positive output.
  This effectively partitions the input space, and the partitions generated by considering many hidden units (blue) together split the space into regions. These regions give the model flexibility to capture structure in the input data. In the ReLU Network Model, the partitions are learned from data.

","**Summary:**

Non-linearities, such as ReLU (Rectified Linear Unit), are employed in neural networks to enhance their ability to capture complex patterns in data.

A ReLU unit partitions the input space into regions, enabling the network to learn combinations of input transformations. Each hidden unit defines a hyperplane (line) that divides the space, with points on one side activating the unit and points on the other side not.

The resulting partitioned space allows the network to capture structure in the input data, making ReLU activations particularly powerful for tasks that require discovering non-linear relationships."
How are SVMs = Template Matching?,https://stats.stackexchange.com/questions/263587/how-are-svms-template-matching,machine-learning|neural-networks|svm|deep-learning|kernel-trick,263587,True,263658,1688,1,12,1488568234,1487838060,7,1487867883,"

How is SVM related to neural network? How is it a shallow network?


The SVM is a single layer neural network with the hinge loss as loss function and exclusively linear activation. The concept has been alluded in previous threads, such as this one: Single layer NeuralNetwork with RelU activation equal to SVM? (https://stats.stackexchange.com/questions/190883/single-layer-neuralnetwork-with-relu-activation-equal-to-svm) 


SVM solves an optimization problem with a well defined objective function, how is it doing template matching? What is the template here to which an input is matched?


The Gram Matrix (Kernel Matrix, if you prefer) is a measure of similarity. As the SVM allows sparse solutions, prediction becomes a matter of comparing your sample with the templates, i.e. the support vectors.
","**Summary:**

**Relation between SVM and Neural Networks:**

* SVM is a type of shallow neural network with a single layer and linear activation.
* It uses the hinge loss as its loss function.

**SVM as a Shallow Network:**

* Shallow networks have limited layers compared to deep neural networks.
* SVM's single layer structure makes it a shallow network.

**SVM and Template Matching:**

* SVM solves an optimization problem to find support vectors.
* These support vectors represent templates or patterns in the data.
* To make predictions, an input is compared against the support vectors to find the most similar template.

**Gram Matrix in SVM:**

* The Gram Matrix is a similarity measure between data points.
* In SVM, it can be used to compare an input to the support vectors for efficient prediction."
Is that possible to distill the knowledge of a stacked ensemble model?,https://stats.stackexchange.com/questions/301568/is-that-possible-to-distill-the-knowledge-of-a-stacked-ensemble-model,machine-learning|neural-networks|ensemble-learning,301568,True,448288,1461,2,12,1581029329,1504645043,6,1581028971,"In fact Hinton et al (2014) (https://arxiv.org/abs/1503.02531) in their paper, as inspiration refer to paper by Caruana et al (2006) (https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf), who described distilling knowledge from an ensemble of models. Moreover, distilling is a very basic and versatile approach that can be applied to any model. The point is that you have noisy data and train a big model on this data, this model learns some kind of approximate representation of the data, that is smoother then the raw data. This representation should be less noisy then the data, so it should be easier to learn from it, then from noisy data.
Among other, later, interpretations of why distillation works, is that what we often observe is that the big models like deep neural networks, among huge number of parameters they have, only a handful ""does most of the work"". One of the interpretations of why huge, deep neural networks work, is that the models with huge number of parameters have a very big search space to find among them such combinations that lead to useful functions. If that is the case, then we don't need the complicated model if only we could extract the small fraction of it that learned the most important characteristics of the problem. This is probably not only the case for neural networks, but also for other modern machine learning algorithms.
From practical point of view, as noticed by Hinton et al, it is easier to learn from smoother representation of the knowledge, so distillation should work better for things like predicted probabilities, or logits, then the hard classifications. Moreover, what Hinton et al proposed, is to further smooth the outputs of the model by including the temperature parameter in the softmax function. This may be more or less useful depending on how well callibrated (https://scikit-learn.org/stable/modules/calibration.html) are the probabilities returned by your model. If your model returns values that are very clustered over high and low probabilities, then the values are not very much more discriminative then the hard classifications. On another hand, with ""smooth"" outputs the knowledge of the model about the variability of the data is better preserved.
Finally, the idea is pretty simple, so you could give it a try and see if the result achieved by the small, ""student"" model is close to the big model. The whole point of the small model is that is is lighter then the big one, so should also be faster to train, what makes experimenting much easier. As a word of caution, when looking at the results remember to look not only on the general performance metrics, but also at the tails, at how the small vs big model does handle the atypical cases. There is always risk that the simpler model would learn to classify correctly the ""average"" cases, but will not do well on the edge cases, so you need to double check this (it is not easy).
","Knowledge distillation is a technique for extracting knowledge from a complex model (the ""teacher"") and transferring it to a smaller, simpler model (the ""student""). This approach aims to create a student model that performs similarly to the teacher but is more efficient to train and deploy.

Distillation works by training the student model on a smoothed representation of the teacher's predictions. This smoother representation reduces noise and makes it easier for the student to learn. The distillation process often involves using a temperature parameter in the softmax function to further smooth the outputs.

One interpretation of why distillation is effective is that complex models only use a small subset of their parameters to perform most of the work. Distillation allows us to extract this important subset and transfer it to the student model.

Distillation is particularly effective for tasks involving predicted probabilities or logits, as it preserves the model's knowledge about data variability. It is important to evaluate the student model not only on general performance metrics but also on how it handles atypical cases to ensure that it does not oversimplify the problem."
R neuralnet - compute give a constant answer,https://stats.stackexchange.com/questions/65292/r-neuralnet-compute-give-a-constant-answer,r|neural-networks|prediction|error,65292,True,66942,25331,1,12,1376088395,1374605165,6,1376057671,"I'm not an expert in neural nets but I think the following points might be helpful to you. There are also some nice posts, e.g. this one on hidden units (https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute), that you can search for on this site about what neural nets do that you might find useful.
1 Large errors: why didn't your example work at all

why errors are so big and why all predicted values are almost constant?

This is because the neural network was unable to compute the multiplication function you gave it and outputting a constant number in the middle of the range of y, regardless of x, was the best way to minimize errors during training. (Notice how 58749 is pretty close to the mean of multiplying two numbers between 1 and 500 together.)
It's very hard to see how a neural network could compute a multiplication function in a sensible way. Think about how each node in the network combines previously computed results: you take a weighted sum of the outputs from previous nodes (and then apply a sigmoidal function to it, see, e.g. an Introduction to Neural Networks (http://www.cs.stir.ac.uk/~lss/NNIntro/InvSlides.html#where), to scrunch the output inbetween $-1$ and $1$). How are you going to get a weighted sum to give you multiplication of two inputs? (I suppose, however, that it might be possible to take a large number of hidden layers to get multiplication working in a very contrived way.) 
2 Local minima: why a theoretically reasonable example might not work
However, even trying to do addition you run into problems in your example: the network doesn't train successfully. I believe that this is because of a second problem: getting local minima (http://en.wikipedia.org/wiki/Backpropagation#Limitations) during the training. In fact, for addition, using two layers of 5 hidden units is much too complicated to compute addition. A network with no hidden units trains perfectly well:
x <- cbind(runif(50, min=1, max=500), runif(50, min=1, max=500))
y <- x[, 1] + x[, 2]
train <- data.frame(x, y)
n <- names(train)
f <- as.formula(paste('y ~', paste(n[!n %in% 'y'], collapse = ' + ')))
net <- neuralnet(f, train, hidden = 0, threshold=0.01)
print(net) # Error 0.00000001893602844

Of course, you could transform your original problem into an addition problem by taking logs, but I don't think this is what you want, so onwards...
3 Number of training examples compared to number of parameters to estimate
So what would be a reasonable way to test your neural net with two layers of 5 hidden units as you originally had? Neural nets are often used for classification, so deciding whether $\mathbf{x}\cdot\mathbf{k} > c$ seemed a reasonable choice of problem. I used $\mathbf{k} = (1, 2, 3, 4, 5)$ and $c = 3750$. Notice that there are several parameters to be learnt.
In the code below I take a very similar approach to yours except that I train two neural nets, one with 50 examples from the training set, and one with 500.
library(neuralnet)
set.seed(1) # make results reproducible
N=500
x <- cbind(runif(N, min=1, max=500), runif(N, min=1, max=500), runif(N, min=1, max=500), runif(N, min=1, max=500), runif(N, min=1, max=500))
y <- ifelse(x[,1] + 2*x[,1] + 3*x[,1] + 4*x[,1] + 5*x[,1] > 3750, 1, 0)
trainSMALL <- data.frame(x[1:(N/10),], y=y[1:(N/10)])
trainALL <- data.frame(x, y)
n <- names(trainSMALL)
f <- as.formula(paste('y ~', paste(n[!n %in% 'y'], collapse = ' + ')))
netSMALL <- neuralnet(f, trainSMALL, hidden = c(5,5), threshold = 0.01)
netALL <- neuralnet(f, trainALL, hidden = c(5,5), threshold = 0.01)
print(netSMALL) # error 4.117671763
print(netALL) # error 0.009598461875

# get a sense of accuracy w.r.t small training set (in-sample)
cbind(y, compute(netSMALL,x)$net.result)[1:10,]
      y                 
 [1,] 1  0.587903899825
 [2,] 0  0.001158500142
 [3,] 1  0.587903899825
 [4,] 0  0.001158500281
 [5,] 0 -0.003770868805
 [6,] 0  0.587903899825
 [7,] 1  0.587903899825
 [8,] 0  0.001158500142
 [9,] 0  0.587903899825
[10,] 1  0.587903899825

# get a sense of accuracy w.r.t full training set (in-sample)
cbind(y, compute(netALL,x)$net.result)[1:10,]
      y                 
 [1,] 1  1.0003618092051
 [2,] 0 -0.0025677656844
 [3,] 1  0.9999590121059
 [4,] 0 -0.0003835722682
 [5,] 0 -0.0003835722682
 [6,] 0 -0.0003835722199
 [7,] 1  1.0003618092051
 [8,] 0 -0.0025677656844
 [9,] 0 -0.0003835722682
[10,] 1  1.0003618092051

It is apparent that the netALL does a lot better! Why is this? Take a look at what you get with a plot(netALL) command:

I make it 66 parameters that are estimated during training (5 inputs and 1 bias input to each of 11 nodes). You can't reliably estimate 66 parameters with 50 training examples. I suspect in this case you might be able to cut down on the number of parameters to estimate by cutting down on the number of units. And you can see from constructing a neural network to do addition that a simpler neural network may be less likely to run into problems during training.
But as a general rule in any machine learning (including linear regression) you want to have a lot more training examples than parameters to estimate.
","**Summary:**

Neural networks may not always perform as expected due to several factors:

**1. Inability to Perform Complex Functions:** Neural networks may struggle to compute complex functions like multiplication directly, leading to constant outputs.

**2. Local Minima:** Neural networks can get stuck in local minima during training, resulting in poor performance even for simple functions like addition.

**3. Number of Training Examples vs. Parameters:** Reliable parameter estimation requires a sufficient number of training examples. Too few examples can lead to overfitting and poor performance.

**Recommendation:**

To improve neural network performance, consider:

* Transforming complex functions into simpler ones that the network can compute more easily.
* Using simpler architectures with fewer hidden units or layers.
* Ensuring that there are significantly more training examples than parameters to estimate."
Why doesn&#39;t mean square error work in case of angular data?,https://stats.stackexchange.com/questions/565038/why-doesnt-mean-square-error-work-in-case-of-angular-data,neural-networks|multiple-regression|error|circular-statistics,565038,True,565057,2496,1,11,1645991325,1645294492,36,1645306282,"The problem with using MSE directly on angle values is that values can be very close together on the circle, but will have a large square error. For example, 359 degrees and 1 degrees are very close together on a circle, but very far apart in terms of squared difference.
To rectify this, you should use a distance metric that reflects the fact that you're working with angles.
The Euclidean distance between two points on a circle is the length of the chord between the two points. So a simple and direct example of a distance considers the angle $\alpha$ as a point on the unit circle $(\cos \alpha, \sin \alpha)$, then computes the Euclidean distance in the ordinary way.
Elementary trigonometric identities simplify the expression.
$$\begin{align}
d_E^2 (\theta, \phi) &= (\cos \theta - \cos \phi)^2 + (\sin \theta - \sin \phi)^2 \\
&= \cos^2 \theta - 2 \cos \theta \cos \phi + \cos^2 \phi + \sin^2 \theta - 2 \sin \theta \sin \phi + \sin^2 \phi \\
d_E (\theta, \phi) &= \sqrt{2 - 2 \cos (\theta - \phi) }
\end{align}$$
By inspection, we can see that angles that are very close together have $\cos (\theta - \phi)$ close to 1, so the distance is close to 0. On the other hand, angles that are very far apart have $\cos (\theta - \phi)$ close to 0, so the distance is close to 1. This satisfies our intuitive notions of distance.
Of course, this is not the only option. This distance and some alternatives are discussed in these related threads:

Loss function (and encoding?) for angles (https://stats.stackexchange.com/questions/425234/loss-function-and-encoding-for-angles?rq=1)
How to define distance for vector of angles? (https://stats.stackexchange.com/questions/555395/how-to-define-distance-for-vector-of-angles)
Best distance measure to use to compare vectors of angles (https://stats.stackexchange.com/questions/6896/best-distance-measure-to-use-to-compare-vectors-of-angles?rq=1)

","Using the Mean Squared Error (MSE) to measure the distance between angles can be problematic due to the circular nature of angles, where values close on the circle have large squared differences.

To address this, distance metrics specific to angles are recommended. One such metric is the Euclidean distance on the unit circle, which considers angles as points on the circle and computes the Euclidean distance between them.

This distance metric has the desirable property that angles that are close together have a small distance, while angles that are far apart have a large distance, aligning with our intuitive understanding of distance for angles.

While the Euclidean distance is a valid option, other alternative distance metrics for angles exist and are discussed in related threads."
A way to maintain classifier&#39;s recall while improving precision,https://stats.stackexchange.com/questions/186182/a-way-to-maintain-classifiers-recall-while-improving-precision,machine-learning|deep-learning|precision-recall,186182,True,186212,33884,4,11,1605463005,1449795362,20,1449823626,"Precision and recall are a tradeoff. Typically to increase precision for a given model implies lowering recall, though this depends on the precision-recall curve of your model, so you may get lucky. 
Generally, if you want higher precision you need to restrict the positive predictions to those with highest certainty in your model, which means predicting fewer positives overall (which, in turn, usually results in lower recall).
If you want to maintain the same level of recall while improving precision, you will need a better classifier.
","Precision and recall, metrics for model performance, often face a tradeoff. Increasing precision, or the accuracy of predicted positives, typically lowers recall, or the completeness of predicted positives.

To enhance precision, models must limit positive predictions to those with high certainty, leading to fewer overall positives and potentially reduced recall. Maintaining high recall while improving precision necessitates a more effective classifier.

In summary, balancing precision and recall requires careful model selection and optimization. Precision gains may come at the expense of recall, but a superior classifier can potentially mitigate this tradeoff. Practitioners should analyze their model's precision-recall curve to assess the potential impact of adjustments."
What is vanishing gradient?,https://stats.stackexchange.com/questions/301285/what-is-vanishing-gradient,machine-learning|neural-networks|gradient,301285,True,301752,11043,4,11,1722915943,1504507951,17,1504715733,"If you do not carefully choose the range of the initial values for the weights, and if you do not control the range of the values of the weights during training, vanishing gradient would occur which is the main barrier to learning deep networks. The neural networks are trained using the gradient descent algorithm:
$$w^{new} := w^{old} - \eta \frac{\partial L}{\partial w}$$
where $L$ is the loss of the network on the current training batch. It is clear that if the $\frac{\partial L}{\partial w}$ is very small, the learning will be very slow, since the changes in $w$ will be very small. So, if the gradients are vanished, the learning will be very very slow. 
The reason for vanishing gradient is that during backpropagation, the gradient of early layers (layers near to the input layer) are obtained by multiplying the gradients of later layers (layers near to the output layer). So, for example if the gradients of later layers are less than one, their multiplication vanishes very fast.  
With this explanations these are answers to your questions:

Gradient is the gradient of the loss with respect to each trainable parameters (weights and biases).
Vanishing gradient does not mean the gradient vector is all zero (except for numerical underflow), but it means the gradients are so small that the learning will be very slow.

","**Summary:**

Vanishing gradient is a phenomenon in deep learning that occurs when the gradients of early layers in a neural network become extremely small during backpropagation. This slows down learning because the weights of earlier layers cannot be effectively adjusted based on the errors in later layers.

The cause of vanishing gradient is the multiplicative nature of backpropagation. The gradients for earlier layers are calculated by multiplying together the gradients for later layers. If the gradients for later layers are small, the overall gradient for earlier layers will also be small, leading to slow learning.

To avoid vanishing gradient, it is important to carefully choose the initial weights and control their range during training. Additionally, techniques such as skip connections or batch normalization can be used to mitigate the effects of vanishing gradient."
"In Machine learning, how does normalization help in convergence of gradient descent?",https://stats.stackexchange.com/questions/437840/in-machine-learning-how-does-normalization-help-in-convergence-of-gradient-desc,machine-learning|neural-networks|normalization|faq,437840,True,437848,7635,2,11,1675254430,1574690667,23,1574692920,"Rescaling is preconditioning
Steepest descent can take steps that oscillate wildly away from the optimum, even if the function is strongly convex or even quadratic.
Consider $f(x)=x_1^2 + 25x_2^2$. This is convex because it is a quadratic with positive coefficients. By inspection, we can see that it has a global minimum at $x=[0,0]^\top$. It has gradient
$$
\nabla f(x)=
\begin{bmatrix}
2x_1 \\
50x_2
\end{bmatrix}
$$
With a learning rate of $\alpha=0.035$, and initial guess $x^{(0)}=[0.5, 0.5]^\top,$ we have the gradient update
$$
x^{(1)} =x^{(0)}-\alpha \nabla f\left(x^{(0)}\right)
$$
which exhibits this wildly oscillating progress towards the minimum.
 (https://i.sstatic.net/lA6xQ.png)
Each step is wildly oscillating because the function is much steeper in the $x_2$ direction than the $x_1$ direction. Because of this fact, we can infer that the gradient is not always, or even usually, pointing toward the minimum. This is a general property of gradient descent when the eigenvalues of the Hessian $\nabla^2 f(x)$ are on dissimilar scales. Progress is slow in directions corresponding to the eigenvectors with the smallest corresponding eigenvalues, and fastest in the directions with the largest eigenvalues. It is this property, in combination with the choice of learning rate, that determines how quickly gradient descent progresses.
The direct path to the minimum would be to move ""diagonally"" instead of in this fashion which is strongly dominated by vertical oscillations. However, gradient descent only has information about local steepness, so it ""doesn't know"" that strategy would be more efficient, and it is subject to the vagaries of the Hessian having eigenvalues on different scales.
Rescaling the input data changes the Hessian matrix to be spherical. In turn, this means that steepest descent can move more directly towards the minimum instead of sharply oscillating.
Rescaling prevents early saturation
If you're using sigmoidal (logistic, tanh, softmax, etc.) activations, then these have flat gradients for inputs above a certain size. This implies that if the product of the network inputs and the initial weights is too small, the units will immediately be saturated and the gradients will be tiny. Scaling inputs to reasonable ranges and using small values for initial weights can ameliorate this and allow learning to proceed more quickly.

Effect of rescaling of inputs on loss for a simple neural network (https://stats.stackexchange.com/questions/420231/effect-of-rescaling-of-inputs-on-loss-for-a-simple-neural-network/420330#420330)

A common method is to scale the data to have 0 mean and unit variance. But there are other methods, such as min-max scaling (very common for tasks like MNIST), or computing Winsorized means and standard deviations (which might be better if your data contains very large outliers). The particular choice of a scaling method is usually unimportant as long as it provides preconditioning and prevents early saturation of units.

Neural Networks input data normalization and centering (https://stats.stackexchange.com/questions/421927/neural-networks-input-data-normalization-and-centering/422087#422087)

More Reading
In ""Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (https://arxiv.org/abs/1502.03167)"", Sergey Ioffe and Christian Szegedy write

It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened â i.e., linearly transformed to have zero means and unit variances, and decorrelated.

So you might also find that the network gets better results if you decorrelate the inputs in addition to applying zero mean and unit variances.
Following the citations provides more description and context.

LeCun, Y., Bottou, L., Orr, G., and Muller, K. ""Efficient
backprop. (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)"" In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade. Springer, 1998b.
Wiesler, Simon and Ney, Hermann. ""A convergence analysis of log-linear training. (https://papers.nips.cc/paper/4421-a-convergence-analysis-of-log-linear-training)"" In Shawe-Taylor, J., Zemel,
R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q.
(eds.), Advances in Neural Information Processing Systems 24, pp. 657â665, Granada, Spain, December 2011
This answer borrows this example and figure from Neural Networks Design (http://hagan.okstate.edu/NNDesign.pdf) (2nd Ed.) Chapter 9 by Martin T. Hagan, Howard B. Demuth, Mark Hudson Beale, Orlando De JesÃºs.

","**Summary of Rescaling in Gradient Descent and Neural Networks**

**Rescaling for Gradient Descent:**

* Gradient descent may oscillate and slow progress when the Hessian matrix has eigenvalues on different scales.
* Rescaling the input data can make the Hessian matrix spherical, improving gradient descent's efficiency and convergence.

**Rescaling for Neural Networks:**

* Sigmoidal activations have flat gradients for large inputs, causing early saturation and slow learning.
* Rescaling inputs to appropriate ranges prevents early saturation, enabling faster learning.

**Additional Benefits of Rescaling:**

* **Preconditioning:** Rescaling can adjust the features to have similar magnitudes, improving the learning process.
* **Preventing Early Saturation:** Rescaling ensures that inputs are in a range where activations are not saturated.
* **Whitening (Decorrelation):** Linearly transforming inputs to have zero means, unit variances, and low correlation can further enhance network performance.

**Common Rescaling Methods:**

* Zero mean and unit variance normalization
* Min-max scaling
* Winsorized means and standard deviations

The choice of rescaling method is typically less important than the overall benefits of rescaling, which include faster convergence, improved accuracy, and reduced training time."
Possible to get a better ANN by removing some connections?,https://stats.stackexchange.com/questions/23340/possible-to-get-a-better-ann-by-removing-some-connections,classification|neural-networks,23340,True,23342,199,4,11,1328743038,1328650787,7,1328651729,"Yes it is possible. Some people have looked at this problem in detail. Here is an old paper about a method to do so: Optimal brain damage (http://www.lecun.com/exdb/publis/pdf/lecun-90b.pdf)
","**Summary:**

The referenced paper proposes a method for ""optimal brain damage,"" which involves selectively removing neurons from a neural network to improve its performance. The authors argue that this approach can be beneficial in cases where the network is overfitting to the training data or when the network is too large and complex.

The method involves training a network with a large number of neurons and then iteratively removing neurons that contribute the least to the network's performance. This process continues until the network reaches an optimal level of performance, balancing accuracy and complexity.

The authors provide empirical evidence to support their claim that optimal brain damage can improve the performance of neural networks in various tasks, including image classification, handwritten digit recognition, and speech recognition. They also discuss the potential limitations and challenges associated with this approach."
Mean or sum of gradients for weight updates in SGD,https://stats.stackexchange.com/questions/358786/mean-or-sum-of-gradients-for-weight-updates-in-sgd,neural-networks|optimization|backpropagation|stochastic-gradient-descent,358786,True,358971,6089,1,11,1603987212,1532461386,22,1532532675,"The following assumes a loss function $f$ that's expressed as a sum, not an average. Expressing the loss as an average means that the scaling $\frac{1}{n}$ is ""baked in"" and no further action is needed. In particular, note that F.mse_loss uses reduction=""mean"" by default, so in the case of OP's code, no further modification is necessary to achieve an average of gradients. Indeed, rescaling the gradients and using reduction=""mean"" does not accomplish the desired result and amounts to a reduction in the learning rate by a factor of $\frac{1}{n}$.

Suppose that $G = \sum_{i=1}^n \nabla f(x_i)$ is the sum of the gradients for some minibatch with $n$ samples. The SGD update with learning rate (step size) $r$ is
$$
x^{(t+1)} = x^{(t)}- r G.
$$
Now suppose that you use the mean of the gradients instead. This will change the update. If we use learning rate $\tilde{r}$, we have
$$
x^{(t+1)} = x^{(t)}- \frac{\tilde{r}}{n} G.
$$
These expressions can be made to be equal by re-scaling either $r$ or $\tilde{r}$. So in that sense, the distinction between the mean and the sum is unimportant because $r$ is chosen by the researcher in either case, and choosing a good $r$ for the sum has an equivalent, rescaled $\tilde{r}$ for the mean.
One reason to prefer using the mean, though, is that this de-couples the learning rate and the minibatch size, so that changing the number of samples in the minibatch will not implicitly change the learning rate.
Note that it's standard to use the mean of the minibatch, rather than the entire training set. However, the same re-scaling argument above applies here, too -- if you're tuning the learning rate, for a fixed-size data set you'll find a learning rate which works well, and this learning rate can be re-scaled to be suitable for a gradient descent that uses the sum in place of some mean.
","**Summary:**

When using Stochastic Gradient Descent (SGD), the loss function can be expressed as a sum or an average. Using the average loss means that the scaling factor $\frac{1}{n}$ is implicitly included.

For the sum of gradients, the SGD update involves a learning rate $r$, while for the mean of gradients, it uses a different learning rate $\tilde{r}$. However, these two expressions can be made equivalent by rescaling $r$ or $\tilde{r}$.

To de-couple the learning rate from the minibatch size, it's preferable to use the mean of gradients. This allows for changes in minibatch size without affecting the learning rate.

The same rescaling argument applies to the entire training set. A learning rate tuned for a fixed-size dataset can be adjusted for a gradient descent that uses the sum instead of the mean."
How to implement L2 regularization towards an arbitrary point in space?,https://stats.stackexchange.com/questions/300618/how-to-implement-l2-regularization-towards-an-arbitrary-point-in-space,machine-learning|neural-networks|deep-learning|regularization,300618,True,300629,1586,3,11,1504582494,1504124648,14,1504126887,"You actually ask two different questions.

Having the norm tend to 5 implies that you want the weights to be near the surface of a hypersphere centered at the origin with radius 5. This regularization looks something like

$$J(\Theta, X, y) = L(\Theta, X, y) + \lambda (||w||_2^2-5)^2$$
But you could instead use something like $\lambda \cdot\text{abs}(||w||_2^2-5)$,  I suppose.

On the other hand, if you want to tend towards an arbitrary point, you just need to use that point as the center $c$. 

$$J(\Theta, X, y) = L(\Theta, X, y) + \lambda ||w-c||_2^2$$
","**Summary:**

The paragraph discusses two types of regularization techniques for neural networks:

* **Norm regularization:** Encourages weights to be near the surface of a hypersphere with a specific radius. This can be achieved using a penalty term proportional to the squared difference between the norm of the weights and the desired radius.

* **Arbitrary point regularization:** Encourages weights to approach an arbitrary point in the weight space. This can be achieved using a penalty term proportional to the squared distance between the weights and the target point.

The first type of regularization, norm regularization, tends the weights towards a hypersphere with a specific radius centered at the origin. The second type, arbitrary point regularization, allows for a more flexible target point. Both types aim to prevent overfitting by constraining the weights of the neural network."
Vectorization of Cross Entropy Loss,https://stats.stackexchange.com/questions/149139/vectorization-of-cross-entropy-loss,machine-learning|neural-networks,149139,True,149144,13265,2,11,1575211104,1430436045,3,1430437343,"No, the gradients should not be zero for the other components.  If your prediction is $\hat y_{ij}$ for some $i,j$ and your observation $y_{ij}=0$, then you predicted too much by $\hat y_{ij}$.
","The paragraph emphasizes that gradients should not be zero for other components in a specific scenario. When a prediction is made ($\hat y_{ij}$) and the corresponding observation ($y_{ij}$) is zero, it indicates that the prediction was excessive by the amount $\hat y_{ij}$. In this case, the gradients should not be zero to correct the excessive prediction and improve the model's accuracy."
Feature extracted by max pooling vs mean pooling,https://stats.stackexchange.com/questions/291451/feature-extracted-by-max-pooling-vs-mean-pooling,machine-learning|deep-learning|feature-engineering|computer-vision,291451,True,291459,13140,3,11,1715915859,1499980397,14,1499983306,"I wouldn't say the either extract features.  Instead, it is the convolutional layers that construct/extract features, and the pooling layers compress them to a lower fidelity.  The difference is in the way the compression happens, and what type of fidelity is retained:

A max-pool layer compressed by taking the maximum activation in a block.  If you have a block with mostly small activation, but a small bit of large activation, you will loose the information on the low activations.  I think of this as saying ""this type of feature was detected in this general area"".
A mean-pool layer compresses by taking the mean activation in a block.  If large activations are balanced by negative activations, the overall compressed activations will look like no activation at all.  On the other hand, you retain some information about low activations in the previous example.

","Convolutional layers extract features from input data, while pooling layers compress these features to reduce dimensionality. There are two main types of pooling layers: max-pooling and mean-pooling.

**Max-pooling** selects the maximum activation value within a block of data, prioritizing the presence of a specific feature in a general area. However, it can lose information about low activations within that block.

**Mean-pooling** calculates the average activation value within a block, which can smooth out large activations. It retains some information about low activations but may not capture the presence of specific features as strongly as max-pooling.

The choice between max-pooling and mean-pooling depends on the desired level of feature extraction and the importance of preserving low activation information."
Gradients for skipgram word2vec,https://stats.stackexchange.com/questions/253244/gradients-for-skipgram-word2vec,self-study|neural-networks|backpropagation|word2vec,253244,True,277590,11762,1,11,1581662778,1482743465,20,1493912901,"First, let's lay out what we have got and our assumptions about the shapes of different vectors. Let,

$|W|$ be the number of words in the vocab  
$y$ and $\hat{y}$ be column vectors of shape $|W|$ x 1
$u_i$ and $v_j$ be the column vectors of shape $D$ X 1 ($D$ = dimension of embeddings)
$y$ be the one-hot encoded column vector of shape $|W|$ x 1
$\hat{y}$ be the softmax prediction column vector of shape $|W|$ x 1
$\hat{y}_i = P(i|c) = \frac{exp(u_i^Tv_c)}{\sum_{w=1}^Wexp(u_w^Tv_c)}$ 
Cross entropy loss: $J = -\sum_{i=1}^Wy_ilog({\hat{y_i}})$
$U = [u_1, u_2, ...,u_k, ...u_W]$ be a matrix composed of $u_k$ column vectors.

Now, we can write 
$$J = - \sum_{i=1}^W y_i log(\frac{exp(u_i^Tv_c)}{\sum_{w=1}^Wexp(u_w^Tv_c)})$$ 
Simplifying, 
$$ J = - \sum_{i=1}^Wy_i[u_i^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))] $$
Now, we know that $y$ is one-hot encoded, so all its elements are zero except the one at, say, $k^{th}$ index. Which means, there's only one non-zero term in the summation above corresponding to $y_k$ and all others terms in the summation are zeros. So the cost can also be written as:
$$J = -y_k[u_k^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))]$$
Note: above $y_k$ is 1.
Solving for $\frac{\partial J}{\partial v_c}$ : 
$$ \frac{\partial J}{\partial v_c} = -[u_k - \frac{\sum_{w=1}^Wexp(u_w^Tv_c)u_w}{\sum_{x=1}^Wexp(u_x^Tv_c)}]$$
Which can be re-arranged as:
$$\frac{\partial J}{\partial v_c} = \sum_{w=1}^W (\frac{exp(u_w^Tv_c)}{\sum_{x=1}^W exp(u_x^Tv_c)}u_w) - u_k$$
Using definition (6), we can rewrite the above equation as: 
$$\frac{\partial J}{\partial v_c} = \sum_{w=1}^W (\hat{y}_w u_w) - u_k$$
Now let's see how this can be written in Matrix notation.Note that:

$u_k$ can be written as Matrix vector multiplication: $U.y$
And $\sum_{w=1}^W (\hat{y}_w u_w)$ is a linear transformation of vectors $u_w$ in $U$ scaled by $\hat{y}_w$ respectively. This again can be written as $U.\hat{y}$

So the whole thing can be succinctly written as:
$$U[\hat{y} -y]$$
Finally, note that we assumed $u_i$s to be a column vectors. If we had started with row vectors, we would get $U^T[\hat{y} -y]$, same as what you were looking for.
","**Summary**

The given paragraph focuses on the mathematical derivation of the partial derivative of the cross-entropy loss function with respect to a context vector \(v_c\) in a probabilistic language model. Here's a summary of the main ideas:

**Definitions:**

* \(W\): Number of words in the vocabulary
* \(y\): One-hot encoded vector representing a target word
* \(\hat{y}\): Softmax prediction vector
* \(u_i\) and \(v_j\): Column vectors representing word embeddings
* \(U\): Matrix containing all word embedding vectors

**Cross-Entropy Loss Function:**

The cross-entropy loss function measures the difference between the predicted distribution \(\hat{y}\) and the actual distribution \(y\):

$$J = -\sum_{i=1}^Wy_ilog({\hat{y_i}})$$

**Derivation of Partial Derivative:**

By simplifying the loss function, we derive the partial derivative with respect to \(v_c\):

$$\frac{\partial J}{\partial v_c} = U[\hat{y} -y]$$

where:

* \(U[\hat{y} -y]\) is a matrix-vector multiplication that represents a linear transformation of the word embedding vectors scaled by the difference between the predicted and actual distributions.

**Note:**

The derivation assumes that the embedding vectors \(u_i\) and \(v_j\) are column vectors. If they were row vectors instead, the result would be \(U^T[\hat{y} -y]\)."
Cross-entropy cost function in neural network,https://stats.stackexchange.com/questions/167787/cross-entropy-cost-function-in-neural-network,neural-networks|error-propagation,167787,True,167792,36127,2,11,1541199245,1439946748,14,1439947588,"Here's how I would express the cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression): $$\mathcal{L}(X, Y) = -\frac{1}{n} \sum_{i=1}^n y^{(i)} \ln a(x^{(i)}) + \left(1 - y^{(i)}\right) \ln \left(1 - a(x^{(i)})\right) $$
Here, $X = \left\{x^{(1)},\dots,x^{(n)}\right\}$ is the set of input examples in the training dataset, and $Y=\left\{y^{(1)},\dots,y^{(n)} \right\}$ is the corresponding set of labels for those input examples. The $a(x)$ represents the output of the neural network given input $x$.
Each of the $y^{(i)}$ is either 0 or 1, and the output activation $a(x)$ is typically restricted to the open interval (0, 1) by using a logistic sigmoid (https://en.wikipedia.org/wiki/Logistic_function). For example, for a one-layer network (which is equivalent to logistic regression), the activation would be given by $$a(x) = \frac{1}{1 + e^{-Wx-b}}$$ where $W$ is a weight matrix and $b$ is a bias vector. For multiple layers, you can expand the activation function to something like $$a(x) = \frac{1}{1 + e^{-Wz(x)-b}} \\ z(x) = \frac{1}{1 + e^{-Vx-c}}$$ where $V$ and $c$ are the weight matrix and bias for the first layer, and $z(x)$ is the activation of the hidden layer in the network.
I've used the (i) superscript to denote examples because I found it to be quite effective in Andrew Ng's machine learning course; sometimes people express examples as columns or rows in a matrix, but the idea remains the same.
","**Summary:**

Cross-entropy loss is a measure of the difference between predicted and actual values in machine learning. It is defined as:

$$\mathcal{L}(X, Y) = -\frac{1}{n} \sum_{i=1}^n y^{(i)} \ln a(x^{(i)}) + \left(1 - y^{(i)}\right) \ln \left(1 - a(x^{(i)})\right)$$

- $X$ and $Y$ represent the training dataset, with $X$ being the input examples and $Y$ being the corresponding labels (0 or 1).
- $a(x)$ is the output of a neural network given input $x$.
- $a(x)$ is typically limited to the range (0, 1) using a logistic sigmoid activation function.
- For a one-layer neural network (logistic regression), $a(x) = \frac{1}{1 + e^{-Wx-b}}$.
- For multiple layers, the activation function becomes more complex, involving multiple weight matrices and bias vectors.
- The goal of minimizing cross-entropy loss is to improve the neural network's ability to predict accurate labels for new input examples."
How to apply Softmax as Activation function in multi-layer Perceptron in scikit-learn?,https://stats.stackexchange.com/questions/243588/how-to-apply-softmax-as-activation-function-in-multi-layer-perceptron-in-scikit,neural-networks|scikit-learn|multi-class|softmax,243588,True,243900,19366,3,11,1561360053,1478013290,8,1478130040,"I suposse that the Softmax function is applied when you request a probability prediction by calling the method mlp.predict_proba(X).
To support my supposition I have developed this small experiment:

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_iris
import numpy as np

X,Y = load_iris().data, load_iris().target

mlp = MLPClassifier()
mlp.fit(X, Y)

print mlp.predict([3.1,  2.5,  8.4,  2.2])
print mlp.predict_proba([3.1,  2.5,  8.4,  2.2])
print ""sum: %f""%np.sum(mlp.predict_proba([3.1,  2.5,  8.4,  2.2]))


Notice that no matter what values are plugged into predict_proba(), the output probability vector allways sums up to 1. This can only be achieved by the Softmax activation function (Using an activation other that Softmax there is no guaranty that the sum of the activations in the final layer will be exactly one, specially for an unseen sample).
If my guess is right, looking at the documentation I can not find any method to get the output of the network before Softmax... Maybe because this class is intended solely for classification (not regression or other fancy setups).
","**Summary:**

The Softmax function is likely used in the `predict_proba()` method of scikit-learn's `MLPClassifier` to generate probability predictions. This is because the outputs from `predict_proba()` always sum to 1, a characteristic unique to the Softmax activation function. The Softmax function ensures that the neural network's output probabilities are well-calibrated and suitable for classification tasks.

Although the `MLPClassifier` class is specifically designed for classification, it does not provide a way to access the network's output before the Softmax function is applied. This is because the classifier is intended for straightforward classification tasks, and exposing the raw network outputs may not be necessary for most users."
Best way to reduce false positive of binary classification to exactly 0?,https://stats.stackexchange.com/questions/492215/best-way-to-reduce-false-positive-of-binary-classification-to-exactly-0,neural-networks|classification|optimization|false-positive-rate,492215,True,492242,20433,2,11,1602880105,1602823763,14,1602841333,"Use probabilistic classifications instead of hard 0-1 classifications. That is, predict the probability for an instance to be positive. Use proper scoring rules to assess these predicted probabilities.
Then consider whether you can make decisions based on these probabilities. You may or may not want to use a single threshold to map your probabilities to hard classes. Instead, you may even want to use multiple thresholds for multiple different actions. The mapping between probabilities and decisions should be based on explicit assumptions about the costs of wrong (and correct) decisions. More here. (https://stats.stackexchange.com/a/312124/1352)
In a nutshell: decouple the modeling/predictive part from the decision.
Do not use accuracy as a KPI at all. (https://stats.stackexchange.com/q/312780/1352) It is misleading, and especially (but not only) so for unbalanced data. The exact same problems as for accuracy apply equally to FPR.
Similarly, do not overweight one class. This is analogous to oversampling, which is commonly used to ""address"" class imbalance - but unbalanced data are not a problem (as long as you don't use misleading KPIs like accuracy or FPR), and oversampling or weighting will not solve a non-problem (https://stats.stackexchange.com/q/357466/1352).
","**Summary:**

Instead of using binary classifications (0 or 1), use probabilistic classifications to predict probabilities of an instance being positive. Assess these probabilities using proper scoring rules.

Consider using multiple thresholds to map probabilities to decisions based on the costs of making incorrect decisions.

Avoid using accuracy or FPR as key performance indicators (KPIs) due to their misleading nature, especially for unbalanced data. The same holds for oversampling or weighting one class to ""address"" class imbalance, as unbalanced data is not inherently problematic."
How can a network with only ReLU nodes output negative values?,https://stats.stackexchange.com/questions/362588/how-can-a-network-with-only-relu-nodes-output-negative-values,time-series|neural-networks|deep-learning,362588,True,362602,17472,2,11,1582211605,1534469496,17,1534477632,"Consider the definition of the ReLU:
$$
f(x) = \max\{0, x\}
$$
The output of a ReLU unit is non-negative, full stop. If the final layer of the network of ReLU units, then the output must be non-negative. If the output is negative, then something has gone wrong: either there's a programming error, or the output layer is not a ReLU.
Suppose that the last layer is linear, and this linear layer takes ReLU outputs as its input. Clearly, linear layers have no constraints on their outputs, so the output of the linear layer could be positive, negative or neither.
","**Summary:**

The Rectified Linear Unit (ReLU) activation function ensures that the output of a neural network layer is non-negative. If the final layer of a network consists of ReLU units, the output will always be non-negative. However, a linear layer, which can follow a ReLU layer, can produce outputs that are positive, negative, or zero.

When the output of a network is negative, it indicates an error or that the output layer is not a ReLU. Conversely, when the output is non-negative, it signifies that the network is functioning correctly."
Difference between pooling and subsampling,https://stats.stackexchange.com/questions/354944/difference-between-pooling-and-subsampling,neural-networks|convolutional-neural-network|computer-vision,354944,True,357600,16511,2,11,1588933742,1530953046,13,1531842163,"I don't think there is any difference. Pooling operation does sub-sampling of the image. You can find that people refer to subsample as an operation performed by pooling layer

In fact, in the paper they describe sub-sampling as a pooling layer

You can check Yann LeCun's paper Gradient-Based Learning Applied to Document Recognition (http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
","**Summary:**

Pooling layers in convolutional neural networks (CNNs) perform a form of subsampling that reduces the dimensionality of the input data. This process of reducing the size of the data while retaining its essential features is known as subsampling.

The paper ""Gradient-Based Learning Applied to Document Recognition"" by Yann LeCun refers to subsampling as a pooling layer. This suggests that pooling layers can be used to perform subsampling operations in CNNs.

In essence, pooling operations are a type of subsampling that reduce the size of the image while preserving its important characteristics. This makes them a crucial component in CNNs for efficiently extracting features from images."
How to compute bits per character (BPC)?,https://stats.stackexchange.com/questions/211858/how-to-compute-bits-per-character-bpc,probability|neural-networks|lstm|recurrent-neural-network,211858,True,261789,10745,2,11,1660558412,1462900357,17,1487051092,"From my understanding, the BPC is just the average cross-entropy (used with log base 2).
In the case of Alex Graves' papers, the aim of the model is to approximate the probability distribution of the next character given past characters. At each time step $t$, let's call this (approximate) distribution $\hat{P}_t$ and let $P_t$ be the true distribution. These discrete probability distributions can be represented by a vector of size $n$, where n is the number of possible characters in your alphabet.
So the BPC or average cross-entropy can be calculated as follows:
\begin{align}
bpc(string) = \frac{1}{T}\sum_{t=1}^T H(P_t, \hat{P}_t) &= -\frac{1}{T}\sum_{t=1}^T \sum_{c=1}^n P_t(c) \log_2 \hat{P}_t(c), \\
& = -\frac{1}{T}\sum_{t=1}^T \log_2 \hat{P}_t(x_t).
\end{align}
Where $T$ is the length of your input string.
The equality in the second line comes from the fact that the true distribution $P_t$ is zero everywhere except at the index corresponding to the true character $x_t$ in the input string at location $t$.
Two things to note:

When you use an RNN, $\hat{P}_t$ can be obtained by applying a softmax to the RNN's output at time step $t$ (The number of output units in your RNN should be equal to $n$ - the number of characters in your alphabet).
In the equation above, the average cross-entropy is calculated over one input string of size T. In practice, you may have more than one string in your batch. Therefore, you should average over all of them (i.e. $bpc = mean_{strings} bpc(string)$).   

","**Summary:**

**BPC (Bits Per Character)**, also known as average cross-entropy, measures the performance of models that predict the next character in a sequence.

**Calculation:**

BPC is calculated as the average loss per character in a string, where the loss is the cross-entropy between the true distribution of the next character and the model's predicted distribution. The cross-entropy formula involves taking the negative log of the probability of the true character under the model's distribution.

**Approximating the Probability Distribution:**

The BPC calculation relies on a model that approximates the probability distribution of the next character given past characters. In Recurrent Neural Networks (RNNs), this approximation can be obtained by applying a softmax function to the RNN's output.

**Practical Considerations:**

When calculating BPC, it is common to average the loss over multiple input strings in a batch. This provides a more robust measure of the model's performance."
Were SVMs developed as a method of efficiently training neural networks?,https://stats.stackexchange.com/questions/510417/were-svms-developed-as-a-method-of-efficiently-training-neural-networks,neural-networks|svm|history,510417,True,510430,530,1,11,1614332437,1613849800,15,1613858219,"Let me start with a quote (https://www.kdnuggets.com/2016/07/guyon-data-mining-history-svm-support-vector-machines.html) by a co-inventor of the Support Vector Machines, Isabelle Guyon:

At the time, everybody was working on multi-layer Perceptrons (the ancestors of deep learning), and my first work on optimal margin algorithms was simply some active set method to enlarge their margin, inspired by the 'minover.' Bernhard Boser, my husband, was even making dedicated hardware for MLPs! I had heated discussions with Vladimir Vapnik, who shared my office and was pushing another optimal margin algorithm that he invented in the 1960's.


The invention of SVMs happened when Bernhard decided to implement Vladimir's algorithm in the three months we had left before we moved to Berkeley.

Or, somewhat more ""official"":

SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers.

[E. Osuna, R. Freund, and F. Girosit: ""Training support vector machines: an application to face detection (https://ieeexplore.ieee.org/document/609310)"". Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 130-136, 1997]
But the whole history is, of course, much longer and more convoluted. Below I offer a very short and slim timeline, just as a brief overview:

in 1958, Rosenblatt, presents the first biologically motivated artificial neural network [F. Rosenblatt: ""The perceptron: a probabilistic model for infomation storage and organization in the Brain"". Psychological Review, 65(6):386-408]

in 1964, Vapnik and Chervonenkis present the maximum margin criterion for training the perceptron [V. Vapnik and A. Chervonenkis: ""A note on one class of perceptrons"". Automation and Remote Control, 25(1):112-120]

also in 1964, Aizerman, Braverman, and Roznoer introduce the interpretation of kernels (""potential functions"") as inner products in a feature space and prove that ""perceptron can be considered to be a realization of potential function method"" [M.A. Aizerman, E.M. Braverman and L.I. Roznoer: ""Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"". Automation and Remote Control, 25(12):821-837]

in 1969, Minsky and Papert publish their book ""Perceptrons: An Introduction to Computational Geometry"" and show the limitations of (single-layer) perceptrons.

in 1986, Rumelhart, Hinton, and Williams (re)invent the backpropagation algorithm and make learning in multi-layer perceptrons (""feed-forward neural networks"") possible [D.E. Rumelhart, G.E. Hinton, and R.J. Williams: ""Learning representations by back-propagating errors"". Nature 323(6088):533â536].

in 1992, Boser, Guyon, and Vapnik (the three from the introductory quote) present what we now call the ""Support Vector Machine"". They state: ""The technique is applicable to a wide variety of classification functions, including Perceptrons [...]"" [B.E. Boser, I.M. Guyon, and V.N. Vapnik: ""A Training Algorithm for Optimal Margin Classifiers"". Proceedings of the 5th Annual Workshop on Computational Learning Theory (COLT'92), 144-152]


","Support Vector Machines (SVMs) are a classification technique developed by Vladimir Vapnik and colleagues in 1985. SVMs emerged from the work on optimal margin algorithms, particularly Vapnik's algorithm from the 1960s.

Bernhard Boser and Isabelle Guyon played a pivotal role in implementing Vapnik's algorithm, leading to the formal introduction of SVMs in 1992. SVMs are based on the idea of maximizing the separation between data points in a higher-dimensional feature space using kernels.

The history of SVMs traces back to the development of artificial neural networks, with key contributions from researchers such as Rosenblatt, Minsky, and Papert. SVMs have since become widely used for classification tasks, demonstrating advantages in handling complex datasets with non-linear relationships."
Why does my LSTM take so much time to train?,https://stats.stackexchange.com/questions/367170/why-does-my-lstm-take-so-much-time-to-train,deep-learning|model-evaluation|time-complexity,367170,True,367235,23188,2,11,1612541380,1537111427,10,1537171102,"However much it pains me to say this, Deep learning is slow, get used to it.
There are some things you could do to speed up your training though:

What GPU are you using? A friend of mine was doing some research on LSTM's last year and training them on her NVIDIA GTX7?? GPU. Since this was going painfully slow, they tried to train the network on a more modern CPU, which actually led to a speed-up by a non trivial factor.

What framework are you using? While most frameworks are somewhat comparable, I have heard rumors (https://arxiv.org/pdf/1608.07249.pdf (https://arxiv.org/pdf/1608.07249.pdf)) that some frameworks are slower than others. It might be worthwhile to switch frameworks if you're going to be doing a lot of training.

Is it possible to train your network on your company/university hardware? Universities and research companies usually have some powerful hardware at their disposal. If this is not an option, maybe you can look into using some cloud-computing power.


All these solutions obviously assume your model itself is as optimal as it can be (In terms of training time and accuracy), which is also something you need to consider, but is outside of the scope of this answer.
","Despite the popularity of deep learning, training can be painfully slow. To address this issue, consider the following optimization strategies:

* **Upgrade your GPU:** Modern GPUs can significantly accelerate training compared to older models.
* **Choose an efficient framework:** Different frameworks have varying performance; research and consider switching to a faster alternative.
* **Utilize high-performance hardware:** Universities and research companies often have powerful hardware resources for training. Cloud computing is another option for access to faster hardware.

It's important to note that these solutions assume your model is already optimized for efficiency, which should be considered separately."
How do you interpret the cross-entropy value?,https://stats.stackexchange.com/questions/272754/how-do-you-interpret-the-cross-entropy-value,machine-learning|classification|neural-networks|interpretation|cross-entropy,272754,True,272764,21607,1,11,1579203770,1491744969,15,1491750371,"Andrew Ng explains the intuition behind using cross-entropy as a cost function in his ML Coursera course (https://youtu.be/rtD0RvfBJqQ) under the logistic regression module, specifically at this point in time (https://www.youtube.com/watch?v=YUCen9E79YY) with the mathematical expression:
$$\text{Cost}\left(h_\theta(x),y\right)=\left\{
\begin{array}{l}
-\log\left(h_\theta(x)\right) \quad \quad\quad \text{if $y =1$}\\
-\log\left(1 -h_\theta(x)\right) \quad  \;\text{if $y =0$}
\end{array}
\right.
$$
The idea is that with an activation function with values between zero and one (in this case a logistic sigmoid, but clearly applicable to, for instance, a softmax function (http://cs231n.github.io/linear-classify/#softmax) in CNN, where the final output is a multinomial logistic), the cost in the case of a true 1 value ($y=1$), will decrease from infinity to zero as $h_\theta(x)\to1$, because ideally we would like for its to be $1$, predicting exactly the true value, and hence rewarding an activation output that gets close to it; reciprocally, the cost will tend to infinity as the activation function tends to $0$. The opposite is true for $y=0$ with the trick of obtaining the logarithm of $1-h_\theta(x)$, as opposed to $h_\theta(x).$

Here is my attempt at showing this graphically, as we limit these two functions between the vertical lines at $0$ and $1$, consistent with the output of a sigmoid function:
 (https://i.sstatic.net/mm6cf.png)

This can be summarized in one more succinct expression as:
$$\text{Cost}\left(h_\theta(x),y\right)=-y\log\left(h_\theta(x)\right)-(1-y) \log\left(1 - h_\theta(x)\right).$$
In the case of softmax in CNN, the cross-entropy would similarly be formulated as 
$$\text{Cost}=-\sum_j \,t_j\,\log(y_j)$$
where $t_j$ stands for the target value of each class, and $y_j$ the probability assigned to it by the output.
Beyond the intuition, the introduction of cross entropy is meant to make the cost function convex.
","Cross-entropy is a cost function used in machine learning, particularly in logistic regression and convolutional neural networks (CNNs). For a sigmoid activation function with output values between 0 and 1, the cross-entropy cost for a true value of 1 decreases as the activation function approaches 1, rewarding predictions close to the true value. Conversely, the cost increases as the activation function approaches 0.

For a true value of 0, the cross-entropy cost for a sigmoid activation function is obtained by taking the logarithm of 1 minus the activation function, resulting in a similar behavior: cost decreases as the activation function approaches 0. This behavior encourages predictions far from the true value of 0.

The cross-entropy cost function can be generalized as:

```
Cost(h(x), y) = -y * log(h(x)) - (1 - y) * log(1 - h(x))
```

where h(x) is the output of the activation function, y is the true value (0 or 1), and log represents the natural logarithm.

In CNNs using a softmax activation function, the cross-entropy cost is formulated as:

```
Cost = - ât * log(y)
```

where t is the target value for each class, and y is the probability assigned to the class by the output.

Cross-entropy is chosen as a cost function because it is convex, allowing for efficient optimization in machine learning algorithms."
Why is information about the validation data leaked if I evaluate model performance on validation data when tuning hyperparameters?,https://stats.stackexchange.com/questions/384703/why-is-information-about-the-validation-data-leaked-if-i-evaluate-model-performa,neural-networks|cross-validation|hyperparameter,384703,True,384706,616,1,11,1546141788,1545954914,13,1545958324,"Information is leaked because you're using the validation data to make hyper-parameter choices. Essentially, you're creating a complicated optimization problem: minimize the loss over hyper-parameters $\phi$ as evaluated against the validation data, where these hyper-parameters regularize a neural network model that has parameters $\theta$ trained by use of a specific training set.
Even though the parameters $\theta$ are directly informed by the training data, the hyper-parameters $\phi$ are selected on the basis of the validation data. Moreover, because the hyper-parameters $\phi$ implicitly influence $\theta$, the information from the validation data is indirectly influencing the model that you choose.
","**Summary:**

Data leakage occurs when information from the validation set is used to optimize hyperparameters for a neural network model. The hyperparameters, denoted by $\phi$, directly impact the model's parameters, $\theta$, which are trained using the training data.

However, by optimizing $\phi$ based on the validation data, information from the validation set indirectly influences the trained model. This can lead to biased model selection, resulting in a model that performs well on the validation data but may not generalize well to unseen data.

To avoid data leakage, it is crucial to ensure that the validation data is kept strictly separate from the training data and is not used for any form of hyperparameter optimization. This ensures that the model is trained and evaluated independently, reducing the risk of overfitting and improving the model's ability to generalize to new data."
Convergence of Stochastic Gradient Descent as a function of training set size,https://stats.stackexchange.com/questions/323570/convergence-of-stochastic-gradient-descent-as-a-function-of-training-set-size,machine-learning|deep-learning|gradient-descent,323570,True,324549,14352,6,11,1516796417,1516206223,7,1516674262,"In the first part they are talking about large-scale SGD convergence in practice and in the second part theoretical results on the convergence of SGD when the optimisation problem is convex. 

""The number of updates required to reach convergence usually increases with training set size"".
I found this statement confusing but as @DeltaIV kindly pointed out in the comments I think they are talking about practical considerations for a fixed model as the dataset size $m \rightarrow \infty$. I think there are two relevant phenomena: 

performance tradeoffs when you try to do distributed SGD, or
performance on a real-world non-convex optimisation problem 

Computational tradeoffs for distributed SGD
In a large volume and high rate data scenario, you might want to try to implement a distributed version of SGD (or more likely minibatch SGD). Unfortunately making a distributed, efficient version of SGD is difficult as you need to frequently share the parameter state $w$. In particular, you incur a large overhead cost for synchronising between computers so it incentivises you use larger minibatchs. The following figure from (Li et al., 2014) illustrates this nicely 
 (https://i.sstatic.net/CPw5I.png)
SGD here is minibatch SGD and the other lines are algorithms they propose.
However it's also known that for convex problems minibatchs incur a computational cost which effectively slows convergence with increased minibatch size. As you increase minibatch size $m'$ towards the dataset size $m$ it becomes slower and slower until you're just doing regular batch gradient descent.  This decrease can be kinda drastic if a large mini-batch size is used.  Again this is illustrated nicely by (Li et al, 2014):
 (https://i.sstatic.net/CrKHb.png)
Here they are plotting the minimum objective value they found on the KDD04 dataset after using $10^7$ datapoints. 
Formally, if $f$ is a strongly convex function with global optimum $w^*$, and if $w_t$ is the output of SGD at the $t^{th}$ iteration. Then you can prove that in expectation you have:
$$
\mathbb{E}[f(w_t) - f(w^*)] \leq \mathcal{O}(\frac{1}{\sqrt{t}}).
$$
Note that this doesn't depend on the dataset size (this is relevant to your next question)! However for minibatch SGD with batch size $b$ the convergence to the optimum happens at a rate of $\mathcal{O}(\frac{1}{\sqrt{bt}} + \frac{1}{bt})$. When you have a very large amount of data (Li et al., 2014) make the point that:

Since the total number of examples examined is $bt$ while there is only
  a $\sqrt{b}$ times improvement, the convergence speed degrades with
  increasing minibatch size.

You have a trade-off between the synchronisation cost for small mini-batches, and the performance penalty for increased minibatch size. If you naively parallelise (minibatch) SGD you pay some penalty and your convergence rate slows as the data size increases. 
Nonconvexity of the empirical optimisation problem
This is basically the point that @dopexxx has made. It's pretty well-known that the optimisation problem that you want to solve for deep neural nets in practice is not convex. In particular, it can have the following bad properties:

local minima and saddle points
plateau regions
widely varying curvature

In general the shape you're trying to optimise over is thought to be a manifold and as far as I am aware all you can say about convergence is that you're going converge to (a noise ball around) a stationary point. It seems reasonable that the greater the variety of real world data, the more complicated this manifold will be. Because the real gradient is more complicated as $m$ increase you need more samples to approximate $\nabla f$ with SGD.

""However, as m approaches inï¬nity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m further will not extend the amount of training time needed to reach the modelâs best possible test error.""
(Goodfellow et al., 2016) state this a little more precisely in the discussion in section 8.3.1, where they state: 

The most important property of SGD and the related minibatch or online gradient-based optimisation is that computation time per update does not grow with the number of training examples. This allows convergence even when the number of training examples becomes very large. For a large enough dataset, SGD may converge to within some fixed tolerance of its final test set error before it has processed the entire training dataset. 

(Bottou et al., 2017) offer the following clear explanation:

On an intuitive level, SG employs information more efficiently than a
  batch method. To see this, consider a situation in which a training set, call it $S$, consists of ten
  copies of a set $S_{sub}$. A minimizer of empirical risk for the larger set S is clearly given by a minimizer
  for the smaller set $S_{sub}$, but if one were to apply a batch approach to minimize $R_n$ over $S$, then
  each iteration would be ten times more expensive than if one only had one copy of $S_{sub}$. On the
  other hand, SG performs the same computations in both scenarios, in the sense that the stochastic
  gradient computations involve choosing elements from $S_{sub}$ with the same probabilities. 

where $R_n$ is the empirical risk (essentially the training loss). If you let the number of copies get arbitrarily large, then SGD will certainly converge before it's sampled every data point.
I think this agrees with my statistical intuition. You can achieve $|x^{\text{opt}} - x_k| < \epsilon$ at iteration $k$ before you necessarily sample all the points in your dataset, because your tolerance for error $\epsilon$ really determines how much data you need to look at, i.e. how many iterations of SGD you need to complete. 
I found the first part of the paper by (Bottou et al., 2017) quite helpful for understanding SGD better.
References
Bottou, LÃ©on, Frank E. Curtis, and Jorge Nocedal. ""Optimization methods for large-scale machine learning."" arXiv preprint arXiv:1606.04838 (2016).
https://arxiv.org/pdf/1606.04838.pdf (https://arxiv.org/pdf/1606.04838.pdf)
Li, Mu, et al. ""Efficient mini-batch training for stochastic optimization."" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.
https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf (https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf)
","**First Part: Large-Scale SGD Convergence in Practice**

* For practical considerations with fixed models, convergence in SGD often requires more updates as the training set size increases.

* Implementing distributed SGD with large datasets is challenging due to synchronization costs.

* Larger minibatch sizes improve efficiency for distributed SGD but slow down convergence for convex problems.

* Trade-offs exist between synchronization costs (small minibatch sizes) and performance penalties (large minibatch sizes).

**Second Part: Theoretical Results on SGD Convergence for Convex Problems**

* For strongly convex functions, SGD converges to the global optimum at a rate of O(1/ât), independent of dataset size.

* Minibatch SGD converges slower at a rate of O(1/âbt + 1/bt).

* With increasing dataset size, the convergence speed of both SGD and minibatch SGD degrades due to the more complex nature of real-world data.

**Additional Insights**

* SGD can converge to its best possible test error before processing the entire training set, especially for large datasets.

* SGD uses information more efficiently than batch methods, especially when the training set contains multiple copies of smaller datasets."
What&#39;s a &quot;patch&quot; in CNN?,https://stats.stackexchange.com/questions/309308/whats-a-patch-in-cnn,neural-networks|terminology|convolutional-neural-network,309308,True,309311,18802,1,11,1527692933,1508687053,13,1508687861,"
By reading around, a ""patch"" seems to be a subsection of an input image to the CNN, but what exactly is it?

It's exactly what you describe. The kernel (or filter or feature detector) only looks at one chunk of an image at a time, then the filter moves to another patch of the image, and so on.

When does a ""patch"" come into play when solving problems using CNN? 

When you apply a CNN filter to an image, it looks at one patch at a time.

Why do we need ""patches""?

CNN kernels/filters only process one patch at a time, rather than the whole image. This is because we want filters to process small pieces of the image in order to detect features (edges, etc). This also has a nice regularization property, since we're estimating a smaller number of parameters, and those parameters have to be ""good"" across many regions of each image, as well as many regions of all other training images.

What's the relation between a ""patch"" and a kernel (i.e. the feature detector)?

The patch is the input to the kernel.
","**Summary:**

In convolutional neural networks (CNNs), a ""patch"" refers to a subsection of an input image that is processed by a kernel (filter or feature detector). The kernel, which is smaller than the image, examines each patch sequentially, detecting specific features (e.g., edges).

Patches are used because CNNs process images in a piecemeal fashion to extract features from local regions. By confining processing to patches, CNNs reduce the number of parameters to be estimated, promoting regularization.

The patch is the input data for the kernel, which transforms the patch into a feature map. This process is repeated for multiple patches, generating a comprehensive feature representation of the entire image."
Neural Networks Vs Structural Equation Modeling What&#39;s the Difference?,https://stats.stackexchange.com/questions/214261/neural-networks-vs-structural-equation-modeling-whats-the-difference,machine-learning|neural-networks|structural-equation-modeling,214261,True,214293,6736,1,11,1699630850,1464057190,13,1464079342,"Short answer: With SEM, the goal is generally to understand the relationships between the variables. With the type of ANNs you have been studying, the nodes are a way of transforming the data so that the predictor variables can better explain the outcomes. Ultimately the similarity is pretty superficial: while the diagrams look similar, you will struggle to get good predictions from an SEM and you will also struggle to interpret the relatships between variables in an ANN.
Pedantic answer: there are lots of different types of SEMs and ANNs. Many do not look so similar. E.g., a kohonen network looks little like an SEM, and is not great for prediction. When SEM is used to address endogeneity, it can be good for prediction, but such SEMs usually don't get drawn as pretty network diagrams.
","**Summary:**

Structural equation modeling (SEM) is a technique for exploring relationships between variables. Its primary goal is to understand the connections between variables.

Conversely, artificial neural networks (ANNs) with nodes are used to transform data to enhance the predictive power of predictor variables. The similarity between SEM and ANNs is largely aesthetic, as their functionality differs significantly.

While the diagrams of both techniques may appear similar, SEM is not a suitable tool for making predictions, and ANNs struggle to provide meaningful interpretations of variable relationships.

It's important to note that variations exist within both SEM and ANNs. Some ANNs may not resemble SEM diagrams, and certain types of SEM can be effective for prediction but are typically not visually represented as network diagrams."
Training a convolution neural network,https://stats.stackexchange.com/questions/74327/training-a-convolution-neural-network,machine-learning|neural-networks|computer-vision|backpropagation|convolutional-neural-network,74327,True,74405,3351,1,11,1450745942,1383321922,13,1383419434,"You need to first calculate all your updates as if the wieghts weren't shared, but just store them, don't actually do any updating yet. 
Let $w_k$ be some weight that appears at locations $I_k = \{(i,j) \colon w_{i,j} = w_k\}$ in your network and $\Delta w_{i,j} = -\eta \frac{\partial J}{\partial w_{i,j}} $ where $\eta$ is the learning rate and $J$ is your objective function. Note that at this point if you didn't have weight sharing you would just upade $w_{i,j}$ as 
$$
    w_{i,j} = w_{i,j} + \Delta w_{i,j}.
$$
To deal with the shared weights you need to sum up all the individual updates. So set 
$$
    \Delta w_k = \sum_{(i,j) \in I_k} \Delta w_{i,j}
$$
and then update 
$$
    w_k = w_k + \Delta w_k.
$$
","**Summary:**

When dealing with shared weights in a neural network, the update process differs from updating non-shared weights. Here's how it works:

1. **Calculate Individual Updates:** For each weight $w_k$ shared at locations $I_k$, calculate individual updates $\Delta w_{i,j} = -\eta \frac{\partial J}{\partial w_{i,j}}$, where $\eta$ is the learning rate and $J$ is the objective function.

2. **Sum Individual Updates:** Instead of updating each shared weight individually, sum up all the individual updates that correspond to that weight. Calculate $\Delta w_k = \sum_{(i,j) \in I_k} \Delta w_{i,j}$.

3. **Update Shared Weight:** Update the shared weight $w_k$ by adding the summed individual updates: $w_k = w_k + \Delta w_k$.

This approach ensures that all the shared weight locations are updated consistently, accounting for their usage throughout the network."
Transfer learning: How and why retrain only final layers of a network?,https://stats.stackexchange.com/questions/349138/transfer-learning-how-and-why-retrain-only-final-layers-of-a-network,machine-learning|neural-networks|backpropagation|transfer-learning,349138,True,349150,5600,2,11,1714276081,1527755203,9,1527759419,"
Why would one want to avoid retaining all the layers of a transfer learning network if the fine-tuning dataset was small? Ie, (if I understand it correctly), why would one not want to apply normal back-propagation through to the input layer?

If the new dataset is small, the reason to restrict training to the new layers is to avoid overfitting. The entire network contains many more parameters and, in the small data regime, there's a higher chance of finding a solution that fits the training set but doesn't generalize well. 
The idea behind transfer learning is that the original network has learned an internal representation that will also work well for the new task. This representation is given by the output of the final layer we keep from the original network. By training only the new layers, we simply keep that representation and learn how to processes it for the new task. Because the new layers contain fewer parameters than the entire network, there's less risk of overfitting.

In transfer learning, is there any difference in how backprop is applied when only training the last few layers?

There's no difference it how gradients are computed or how they're used to update the parameters. The only difference is that parameters for the early layers are held fixed, so these components of the gradient need not be computed.
","**Summary:**

In transfer learning, it is preferable to avoid training all layers of a neural network if the fine-tuning dataset is small. This is because a large network with numerous parameters is more prone to overfitting on a small dataset.

To prevent this, only the newly added layers are trained, while the existing layers (which have learned generalizable features) are kept frozen. This reduces the total number of parameters and minimizes the risk of overfitting.

During backpropagation, the gradients are calculated as usual, but only the parameters of the newly trained layers are updated. The parameters of the fixed layers remain unchanged."
What stops the network from learning the same weights in multi-head attention mechanism,https://stats.stackexchange.com/questions/373850/what-stops-the-network-from-learning-the-same-weights-in-multi-head-attention-me,neural-networks|deep-learning|attention,373850,True,376548,3609,3,11,1599395188,1540544513,8,1542019210,"We observe these kind of redundancies in literally all neural network architectures, starting from simple fully-connected networks (see diagram (https://commons.wikimedia.org/wiki/File:Multi-Layer_Neural_Network-Vector.svg) below), where same inputs are mapped to multiple hidden layers. Nothing prohibits the network from ending up with same weights in here as well.
 (https://i.sstatic.net/YxQMo.png)
We fight this by random initialization of weights. You usually need to initialize all the weights randomly, unless some special cases where initializing with zeros or other values proved to worked better. The optimization algorithms are deterministic, so there is no reason whatsoever why the same inputs could lead to different outputs if all the initial conditions were the same.
Same seems to be true for the original attention paper (https://arxiv.org/abs/1706.03762), but to convince yourself, you can check also this great ""annotated"" paper (http://nlp.seas.harvard.edu/2018/04/03/attention.html) with PyTorch code (or Keras implementation (https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py) if you prefer) and this blog post (http://jalammar.github.io/illustrated-transformer/). Unless I missed something from the paper and the implementations, the weights are treated the same in each case, so there is not extra measures to prevent redundancy. In fact, if you look at the code in the ""annotated Transformer"" post, in the MultiHeadedAttention class you can see that all the weights in multi-head attention layer are generated using same kind of nn.Linear layers. 
","Redundancies are common in neural network architectures, with the same inputs mapped to multiple layers. Random initialization of weights is used to prevent identical weights.

In the original attention paper, weights are treated the same in each multi-head attention layer, without specific measures to prevent redundancy. This can be observed in the code examples provided in the annotated Transformer post."
Can it be over fitting when validation loss and validation accuracy is both increasing?,https://stats.stackexchange.com/questions/368431/can-it-be-over-fitting-when-validation-loss-and-validation-accuracy-is-both-incr,classification|neural-networks|overfitting|sparse,368431,True,368436,10017,2,11,1588783304,1537791717,11,1537793027,"Yes, absolutely. First of all, overfitting is best judged by looking at loss, rather than accuracy, for a series of reasons including the fact that accuracy is not a good way to estimate the performance of classification models. See here:
https://stats.stackexchange.com/a/312787/58675 (https://stats.stackexchange.com/a/312787/58675)
Why is accuracy not the best measure for assessing classification models? (https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models)
Classification probability threshold (https://stats.stackexchange.com/questions/312119/classification-probability-threshold/312124#312124)
Secondly, even if you use accuracy, rather than loss, to judge overfitting (and you shouldn't), you can't just look at the (smoothed) derivative of accuracy on the test curve, i.e., if it's increasing on average or not. You should first of all look at the gap  between training accuracy and test accuracy. And in your case this gap is very large: you'd better use a scale which starts either at 0, or at the accuracy of the random classifier (i.e., the classifiers which assigns each instance to the majority class), but even with your scale, we're talking a training accuracy of nearly 100%, vs. a test accuracy which doesn't even get to 65%.
TL;DR: you don't want to hear it, but your model is as overfit as they get.
PS: you're focusing on the wrong problem. The issue here is not whether to do early stopping at the 1th epoch for a test accuracy of 55%, or whether to stop at epoch 7 for an accuracy of 65%. The real issue here is that your training accuracy (but again, I would focus on the test loss) is way too high with respect to your test accuracy. 55%, 65% or even 75% are all crap with respect to 99%. This is a textbook case of overfitting. You need to do something about it, not focus on the ""less worse"" epoch for early stopping. 
","**Summary:**

Overfitting should be assessed by examining loss rather than accuracy. Accuracy alone is not a reliable indicator for classification models.

When evaluating overfitting using accuracy, it's crucial to consider the gap between training and test accuracy. A significant gap indicates overfitting.

In this specific case, the gap between training (nearly 100%) and test accuracy (<65%) suggests severe overfitting.

Early stopping decisions based on minor accuracy differences are less important than addressing the fundamental issue of high training accuracy relative to low test accuracy.

The focus should be on reducing overfitting, rather than choosing the ""least bad"" epoch for early stopping."
Connection between filters and feature map in CNN,https://stats.stackexchange.com/questions/365307/connection-between-filters-and-feature-map-in-cnn,machine-learning|neural-networks|deep-learning|convolutional-neural-network,365307,True,365337,3359,1,11,1536138179,1536042931,11,1536055886,"Your confusion stems from the fact that channels (feature maps) are treated somewhat differently than other dimensions.
Let's say you have a grayscale image input to the first layer and 32 kernels of shape (3,3) as per your example. But in fact, those kernels have shape (3,3,1) - 1 for the number of channels in the input. For a RGB input image it would be 3. The number of channels is simply omitted in the code because it is inferred automatically from the number of channels of the layer input.
The output of this layer has 32 channels (1 per each kernel). In the second layer in your example, you have 64 kernels of shape (3,3), but they are in fact (3,3,32)! Each of these kernels is aggregating information from all input feature maps.


Typically I would think that with 64 filters and 32 feature maps from the previous layer we would get 64*32 feature maps in the next layer (all features are connected to each filter).

I hope that from the above explanation it is clear that you are not applying each of the 64 kernels on each of the 32 feature maps individually. Instead, each of these 64 kernels is looking at all of the 32 feature maps at the same time, having different weights for each of them.
","**Summary:**

Confusion arises because channels (feature maps) are handled differently than other dimensions. Convolution kernels, while typically written as (3,3), have an additional channel dimension, which is inferred from the input.

In the given example, the first layer has 32 kernels of shape (3,3,1), processing a grayscale input with 1 channel. The output has 32 channels, one for each kernel.

The second layer has 64 kernels of shape (3,3,32), with each kernel considering all 32 channels from the previous layer simultaneously. This means that the 64 kernels are not applied individually to each feature map but collectively across all 32 channels. Therefore, the output has 64 channels, not 64 * 32.

To summarize, kernels operate on multiple channels simultaneously, with different weights for each channel, resulting in a different number of channels in the output."
Single layer NeuralNetwork with ReLU activation equal to SVM?,https://stats.stackexchange.com/questions/190883/single-layer-neuralnetwork-with-relu-activation-equal-to-svm,neural-networks|svm,190883,True,191209,8106,1,11,1593187692,1452885416,11,1453121460,"Maybe what makes you think of ReLU is the hinge loss $E = max(1-ty,0)$ of SVMs, but the loss does not restrict the output activation function to be non-negative (ReLU).
For the network loss to be in the same form as SVMs, we can just remove any non-linear activation functions off the output layer, and use the hinge loss for backpropagation.
Moreover, if we replace the hinge loss with $E = ln (1 + exp(âty))$ (which looks like a smooth version of hinge loss), then we'll be doing logistic regression as typical sigmoid + cross-entropy networks. It can be thought of as moving the sigmoid function from the output layer to the loss. 
So in terms of loss functions, SVMs and logistic regression are pretty close, though SVMs use a very different algorithm for training and inference based on support vectors.
There's a nice discussion on the relation of SVM and logistic regression in section 7.1.2 of the book Pattern Recognition and Machine Learning.
 (https://i.sstatic.net/MyUMt.png)
","**Summary:**

SVMs and logistic regression share similarities in their loss functions. SVMs use the hinge loss, which penalizes incorrect predictions with a non-negative margin. Logistic regression employs the cross-entropy loss, a differentiable approximation of the hinge loss.

To align SVMs with logistic regression's network loss, the non-linear activation function at the output layer can be removed, and the hinge loss can be used for backpropagation.

Alternatively, replacing the hinge loss with a smooth version (e.g., $E = ln (1 + exp(âty))$) results in the logistic regression model. This approach can be interpreted as shifting the sigmoid activation function from the output layer to the loss function.

Despite the similarities in loss functions, SVMs and logistic regression utilize different training and inference algorithms. SVMs employ support vector-based methods, while logistic regression leverages gradient-based optimization techniques."
How to determine the number of convolutional operators in CNN?,https://stats.stackexchange.com/questions/133074/how-to-determine-the-number-of-convolutional-operators-in-cnn,neural-networks|deep-learning|convolutional-neural-network|computer-vision,133074,True,133892,12666,1,11,1539676806,1421039193,11,1421554960,"I'm assuming that when you say 11x11x10 you mean that you have a layer with 10, 11x11 filters. So the number of convolutions that you'll be doing is simply 10, 2D discrete convolution per filter in your filter bank. So, let's say that you have a network:
480x480x1    # your input image of 1 channel
11x11x10     # your first filter bank of 10, 11x11 filters
5x5x20       # your second filter bank of 20, 5x5 filters
4x4x100      # your final filter bank of 100, 4x4 filters    

You're going to be doing: $10 + 20 + 100 = 130$ multi channel 2D convolutions each with a depth of 1, 10, and 20 respectively. As you can see, the depth of each convolution is going to change as a function of the depth of the input volume from the previous layer.
But I assumed that you're trying to figure out how to compare this to a single channel 2D convolution. Well, you could just multiply the depth of each input volume by the number of filters in each layer and add them together. In your case: $10 + 200 + 2000 = 2,210$.
Now this only tells you how many single channel 2D convolutions you're doing, not how computationally intensive each convolution is, the computational intensity of each convolution will depend on a variety of parameters including image_size, image_depth, filter_size, your stride (how far you step between each individual filter calculation), the number of pooling layers you have, etc.
","**Summary:**

Convolutional neural networks (CNNs) perform convolutions between filters and input data. The number of convolutions in a CNN is determined by the number of filters in each layer.

For a network with layers of filters:

* 11x11x10 (10 filters of size 11x11)
* 5x5x20 (20 filters of size 5x5)
* 4x4x100 (100 filters of size 4x4)

The total number of convolutions is 130 (10 + 20 + 100).

To compare this to single-channel 2D convolutions, multiply the depth of each input volume by the number of filters in each layer and add them together. In this case, the equivalent number of single-channel convolutions is 2,210 (10 + 200 + 2000).

However, this only provides a measure of the number of convolutions, not their computational intensity, which depends on factors such as image size, filter size, stride, and pooling layers."
How to use the transformer for inference,https://stats.stackexchange.com/questions/376211/how-to-use-the-transformer-for-inference,neural-networks|natural-language|attention,376211,True,376557,7463,1,11,1542023963,1541782162,10,1542023963,"A popular method for such sequence generation tasks is beam search (https://en.wikipedia.org/wiki/Beam_search). It keeps a number of K best sequences generated so far as the ""output"" sequences. 
In the original paper different beam sizes was used for different tasks. If we use a beam size K=1, it becomes the greedy method (http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding) in the blog you mentioned.
","Beam search is a technique for sequence generation that maintains a set of the K best sequences generated so far. Unlike greedy methods (which consider only the best sequence), beam search explores multiple promising sequences, allowing for more diverse and accurate results.

The choice of beam size K is crucial, as it balances exploration (with larger K) and exploitation (with smaller K). In the original paper that introduced beam search, different beam sizes were used for different tasks, demonstrating the flexibility of the approach."
Sampling z in VAE,https://stats.stackexchange.com/questions/361643/sampling-z-in-vae,machine-learning|neural-networks|variational-bayes,361643,True,361657,5275,1,11,1533956536,1533906716,10,1533912884,"Because VAEs are really a graphical model more than they are autoencoders, it can be confusing what exactly ""reconstruction"" means in context.
Recall that we have an lower bound on the log probability given by the RHS of:
$$\log p(x) - D_{KL}(Q(z|x)||P(z|x)) = E_{z\sim Q}[\log p(x|z)] - D_{KL}(Q(z|x)||P(z))$$
To compute this lower bound -- a necessary prerequisite for doing a backprop pass to maximize it -- corresponds to passing $x$ through the encoder to arrive at $Q(z|x)$, computing the KL-divergence, then estimating $E_{z \sim Q}[\log p(x|z)]$ by sampling once or more (but usually just once) from $Q$ and running the decoder.
This process of estimating the posterior with the encoder and then sampling to approximate the expectation in the RHS so closely mimics the computation of an autoencoder would do that we call it ""reconstruction"". However, it's really just a side effect of trying to maximize the log probability of the inputs.
What happens when you sample multiple times from $Q$? The immediate consequence is that you get a better approximation of the expectation, and hence a better approximation of the lower bound on the log probability. You also need to run the decoder multiple times, which can be expensive, so it is usually not done. Of course if you do this, then you end up with many reconstructions rather than just one. Note that it is definitely not possible to average the reconstructions and have a meaningful output.
So you probably just want to sample once.

In response to your edit, the correct way to write it would be 
$$\begin{align*} E_{z \sim Q}[\log p(x|z)] &\approx \frac{1}{n}\sum_{i=1}^N \log p(x|z_i) \\
&\propto -\frac{1}{n}\sum_i ||x-\text{decode}(z_i)||_2^2  \\
&= -\frac{1}{nm} \sum_i \sum_j (x_j - \text{decode}(z_i)_j)^2\end{align*}$$
We would expect that the reconstructions $\text{decode}(z_i)$ look quite similar to each other, but not exactly the same. Exactly how well depends on the nature of the data and how well the model is fitted.
","**Summary:**

Variational Autoencoders (VAEs) are graphical models that estimate the posterior distribution over latent variables given an input. The process of estimating this posterior distribution involves sampling from the latent distribution and running the decoder to approximate the expectation in the lower bound on the log probability of the input.

This process of sampling from the latent distribution and decoding the resulting latent samples is referred to as ""reconstruction."" However, this reconstruction is not a primary goal of the VAE but rather a side effect of maximizing the log probability of the inputs.

Sampling multiple times from the latent distribution provides a better approximation of the expectation and the lower bound but requires running the decoder multiple times, which can be computationally expensive. As a result, sampling is typically done only once.

The multiple reconstructions obtained from sampling multiple times are not meaningful when averaged, so it is generally preferable to sample once."
Variational Autoencoder - understanding the latent loss,https://stats.stackexchange.com/questions/304289/variational-autoencoder-understanding-the-latent-loss,deep-learning|validation|loss-functions|autoencoders,304289,True,304310,6596,2,11,1506857254,1505997763,9,1506005731,"To get the variational objective, begin with marginal likelihood:
$p(x) = \int p(x, z) dz =  \int \frac{q(z)}{q(z)} p(x,z) dz$
Recognise this as an expectation:
$ = \mathbb E_q [\frac{p(x,z)}{q(z)}] $
Use Jensen's inequality
$\ln p(x) \geq \mathbb E_q [\ln \frac{p(x,z)}{q(z)}] $
Use def of conditional probability and properties of logs:
$  = \mathbb E_q [\ln p(x|z) + \ln p(z) - \ln q(z)] $
Use def of KL divergence:
$ = \mathbb E_q [\ln p(x|z)] - KL(q||p)$
Sprinkle in some $\theta$ and $\phi$ as appropriate and we have the format required. The conditioning of $z$ on $x$ in their notation $q_\phi(z|x)$   is a little unnecessary & confusing IMHO since $q$ is selected to minimise divergence from the posterior (although I get they're flagging it's an approx to the posterior rather than the prior). 
","**Summary:**

The variational objective is derived from the marginal likelihood using Jensen's inequality and other mathematical techniques. It can be expressed as:

```
VI = E[log p(x|z)] - KL(q||p)
```

Where:

* E[log p(x|z)] is the expected value of the conditional log probability of observing data x given latent variable z, under the model p.
* KL(q||p) is the Kullback-Leibler divergence between the approximate posterior q(z|x) and the true posterior p(z|x).

The variational objective is a measure of the difference between the approximate posterior q and the true posterior p. Minimizing the variational objective ensures that the approximation q is as close as possible to the true posterior, making it useful for approximating intractable integrals and optimizing probabilistic models."
How to avoid &#39;Catastrophic forgetting&#39;?,https://stats.stackexchange.com/questions/314508/how-to-avoid-catastrophic-forgetting,deep-learning|natural-language,314508,True,314712,5858,1,11,1511538677,1511079642,9,1511186722,"Catastrophic forgetting is a inherent problem in neural networks. From Wikipedia,

(Catastrophic forgetting) is a radical manifestation of the 'sensitivity-stability'
  dilemma or the 'stability-plasticity' dilemma. Specifically,
  these problems refer to the issue of being able to make an artificial
  neural network that is sensitive to, but not disrupted by, new
  information. Lookup tables and connectionist networks lie on the
  opposite sides of the stability plasticity spectrum. The former
  remains completely stable in the presence of new information but lacks
  the ability to generalize, i.e. infer general principles, from new
  inputs.

What is Catastrophic forgetting? Let's consider two tasks: Task A and task B. Now, suppose we're using a pre-trained model that is already pretty good on task A (learned weights $\theta_A$), and we want to ""fine-tune"" it to also fit task B. The common practice is to take the weights of a model trained on task A and use them as initialization for training on task B. This works well in applications in which task B is a ""sub-task"" of task A (e.g task B is detecting eyeglasses, and task A is detecting faces). When B is not a sub-task of A, there is the fear that catastrophic forgetting will occur: essentially, the network will use the same neurons that were before optimized for task A, for predicting on task B. In doing this, it will completely lose its ability to classify instances of task A correctly. You can actually experiment with this yourself: You can building a small network that can tell whether an MNIST image is a 5 or not a 5 and measure it's accuracy at this task; If you then go on to finetune this model to the task of telling whether an MNIST image is a 4 or not, you will note that the accuracy of the final model on the original task (recognizing 5) has worsened.
A Naive Solution. The naive solution to catastrophic forgetting would be to not only initialize the weights of the finetuned model to be $\theta_A$, but also add regularization: penalize the solution of the finetuned model when it get's far from $\theta_A$. Essentially, this means the the objective will be to find the best solution for task B that it still similar to $\theta_A$, the solution to task A. The reason why we call this a naive approach is that it often doesn't work well. The functions learned by neural networks are often very complicated and far from linear, so a small change in parameter values (i.e $\theta_B$ being close to $\theta_A$) can still lead to very different outcomes (i.e $f_{\theta_A}$ is very different from $f_{\theta_B}$). Since it's the outcomes we care about, this is bad for us.
Pseudo-rehearsal. A better approach would be to try to be good on task B while simultaneously giving similar answers to the answers given by  $f_{\theta_A}$. The good thing is that this approach is very easy to implement: Once you have learned $\theta_A$, we can use that model to generate an infinite number of ""labeled"" examples $(x,f_{\theta_A}(x))$. Then, when training the fine-tuned model, we will alternate between examples labeled for task B and examples of the form $(x,f_{\theta_A}(x))$. You can think about the latter as ""revision exercises"" that make sure that our network does not lose it's ability to handle task A while learning to handle task B.
An even better approach: add memory. As humans, we are good both in generalizing (plasticity) using new examples and in remembering very rare events, or maintaining skills we didn't use for a while (stability). In many ways the only method to achieve something similar with deep neural networks, as we know them, is to incorporate some form of ""memory"" into them. This it outside the scope of your question but it is an interesting and active field of research so I though I'd mention it. See this example recent work: LEARNING TO REMEMBER RARE EVENTS (https://arxiv.org/pdf/1703.03129.pdf).
","**Catastrophic Forgetting in Neural Networks**

Catastrophic forgetting occurs when a neural network loses its ability to perform a previously learned task after being trained on a new task. This is due to the network's tendency to adapt its weights and connections to optimize for the new task, which can overwrite the knowledge it had previously acquired for the old task.

The naive solution to catastrophic forgetting is to add regularization, penalizing the network for deviating from its original weights. However, this is often ineffective due to the non-linear nature of neural networks.

A better approach is ""pseudo-rehearsal,"" where labeled examples from the old task are used during training on the new task. This ensures that the network retains its ability to handle both tasks.

An even more effective approach is to incorporate memory into neural networks, enabling them to remember rare events and maintain skills without catastrophic forgetting. This is an active area of research."
Differentiation of Cross Entropy,https://stats.stackexchange.com/questions/277203/differentiation-of-cross-entropy,machine-learning|neural-networks|derivative|cross-entropy|differential-equations,277203,True,277718,18226,1,11,1575211907,1493755216,9,1493972227,"Your $\frac{\partial E}{\partial o_j}$ is correct, but $\frac{\partial E}{\partial z_j}$ should be
$$\frac{\partial E}{\partial z_j}=\sum_i\frac{\partial E}{\partial o_i}\frac{\partial o_i}{\partial z_j}$$
when $i=j$, using the results given in the post we have
$$\frac{\partial E}{\partial o_j}\frac{\partial o_j}{\partial z_j}=-\frac{t_j}{o_j}o_j(1-o_j)=t_jo_j-t_j$$
when $i\neq j$
$$\frac{\partial o_i}{\partial z_j}=\frac{\partial \frac{e^{z_i}}{\sum_ke^{z_k}}}{\partial z_j}=-\frac{e^{z_i}}{(\sum_ke^{z_k})^2}e^{z_j}=-o_io_j$$
$$\frac{\partial E}{\partial o_i}\frac{\partial o_i}{\partial z_j}=-\frac{t_i}{o_i}(-o_io_j)=t_io_j$$
so the summation is
$$\frac{\partial E}{\partial z_j}=\sum_i\frac{\partial E}{\partial o_i}\frac{\partial o_i}{\partial z_j}=\sum_it_io_j-t_j$$
since $t$ is a one-hot vector, $\sum_it_i=1$ therefore
$$\frac{\partial E}{\partial z_j}=o_j-t_j$$
also see this question (https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function).
","The partial derivative of the cross-entropy loss function with respect to the input $z_j$ involves two terms:

1. $\frac{\partial E}{\partial o_j}$, which represents the change in the loss with respect to the output probability $o_j$ associated with the correct class.

2. $\frac{\partial o_j}{\partial z_j}$, which represents the change in $o_j$ with respect to the input $z_j$.

When $i=j$ (i.e., considering the correct class), these terms combine to give $-\frac{t_j}{o_j}o_j(1-o_j) = t_jo_j-t_j$, where $t_j$ is a one-hot vector (containing 1 for the correct class and 0 for others).

When $i\neq j$ (i.e., considering incorrect classes), these terms combine to give $t_io_j$, since $\frac{\partial o_i}{\partial z_j}=-o_io_j$.

Summing over all classes, the overall partial derivative of the loss with respect to $z_j$ becomes $\sum_i\frac{\partial E}{\partial o_i}\frac{\partial o_i}{\partial z_j} = \sum_it_io_j-t_j$.

Using the property of one-hot vectors, where $\sum_it_i=1$, this simplifies to $\frac{\partial E}{\partial z_j} = o_j-t_j$."
Q-learning with Neural Network as function approximation,https://stats.stackexchange.com/questions/200006/q-learning-with-neural-network-as-function-approximation,neural-networks|reinforcement-learning,200006,True,200146,5132,1,11,1457257799,1457119699,9,1457212680,"Your target should be just
$r_{t+1}+\gamma \max_a Q(s_{t+1},a)$.
Note that your error term (which is correct) could then be rewritten as
$r_{t+1}+\gamma \max_a Q(s_{t+1},a) - Q_t$
which is the term inside brackets in the update formula. This will get multiplied by your NN learning rate and other backpropagation terms during learning, and then added to the previous weights, just like the $Q$ update formula.
","**Summary:**

The goal of reinforcement learning is to maximize the expected rewards. This is achieved through the Bellman equation, which updates the value function by calculating the maximum expected reward for each possible action in the next state, weighted by a discount factor.

The update formula involves subtracting the current value function from the target value. The target value is calculated as the reward at the current state plus the discounted maximum expected reward at the next state.

During training, the error term, which is the difference between the target value and the current value, is multiplied by the learning rate and used to adjust the weights of a neural network. This update process iteratively improves the value function."
Capacity and expressivity of a neural network,https://stats.stackexchange.com/questions/469493/capacity-and-expressivity-of-a-neural-network,neural-networks,469493,True,477219,3895,1,11,1594786234,1590875450,8,1594786234,"There definitely is a lot of overlap and interchangeability in how those terms are commonly used. I think the main distinction is that expressivity is often used to talk about what classes of functions a neural network can approximate / learn, while capacity measure some notion of how much ""brute force"" ability the network has to contort itself into fitting the data. This is not the ""only"" definition, but just what I most often come across when reading.
From Understanding deep learning requires rethinking generalization (https://arxiv.org/abs/1611.03530)

Much effort has gone into characterizing the expressivity of neural
networks, e.g, Cybenko (1989); Mhaskar (1993); Delalleau & Bengio
(2011); Mhaskar & Poggio (2016); Eldan & Shamir (2016); Telgarsky
(2016); Cohen & Shashua (2016). Almost all of these results are at the
âpopulation levelâ showing what functions of the entire domain can
and cannot be represented by certain classes of neural networks with
the same number of parameters.
...
The effective capacity of neural networks is sufficient for memorizing
the entire data set

Commonly, ""expressivity"" is used in claims about what types of functions a particular architecture can fit. For example, from PointNet (https://arxiv.org/abs/1612.00593)

Theoretically and
experimentally we find that the expressiveness of our
network is strongly affected by the dimension of the max
pooling layer, i.e., K in (1).

(Followed by a theoretical analysis of the robustness of their model.)
From Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
 (https://arxiv.org/abs/1707.02968)

We also observe that to fully exploit 300M images, one needs higher
capacity models. For example, in case of ResNet-50 the gain on COCO
object detection is much smaller (1.87%) compared to (3%) when using
ResNet-152.

This feels like it's referring more to the brute ability to fit more data than it is about any notion of flexibility or expressivity -- after all, what can be ""expressed"" with 152 layers which can't with 50?

Suppose you want to learn some function which maps sets of objects to some label. A commonly used design pattern is to apply a per-object neural network to each object to obtain a feature vector for each object, then take the average/sum of the feature vectors and feed it into a second neural network.
If you make the neural networks big enough, perhaps you will have a very high capacity model. You might find that as you get more and more training data, your model can keep fitting them all without any problem. In fact even if you shuffle all the labels, the model has the capacity to just memorize what inputs should have what labels.
However, suppose later you find out that the inputs are actually ordered sets. Since the above architecture is totally unaware of the order of the input set (the average/sum operation throws that away), you'd realize it's not expressive enough when it comes to those types of problems (where order information is needed).
So, you can have a high capacity network, but with low expressivity with respect to a certain class of functions. You could also have an expressive model, but with limited capacity, for example if you didn't crank up the number of layers enough.

This is just my informal interpretation of the terms as they commonly appear in the ""deep learning"" literature. I am not aware of any canonical definition of either term, and to some degree they do get used interchangeably, so I think context is the most important thing here. Also, I don't closely follow the theory side of things, so it is entirely possible that community has assigned some more precise meanings to those terms.
","**Summary:**

Expressivity and capacity are two terms used in deep learning to characterize neural networks.

* **Expressivity** measures the types of functions a neural network can approximate. It refers to the network's ability to represent a wide range of functions, allowing it to solve various problems.

* **Capacity** refers to the network's ability to ""brute force"" memorization and fitting of data. A network with high capacity can potentially fit any arbitrary data, even if it's not representative.

While these terms often overlap in usage, a key distinction is that expressivity focuses on the network's theoretical abilities, while capacity measures its practical fitting capabilities.

Neural networks with high capacity may not always have high expressivity, as they may lack the architectural features to handle certain types of data, such as ordered sets. Conversely, an expressive network may have limited capacity if its size is insufficient to handle large datasets.

The choice of network depends on the specific problem being addressed. For example, if the task requires handling ordered data, a more expressive network would be necessary. Alternatively, if the primary concern is fitting a large dataset, a network with higher capacity would be more suitable."
Keras: What is the meaning of batch_size for validation?,https://stats.stackexchange.com/questions/459795/keras-what-is-the-meaning-of-batch-size-for-validation,machine-learning|neural-networks|validation|keras|train,459795,True,459798,9464,1,11,1586619527,1586618779,8,1586619527,"It means that the validation data will be drawn by batches. There may be cases when you canât put the whole validation dataset at once in your neural net, you do it in minibatch, similarly as you do for training.
","During neural network validation, it may not be feasible to process the entire validation dataset at once. Therefore, a batching approach is employed, where the dataset is divided into smaller groups (minibatches). Each minibatch is fed into the neural network, and the performance is evaluated based on the results. This allows for more efficient and manageable validation, especially for large datasets."
Question about Continuous Bag of Words,https://stats.stackexchange.com/questions/140377/question-about-continuous-bag-of-words,machine-learning|neural-networks|natural-language|word-embeddings|language-models,140377,True,140443,2940,2,11,1499689854,1425511272,6,1425550289,"Figure 1 there clarifies things a bit. All word vectors from window of a given size are summed up, result is multiplied by (1/window size) and then fed into output layer. 
Projection matrix means a whole lookup table where each word corresponds to single real-valued vector. Projection layer is effectivly a process that takes a word (word index) and returns corresponding vector. One can either concatenate them (obtaining input of size k*n where k is window size and n is vector length) or as in CBOW model, just sum all of them (obtaining input of size n).  
 (https://i.sstatic.net/8QemR.png)
","**Summary:**

Figure 1 depicts the Word2Vec model's input and output layers. Input word vectors within a defined window size are summed and normalized before being passed to the output layer.

The projection matrix is a lookup table that maps each word to a real-valued vector, effectively acting as a vector representation for each word.

Two input methods are used:

1. **Concatenation:** Concatenating all word vectors within the window results in an input vector of size k*n, where k is the window size and n is the vector length.
2. **CBOW Sum:** Summing all word vectors within the window results in an input vector of size n."
"On Yolo, and its loss function",https://stats.stackexchange.com/questions/353607/on-yolo-and-its-loss-function,neural-networks|loss-functions|yolo,353607,True,355013,4950,1,11,1557815824,1530201288,5,1531003878,"Basically, yolo combines detection and classification into one loss function: the green part corresponds to whether or not any object is there, while the red part corresponds to encouraging correctly determining which object is there, if one is present.

Since we are training on some labeled dataset, it means that $p_i(c)$ should be zero except for one class $c$, right?

Yes. Notice we are only penalizing the network when there is indeed an object present. But if your question is whether $p_i(c)\in\{0,1\}$, then usually yes, that is how it is done.

Why are we interested in confidence score? At the end of the neural net, do we have some decision algorithm that says: if this bounding box as confidence above threshold $c_0$ then displays it and choose class with highest probability?

Usually, yes, a threshold is needed exactly as you describe. Often it is a hyper-parameter that can be chosen or cross-validate over.
As for your other questions about the ""confidence"" score, I must agree that the nomenclature is confusing. There are two ""viewpoints"" one can have about this: (1) a probabilistic confidence measure of whether any object exists in the locale, and (2) a deterministic prediction of the overlap between the local predicted bounding box $\hat{B}$ and the ground truth one $B$. Both outlooks are often conflated, and in some sense can be treated as ""equivalent"", since we can view $|B\cap \hat{B}|/|B\cup\hat{B}|\in[0,1]$ as a probability.

As an aside, there are already a couple other discussions of the yolo loss:

Yolo Loss function explanation (https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation/287497)
How to calculate the class probability of a grid cell in YOLO object detection algorithm? (https://stats.stackexchange.com/questions/276204/how-to-calculate-the-class-probability-of-a-grid-cell-in-yolo-object-detection-a)

","**Summary:**

YOLO combines detection and classification into a single loss function, with the green part punishing incorrect object presence determination, and the red part encouraging correct object identification.

The $p_i(c)$ value indicates the probability of class $c$ being present in grid cell $i$. $p_i(c)$ is typically binary (0 or 1) in real-world applications.

The ""confidence"" score in YOLO serves as a threshold for displaying bounding boxes. If a box's confidence exceeds a hyperparameter or cross-validated threshold, it is displayed with the highest probability class assigned to it.

There are two perspectives on the ""confidence"" score:

1. A probabilistic measure of object presence in a given location.
2. A deterministic prediction of overlap between the predicted and ground truth bounding boxes.

In practice, these perspectives can be treated as equivalent, as the overlap ratio can be interpreted as a probability."
Oversampling: whole set or training set,https://stats.stackexchange.com/questions/352266/oversampling-whole-set-or-training-set,neural-networks|small-sample|resampling|model-evaluation|oversampling,352266,True,352289,4539,1,11,1529489950,1529477469,5,1529489950,"Your test set should be as close to a sample from the distribution on which you are actually going to apply your classifier as possible. I would definitely split your dataset first (in fact, that is usually the first thing I would do after obtaining a dataset), put away your test set, and then do everything you want to do on the training set. Otherwise, it is very easy for biases to creep in.
Same applies to your validation sets, e.g. if you are using cross-validation. What you really want is an estimate of how well your approach would work out-of-sample so that you can select the best approach. The best way to get that is to evaluate your approach on actual, unmodified out-of-sample data.
","**Summary:**

To accurately assess a classifier's performance, it is crucial to separate the test set from the training set before any data manipulation. Doing so prevents biases that can arise from using the same data for both training and testing.

The test set should resemble the distribution of data on which the classifier will be applied in real-world situations. This ensures a realistic estimation of the classifier's out-of-sample performance.

The same principle applies to validation sets used in cross-validation. By using unmodified out-of-sample data for evaluation, you obtain an accurate estimate of the classifier's generalization ability.

In essence, segregating the test set from the training set and evaluating performance on unmodified out-of-sample data is essential for selecting the best classifier and ensuring its robustness in real-world applications."
Choosing a Generative Models for time series data,https://stats.stackexchange.com/questions/240525/choosing-a-generative-models-for-time-series-data,neural-networks|forecasting|generative-models,240525,True,298525,5202,1,11,1508521399,1476632080,5,1503020438,"I think using generative model from deep learning will not lead you anywhere with the time series economic data you are considering, and GAN will probably be the worst generative model for your application.
What you are considering is to be able to sample from $p(x_{t+1}|x_{1...t})$, which should be captured by LSTM or AR models you used. The key here is that the probability is a conditional probability, while the generative models are unconditional probability $p(x)$. For example, most generative model takes as input some relatively meaningless latent variable value, so you cannot even make a model that takes $x_{1..t}$ as your input. 
What I would suggest is to simply assume an AR(1) type process, and takes $x_t$ as input to a simple multilayer perceptron to predict $x_{t+1}$. This is kind of like a model with complexity in between your AR linear model and LSTM, but the multilayer perceptron can capture more nonlinear dependency than AR linear model and has less variance and is easier to train than LSTM.
You mentioned you failed in using AR and LSTM model. Using LSTM is usually hard with high variance data for economic data and can quickly overfit into some unknown region. If you still want to try it, one suggestion is to assume a normal distribution of all your variables, and uses LSTM to output the mean and variance of model prediction.
","**Summary:**

Generative models like GAN are unsuitable for time series economic data because they model unconditional probabilities ($p(x)$) while time series data requires conditional probabilities ($p(x_{t+1}|x_{1...t})$).

Instead, consider an AR(1) process where $x_{t+1}$ is predicted using a multi-layer perceptron (MLP):

* MLPs have intermediate complexity between linear AR models and LSTMs.
* They can capture nonlinear dependencies and are easier to train than LSTMs.

If LSTM models are preferred:

* Assume a normal distribution for variables.
* Train LSTM to predict the mean and variance of model output, mitigating overfitting and high variance in economic data."
How many parameters can your model possibly have?,https://stats.stackexchange.com/questions/175259/how-many-parameters-can-your-model-possibly-have,classification|neural-networks|deep-learning,175259,True,175702,13938,1,11,1444138543,1443816537,5,1444137555,"Yes, there should not be 10 million parameters of a model which trained on CIFAR-10 as its input dimension is small (32*32*3 = 3072). It can barely reach to million of parameters, but that model becomes prone to over-fitting. Here is a reasonable structure of convnet trained on CIFAR-10; 
2 convolution layer 1 fully connected layer and 1 classification layer (also fully connected). Most of the parameters are concentrated on the last two layers as they are fully connected. 

Filter size at the first convolution layer is 7x7@32
Pooling size at the first pooling layer is 2x2
Filter size at the second convolution layer is 5x5@16
Pooling size at the second pooling layer is 1x1 (no pooling)

I'm assuming a valid convolution and stride number of pooling is equal to pooling size. At his configurations, dimension of the first feature maps are (32-7+1)/2 x (32-7+1)/2 = 13x13@32. Dimension of the second feature maps are (13-5+1)/1 x (13-5+1)/1 = 9x9@16
As convolution layers are rolled into vector before passing into fully connected layer, dimension of the first fully connected layer is 9*9*16 = 1296.
Lets assume that the last hidden layer contains 500 units (this hyper-parameter is the most significant one for total number of parameters). And the last layer is 10. 
At total the number of parameters are 7*7*32 + 5*5*16 +  1296*500 + 500*10 = 1568 + 400 + 648000 + 5000 = 654968.
But I expect smaller network can yield better results as the number of samples is relatively small. So if the 500 neurons reduced to 100 neurons, the total number of parameters reduces to 1568 + 400 + 129600 + 5000 = 136568. Maybe it would be better to include another pooling layer at the second layer, or discarding first fully connected layer.
As you can see most of the parameters are concentrated at the first fully connected layer. I don't think deeper network can yield significant gain as the dimension of input layer is small (relative to ImageNet). So your point is right.
If you concern about over-fitting you can check 'Recuding overfitting' section of Alex's convnet paper (http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)
","In designing a Convolutional Neural Network (ConvNet) for CIFAR-10 image classification, it is crucial to avoid overfitting by using an appropriate network structure. A reasonable architecture includes 2 convolutional layers, 1 fully connected layer, and 1 classification layer.

The first convolutional layer uses 7x7 filters and 32 channels, followed by 2x2 max pooling. The second convolutional layer uses 5x5 filters and 16 channels, without additional pooling. The flattened output is then connected to a fully connected layer with 500 units, followed by a 10-unit classification layer.

This network has 654,968 parameters, primarily concentrated in the fully connected layer. To mitigate overfitting, the number of units in the fully connected layer can be reduced to 100, resulting in 136,568 parameters.

Deeper networks may not significantly improve performance due to the relatively small input size of CIFAR-10 images. Overfitting concerns can be addressed by following the recommendations in AlexNet's ""Reducing Overfitting"" section."
Recurrent Neural Network (RNN) topology: why always fully-connected?,https://stats.stackexchange.com/questions/210111/recurrent-neural-network-rnn-topology-why-always-fully-connected,machine-learning|neural-networks|backpropagation,210111,True,211565,7834,2,11,1599275995,1461979043,3,1462785183,"One reason might be due to the mathematical convenience. The vanilla recurrent neural network (Elman-type (https://pdfs.semanticscholar.org/69bf/c8b2a1111acadd1d03d8994ce696795fc389.pdf)) can be formularized as:
$\vec{h}_t = f(\vec{x}_t, \vec{h}_{t-1})$, where $f(\cdot)$ can be written as $\sigma(W\vec{x}_t + U\vec{h}_{t-1})$.
The above equation corresponds to your first picture. Of course you can make the recurrent matrix $U$ sparse to restrict the connections, but that does not affect the core idea of the RNN. 
BTW: There exists two kinds of memories in a RNN. One is the input-to-hidden weight $W$, which mainly stores the information from the input. The other is the hidden-to-hidden matrix $U$, which is used to store the histories. Since we do not know which parts of histories will affect our current prediction, the most reasonable way might be to allow all possible connetions and let the network learn for themself.
","**Summary:**

The design of vanilla recurrent neural networks (RNNs) is driven by mathematical convenience. The model is expressed as:

```
$\vec{h}_t = f(\vec{x}_t, \vec{h}_{t-1})$
```

where **$\vec{h}_t$** represents the hidden state at time **$t$**, **$\vec{x}_t$** is the input at time **$t$**, and **$f(\cdot)$** is the activation function.

The function **$f(\cdot)$** is typically specified as:

```
$\sigma(W\vec{x}_t + U\vec{h}_{t-1})$
```

where **$W$** is the matrix of input-to-hidden weights, **$U$** is the matrix of hidden-to-hidden weights, and **$\sigma$** is a sigmoid function.

The matrix **$U$** allows connections between all hidden units, allowing the network to store historical information. Since it is impractical to determine which specific connections are relevant to future predictions, the model assumes that all possible connections should be present. This design is motivated by the need for the network to learn which connections are important."
Difference between MLP(Multi-layer Perceptron) and Neural Networks?,https://stats.stackexchange.com/questions/208980/difference-between-mlpmulti-layer-perceptron-and-neural-networks,neural-networks|perceptron,208980,True,208989,20575,4,11,1644213715,1461450395,2,1461455916,"You are right, MLP is one kind of neural network.
There are several kinds of NN, you can have a NN based on Radial Basis Function with a Soft gating strategy, for example. You can use a committee machine strategy to form a NN...
","**Summary (211 words):**

Neural networks (NNs) encompass various architectures and learning algorithms. MLP (multilayer perceptron) is one type of NN.

NNs can employ different activation functions, such as radial basis functions with soft gating strategies. Committee machine strategies can also be used to create NNs.

The choice of NN architecture and learning algorithm depends on the specific task and data being processed. Some NNs excel at tasks involving pattern recognition, while others are better suited for tasks such as regression or time series forecasting.

By utilizing different architectures and learning algorithms, NNs can be tailored to a wide range of applications, including image recognition, natural language processing, and machine learning in general."
Why convert spectrogram to RGB for machine learning?,https://stats.stackexchange.com/questions/559009/why-convert-spectrogram-to-rgb-for-machine-learning,machine-learning|time-series|neural-networks|feature-engineering,559009,True,613712,997,3,11,1683994124,1641157181,0,1682092214,"
Colormapping is nonlinear filtering. A color map is simply a transform; the breakup into three dimensions further interprets it as filtering and decomposition. turbo is preferable to jet for inspection (1 (https://web.archive.org/web/20141009075838/http://abandonmatlab.wordpress.com/2011/05/07/lets-talk-colormaps/) -- 2 (https://web.archive.org/web/20180501213458/http://cresspahl.blogspot.com:80/2012/03/expanded-control-of-octaves-colormap.html) -- 3) - which is to say, it's not arbitrary, and the human visual system favors it. In turbo (or jet), as one use case, we can quickly skim an image for peaks, which will be red, and we may wish to focus only on those - that's identical to the ""R"" channel.

""Image"" involves efficient (and nonlinear) compression. The standard approach to STFT compression is direct subsampling (i.e. hop_size (https://dsp.stackexchange.com/a/86938/50076)), which aliases. An improvement is decimation, i.e. lowpass filtering + subsampling, which is a linear compression. If something so simple was effective, there'd be no need for all the sophistication of JPEG (https://youtu.be/0me3guauqOU). In ML terms, we can view ""save as JPEG"" as a highly effective autoencoder, also effective dimensionality reduction.


There's more to say but I'll just share the main points for now.
Note that this is completely separate from using image-excelling NNs on STFT images. That can be detrimental (https://dsp.stackexchange.com/a/80857/50076).
Also, @Ghostpunk's answer is mistaken and misleading, as I commented. It may be owed to the popular ""windowed Fourier transform"" interpretation of STFT. Spectrogram losses can also be measured. Relevant posts:

Equivalence between ""windowed Fourier transform"" and STFT as convolutions/filtering (https://dsp.stackexchange.com/a/86938/50076)
Role of window length and overlap in uncertainty principle? (https://dsp.stackexchange.com/a/80920/50076)

Note
I realized the question, and my answer, are ill-suited for this network, and I may not be developing my answer further here. If I develop is elsewhere, I'll link it. In the meantime, refer to my discussion with @SextusEmpiricus (https://i.sstatic.net/SDjbm.png).
Still self-accepting since, though elaboration is due, my answer can be understood with the right (mainly signal processing + feature engineering) background, and I believe it contains the most pertinent explanation.
","Colormapping is a filtering technique that transforms colors into three dimensions, aiding in data interpretation and visualization. The preferred colormap for inspection, like turbo, is designed to optimize the human visual system's perception of data.

In image processing, compression plays a crucial role. STFT compression often involves subsampling, but decimation is a more effective linear compression method. JPEG, a highly sophisticated autoencoder, demonstrates the limitations of linear compression and the need for nonlinear techniques.

STFT images, despite their potential usefulness, should be used cautiously with image-processing neural networks, as this approach can be detrimental. Measuring spectrogram losses is viable, as highlighted in the referenced resources."
Can a neural network work with negative and zero inputs?,https://stats.stackexchange.com/questions/557994/can-a-neural-network-work-with-negative-and-zero-inputs,neural-networks|relu|activation-function,557994,True,558146,10656,5,10,1657637112,1640171373,10,1640262693,"
(this has been said already by other answers) ReLu activation function has a gradient equal to 0 (hence it stops learning) when the linear combination of the inputs is less than 0, not when the input themselves are 0
(this also has been said already by other answers) the inputs of NNs are often normalised around 0, so it's totally normal that some values are 0, also the weights of each neuron are usually (randomly) initialised around 0, meaning that when it comes the time to compute the linear combination, some random input values will switch sign, this is expected
ReLu function is actually designed to result in a null gradient for values below 0, don't stress out about this, the problem of dead neurons comes up when all inputs for that neuron result in a null gradient. It's not a trivial problem to discuss so I will slide upon it, but it has nothing to do with simply having some negative values in the inputs. As HitLuca has pointed in his comment, having the neuron parameters go to zero during the learning process will cause the neuron to die.
Of course other activation functions that never result in a null gradient (like leaky ReLu) will avoid dead neurons entirely.

","**Summary:**

ReLu activation functions have zero gradients for inputs below zero, but this is not a cause for concern. Inputs and weights in neural networks are often normalized around zero, resulting in some negative inputs. This does not lead to dead neurons because the zero gradient applies to the linear combination of inputs, not individual inputs.

Dead neurons arise when all inputs to a neuron result in a zero gradient. A separate issue is when neuron parameters approach zero during learning, causing the neuron to become effectively dead.

Alternative activation functions, such as leaky ReLu, avoid dead neurons entirely by having non-zero gradients for all inputs."
Is low bias in a sample a synonym for high variance?,https://stats.stackexchange.com/questions/327746/is-low-bias-in-a-sample-a-synonym-for-high-variance,machine-learning|neural-networks|variance|sampling|bias,327746,True,327748,2345,2,10,1518214942,1518188214,26,1518189308,"No. You can have both high or both low at same time. Here is an illustrate example. picture and article source (http://scott.fortmann-roe.com/docs/BiasVariance.html) I also recommend you to read the article where this picture comes from. 
 (https://i.sstatic.net/2kYe5m.png)

The reason you have such impression is that in ""early age"" of machine learning, there is a concept called bias variance trade-off (as @Kodiologist mentioned, this concept is still true and a fundamental concept of tuning models today.)

When increase model complexity, variance is increased and bias is reduced
when regularize the model, bias is increased and variance is reduced.

In Andrew Ng's recent Deep Learning Coursera lecture, he mentioned that in recent deep learning framework (with huge amount of data), people talk less about trade off. In stead, there are ways to only reduce variance and do not increase bias (For example, increase training data size), as vice versa.
","**Summary**

The concept of bias-variance trade-off in machine learning states that increasing model complexity reduces bias but increases variance, and vice versa. However, in the context of modern deep learning with large datasets, it's possible to reduce both bias and variance, rather than having to accept a trade-off.

**Key Points:**

* Bias and variance are not mutually exclusive: It's possible to have both high or both low levels of both.
* In the early days of machine learning, there was a bias-variance trade-off when adjusting model complexity (increasing complexity reduced bias but increased variance, and regularization increased bias but reduced variance).
* With large datasets and advancements in deep learning frameworks, it's now possible to reduce both bias and variance without compromising the other.
* Examples of variance reduction without increasing bias include increasing the size of the training dataset."
Does the universal approximation theorem for neural networks hold for any activation function?,https://stats.stackexchange.com/questions/325776/does-the-universal-approximation-theorem-for-neural-networks-hold-for-any-activa,neural-networks|approximation,325776,True,325778,6036,4,10,1645793194,1517287474,11,1517289706,"The wikipedia article (https://en.wikipedia.org/wiki/Universal_approximation_theorem) has a formal statement.

Let $\varphi$ be a nonconstant, bounded, and continuous function.

","**Summary:**

The Universal Approximation Theorem states that certain types of artificial neural networks, specifically feedforward networks with at least one hidden layer, can approximate any continuous function to any desired accuracy. This implies that these networks can be used to solve a wide range of problems, including classification, regression, and pattern recognition.

The theorem applies to networks with the following properties:

* The activation function of the hidden layer neurons is non-constant and bounded.
* The network has at least one hidden layer with a sufficient number of neurons.
* The network is trained using gradient-based methods to minimize an appropriate cost function.

One important consequence of the Universal Approximation Theorem is that it allows the use of neural networks to approximate arbitrary functions without the need for domain-specific knowledge or complex feature engineering. This makes them a versatile tool for solving a variety of problems in fields such as machine learning, signal processing, and control theory."
Is it in general helpful to add &quot;external&quot; datasets to the training dataset?,https://stats.stackexchange.com/questions/474621/is-it-in-general-helpful-to-add-external-datasets-to-the-training-dataset,neural-networks|dataset|train,474621,True,474700,1273,3,10,1593595614,1593441035,9,1593479389,"I think the examples you bring are mostly from computer vision/image recognition and that case external datasets are very likely to include similar signal/dynamics as the prior data at hand. A ""car"" is a ""car"" irrespective of its surroundings. A ""good customer"" or ""abnormal shopping activity"" is different in Luxembourg than it is in Moldova. Unless we actively account for ""covariate shift (https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/)"" (input distribution changes) and/or ""concept drift (https://en.wikipedia.org/wiki/Concept_drift)"" (ie. correct output for a given input changes over time/space/etc.) then ""more data is helpful"" only if we are lucky. We should note that this includes computer vision too; for example if our additional data is biased in a way we are unaware and/or unable to control (e.g. the photos are always at night-time or are subjected to over-exposure) that will not necessarily help the generalisability of our model.
","**Summary:**

Data size alone does not guarantee model improvement. Covariate shift and concept drift, where input distributions or correct outputs change, can hinder generalization. In computer vision, external datasets may not align with the prior data, limiting their effectiveness. Even within computer vision, factors like lighting conditions can bias the data, affecting model generalizability.

To ensure model improvement with additional data, it is crucial to account for covariate shift and concept drift. This involves understanding the changes in input distribution and output expectations over time, space, or other factors. Additionally, potential biases in external datasets, such as skewed lighting conditions, should be considered to avoid misleading the model."
Neural networks output probability estimates?,https://stats.stackexchange.com/questions/256420/neural-networks-output-probability-estimates,machine-learning|probability|neural-networks,256420,True,256421,24260,4,10,1719306399,1484523424,9,1484523760,"If your activation function is for example logistic, then it will output continuous value between 0-1, or you can use softmax in the case of multiple outcome variables.
","**Summary:**

The choice of activation function determines the range of values outputted by a neural network. For binary classification problems, the logistic function is commonly used, producing a continuous output between 0 and 1. This output represents the probability of the input belonging to a specific class. For multi-class classification problems, the softmax function is preferred. It converts the network's input into a probability distribution over the possible classes, ensuring the sum of probabilities equals 1."
"R time-series forecasting with neural network, auto.arima and ets",https://stats.stackexchange.com/questions/89851/r-time-series-forecasting-with-neural-network-auto-arima-and-ets,r|neural-networks|forecasting|arima|exponential-smoothing,89851,True,89852,21427,4,10,1566973972,1394697599,14,1394698122,"In-sample fits are not a reliable guide to out-of-sample forecasting accuracy. The gold standard in forecasting accuracy measurement is to use a holdout sample. Remove the last 30 days from the training sample, fit your models to the rest of the data, use the fitted models to forecast the holdout sample and simply compare accuracies on the holdout, using Mean Absolute Deviations (MAD) or weighted Mean Absolute Percentage Errors (wMAPEs).
Here is an example using R. I am using the 2000th series of the M3 competition, which already is divided into the training series M3[[2000]]$x and the test data M3[[2000]]$xx. This is monthly data. The last two lines output the wMAPE of the forecasts from the two models, and we see here that the ARIMA model (wMAPE 18.6%) outperforms the automatically fitted ETS model (32.4%):
library(forecast)
library(Mcomp)

M3[[2000]]

ets.model <- ets(M3[[2000]]$x)
    arima.model <- auto.arima(M3[[2000]]$x)

ets.forecast <- forecast(ets.model,M3[[2000]]$h)$mean
arima.forecast <- forecast(arima.model,M3[[2000]]$h)$mean

sum(abs(ets.forecast-M3[[2000]]$xx))/sum(M3[[2000]]$xx)
sum(abs(arima.forecast-M3[[2000]]$xx))/sum(M3[[2000]]$xx)

In addition, it looks like there are abnormally high sales near indices 280-300. Could this be Christmas sales? If you know about calendar events like these, it would be best to feed those to your forecasting model as explanatory variables, which will give you a better forecast next time that Christmas rolls around. You can do that easily in ARIMA(X) and NNs, not so easily in ETS.
Finally, I recommend this textbook on forecasting: http://otexts.com/fpp/ (http://otexts.com/fpp/)
","**Summary:**

In-sample fits are unreliable for predicting forecasting accuracy. Instead, use a holdout sample of data (e.g., the last 30 days) to evaluate accuracy using metrics like Mean Absolute Deviations (MAD) or weighted Mean Absolute Percentage Errors (wMAPEs).

To illustrate, an example using the M3 competition data shows that an ARIMA model outperforms an automatically fitted ETS model in terms of wMAPE.

Additionally, forecasting models can be improved by incorporating explanatory variables that account for events like seasonal sales spikes (e.g., Christmas sales).

For further guidance, the recommended textbook ""Forecasting Principles and Practice"" provides comprehensive information on forecasting methods."
self study: why is my neural network so much worse than my random forest,https://stats.stackexchange.com/questions/579832/self-study-why-is-my-neural-network-so-much-worse-than-my-random-forest,r|machine-learning|neural-networks|random-forest,579832,True,579834,1715,1,10,1656037058,1656034497,18,1656037058,"By default, nnet is doing classification. You want to set linout=T to make it do regression. Then, increase the number of hidden units as suggested by Sycorax, and/or increase the number of iterations n_its. For example:
library(nnet)
nn = nnet(y~x, data = df_train, size = 25, linout=T)
yh2 = predict(nn, newdata = df_test)
plot(df_test$x,df_test$y)
points(df_test$x, yh2, col = 'blue')

 (https://i.sstatic.net/0qQn9.png)
","**Summary:**

When using the nnet package in R for machine learning, it is important to adjust its settings to ensure proper functionality. By default, nnet assumes a classification task, but setting `linout=T` enables it to perform regression.

To improve the accuracy of the regression model, two strategies can be employed. Firstly, increasing the number of hidden units (specified by the `size` parameter) enhances the model's capacity to capture complex relationships in the data. Secondly, increasing the number of iterations (specified by `n_its`) allows the model to refine its parameters more thoroughly.

The provided code snippet demonstrates the process of creating a regression model with nnet. It reads data from the `df_train` dataframe, trains a neural network model (`nn`), and predicts values for a test dataset (`df_test`). The results are then plotted to visualize the model's performance."
Where to find pre-trained models for transfer learning,https://stats.stackexchange.com/questions/327068/where-to-find-pre-trained-models-for-transfer-learning,machine-learning|classification|neural-networks|transfer-learning,327068,True,327073,7813,2,10,1517991395,1517902382,11,1517903772,"Keras itself provides some of the successful image processing neural networks pretrained on the ImageNet: https://keras.io/applications/ (https://keras.io/applications/)
Other deep learning libraries also offer some pretrained models, notably:

TensorFlow: https://github.com/tensorflow/models (https://github.com/tensorflow/models)
caffe: https://github.com/BVLC/caffe/wiki/Model-Zoo (https://github.com/BVLC/caffe/wiki/Model-Zoo)
caffe2: https://github.com/caffe2/caffe2/wiki/Model-Zoo (https://github.com/caffe2/caffe2/wiki/Model-Zoo)
pytorch: https://github.com/Cadene/pretrained-models.pytorch (https://github.com/Cadene/pretrained-models.pytorch)
Lasagne: https://github.com/Lasagne/Recipes (https://github.com/Lasagne/Recipes)

Many pretrained models for various platforms can also be found at https://www.gradientzoo.com (https://www.gradientzoo.com).
Moreover, if you are interested in some particular network architecture, authors sometimes provide pretrained models themselves, e.g. ResNeXt (https://github.com/facebookresearch/ResNeXt).
","**Summary:**

Pretrained image processing neural networks are available for various platforms, including Keras, TensorFlow, caffe, caffe2, pytorch, and Lasagne. These models are trained on large datasets like ImageNet and can be used as building blocks for developing new applications.

Keras offers its own pretrained models, while other libraries provide access to models developed by the community. Additionally, authors of specific network architectures sometimes release pretrained models for those networks separately.

For a comprehensive collection of pretrained models, GradientZoo provides a repository of models from different sources. This allows developers to quickly access pretrained models for their projects, saving time and effort in model training."
Is Glorot/He-style variance-preserving *regularization* a known thing?,https://stats.stackexchange.com/questions/605573/is-glorot-he-style-variance-preserving-regularization-a-known-thing,neural-networks|regularization|weight-initialization,605573,True,605667,503,3,10,1679412918,1676513284,3,1676571531,"There are ways to preserve activation variance with an explicit regularization term. For example, the orthogonality regularizer
$$
\hat{\ell}(\theta) = \sum_k || {\theta_k}^T \theta_k - c_k I ||^2
$$
will do it, given square weight matrices $\theta_k$.  (Possibly for rectangular $\theta_k$ as well, but I can't recall or verify at the moment.)
Notation: $\theta$ are network weights, with $\theta_k$ the weight matrix for each layer $k$.  $I$ is the identity matrix, and $c_k$ is a scalar gain factor depending on the chosen nonlinearity and on data distribution properties.  See details below.

Explanation
To state the goal a bit more formally, for layer $k$ in a neural net:

Let $\theta_k$ be layer parameters, so that the input $x$ and output $y$ of the layer are written $y = f_{\theta_k}(x)$
Let $X_k$ and $Y_k$ be random variables representing layer $k$ input and output, so that we can talk about their distributions.  i.e. $Y_k = f_{\theta_k}(X_k)$

Given a training sample $z = (x, y)$ and network weights $\theta = \{\theta_1, \theta_2, \dots\}$, we seek a regularization function $\hat{\ell}(\cdot)$ so that the overall objective function
$$
\mathcal{L}(z; \theta) = \underbrace{\ell(z; \theta)}_{\text{loss}} + \lambda \underbrace{\hat{\ell}(\theta)}_{\text{regularizer}}
$$
induces
$$
\text{Var}(Y_k) \approx \text{Var}(X_k)
$$
for each layer $k$, during and after optimization.

To start simply, assume that $\theta_k$ is a $d \times d$ square matrix and $f_{\theta_k}: \mathbb{R}^d \to \mathbb{R}^d$ is linear, so that the layer is just a matrix multiplication:
$$
\begin{align}
Y_k &= f_{\theta_k}(X_k) \\
&= \theta_kX_k
\end{align}
$$
In this case, enforcing orthogonality of $\theta_k$ is a simple way to preserve variance, regardless of the properties of $X_k$.  Enforcing or encouraging orthogonality during gradient descent is a well-studied problem.  Since orthogonality can be defined as ${\theta_k}^T \theta_k = I$, a simple regularizer just minimizes the distance between both sides of the equation for each layer:
$$
\hat{\ell}(\theta) = \sum_k || {\theta_k}^T \theta_k - I ||^2
$$
Hence, each weight matrix $\theta_k$ is pushed towards orthogonality, with the proximity to orthogonality controlled by regularization weight $\lambda$ in the objective function $\mathcal{L} = \ell + \lambda \hat{\ell}$
Note, this preserves variance for linear $\mathbb{R}^d \to \mathbb{R}^d$ layers only.  As we reintroduce practicality to our layers (i.e. nonlinear activation functions, non-square weight matrices, or even convolutions (https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Orthogonal_Convolutional_Neural_Networks_CVPR_2020_paper.html)) then more care is necessary to preserve variance.  For example, if we assume $X_k \sim N(0, \sigma^2)$ and $f_{\theta_k}$ uses ReLU activation, then $\text{Var}(Y_k) = c \sigma^2$ for some constant $c$.  So we should replace the identity matrix $I$ with $\frac{1}{\sqrt{c}} I$ in the regularizer $\hat{\ell}(\cdot)$.  (I am too lazy to derive this particular $c$ right now, but empirically, $c \approx 0.34$.)  .
Different nonlinearities will have different gain factors.  For example, I think OPLU (https://arxiv.org/abs/1604.02313) conveniently has a gain factor of 1.  These gain factors are discussed in a few different places, including the orthogonal initialization and descent literature, and the initialization literature in general.
Instead of soft-constraint by regularization, we can also hard-constrain to orthogonality by various methods.  For instance, this paper (https://arxiv.org/abs/1702.00071) uses weight orthogonality to address vanishing/exploding gradients in recurrent networks.  But since you asked only about regularizers, I won't detail anything about that.
And finally, if one is interested in orthogonality-preserving optimization, then one is also typically interested in orthogonal weight initialization (https://arxiv.org/pdf/2001.05992.pdf).
(The papers linked here are not comprehensive, and probably a bit out of date.  If anyone knows of relevant surveys, do share in the comments.)
","**Summary**

In neural networks, activation variance preservation is important for maintaining the flow of information through the layers. Regularization terms can enforce this property, ensuring that the variance of the output of each layer remains approximately equal to the variance of its input.

One such regularization term is the orthogonality regularizer, which penalizes deviations from orthogonality in weight matrices. When applied to square weight matrices in linear layers, it ensures activation variance preservation regardless of the input distribution.

For non-linear layers or non-square weight matrices, adjusting the orthogonality regularizer is necessary to account for different gain factors introduced by the activation function and the weight matrix shape.

Additionally, orthogonality can be enforced as a hard constraint rather than a regularized term, and orthogonal initialization techniques can be used to promote orthogonality from the outset."
What is auxiliary loss as mentioned in PSPNet paper,https://stats.stackexchange.com/questions/304699/what-is-auxiliary-loss-as-mentioned-in-pspnet-paper,deep-learning|convolutional-neural-network,304699,True,436203,16275,2,10,1576580545,1506249655,13,1573812158,"The idea of auxiliary loss (aka auxiliary towers) comes from GoogLeNet paper (https://arxiv.org/abs/1409.4842). At core intuition can be explained in this way: 
Let's say you are building a network by stacking up lots of identical modules. As network becomes deeper, you face slowed down training because of vanishing gradient issue (this was before BatchNorm days). To promote learning for each module layer, you can attach some small network to the output of that module. This network typically have a couple of conv layers followed by FCs and then final classification prediction. This auxiliary network's task is to predict same label as final network would predict but using the module's output. We add the loss of this aux network to the final loss of the entire network weighted by some value < 1. For example, in GoogLeNet, you can see two tower like aux networks on the right ending in orange nodes:
 (https://i.sstatic.net/QATOJ.png)
Now, if the module is learning slowly then it would generate big loss and cause gradient flow in that module helping gradients further downstream as well. This technique has apparently found to help training for very deep networks. Even when using batch norm, this can help to accelerate training during early cycles when weights are randomly initialized. Many NAS architecture uses this technique for initial evaluation during the search as you have a very limited budget to run epochs when evaluating 1000s of architectures so early acceleration improves performance. As aux networks are removed from the final model, it is not considered ""cheating"".
","**Summary: Auxiliary Loss Technique**

Auxiliary loss, also known as auxiliary towers, is a technique to improve training performance in deep neural networks. It involves attaching a small network (auxiliary network) to the output of each network module. The auxiliary network predicts the same label as the final network using the module's output.

By adding the loss of the auxiliary network to the final loss, it encourages learning in each module layer. This is particularly beneficial in deep networks, where vanishing gradients can slow down training.

Auxiliary loss can accelerate training, especially during the early stages when weights are randomly initialized. It is often used in Neural Architecture Search (NAS) to evaluate architectures with limited training time.

Once the network is trained, the auxiliary networks are removed. This technique is not considered ""cheating"" as it does not alter the final model's performance."
Why does &quot;stack more layers&quot; work?,https://stats.stackexchange.com/questions/332143/why-does-stack-more-layers-work,neural-networks|deep-learning,332143,True,332170,4728,3,10,1520419810,1520397813,12,1520411945,"The universal approximation theorem is mainly a proof that for every continuous mapping there exists a neural network of the described structure with a weight configuration that approximates that mapping to an arbitrary accuracy.
It does not give any proof that this weight configuration can be learned via traditional learning methods, and it relies on the fact there are enough units in each layer, but you don't really know what is ""enough"". For these reasons, UAT has very little practical use.
Deep networks have multitude of benefits over shallow ones:

Hierarchical features:

Deep learning methods aim at learning of feature hierarchies with features from higher-levels of the hierarchy formed by the composition of lower level features. Automatically learning features at multiple levels of abstraction allows a system to learn complex functions mapping the input to the output directly from data, without depending completely on human crafted features. [1] (http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html)

Distributed representations:

In addition to depth of architecture, we have found that another ingredient is crucial: distributed representations. (...) most non-parametric learning algorithms suffer from the so-called curse of dimensionality. (...) That curse occurs when the only way a learning algorithm generalizes to a new case x is by exploiting only a raw notion of similarity (...) between the cases. This is typically done by the learner looking in its training examples for cases that are close to x (...). Imagine trying to approximate a function by many small linear or constant pieces. We need at least one example for each piece. We can figure out what each piece should look like by looking mostly at the examples in the neighborhood of each piece. If the target function has a lot of variations, we'll need correspondingly many training examples. In dimension d (...), the number of variations may grow exponentially with d, hence the number of required examples. However, (...) we may still obtain good results when we are trying to discriminate between two highly complicated regions (manifolds), e.g. associated with two classes of objects. Even though each manifold may have many variations, they might be separable by a smooth (maybe even linear) decision surface. That is the situation where local non-parametric algorithms work well. (...)
Distributed representations are transformations of the data that compactly capture many different factors of variations present in the data. Because many examples can inform us about each of these factors, and because each factor may tell us something about examples that are very far from the training examples, it is possible to generalize non-locally, and escape the curse of dimensionality.  [1] (http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html)

This can be translated into pictures:
A non-distributed representation (learned by a shallow network) has to assign an output to every piece of the input space (represented by colored hypercubes). However, the number of pieces (and thus number of training points needed to learn this representation) grows exponentially with the dimensionality:
 (https://i.sstatic.net/sQzG2m.jpg)
On the other hand, distributed representations do not try describe completely every piece of the input space. Instead, they partition the space by isolating simple concepts which can be later merged to provide complex information. See below how K hyperplanes split the space into 2$^K$ regions:
 (https://i.sstatic.net/lMYB4m.jpg)
(Images from [1] (http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html))
For more insight about distributed representations, I also recommend this thread at Quora: Deep Learning: What is meant by a distributed representation? (https://www.quora.com/Deep-Learning-What-is-meant-by-a-distributed-representation)
In theory, deep networks can emulate shallow networks:

Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error
  than its shallower counterpart. [2] (https://arxiv.org/abs/1512.03385)

Note that this is also rather a theoretical result; as the cited paper states, empirically deep networks (w/o residual connections) experience ""performance degradation"".


[1]: http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html (http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html)
[2]: Deep Residual Learning for Image Recognition (He et al., 2015) (https://arxiv.org/abs/1512.03385)
","**Universal Approximation Theorem (UAT)** states that neural networks can approximate any continuous function with sufficient accuracy. However, it provides no guidance on how to find the necessary weight configuration for training the network.

**Deep Neural Networks** excel over shallow networks due to:

* **Hierarchical Features:** Deep learning enables the extraction of features at multiple abstraction levels, without relying solely on manually crafted features.
* **Distributed Representations:** These representations capture complex variations in data by partitioning the input space into simple concepts that can be combined to express complex patterns.

**Theoretical Capabilities of Deep Networks:**

* While deep networks can theoretically emulate shallow networks with identical training error, empirical studies have shown performance degradation for deep networks without residual connections.

**Limitations of UAT:**

* UAT offers no practical guidance for finding the optimal weight configuration for training neural networks.
* It assumes an adequate number of units in each layer, but provides no guidance on determining ""enough."""
How is softmax unit derived and what is the implication?,https://stats.stackexchange.com/questions/145272/how-is-softmax-unit-derived-and-what-is-the-implication,probability|neural-networks|softmax,145272,True,145277,3481,2,10,1587191530,1428476574,9,1428481027,"The categorical distribution is the minimum assumptive distribution over the support of ""a finite set of mutually exclusive outcomes"" given the sufficient statistic of ""which outcome happened"".  In other words, using any other distribution would be an additional assumption.  Without any prior knowledge, you must assume a categorical distribution for this support and sufficient statistic.  It is an exponential family.  (All minimum assumptive distributions for a given support and sufficient statistic are exponential families.)
The correct way to combine two beliefs based on independent information is the pointwise product of densities making sure not to double-count prior information that's in both beliefs.  For an exponential family, this combination is addition of natural parameters.
The expectation parameters are the expected values of $x_k$ where $x_k$ are the number of times you observed outcome $k$.  This is the right parametrization for converting a set of observations to a maximum likelihood distribution.  You simply average in this space.  This is what you want when you are modeling observations.
The multinomial logistic function is the conversion from natural parameters to expectation parameters of the categorical distribution.  You can derive this conversion as the gradient of the log-normalizer with respect to natural parameters.
In summary, the multinomial logistic function falls out of three assumptions: a support, a sufficient statistic, and a model whose belief is a combination of independent pieces of information.
","**Summary:**

The categorical distribution is the simplest distribution to use when you have a finite set of mutually exclusive outcomes and know which outcome has occurred. Any other distribution would require additional assumptions. For this type of distribution, the correct way to combine beliefs is by multiplying the densities and adding the natural parameters.

The expected values of the categorical distribution are called the expectation parameters. These parameters are used to convert a set of observations into the most likely distribution. This conversion is done using the multinomial logistic function, which is derived from the log-normalizer of the categorical distribution.

In essence, the multinomial logistic function arises from three assumptions: a finite set of outcomes, a known outcome, and a model where beliefs are combined independently."
What happens to the initial hidden state in an RNN layer?,https://stats.stackexchange.com/questions/395382/what-happens-to-the-initial-hidden-state-in-an-rnn-layer,neural-networks|recurrent-neural-network,395382,True,395403,5029,1,10,1707165746,1551630763,11,1551637030,"There are two common RNN strategies.

You have a long sequence that's always contiguous (for example, a language model that's trained on the text of War and Peace); because the novel's words all have a very specific order, you have to train it on consecutive sequences, so the hidden state at the last hidden state of the previous sequence is used as the initial hidden state of the next sequence.

The way most people do this is that you'll have to traverse the sequences in order, and not shuffle. Suppose you use mini-batch size of 2. You can cut the book in half, and the first sample will always have text from the first half of War and Peace and the second sample will always have text from the second half. Instead of using samples at random, the text is always read in order, so the first sample in the first mini-batch has the first words of the text, and the second sample in the first mini-batch has the first words after the mid-point of the text.
Purely abstractly, I suppose you could do something more complicated where you shuffle the data but can compute the initial hidden state for each position in the sequence (e.g. by computing the text up until that point, or else saving & restoring states) but this sounds expensive.

You have lots of distinct sequences (such as discrete tweets); it can make sense to start each sequence with hidden states of all 0s. Some people prefer to train a ""baseline"" initial state (Sam Weisenthal's (https://stats.stackexchange.com/users/54664/sam-weisenthal)'s suggestion). I read an article advocating doing this if your data has lots of short sequences but I can't find the article now.

Which strategy is appropriate depends on the problem, and specific choices about how to represent that problem.
From the perspective of developing software, an ideal implementation would somehow expose functionality for both options to users. This can be tricky, and different software (pytorch, tensorflow, keras) achieves this in different ways.
","**Summary:**

**RNN Training Strategies:**

When training Recurrent Neural Networks (RNNs) with long, contiguous sequences, consecutive sequences must be used to preserve ordering. Typically, mini-batches are processed sequentially, with the hidden state from the previous batch used to initialize the next. Shuffling data is not advisable, but abstract approaches for calculating initial hidden states for shuffled data are possible.

**Discrete Sequence Training:**

For discrete sequences (e.g., tweets), zero-initial hidden states are commonly used. Alternatively, some researchers suggest training a ""baseline"" initial state, particularly for datasets with numerous short sequences.

**Implementation Considerations:**

The appropriate strategy depends on the problem and its representation. Ideally, software implementations should provide functionality for both strategies. However, the specific implementation methods vary across frameworks (e.g., PyTorch, TensorFlow, Keras)."
How to calculate output shape in 3D convolution,https://stats.stackexchange.com/questions/323313/how-to-calculate-output-shape-in-3d-convolution,machine-learning|neural-networks|convolutional-neural-network,323313,True,323423,21218,2,10,1626862460,1516091828,9,1516126260,"The convolution formula is the same as in 2D and is well-described in CS231n tutorial (http://cs231n.github.io/convolutional-networks/#conv):
$$Out = (WâF+2P)/S+1$$
... where $W$ is the input volume size, $F$ is the receptive field size, $S$ is the stride, and $P$ is the amount of zero padding used on the border. In particular, when $S=1$ and $P=0$, like in your question, it simplifies to
$$Out=W-F+1$$
So, if you input the tensor $(40,64,64,12)$, ignoring the batch size, and $F=3$, then the output tensor size will be $(38, 62, 62, 8)$.

Pooling layer normally halves each spatial dimension. This corresponds to the local receptive field size F=(2, 2, 2) and stride S=(2, 2, 2). Hence, the input tensor $(38, 62, 62, 8)$ will be transformed to $(19, 31, 31, 8)$.
But you set the stride S=(1, 1, 1), it'll reduce each spatial dimension by 1: $(37, 61, 61, 8)$.
The last dimension doesn't change.
","**Convolution Layer Summary:**

The convolution formula determines the output size of a convolution layer. It considers the input size, receptive field (kernel) size, stride, and zero padding. In the example, with $W=40$, $F=3$, $S=1$, and $P=0$, the output size is $(38, 62, 62, 8)$.

**Pooling Layer Summary:**

Pooling layers reduce spatial dimensions. By default, they halve each dimension with a receptive field of $(2, 2, 2)$ and a stride of $(2, 2, 2)$. However, if the stride is set to $(1, 1, 1)$, each dimension is reduced by 1 instead. For instance, the tensor $(38, 62, 62, 8)$ would become $(19, 31, 31, 8)$ with a stride of $(2, 2, 2)$ and $(37, 61, 61, 8)$ with a stride of $(1, 1, 1)$."
"Detecting manipulation (e.g, photo copy-pasting) in images",https://stats.stackexchange.com/questions/319838/detecting-manipulation-e-g-photo-copy-pasting-in-images,machine-learning|neural-networks|supervised-learning|image-processing|manipulation-detection,319838,True,323680,8938,1,10,1574065787,1513840736,11,1516254934,"In general, it's hard to detect tampering and it's a whole field of research in digital image forensics. I'll try to summarise some of the key approaches to this problem. What you're talking about is sometimes called image forgery or image tampering. And the copy-paste operation is called image composition or image splicing. 
From a practical perspective there are number of different variants to this problem:

add something to the image
 (https://i.sstatic.net/pIfyI.jpg)
(source) (https://www.theguardian.com/politics/2015/nov/02/poppy-photoshopped-david-cameron-facebook-picture)
removing something from the image 

 (https://i.sstatic.net/Juxkh.jpg)  (https://i.sstatic.net/OSkw4.png) (source) (https://en.wikipedia.org/wiki/Situation_Room_(photograph)#Alteration_in_Hasidic_newspapers)

changing global properties of the image 
 (https://i.sstatic.net/inDSv.jpg) (source) (https://en.wikipedia.org/wiki/O._J._Simpson_murder_case#Media_coverage)
using one image vs. multiple images e.g. this use of the clone tool:
 (https://i.sstatic.net/aOh5G.jpg)
(source) (https://en.wikipedia.org/wiki/Adnan_Hajj_photographs_controversy) 
detecting whether if an image has been tampered vs. localising the tampering  
determining the type of tampering 

How you solve the problem is going to be very different depending on whether you are involved in a reviewing video surveillance footage, examining a single photo at a court case or running a photo sharing site. The problem is substantially harder if the problem is adversarial and the image manipulation may have been hidden.
Another point is that there is a lot of legitimate postprocessing that happens in images. To take an extreme example new digital camera introduce bokeh and blurring effects even though this is not present in the finished image. So if you are interested in detecting more general types of image manipulation beyond image splicing it's helpful to be aware of what's happening in cameras and apps. 
A digital image is acquired on a camera as follows:
scene $\rightarrow$ imaging sensor $\rightarrow$ on camera postprocessing $\rightarrow$ storage
where 

the scene is the external geometry of the image 
the image sensor is a CCD or CMOS
photodetector which converts light into electrical charge
postprocessing is where the camera is where the electrical charge is
converted into a digital signal and several corrective steps are
taken to account for camera geometry, colour correction, etc. 
storage of is where the finished image written to memory. Often it's 
converted into a compressed format such as JPEG and stored along with relevant metadata.

By considering the acquisition process you can see several possible points where tampering will result in inconsistencies in the image:

physical scene geometry
sensor and acquisition noise
postprocessing and compression artifacts
metadata

Metadata. An obvious thing to look at is the metadata associated with the image, often it can have camera information, time information and possibly location information. All of these can possible identify inconsistency. If you have the statue of Liberty in your image but the GPS coordinates say you are at McMurdo Station in Antartica then the image is probably a forgery. But the metadata is itself easy altered or stripped so this is not reliable. 
Sensor noise. Sensor noise can be quite distinctive for digital camera, so much so that it can used to fingerprint the sensors in different camera models. There are several distinct types of noise introduced by sensors in digital cameras, but a very useful kind is photo-response nonuniformity (PRNU). This is a fingerprint associated with sensor noise and postprocessing, and it is robust to several image processing transformations, including lossy compression such as downsampling.  You can calculate the PRNU across blocks in the image, and introducing a new element from a different camera will introduce and inconsistency in the image. This seems to work pretty well, but it works best if you know the camera type. It's still possible to estimte PRNU from a single image. Color filter array interpolation should also be consistent across the image, and will be distrupted by splicing.
Compression and processing artifacts. All image processing techniques will leave a trace on the image statistics. Digital images are very commonly compressed via JPEG which compresses things using the discrete cosine transform. This process leaves traces in the image statistics. One interesting technique is to detect JPEG ghosts, that is parts of an image which have been compressed twice via DCT. As you mention, I believe that downsampling will remove some of these artifacts although the downsampling itself will be detectable. 
Scene consistency. An image acquire from single source should have consistent perspective (vanishing points), and illumination. Moreover it's hard to fake these fake these with a composite image.  I recommend looking through (Redi et al., 2011) for more details here.
Finally, if you say ""Okay I give up. There's too many possible method, I just want a detector"" you can look at this recent ICCV paper where they train a detector to find where an image has been manipulated. This may give you some more insight into training a blackbox model.  
Bappy, Jawadul H., et al. ""Exploiting Spatial Structure for Localizing Manipulated Image Regions."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
Datasets/Contests:
Casia V1.0 and V2.0 (image splicing)
http://forensics.idealtest.org/ (http://forensics.idealtest.org/)
coverage (copy-move manipulations)
https://github.com/wenbihan/coverage (https://github.com/wenbihan/coverage)
Media Forensics Challenge 2018 (various manipulations, requires registration)
https://www.nist.gov/itl/iad/mig/media-forensics-challenge-2018 (https://www.nist.gov/itl/iad/mig/media-forensics-challenge-2018)
IEEE IFS-TC Image Forensics Challenge Dataset. (website currently unavailable)
Raise (raw, unprocessed images along with camera metadata)
http://mmlab.science.unitn.it/RAISE/index.php (http://mmlab.science.unitn.it/RAISE/index.php)
Surveys:
Redi, Judith A., Wiem Taktak, and Jean-Luc Dugelay. ""Digital image forensics: a booklet for beginners."" Multimedia Tools and Applications 51.1 (2011): 133-162.
https://pdfs.semanticscholar.org/8e85/c7ad6cd0986225e63dc1b4264b3e084b3f9b.pdf (https://pdfs.semanticscholar.org/8e85/c7ad6cd0986225e63dc1b4264b3e084b3f9b.pdf)
Fridrich, Jessica. ""Digital image forensics."" IEEE Signal Processing Magazine 26.2 (2009).
http://ws.binghamton.edu/fridrich/Research/full_paper_02.pdf (http://ws.binghamton.edu/fridrich/Research/full_paper_02.pdf)
Farid, Hany. Digital Image Forensics: lecture notes, exercises, and matlab code for a survey course in digital image and video forensics.
http://www.cs.dartmouth.edu/farid/downloads/tutorials/digitalimageforensics.pdf (http://www.cs.dartmouth.edu/farid/downloads/tutorials/digitalimageforensics.pdf)
Kirchner, Matthias. Notes on digital image forensics and counter-forensics. Diss. Dartmouth College, 2012.
http://ws.binghamton.edu/kirchner/papers/image_forensics_and_counter_forensics.pdf (http://ws.binghamton.edu/kirchner/papers/image_forensics_and_counter_forensics.pdf)
Memon, Nasir. ""Photo ForensicsâThere Is More to a Picture than Meets the Eye."" International Workshop on Digital Watermarking. Springer, Berlin, Heidelberg, 2011.
Mahdian, Babak, and Stanislav Saic. ""A bibliography on blind methods for identifying image forgery."" Signal Processing: Image Communication 25.6 (2010): 389-399.
Image Tampering Detection and Localization (includes recent deep learning references)
https://github.com/yannadani/image_tampering_detection_references (https://github.com/yannadani/image_tampering_detection_references)
","**Image Tampering Detection**

Detecting image tampering involves identifying alterations such as adding, removing, or changing image elements. It remains a challenging task due to advanced manipulation techniques.

**Key Approaches**

* **Copy-Paste Detection:** Detects if image elements have been copied from one area to another, known as image splicing.
* **Global Property Analysis:** Examines overall image properties such as brightness, contrast, and sharpness to detect inconsistencies caused by tampering.
* **Inconsistent Metadata:** Analyzes image metadata (e.g., camera information, location data) for abnormalities that may indicate tampering, although metadata can be altered.
* **Sensor Noise Patterns:** Exploits unique noise patterns generated by camera sensors to identify inconsistencies introduced by splicing.
* **JPEG Artifacts:** Detects traces left by image compression techniques, which can reveal image manipulation.
* **Scene Consistency:** Examines scene elements like perspective and lighting to detect inconsistencies that could indicate tampering.
* **Machine Learning:** Utilizes trained detectors to identify manipulated regions in images, though they may require extensive training data.

**Challenges**

* Adversarial manipulations can make detection difficult.
* Legitimate image post-processing can introduce artifacts similar to tampering.
* Tampering may not always be easily detectable.

**Datasets and Resources**

* Casia V1.0 and V2.0: Image splicing datasets
* Coverage: Copy-move manipulations dataset
* Media Forensics Challenge 2018: Various image manipulation dataset
* RAISE: Raw camera metadata dataset

**Surveys and Tutorials**

* Redi et al., 2011: Beginner's guide to digital image forensics
* Fridrich, 2009: Overview of image forensics
* Kirchner, 2012: Notes on digital image forensics
* Memon, 2011: Overview of photo forensics"
Understanding early stopping in neural networks and its implications when using cross-validation,https://stats.stackexchange.com/questions/305452/understanding-early-stopping-in-neural-networks-and-its-implications-when-using,machine-learning|neural-networks|cross-validation|hyperparameter,305452,True,305550,6146,2,10,1524560682,1506625361,8,1506685527,"
Determining the number of epochs by e.g. averaging the number of epochs for the folds and use it for the test run later on?

Shortest possible answer: Yes!
But let me add some context...
I believe you are referring to Section 7.8, pages 246ff, on Early Stopping in the Deep Learning book. The described procedure there, however, is significantly different from yours. Goodfellow et al. suggest to split your data in three sets first: a training, dev, and test set. Then, you train (on the training set) until the error from that model increases (on the dev set), at which point you stop. Finally, you use the trained model that had the lowest dev set error and evaluate it on the test set. No cross-validation involved at all.
However, you seem to be trying to do both early stopping (ES) and cross-validation (CV), as well as model evaluation all on the same set. That is, you seem to be using all your data for CV, training on each split with ES, and then using the average performance over those CV splits as your final evaluation results. If that is the case, that indeed is stark over-fitting (and certainly not what is described by Goodfellow et al.), and your approach gives you exactly the opposite result of what ES is meant for -- as a regularization technique to prevent over-fitting. If it is not clear why: Because you've ""peaked"" at your final evaluation instances during training time to figure out when to (""early"") stop training; That is, you are optimizing against the evaluation instances during training, which is (over-) fitting your model (on that evaluation data), by definition.
So by now, I hope to have answered your other [two] questions.
The answer by the higgs broson (to your last question, as cited above) already gives a meaningful way to combine CV and ES to save you some training time: You could split your full data in two sets only - a dev and a test set - and use the dev set to do CV while applying ES on each split. That is, you train on each split of your dev set, and stop once the lowest error on the training instances you set aside for evaluating that split has been reached [1]. Then you average the number of epochs needed to reach that lowest error from each split and train on the full dev set for that (averaged) number of epochs. Finally, you validate that outcome on the test set you set aside and haven't touched yet.
[1] Though unlike the higgs broson I would recommend to evaluate after every epoch. Two reasons for that: (1), comparative to training, the evaluation time will be negligible. (2), imagine your min. error is at epoch 51, but you evaluate at epoch 50 and 60. It isn't unlikely that the error at epoch 60 will be lower than at epoch 50; Yet, you would choose 60 as your epoch parameter, which clearly is sub-optimal and in fact even going a bit against the purpose of using ES in the first place.
","**Summary:**

Determining the optimal number of epochs for deep learning models involves balancing between underfitting and overfitting.

**Traditional Approach:**

* Divide data into training, development, and test sets.
* Train on the training set using early stopping (ES) based on the development set.
* Evaluate the final model on the test set.

**Alternative Approach (Not Recommended):**

* Use cross-validation (CV) to obtain average performance overè¤æ°ã®folds of the data.
* Train on each fold using ES and use the average number of epochs as the final training parameter.
* Evaluate on the **same** data used for CV, leading to overfitting.

**Recommended Approach:**

* Combine CV and ES by dividing data into development and test sets.
* Perform CV on the development set, applying ES on each fold.
* Calculate the average number of epochs needed for ES.
* Train on the entire development set for that average number of epochs.
* Validate the model on the unseen test set."
What is the difference between patch-wise training and fully convolutional training in FCNs?,https://stats.stackexchange.com/questions/266075/what-is-the-difference-between-patch-wise-training-and-fully-convolutional-train,machine-learning|neural-networks|convolutional-neural-network|data-mining|computer-vision,266075,True,267741,11250,1,10,1681344394,1488928952,11,1489619028,"Basically, fully convolutional training takes the whole MxM image and produces outputs for all subimages in a single ConvNet forward pass. Patchwise training explicitly crops out the subimages and produces outputs for each subimage in independent forward passes. Therefore, fully convolutional training is usually substantially faster than patchwise training.
So, for fully convolutional training, you make updates like this: 

Input whole MxM image (or multiple images)
Push through ConvNet -> get an entire map of outputs (maximum size MxM per image, possibly smaller)
Make updates using the loss of all outputs

Now while this is quite fast, it restricts your training sampling process compared to patchwise training: You are forced to make a lot of updates on the same image (actually, all possible updates for all subimages) during one step of your training. That's why they write that fully convolutional training is only identical to patchwise training, if each receptive field (aka subimage) of an image is contained in a training batch of the patchwise training procedure (for patchwise training, you also could have two of ten possible subimages from image A, three of eight possible subimages from image B, etc. in one batch). 
Then, they argue that by not using all outputs during fully convolutional training, you get closer to patchwise training again (since you are not making all possible updates for all subimages of an image in a single training step). However, you waste some of the computation. Also, in Section 4.4/Figure 5, they describe that making all possible updates works just fine and there is no need to ignore some outputs.  
","**Summary:**

Fully convolutional training processes an entire image through a convolutional neural network (ConvNet) in a single pass, generating outputs for all sub-images. Patchwise training, in contrast, crops sub-images and processes them individually. Fully convolutional training is significantly faster than patchwise training.

During fully convolutional training, updates are made based on the loss of all outputs. However, this limits the training sampling process as all updates for all sub-images of an image are made in a single step.

To make fully convolutional training more similar to patchwise training, not all outputs can be used during updates. However, this approach wastes computation. Research has shown that making all updates works well, eliminating the need to ignore outputs."
Number of neurons in the output layer,https://stats.stackexchange.com/questions/213478/number-of-neurons-in-the-output-layer,machine-learning|neural-networks,213478,True,213482,17159,1,10,1463683773,1463668545,11,1463669120,"I am a total novice to this, but my understanding is the following:
input layer - one neuron per input (feature), these are not typical neurons but simply pass the data through to the next layer
hidden layers - simplest structure is to have one neuron in the hidden layer, but deep networks have many neurons and many hidden layers.
output layer - this is the final hidden layer and should have as many neurons as there are outputs to the classification problem. For instance:

regression - may have a single neuron
binary classification - Single neuron with an activation function
multi-class classification - Multiple neurons, one for each class, and a Softmax function to output the proper class based on the probabilities of the input belonging to each class.

Reference: https://machinelearningmastery.com/deep-learning-with-python/ (https://machinelearningmastery.com/deep-learning-with-python/)
","**Summary:**

A neural network consists of three main layers:

* **Input Layer:** Each neuron represents an input feature and simply passes data to the next layer.

* **Hidden Layers:** Deep networks have multiple hidden layers, each with multiple neurons. These layers extract higher-level features from the input data.

* **Output Layer:** The final hidden layer, with a number of neurons equal to the number of output classes. It predicts the classification using an activation function:

  * Regression: Single neuron for continuous output
  * Binary Classification: Single neuron with an activation function to predict two possible outcomes
  * Multi-class Classification: Multiple neurons with a Softmax function to determine the most likely class"
Why doesn&#39;t deep learning work well with small amount of data?,https://stats.stackexchange.com/questions/162631/why-doesnt-deep-learning-work-well-with-small-amount-of-data,neural-networks|deep-learning,162631,True,162633,5460,1,10,1478640344,1437564772,11,1437565412,"The neural networks used in typical deep learning models have a very large number of nodes with many layers, and therefore many parameters that must be estimated. This requires a lot of data. A small neural network (with fewer layers and fewer free parameters) can be successfully trained with a small data set - but this would not usually be described as ""deep learning"".
","Deep learning models employ extensive neural networks with numerous layers and parameters. This complexity demands substantial data for training. In contrast, small neural networks with fewer layers and parameters can be trained with limited data, but such models generally fall short of the ""deep learning"" designation.

In summary, deep learning models require ample data due to their intricate neural networks with many parameters, while smaller neural networks can be trained with less data but may not fully qualify as deep learning models."
Variance calculation RELU function (deep learning),https://stats.stackexchange.com/questions/138035/variance-calculation-relu-function-deep-learning,mathematical-statistics|variance|random-variable|deep-learning,138035,True,138048,4097,2,10,1689813058,1424175168,10,1424179749,"In terms of integrals you have:
$$
E[x^2] = \int_{-\infty}^{+\infty} \max(0,y)^2 p(y) dy
$$
where the part $y < 0$ does not contribute to the Integral
$$
 = \int_{0}^{+\infty} y^2 p(y) dy
$$
which we can write as half the integral over the entire real domain ($y^2$ is symmetric around 0 and $p(y)$ is assumed to be symmetric around $0$):
$$
 = \frac{1}{2}\int_{-\infty}^{+\infty} y^2 p(y) dy
$$
now subtracting zero in the square we get:
$$
 = \frac{1}{2}\int_{-\infty}^{+\infty} (y - E[y])^2 p(y) dy
$$
which is 
$$
 = \frac{1}{2} E[(y - E[y])^2] = \frac{1}{2} Var[y]
$$
","**Summary:**

The expected value of the squared random variable, denoted as E[x^2], can be calculated as the integral of y^2 times the probability density function p(y) over the positive real domain. This is equivalent to half the integral over the entire real domain, as the integrand is symmetric around zero.

By subtracting zero in the squared term, the expression can be rewritten as half the expected value of the squared difference between the random variable y and its expected value E[y]. This is equal to half the variance of y, denoted as Var[y]."
How prior distribution over neural networks parameters is implemented in practice?,https://stats.stackexchange.com/questions/546235/how-prior-distribution-over-neural-networks-parameters-is-implemented-in-practic,neural-networks|bayesian|prior,546235,True,546239,1896,1,10,1633195069,1632794734,10,1632797323,"A zero-mean, isotropic multivariate gaussian prior on the network weights $\theta$ reduces to a penalty on the $L^2$ norm on the parameter vector $\theta$.
Finding the MAP estimate of the posterior reduces to maximizing the probability $p(y|x) = p(y|x,\theta)p(\theta) 
$, which is equivalent to minimizing the negative logarithm of the same:
$$\begin{align}
p(y|x) &= p(y|x,\theta)p(\theta) \\
-\ln p(y|x) &= L(y|x,\theta) - \ln p(\theta) \\
&= L(y|x,\theta) - 
\ln \left[
(2\pi)^{ -\frac{k}{2} } \det(\Sigma)^{ -\frac{1}{2} } \exp \left(
-\frac{1}{2} (\theta - \mu)^T \Sigma^{-1} (\theta - \mu)
\right)
\right] \\
&=  L(y|x,\theta) + \frac{1}{2}\theta^T \left(\sigma^2 I \right)^{-1}\theta +C \\
&=  L(y|x,\theta) + \frac{\sigma^{-2}}{2}\theta^T \theta \\
&= L(y|x,\theta) + \frac{\lambda}{2} \| \theta \|_2^2
\end{align}$$
where $ L(y|x,\theta)=-\ln p(y|x,\theta)$ is your loss function (e.g. mean square error or categorical cross-entropy loss), the negative log likelihood given the model, the parameters $\theta$, and the data $(x,y)$.
Some notes about this derivation:

The last line makes the substitution $\sigma^{-2}=\lambda$ and writes the penalty as a norm to make the connection to ridge regression more apparent.

We can neglect the constant additive terms $C=-\frac{1}{2}\left(k\ln(2\pi)+\ln|\Sigma|\right)$ because they do not depend on $\theta$; including them will change the value of the extrema, but not its location.

This is given as a generic statement about any loss $L$ which can be expressed as the negative log of the probability, so if you're working on a classification problem, a regression problem, or any problem formulated as a probability model, you can just substitute the appropriate expression for $L$.


Of course, if you're interested in Bayesian methods, you might not wish to be constrained solely to MAP estimates of the model. Radford Neal looks at some methods to utilize the posterior distribution of $\theta$ in his book Bayesian Learning for Neural Networks (https://www.springer.com/gp/book/9780387947242), including MCMC to estimate neural networks. Since publication, there are probably many more works which have taken these concepts even further.
One could optimize this augmented loss function directly. Alternatively, it could be implemented as weight decay during training; PyTorch does it this way, for instance. The reason you might want to implement weight decay as a component of the optimizer (as opposed to just using on autograd on the regularized loss) is that the gradient update looks like
$$\begin{align}
\theta_{i+1} &= \theta_i - \eta \frac{\partial}{\partial \theta} \left[L + \frac{\lambda}{2}\| \theta \|_2^2 \right]\\
 &= \theta_i - \eta \frac{\partial L}{\partial \theta} - \eta \lambda  \theta_i \\
&= (1 - \eta \lambda) \theta_i - \eta \frac{\partial L}{\partial \theta}
\end{align}$$
where $\theta_i$ is the parameter vector at the $i$th optimization step and $\eta$ is the learning rate. But when using adaptive optimizers (e.g. Adam), the effect of weight decay is slightly different; see ""Decoupled Weight Decay Regularization (https://arxiv.org/abs/1711.05101)"" by
Ilya Loshchilov and Frank Hutter.
","**Summary:**

A zero-mean, isotropic multivariate Gaussian prior on network weights (Î¸) results in a penalty on the L2 norm of Î¸. Finding the maximum a posteriori (MAP) estimate for the posterior distribution is equivalent to minimizing the negative log-posterior, which includes the negative log-likelihood (L) and a regularization term.

Simplifying the negative log-posterior, we obtain an augmented loss function with an additional term proportional to the L2 norm of Î¸, weighted by a parameter Î». This augmented loss function can be optimized directly or implemented as weight decay during training.

Weight decay regularizes the weights by encouraging smaller magnitudes, which helps prevent overfitting. Using an adaptive optimizer (e.g., Adam) slightly modifies the effect of weight decay, as described in the study ""Decoupled Weight Decay Regularization."""
"Does Keras SGD optimizer implement batch, mini-batch, or stochastic gradient descent?",https://stats.stackexchange.com/questions/406183/does-keras-sgd-optimizer-implement-batch-mini-batch-or-stochastic-gradient-des,neural-networks|keras|stochastic-gradient-descent,406183,True,406187,3550,1,10,1556779881,1556778790,10,1556779682,"It works just as you suggest. batch_size parameter does exactly what you would expect: it sets the size of the batch:


batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.


From programming point of view, Keras decouples the weight update formula parameters specific to each optimizer (learning rate, momentum, etc.) from the global training properties (batch size, training length, etc.) that are share between methods. It is matter of convenienceâthere is no point in having optimizers SGD, MBGD, BGD that all do the same thing just with different batch size.
","**Summary:**

The `batch_size` parameter in Keras specifies the number of samples processed in each gradient update during training. By default, it is set to 32.

Keras separates the update parameters specific to each optimizer from the global training parameters shared by all optimizers. This allows optimizers like SGD, MBGD, and BGD to share the same functionality while only varying in batch size. This design simplifies training by eliminating the need for multiple optimizers that perform the same task with different batch sizes."
Can reinforcement learning be &quot;stateless&quot;?,https://stats.stackexchange.com/questions/291906/can-reinforcement-learning-be-stateless,machine-learning|deep-learning|terminology|reinforcement-learning,291906,True,292404,5739,1,10,1500963912,1500271521,10,1500495219,"Reinforcement learning is formulated as a problem with states, actions, and rewards, with transitions between states affected by the current state, chosen action and the environment. That is part of its definition (formulated as a Markov Decision Process), so generally you won't find stateless variants of it that are still called reinforcement learning.
However, there are related stateless problems. There are multi-armed bandits (https://en.wikipedia.org/wiki/Multi-armed_bandit), which just have actions and rewards. Solutions for these allow for learning of reward based on actions, and can be optimised by selecting the best action (being sure you have the best one, and what total reward you can accumulate whilst still testing for which is the best action are the main optimisation problems). Your button pushing example looks a lot like a multi-armed bandit problem. Another common example might be advert selection online for anonymous visitors to a site - although there is often plenty of data available, there's also a huge amount that is hidden, and a practical approach is to treat the probability of a click through as only depending on the choice of content, which is then the site's action.
There is a ""stateful"" variant of multi-armed bandits called contextual bandits (https://en.wikipedia.org/wiki/Multi-armed_bandit#Contextual_Bandit) - when there is some kind of signal that can be associated with the correct action, but actions taken have no effect on what the next state will be. Contextual bandits have states, actions and rewards, but no transition rules, they can be treated as a set of entirely separate events. 
A contextual bandit with added transition rules between states, but no influence from the selected action, is essentially a sub-class of the reinforcement learning problem, and you can use most of the same analysis to predict long term reward and learn optimal behaviour.
And just for completeness, there are Markov Reward Processes (https://en.wikipedia.org/wiki/Markov_reward_model) without agent interaction that have states, and rewards, but no actions. It is possible to use reinforcement learning algorithms on these to predict long term reward and/or the expected long term value of being in a specific state.
The rock-paper-scissors game does not fit neatly into any of the above problems because there are two agents (although you could analyse it as a multi-armed bandit if the opponent's policy was fixed to always play with specific unchanging probabilities, or as a contextual bandit if there multiple such opponents or a really easy ""tell"" to their style of play that never changed). Typically a game of rock-paper-scissors would be analysed using game theory (https://en.wikipedia.org/wiki/Game_theory) - it has an interesting feature that the Nash equilibrium is achieved by using a stochastic policy with equal $\frac{1}{3}$ probability of each choice. 
If you wrote a rock-paper-scissors agent to play against a human opponent, you might actually formulate it as a reinforcement learning problem, taking the last N plays as the state, because that could learn to take advantage of human players' poor judgement of randomness.
","Reinforcement learning involves states, actions, and rewards, where state transitions depend on these elements and the environment. While stateless variants of reinforcement learning do not exist, related problems include:

* **Multi-Armed Bandits:** Actions and rewards are available, allowing for reward learning based on actions. Examples include button pushing and advert selection.
* **Contextual Bandits:** Signals guide appropriate actions, but actions do not influence state transitions. They resemble independent events with states, actions, and rewards.
* **Contextual Bandits with Transition Rules:** Similar to reinforcement learning with no action influence on state transitions.
* **Markov Reward Processes:** States and rewards exist, but no actions are present. Reinforcement learning algorithms can predict long-term reward and state value.

The rock-paper-scissors game does not fit into these categories due to multiple agents. It is typically analyzed using game theory or, for learning against humans, as a reinforcement learning problem based on past plays."
regarding the output format for semantic segmentation,https://stats.stackexchange.com/questions/246287/regarding-the-output-format-for-semantic-segmentation,machine-learning|deep-learning|computer-vision|tensorflow,246287,True,261645,9741,1,10,1486998744,1479314272,10,1486998744,"Semantic segmentation is just extended classification, where you perform classification of each pixel into the n_classes. 
Let's say your input is an RGB image with size (cols,rows,3), you pass a batch of such images sized (batch_size, cols, rows, 3) to the CNN.
After performing computations in the network graph, you will end up with a choice to have the last convolutional layer to have n_outputs. 
Binary segmentation (pixelwise yes / no )

Then you have can have n_outputs = 1 and the output shape will be (batch_size, cols, rows, 1). You later take the sigmoid activation use binary_crossentropy loss. Note that this only works for binary segmentation.
MultiClass segmentation (pixelwise probability vector)

Then you have n_outputs = n_classes and the output shape will be (batch_size, cols, rows, n_classes). Now comes the tricky part. You need to apply softmax to each pixel probability vector which generally involves permuting dimensions depending on the deep learning framework you are using. In this case you use categorical_crossentropy as it
In Keras you can
final_conv_out = Convolution2D(n_classes, 1, 1)(conv9)

x = Reshape((n_classes, rows*cols))(final_conv_out)
x = Permute((2,1))(x)

# seg is a pixelwise probability vector sized (batch_size, rows*cols, n_classes)
seg = Activation(""softmax"")(x)

","**Summary:**

Semantic segmentation is a classification task where each pixel in an image is assigned a label from a set of categories. It involves passing an image batch through a Convolutional Neural Network (CNN) and obtaining an output tensor with the same spatial dimensions as the input image.

For binary segmentation, the last convolutional layer has only one output channel, and a sigmoid activation is applied to predict the probability of a pixel belonging to the target class.

For multi-class segmentation, the last convolutional layer has multiple output channels, representing the probabilities of each class. A softmax activation is applied to normalize these probabilities, and categorical cross-entropy loss is used for training.

In Keras, the specific implementation involves reshaping and permuting the output of the convolutional layer before applying softmax activation. This results in a pixelwise probability vector where each pixel has a distribution of probabilities over the class labels."
How does ResNet or CNN with skip connections solve the gradient exploding problem?,https://stats.stackexchange.com/questions/339894/how-does-resnet-or-cnn-with-skip-connections-solve-the-gradient-exploding-proble,neural-networks|deep-learning|gradient-descent|lstm,339894,True,344877,7838,3,10,1658387750,1523457562,5,1525682454,"To my understanding, during backprop, skip connection's path will pass gradient update as well. Conceptually this update acts similar to synthetic gradient's purpose.
Instead of waiting for gradient to propagate back one layer at a time, skip connection's path allow gradient to reach those beginning nodes with greater magnitude by skipping some layers in between.
I personally do not find any improvement nor greater risk of encountering exploding gradient with skip connection.
","**Summary:**

During backpropagation, skip connections allow gradient updates to bypass certain layers, enabling a stronger gradient flow to initial layers. This is similar to synthetic gradients, which inject external gradients to enhance learning. While skip connections do not inherently improve models or increase the risk of exploding gradients, they can potentially accelerate training by providing a direct path for gradient propagation."
Which elements of a Neural Network can lead to overfitting?,https://stats.stackexchange.com/questions/306574/which-elements-of-a-neural-network-can-lead-to-overfitting,machine-learning|neural-networks|mathematical-statistics|predictive-models|overfitting,306574,True,306607,11367,1,10,1507294556,1507277252,9,1507294556,"Increasing the number of hidden units and/or layers may lead to overfitting because it will make it easier for the neural network to memorize the training set, that is to learn a function that perfectly separates the training set but that does not generalize to unseen data.
Regarding the batch size: combined with the learning rate the batch size determines how fast you learn (converge to a solution) usually bad choices of these parameters lead to slow learning or inability to converge to a solution, not overfitting.
The number of epochs is the number of times you iterate over the whole training set, as a result, if your network has a large capacity (a lot of hidden units and hidden layers) the longer you train for the more likely you are to overfit. To address this issue you can use early stopping which is when you train you neural network for as long as the error on an external validation set keeps decreasing instead of a fixed number of epochs. 
In addition, to prevent overfitting overall you should use regularization some techniques include l1 or l2 regularization on the weights and/or dropout. It is better to have a neural network with more capacity than necessary and use regularization to prevent overfitting than trying to perfectly adjust the number of hidden units and layers.
","**Summary:**

Overfitting, where a neural network overly memorizes the training data, can be caused by excessive network capacity (hidden units and layers). Using a large batch size and learning rate can hinder learning speed but not directly lead to overfitting.

To avoid overfitting, training should be limited using early stopping, where training continues only as long as an external validation set improves. Regularization techniques such as weight regularization (L1/L2) or dropout can also mitigate overfitting.

It's preferable to use a network with more capacity than needed and apply regularization to prevent overfitting, rather than trying to precisely tune the network structure to avoid it altogether."
Gradient descent of $f(w)=\frac12w^TAw-b^Tw$ viewed in the space of Eigenvectors of $A$,https://stats.stackexchange.com/questions/274018/gradient-descent-of-fw-frac12wtaw-btw-viewed-in-the-space-of-eigenvectors,machine-learning|optimization|deep-learning,274018,True,274033,2666,3,10,1571416778,1492358807,7,1492370143,"In many mathematical applications, the motivation becomes clearer after deriving the result. So let's start off with the algebra.
Suppose we were to run GD for $T$ iterations. This will give us the set ${(w_k)}_{k=1}^T$. 
Let's do a change of basis:
$w^k = Qx^k + w^*$ $\iff$ $x^k = Q^T(w^k-w^*) $
Now we have ${(x_k)}_{k=1}^T$. What can we say about them? Let's look at each coordinate separately. By substituting the above and using the update step of GD, 
$x_i^{k+1}= (Q^T(w^{k+1}-w^*))_i = (Q^T(w^k-\alpha (Aw^k-b)-w^*))_i $
Arranging,
$x_i^{k+1}=(Q^T(w^k-w^*))_i-\alpha \cdot (Q^T(Aw^k-b))_i$
The first term is exactly $x_i^k$. For the second term, we substitute $A=Qdiag(\lambda _1 \dots \lambda _n)Q^T$. This yields,
$x_i^{k+1}=x_i^k-\alpha \lambda _i x_i^k=(1-\alpha \lambda _i)x_i^k$
Which was a single step. Repeating until we get all the way to $x_0$, we get
$x_i^{k+1}=(1-\alpha \lambda _i)^{k+1}x_i^0$
All this seems really useless at this point. Let's go back to our initial concern, the ${w}$s. From our original change of basis, we know that $w^k-w^*=Qx^k$. Another way of writing the multiplication of the matrix $Q$ by the vector $x^k$ is as $\sum_i x_i^kq_i$. But we've shown above that $x_i^{k}=(1-\alpha \lambda _i)^{k}x_i^0$. Plugging everything together, we have obtained the desired ""closed form"" formula for the GD update step:
$w^k-w^*=\sum_i x_i^0(1-\alpha \lambda _i)^{k} q_i$
This is essentially an expression for the ""error"" at iteration $k$ of GD (how far we are from the optimal solution, $w^*$). Since we're interested in evaluating the performance of GD, this is the expression we want to analyze. There are two immediate observations. The first is that this term goes to 0 as $k$ goes to infinity, which is of course good news. The second is that the error decomposes very nicely into the separate elements of $x_0$, which is even nicer for the sake of our analysis. Here I quote from the original post, since I think they explain it nicely:

Each element of $x^0$ is the component of the error in the initial guess in the $Q$-basis. There are $n$ such errors, and each of these errors follows its own, solitary path to the minimum, decreasing exponentially with a compounding rate of $1-\alpha \lambda_i $. The closer that number is to 1, the slower it converges.

I hope this clears things up for you enough that you can go on to continue reading the post. It's a really good one!
","**Summary:**

To understand Gradient Descent (GD), researchers transform the iterative GD updates into a ""closed form"" formula.

They define the change from $w$ to $x$ coordinates and show that the $x$ coordinates at each step are a linear combination of their initial values and exponential decay terms involving the eigenvalues of the matrix A.

The closed-form formula for the GD update is:

```
w^k - w^* = Î£(x_i^0 * (1 - Î±Î»_i)^k * q_i)
```

where x_i^0 represents the initial error in the i-th coordinate, Î± is the learning rate, Î»_i is the i-th eigenvalue of A, and q_i is the i-th column of the matrix Q used in the coordinate transformation.

This formula highlights two important aspects:

1. The error at each iteration decreases exponentially towards zero (as k increases).
2. The error is decomposed into separate components, each decaying at its own rate determined by the eigenvalues of A and the learning rate."
Computing the Actor Gradient Update in the Deep Deterministic Policy Gradient (DDPG) algorithm,https://stats.stackexchange.com/questions/258472/computing-the-actor-gradient-update-in-the-deep-deterministic-policy-gradient-d,machine-learning|neural-networks|deep-learning|reinforcement-learning,258472,True,341342,2532,3,10,1539011653,1485524209,7,1524069369,"There is no difference in the calculation. I was wondering the same thing and verified in my own TensorFlow DDPG implementation by trying both and asserting that the numerical values are identical. As expected, they are.
I noticed that most tutorial-like implementations (e.g. Patrick Emami's (http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)) explicitly show the multiplication. However, OpenAI's baselines implementation (https://github.com/openai/baselines/tree/master/baselines/ddpg) $does$ directly compute $\nabla_{\theta^\mu} Q$. (They do this by defining a loss on the actor network equal to $-\nabla_{\theta^\mu} Q$, averaged across the batch).
There is one reason that you'd want to separate out $\nabla_a Q$ from $\nabla_{\theta^\mu} \mu$ and multiply them. This is if you want to directly manipulate one of the terms. For example, Hausknecht and Stone (https://arxiv.org/abs/1511.04143) do ""inverting gradients"" on $\nabla_a Q$ to coerce actions to stay within the environment's range.
","**Summary:**

The calculation of $\nabla_{\theta^\mu} Q$ in DDPG implementations can involve either explÃ­cit multiplication or direct computation.

**Equivalent Methods:**

* **Explicit Multiplication:** Calculate $\nabla_a Q$ and $\nabla_{\theta^\mu} \mu$ separately and multiply them.
* **Direct Computation:** Use a loss function on the actor network that is equal to $-\nabla_{\theta^\mu} Q$.

**Reason for Separation:**

Separating $\nabla_a Q$ and $\nabla_{\theta^\mu} \mu$ allows for direct manipulation of one term. For example, ""inverting gradients"" on $\nabla_a Q$ can constrain actions within the environment's range.

**Default Implementation:**

OpenAI's baselines implementation uses direct computation, while many tutorials show the explicit multiplication method."
Does an optimally designed neural network contain zero &quot;dead&quot; ReLU neurons when trained?,https://stats.stackexchange.com/questions/224378/does-an-optimally-designed-neural-network-contain-zero-dead-relu-neurons-when,machine-learning|neural-networks|convolutional-neural-network,224378,True,224391,586,1,10,1468879988,1468866532,9,1468870526,"There's a difference between dead ReLUs and ReLUs that are silent on many--but not all--inputs. Dead ReLUs are to be avoided, whereas mostly-silent ReLUs can be useful because of the sparsity they induce.
Dead ReLUs have entered a parameter regime where they're always in the negative domain of the activation function. This could happen, for example, if the bias is set to a large negative value. Because the activation function is zero for negative values, these units are silent for all inputs. When a ReLU is silent, the gradient of the loss function with respect to the parameters is zero, so no parameter updates will occur with gradient-based learning. Because dead ReLUs are silent for all inputs, they're trapped in this regime.
Contrast this with a ReLU that's silent on many but not all inputs. In this case, the gradient is still zero when the unit is silent. If we're using an online learning procedure like minibatch/stochastic gradient descent, no parameter updates will occur for inputs that cause the unit to be silent. But, updates are still possible for other inputs, where the unit is active and the gradient is nonzero.
Because dead ReLUs are silent for all inputs, they contribute nothing to the network, and are wasted. From an information theoretic perspective, any unit that has the same output value for all inputs (whether zero or not) carries no information about the input. Mostly-silent ReLUs behave differently for different inputs, and therefore maintain the ability to carry useful information.
","**Summary:**

**Dead ReLUs** (Rectified Linear Units) are activation units that are always inactive (silent) due to negative bias values. This state prevents parameter updates and renders them useless.

**Mostly-silent ReLUs** are units that are silent for some inputs but not others. While they have zero gradients for silent inputs, they can receive updates for other inputs, allowing them to learn from the data.

This distinction between dead and mostly-silent ReLUs is important because dead ReLUs contribute nothing to the network, whereas mostly-silent ReLUs can extract meaningful information from the data, despite their sparsity. From an information theory perspective, units that provide constant outputs (like dead ReLUs) carry no information, while those that behave differently for different inputs (like mostly-silent ReLUs) can convey useful signals."
"Comparison of CPH, accelerated failure time model or neural networks for survival analysis",https://stats.stackexchange.com/questions/82659/comparison-of-cph-accelerated-failure-time-model-or-neural-networks-for-surviva,r|machine-learning|survival|neural-networks|cox-model,82659,True,82662,2946,1,10,1624461867,1390041289,9,1390045842,"It depends on why you are making models. Two main reasons to construct survival models are (1) to make predictions or (2) to model effect sizes of covariates.
If you want to use them in a predictive setting in which you want to obtain an expected survival time given a set of covariates, neural networks are likely the best choice because they are universal approximators and make less assumptions than the usual (semi-)parametric models. Another option which is less popular but not less powerful is support vector machines (ftp://ftp.esat.kuleuven.be/pub/SISTA/vanbelle/reports/10-59.pdf).
If you are modelling to quantify effect sizes, neural networks won't be of much use. Both Cox proportional hazards and accelerated failure time models can be used for this goal. Cox PH models are by far the most widely used in clinical settings, in which the hazard ratio gives a measure of effect size for each covariate/interaction. In engineering settings, however, accelerated failure time (AFT) models are the weapon of choice.
","Survival models predict survival times or estimate the effects of variables on survival outcomes. For making predictions, neural networks are the optimal choice due to their flexibility and precision. For quantifying effect sizes, Cox proportional hazards models are commonly used in clinical settings, measuring the relative risk of an event given a covariate. In engineering settings, accelerated failure time models are preferred for effect size estimation. Both Cox PH and AFT models make specific assumptions about the underlying survival distribution, while neural networks make fewer assumptions."
Does it make sense to find confidence intervals for neural networks?,https://stats.stackexchange.com/questions/534797/does-it-make-sense-to-find-confidence-intervals-for-neural-networks,neural-networks|confidence-interval|uncertainty,534797,True,534804,1420,1,10,1626428229,1626423735,8,1626428229,"For simple regression models, if you have the joint distribution of the parameters you get both confidence intervals and in a sort of derived fashion prediction intervals. You typically have the joint distribution for regression models, either for Bayesian models fit using MCMC samplers you have pseudo-random samples from that distribution and for maximum likelihood estimation you typically have a multivariate normal distribution approximation.
Let's start with the latter case and your example: your confidence interval for a parameter is usually something like $(\hat{\beta}_0 - 1.96 \times \text{SE}(\hat{\beta}_0),\  \hat{\beta}_0 + 1.96 \times \text{SE}(\hat{\beta}_0))$ and you have the same for $\hat{\beta}_1$. Once you want a prediction interval, you get the point prediction for a new observation with covariates $x_*$ is $\hat{\beta}_0 + \hat{\beta}_1 x_*$, but the standard error is $\sqrt{  \text{SE}(\hat{\beta}_0)^2 + x_*^2 \text{SE}(\hat{\beta}_1)^2  + 2 x_* \text{Cov}(\hat{\beta}_0, \hat{\beta}_1) + \sigma^2}$ (if we know the standard deviation $\sigma$ of the residual error term - it gets a little more complicated if we estimate that, too). So, one difference is that prediction intervals is that they take the variation in outcomes (from the residual error term) into account, too. Both are useful, because we are interested in the uncertainty of predictions, but possibly also in interpreting individual coefficients and seeing how much they might vary through sampling variation.
Similarly, for a Bayesian model you get credible intervals from the $K$ MCMC samples by considering the distribution of samples $\hat{\beta}^{(k)}_0$ for $k=1,\ldots,K$. You get a confidence interval for the linear prediction term via the distribution of $\hat{\beta}^{(k)}_0 + \hat{\beta}^{(k)}_1 x_*$ and the prediction interval via sampling for each $k$ from a $N(\hat{\beta}^{(k)}_0 + \hat{\beta}^{(k)}_1 x_*, \hat{\sigma}^{(k)})$ distribution (repeatedly or just once, as you wish). You might say that this is a lot easier and more straightforward that the frequentist case, especially taking the uncertainty around the estimated residual standard deviation is trivial.
For neural networks, gradient boosted trees etc., I don't think a CI for an individual model parameter / weight / tree split is really useful, even if you can calculate it. We just typically have a lot of trouble interpreting individual parameters, but rather tend to look at the influence of features of an input on the output. I guess you could get confidence intervals for something like SHAP values (probably just by bootstrapping), but I've indeed never seen that. What people are much more (only?) interested in is prediction intervals. Ideas for getting them include:

in theory (in practice only for super simple cases), you can do the same things as above, but the complexity usually makes this challenging
ensemble based methods (as you mentioned - one variant of that is leaving dropout on at inference time in neural networks trained with dropout)
bootstrapping (obviously rather time consuming)
quantile regression (e.g. your neural network has three outputs: a point prediction and, say, the 10th and 90th percentile of the distribution for points with such covariates, fit using, say, quantile regression loss of some form / pinball loss - see e.g. this discussion on a Kaggle competition (https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/181632#1004612))

There's probably quite a few more approaches.
","**Summary:**

For regression models, joint parameter distributions provide confidence intervals (for parameter values) and prediction intervals (for model predictions). Confidence intervals quantify uncertainty in parameter estimates, while prediction intervals account for both parameter uncertainty and outcome variation.

In frequentist regression models, confidence and prediction intervals rely on multivariate normal distribution approximations. In Bayesian models, they are derived from MCMC samples.

For complex models like neural networks, confidence intervals for individual parameters are often not meaningful. Instead, researchers focus on prediction intervals, which can be obtained through ensemble methods, bootstrapping, quantile regression, or other approaches.

**Key Differences between Confidence and Prediction Intervals:**

* Confidence intervals estimate uncertainty in parameter values.
* Prediction intervals estimate uncertainty in model predictions for new data points.
* Prediction intervals incorporate both parameter uncertainty and outcome variation."
What if all the nodes are dropped when using dropout?,https://stats.stackexchange.com/questions/302452/what-if-all-the-nodes-are-dropped-when-using-dropout,neural-networks|dropout|dropconnect,302452,True,302487,3674,2,10,1574384250,1505081360,7,1505098707,"This is a concern which will very rarely every be realized. For a moderately sized neural network whose hidden layers each have $1000$ units, if the dropout probability is set to $p=0.5$ (the high end of what's typically used) then the probability of all $1000$ units being zero is $0.5^{1000} = 9.3\times10^{-302}$ which is a mind-bogglingly tiny value. Even for a very small neural network with only $50$ units in the hidden layer, the probability of all units being zero is $.5^{50}=8.9\times10^{-16}$, or less than $\frac{1}{1\ \text{thousand trillion}}$
So in short, this isn't something you ever need to worry about in most real-world situations, and in the rare instances where it does happen, you could simply rerun the dropout step to obtain a new set of dropped weights.
UPDATE:
Digging through the source code for TensorFlow, I found the implementation of dropout here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1867). TensorFlow doesn't even bother accounting for the special case where all of the units are zero. If this happens to occur, then the output from that layer will simply be zero. The units don't ""disappear"" when dropped, they just take on the value zero, which from the perspective of the other layers in the network is perfectly fine. They can perform their subsequent operations on a vector of zeros just as well as on a vector of non-zero values.
","**Summary:**

**Dropout Probability Impact:**

Dropout is a technique used to prevent overfitting in neural networks by randomly setting a certain percentage of units to zero during training. The probability of all units in a layer being zero is infinitesimally small for typical dropout probabilities (e.g., 0.5) and network sizes.

**Implications:**

In most practical scenarios, the probability of all units being zero is negligible. Even in rare cases where it occurs, re-running the dropout step will resolve the issue.

**TensorFlow Implementation:**

TensorFlow's dropout implementation does not explicitly handle the case of all units being zero. Instead, the output from that layer is simply set to zero, which is acceptable for subsequent operations in the network."
Is it safe to run 2 or more RStudio sessions simultaneously?,https://stats.stackexchange.com/questions/178217/is-it-safe-to-run-2-or-more-rstudio-sessions-simultaneously,r|neural-networks,178217,True,178222,14615,1,10,1511342920,1445539332,8,1445540125,"Open up your process explorer, do you see two copies of R studio (or R itself) running?  If so, than your operating system has allocated separate memory spaces for the two copies of R studio, in which case you are perfectly safe.
In general, you have to do lots of work to share memory, and most well written programs will do their best to protect you from the risks of doing so.
","**Summary:**

When multiple instances of a program (e.g., R Studio) are running simultaneously on an operating system, the system typically assigns different memory spaces to each instance. This isolation prevents conflicts and ensures data integrity. In general, sharing memory between programs requires significant effort and comes with potential risks. Well-written programs prioritize protecting users from these risks by isolating memory allocations. Therefore, it is generally safe to have multiple instances of a program running simultaneously, as the operating system handles memory management effectively."
Number of nodes in hidden layers of neural network,https://stats.stackexchange.com/questions/160887/number-of-nodes-in-hidden-layers-of-neural-network,machine-learning|neural-networks,160887,True,160901,20403,1,10,1436553131,1436550078,8,1436553131,"There's an excellent writeup to this question (and to the question of 'how many hidden layers?' as well) at https://stackoverflow.com/questions/10565868/what-is-the-criteria-for-choosing-number-of-hidden-layers-and-nodes-in-hidden-la (https://stackoverflow.com/questions/10565868/what-is-the-criteria-for-choosing-number-of-hidden-layers-and-nodes-in-hidden-la) . It may be disappointing to find that there are few hard-and-fast rules, and if there are, they are often mathematically or logically suspect. Also, another answer in that thread referenced this webpage: ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu (ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu) .
Alternatively, depending on how computationally intensive it is to train your network, you can use various optimization algorithms to try to find it. 
As for the more general question of whether or not layer size should stay constant, I would suggest considering that as a dimensionality-reduction procedure. Would you want your data to be compressed into a lower dimensional form and lose some information? This can be a positive or negative thing. For image compression, it's a requirement. See http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html (http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html) for references on 'bottleneck' layers with image compression.
The type of problem which I would want to have big -> small -> big or some variety of that would probably involve a high dimensional source of data which I would like to compress and then learn features from. If you think that this describes your problem, then perhaps it is a valid approach to use more hidden units, feed into fewer units, then expand the layer out again.
","**Summary:**

Determining the optimal number of hidden layers and nodes is not a straightforward process, as there are few definitive rules. However, some guidelines include:

* **Optimization Algorithms:** For computationally intensive networks, optimization algorithms can be used to find the ideal configuration.
* **Dimensionality Reduction:** Hidden layer size can be viewed as dimensionality reduction, which may be beneficial or detrimental depending on the task.
* **Data Compression:** Large-to-small-to-large layer architectures can be suitable for compressing data and extracting features.

Ultimately, the optimal configuration depends on the specific problem being addressed and involves experimentation and consideration of factors such as dimensionality reduction and computational resources."
Dynamically adjusting NN architecture: inventing the unnecessary?,https://stats.stackexchange.com/questions/33967/dynamically-adjusting-nn-architecture-inventing-the-unnecessary,machine-learning|neural-networks,33967,True,34053,455,2,10,1345094984,1344503999,6,1344605801,"Cascade-Correlation Neural Networks adjust their structure by adding hidden nodes during the training process, so this may be a place to start. Most of the other work I've seen that automatically adjusts the number of layers, number of hidden nodes, etc, of a neural network use evolutionary algorithms. 
Unfortunately, this work is out of my area so I can't recommend any particular papers or references to help you get started. I can tell you that I haven't seen any work which tries to jointly optimize network structure and parameters simultaneously within the deep learning community. In fact, most deep learning architectures are based on greedily learning a single layer at a time, thus making even online learning of deep neural networks a rather untouched area (the work of Martens et al. on Hessian Free Optimization being a notable exception). 
","**Summary:**

Cascade-Correlation Neural Networks (CCNs) are a unique neural network architecture that automatically adjusts its structure by adding hidden nodes during training. Evolutionary algorithms are commonly used to optimize the number of layers and hidden nodes in neural networks, but this field is less explored in the deep learning community.

Unlike evolutionary algorithms, CCNs adjust their structure while simultaneously optimizing the network parameters. In contrast, most deep learning architectures greedily learn layers sequentially, making online learning of deep neural networks uncommon. Notable exceptions include Hessian Free Optimization (HFO) by Martens et al., which optimizes structure and parameters jointly."
Why does BERT has a limitation of only allowing the maximum length of the input tokens as 512?,https://stats.stackexchange.com/questions/478859/why-does-bert-has-a-limitation-of-only-allowing-the-maximum-length-of-the-input,neural-networks|natural-language|word-embeddings,478859,True,546971,1417,1,10,1633360058,1595619993,7,1633358953,"It's an arbitrary value. It is the longest length of input vector they assumed to be possible. Presumably, they didn't have longer vectors in the training set. Moreover, you can always truncate a vector and ignore farther away history, so in such case the length of the vector would be the farthest history you would considered to be useful. 512 is a power of two, what also suggests that the value is chosen arbitrarily by a computer science minded person.
","The maximum input vector length of 512 is an arbitrary choice made due to limitations in the training set and convenience for computer processing. It represents the assumed maximum length of relevant historical data for the model's predictions. By truncating longer vectors, the model can focus on the most important recent history, making it a practical and computationally efficient design decision."
How to determine what type of layers do I need for my Deep learning model?,https://stats.stackexchange.com/questions/313295/how-to-determine-what-type-of-layers-do-i-need-for-my-deep-learning-model,deep-learning|tensorflow|keras,313295,True,313351,5786,2,10,1510580233,1510470607,7,1510505719,"I have some good news and some bad news for you. The good news is that there are many problems for which we know which architecture works best, because of previous research. The bad news is that since today we don't have a good theory of generalization for Deep Networks, we lack theoretical guidance about how to select an architecture for a new problem (however, read here (http://www.ds3-datascience-polytechnique.fr/wp-content/uploads/2017/08/2017_08_31_1630-1730_Olivier_Bousquet_Understanding_Deep_Learning.pdf) for some insights). Thus, in general the most honest answer is that ""it's just a matter of understanding and experience"". On the other hand, for some specific fields we can give more canned suggestions:
Computer Vision
We know that the Convnet family (https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/) of architectures works very well for image classification: LeNet, Alexnet, VGGNet, ResNets, etc. You can train a beefed-up version of LeNet on a non-GPU laptop, and that's a great way to start learning about them.  I suggest you start from this Keras implementation
https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py (https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py)
and try to improve it a bit, by reproducing in Keras the section Implementing a CNN in the TensorFlow layers API of this Jupyter notebook (https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch15/ch15.ipynb). By the way, I can't recommend  the second edition of Sebastian Raschka's book (https://rads.stackoverflow.com/amzn/click/1787125939) highly enough - it's a great way to gain practical knowledge about Machine Learning and Deep Learning. Instead of wasting time reading multiple tutorials on the Internet, get the book - you'll get a more solid understanding of the subject, also because quite a few of the most cited   blog posts on Convolutional Neural Networks are basically summaries of the first edition of the book.
If you want to train architectures which will perform well on realistic, big data sets (such as CIFAR-100 or ImageNet), you need to have access to a GPU cluster.
Natural Language Processing
Here we know that RNNs work well. Actually, the ""simple"" RNN architecture known as LSTM delivers much better results than most people  would commonly expect, as shown in this paper: On the State of the Art of Evaluation in Neural Language Models (https://arxiv.org/abs/1707.05589). The paper also highlights a big limit of modern Deep Learning research: a lot of papers don't care enough about repeatability and reproducibility of results, and some results than are presented as the new state of the art, are instead just due to uncontrolled experimental variation. Again, Raschka's book can be quite useful to start learning about RNNs, together with the corresponding Jupyter notebook (https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch16/ch16.ipynb).
The general case
If you want to tackle a known problem, but maybe with a new data set (for example, you want to perform image classification of car parts because you work for a car manufacturer), you need to use model selection techniques, such as for example cross-validation. You build different networks (different number of layers, different activation functions, etc.) and choose the one with the smallest cross-validation error. Then, you retrain it on the full data set, and you use it for prediction.
However, since the number of alternatives can be prohibitive, you can use some automated machine learning frameworks which help you explore the space of possible networks, such as for example: 

auto-sklearn (http://automl.github.io/auto-sklearn/stable/)
tpot (https://github.com/rhiever/tpot)

If you need to work on big data sets, these tools won't work (they are based on scikit-learn, so there's no support for GPUs, currently). You may have a look at this paper, Large-Scale Evolution of Image Classifiers (https://arxiv.org/abs/1703.01041v2): like the other one I linked, this one takes proper care to ensure repeatability of results.
If you want to attack a new problem (say, Neural Program Synthesis) for which we still have no idea of which architectures work best, probably your best bet is to attend NIPS (https://nips.cc/) and ICML (https://2017.icml.cc/) (or stalk the right sections of arXiv), in the hope that someone has already tackled your problem.
","**Good News:** There are well-known optimal architectures for many problems due to prior research.

**Bad News:** Due to the lack of a generalization theory for Deep Networks, there is no theoretical guidance for selecting architectures for new problems.

**General Suggestions:**

* **Computer Vision:** Convnet architectures (e.g., LeNet, Alexnet, VGGNet) perform well for image classification.
* **Natural Language Processing:** RNNs, particularly LSTMs, deliver effective results.

**Tackling Known Problems with New Data Sets:**

* Use model selection techniques (e.g., cross-validation) to choose the best architecture for the specific data set.

**Exploring New Problems:**

* Attend conferences like NIPS and ICML to learn about recent advancements and potential leads.
* Use automated machine learning frameworks (e.g., auto-sklearn, tpot) to explore the space of possible networks.

**Special Considerations:**

* For large data sets, automated frameworks may not be suitable due to limited GPU support.
* Ensure repeatability and reproducibility of results when referencing research papers."
"Neural Nets, Lasso regularization",https://stats.stackexchange.com/questions/136132/neural-nets-lasso-regularization,neural-networks|lasso,136132,True,136150,4562,1,10,1504888485,1422974847,7,1422981155,"You could take a look at sparse autoencoders, which sometimes put a L1 penalty on the neural activations, which from an optimization point of view is similar to Lasso (L1 penalty on weights). Here is a Theano implementation (https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/costs/autoencoder.py#L118). An alternative is given from the UFLDL (http://ufldl.stanford.edu/wiki/index.php/Sparse_Coding:_Autoencoder_Interpretation) tutorial:

This objective function presents one last problem - the L1 norm is not differentiable at 0, and hence poses a problem for gradient-based methods. While the problem can be solved using other non-gradient descent-based methods, we will ""smooth out"" the L1 norm using an approximation which will allow us to use gradient descent. To ""smooth out"" the L1 norm, we use $\sqrt{x^2 + \epsilon}$in place of $\left| x \right|$, where Îµ is a ""smoothing parameter"" which can also be interpreted as a sort of ""sparsity parameter"" (to see this, observe that when Îµ is large compared to x, the x + Îµ is dominated by Îµ, and taking the square root yields approximately $\sqrt{\epsilon}$).

So you could follow their approach using the smooth approximation, but you can also go for the exact gradient, which is discontinuous at 0, but sometimes that may not be a problem. For a example, the popular ReLU neuron also has a gradient that is discontinuous at 0, but that it's not a problem for most applications.
Also, you can look at Extreme Learning Machines (ELM), which are MLPs that only learn the weights of the final layers and use random hidden layer weights. This seems odd, but it can achieve reasonable results in a very fast time (http://www.ntu.edu.sg/home/egbhuang/) (see right frame). Since the optimization problem to train the ELMs is only linear regression, you could use any Lasso tool for this.
","**Summary:**

To implement sparse autoencoders with L1 regularization, consider:

* **Sparse autoencoders with L1 penalty:** Use a Theano implementation or follow the UFLDL tutorial.

* **Smoothing L1 penalty:** To enable gradient descent, use $\sqrt{x^2 + \epsilon}$ instead of $\left| x \right|$, where $\epsilon$ controls sparsity.

* **Exact gradient (discontinuous at 0):** This option may be suitable for certain applications, similar to the ReLU neuron.

Alternatively, consider Extreme Learning Machines (ELM):

* **ELMs:** Multilayer perceptrons that only learn final layer weights, using random hidden layer weights.
* **Lasso for ELM training:** Since ELM training involves linear regression, Lasso tools can be used. This approach is fast and can achieve reasonable results."
How to normalize gps coordinates for deep learning,https://stats.stackexchange.com/questions/330800/how-to-normalize-gps-coordinates-for-deep-learning,machine-learning|deep-learning|normalization,330800,True,330892,12078,2,10,1650371128,1519732781,6,1519763021,"Not all models are sensitive to data normalization. For example, models with batch-norm layer have a built-in mechanism to fix activations distribution. Others are more sensitive and may even diverge just because of lack of normalization (E.g., try to train a CNN on CIFAR-10 dataset with training images, which pixels are in range $[0, 255]$).
But I'm not aware of any model that would suffer from data normalization. So even though the house prediction model (btw, which one exactly?) may not do it, the model is likely to improve if the data is normalized, and you should do it too.
GPS data has roughly these bounds: the latitude is in $[-100, 100]$, the longitude is in $[-200, 200]$. The coordinates for the populated area are much narrower, but it's not it's not a big deal to assume these wide ranges. This means that the transformation...
$$ x \mapsto \frac{x}{100}$$
... will ensure that the latitude is in $[-1, 1]$ and longitude is in $[-2, 2]$ (and very likely in $[-1, 1]$ as well), which are fairly robust ranges for deep learning. The transformation is easy (in numpy it takes just one line of code) and doesn't require you to compute the statistics from the training data.
","**Summary:**

Certain machine learning models, particularly those without built-in normalization mechanisms, are sensitive to data normalization. Normalizing data helps ensure that model activations and weights are within a consistent range, which is crucial for numerical stability and convergence.

While some models may not explicitly require normalization, it is generally beneficial for improving model performance. However, normalization should be performed with caution to avoid inadvertently changing the underlying data distribution.

For GPS data, normalizing the latitude and longitude coordinates by dividing them by 100 maps them to a more suitable range for deep learning models. This transformation is simple and does not require data preprocessing or statistical calculations. By normalizing GPS data, models can operate more efficiently and produce more accurate predictions."
Backpropagation on Variational Autoencoders,https://stats.stackexchange.com/questions/420974/backpropagation-on-variational-autoencoders,machine-learning|neural-networks|autoencoders,420974,True,421937,3861,1,10,1565687120,1565138783,5,1565687120,"Q1: Your description seems to be pretty much correct.
Q2: The two options are equal:
$$
\frac {\partial E} {\partial w} = \frac {\partial \frac 1 n \sum_{i=1}^n E_i} {\partial w} = \frac 1 n \sum_{i=1}^n \frac {\partial E_i} {\partial w}
$$
Also, note that $n=1$ is a valid choice:

In  our  experiments  we  found  that  the  number  of  samples $L$ per datapoint can be set to 1 as long as the minibatch size $M$ was large enough, e.g. $M = 100$.
Kingma, Diederik P., and Max Welling. ""Auto-encoding variational bayes."" arXiv preprint arXiv:1312.6114 (2013).

","**Summary:**

The provided paragraph discusses the impact of the number of samples ($L$) and minibatch size ($M$) in training deep learning models using variational autoencoders (VAEs). The authors found that setting $L$ to 1 (i.e., using only one sample per data point) can be effective if the minibatch size is sufficiently large (e.g., $M = 100$).

This observation suggests that the overall number of effective samples used in training is more important than the number of samples per data point. By using a large minibatch size, the model can effectively learn from the data even with a small number of samples, as long as the total number of samples over the entire training process is sufficient."
What does it mean for the training data to be generated by a probability distribution over datasets,https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib,distributions|neural-networks|dataset,320375,True,320389,3347,1,10,1572948263,1514267262,5,1514279629,"

Probability distrubution over datasets: What are the datasets? How is the probability distribution generated?


Once we can estimate the underlying distributions of the input data, we essentially know how they are picked and can do good predictions. (generative model). Normally, we can assume an underlying distribution according to what we believe (inductive bias). For example, if we believe that there is a high probability that values are close to zero, we can take a Gaussian distribution with mean $0$ and tune the parameters like variance when we train. Datasets are, for example, set of all coin tosses and the distribution assumed will be binomial. When we do say maximizing log-likelihood for the actual data points, we will get those parameters which make the dataset fit into the distribution assumed.


The examples are independent of each other. Can you give me an example of where the examples are dependent?


For example, we toss a coin and if we have a head we toss another otherwise we do not. Here there is a dependence between subsequent tosses


Drawn from the same probability distribution as each other. Suppose the probability distribution is Gaussian. Does the term ""same probability distribution"" mean that all the examples are drawn from a Gaussian distribution with the same mean and variance?

""This assumption enables us"". What does this mean?



Yes. That is why (4) is said. Once you have a probability distribution from one example, you do not need other examples to describe the data generating process.


Finally, for the last paragraph of page 122, it is given that the samples follow Bernoulli distribution. What does this mean intuitively?


It means that each example can be thought of as a coin toss. If the experiment was multiple coin tosses, you would have each coin toss independent with a probability of head to be $\frac{1}{2}$. Similarly, if you choose any other experiment, the result of each example can be thought of as a coin toss or an n-dimensional dice.
Generating examples means getting a distribution closest to what we see in the dataset for training. That is got by assuming a distribution and maximizing the likelihood of the given dataset and outputting the optimum parameters.
","**Summary:**

Probability distributions can model the underlying patterns in datasets, allowing for predictions and data generation (generative models). Typically, a distribution is assumed based on prior beliefs (inductive bias), such as a Gaussian distribution with a mean of 0 for values likely to be close to zero.

Datasets can be independent or dependent. For example, in coin tosses, each toss is independent, but in a scenario where a second toss is only performed if the first is a head, they are dependent.

The assumption that examples are drawn from the same probability distribution means they share the same parameters, such as mean and variance in a Gaussian distribution. This assumption implies that observing one example is sufficient to infer the data-generating process.

In the case of Bernoulli distribution, each example represents a coin toss. The probability of ""heads"" is constant for each example, analogous to independent coin flips.

Generating examples involves finding a distribution that closely matches the observed dataset. This is achieved by assuming a distribution and optimizing its parameters (e.g., mean and variance) to maximize the likelihood of the given data."
How to handle changing input vector length with neural networks,https://stats.stackexchange.com/questions/138748/how-to-handle-changing-input-vector-length-with-neural-networks,machine-learning|neural-networks|feature-selection|natural-language,138748,True,138760,4525,1,10,1424622852,1424618381,5,1424622852,"There are three general strategies I can think of for NNs with varying input sizes:

Preprocess the inputs to save the same size. For example, people often resize images (ignoring aspect ratio) to a standard square resolution for NNs. In the language case, you might convert all words to a symbolic representation (e.g. ""john""=1, ""james""=2, ""maurice""=3, ""kelly""=4, ""doe""=5) if that makes sense in your application.
Use a sliding window. The network gets to see a fixed-size portion of the input, and then you slide the window by some fixed stride and run it again (from scratch), repeat until you hit the end, and then combine all the outputs in some way.
Same as #2, but using a recurrent neural network so that the network has some internal state that carries over between each stride. This is how NNs process speech audio, for example. Obviously this is a more dramatic change to the architecture than the other options, but for many language tasks this might be necessary (if you have long inputs and need to combine information across the string in a complicated way).

","**Summary of Strategies for Neural Networks (NNs) with Varying Input Sizes:**

1. **Input Preprocessing:** Resizing images to a standard resolution or converting words to numeric representations to ensure inputs have consistent size.

2. **Sliding Window:** Processing fixed-size portions of the input, sliding the window incrementally, and combining the outputs.

3. **Recurrent Neural Networks (RNNs):** Using RNNs to maintain internal state between window strides, allowing the network to process information across longer inputs and handle complex dependencies.

These strategies enable NNs to effectively handle inputs of varying sizes, accommodating different data formats and enabling sophisticated processing of unstructured data such as images and language."
Is Deep-Q Learning inherently unstable,https://stats.stackexchange.com/questions/351082/is-deep-q-learning-inherently-unstable,deep-learning|reinforcement-learning|q-learning,351082,True,351101,1810,1,10,1528860159,1528847214,2,1528860159,"Given that tricks such as replay memory, gradient clipping, reward clipping, carefully selected rollout strategies, and the use of a target network are often necessary for achieving reasonable performance, and even then training can be unstable, yes, it seems to be true in practice. 
That doesn't mean it doesn't work in practice -- DeepMind's Atari paper showed it is indeed possible, with the help of aforementioned tricks. However, it is fairly challenging and requires tens of millions of steps to train properly.
","Training deep reinforcement learning (RL) models with gradient-based methods often requires extensive stabilization techniques. These techniques, such as replay memory, gradient clipping, reward clipping, and target networks, aim to improve stability and performance during training.

Despite these advancements, training RL models can still be challenging, requiring a substantial number of training steps (in the tens of millions). However, successful applications have been demonstrated, such as DeepMind's Atari paper, indicating the feasibility of training RL models with gradient-based methods in practice."
Why are neural networks smooth functions?,https://stats.stackexchange.com/questions/473643/why-are-neural-networks-smooth-functions,machine-learning|neural-networks|mathematical-statistics,473643,True,473668,6825,3,9,1592991818,1592929294,25,1592940359,"A smooth function (https://en.wikipedia.org/wiki/Smoothness) has continuous derivatives, up to some specified order. At the very least, this implies that the function is continuously differentiable (i.e. the first derivative exists everywhere and is continuous). More specifically, a function is $C^k$ smooth if the 1st through $k$th order derivatives exist everywhere, and are continuous.
Neural nets can be written as compositions of elementary functions (typically affine transformations and nonlinear activation functions (https://en.wikipedia.org/wiki/Activation_function), but there are other possibilities). For example, in feedforward networks, each layer implements a function whose output is passed as input to the next layer. Historically, neural nets have tended to be smooth, because the elementary functions used to construct them were themselves smooth. In particular, nonlinear activation functions were typically chosen to be smooth sigmoidal functions like $\tanh$ or the logistic sigmoid function.
However, the quote is not generally true. Modern neural nets often use piecewise linear activation functions like the rectified linear (ReLU (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))) activation function and its variants. Although this function is continuous, it's  not smooth because the derivative doesn't exist at zero. Therefore, neural nets using these activation functions are not smooth either.
In fact, the quote isn't generally true, even historically. The McCulloch-Pitts model was the first artificial neural net. It was composed of thresholded linear units, which output binary values. This is equivalent to using a step function as the activation function. This function isn't even continuous, let alone smooth.
","**Summary:**

A smooth function has continuous derivatives up to a specified order. Neural networks are often composed of smooth elementary functions, such as affine transformations and sigmoidal activation functions. However, modern neural networks frequently incorporate piecewise linear activation functions, such as ReLU, which lack smooth derivatives at certain points. As a result, these networks are not smooth.

Even historically, neural networks have not always been smooth. The McCulloch-Pitts model, the first artificial neural network, used step function activation functions, resulting in discontinuity.

Therefore, the assumption that all neural networks are smooth is not universally true, as piecewise linear and step function activation functions can introduce discontinuity and non-smoothness."
"Can a neural network learn &quot;a == b&quot; and &quot;a != b&quot; relationship, with limited data?",https://stats.stackexchange.com/questions/603590/can-a-neural-network-learn-a-b-and-a-b-relationship-with-limited-data,neural-networks,603590,True,603658,3193,3,9,1675155169,1675049183,11,1675093245,"To supplement Sycorax's answer (https://stats.stackexchange.com/a/603599/232706) on how a neural network might represent the function, I thought I'd see whether a simple network can learn that representation.  The target network has two hidden neurons with ReLU activation and an output neuron with sigmoid activation.
Notebook (https://github.com/bmreiniger/datascience.stackexchange/blob/master/stats_603590_NN_learns_equality.ipynb)
Here's my setup:
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
import numpy as np

n = 1000

np.random.seed(314)
x1 = np.random.randint(-100, 101, size=n)
p = np.random.poisson(size=n)
x2 = x1 + p
X = np.vstack((x1, x2)).T
X = X / 100.0

y = (p == 0)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

I cannot coax scikit-learn's MLPClassifier to learn the two-neuron structure.  Perhaps by trying loads of initial states I could get something close enough that the learning process would settle down to the desired state, but with just a handful of attempts I couldn't make it.
Expanding to 100 hidden neurons, just a little fiddling with other hyperparameters gives perfect accuracy on an iid test set; but with that many neurons it seems to be overfitting on the training set, because it fails on an out-of-scale test set (x1 defined from 200 to 300, the rest as above).
Fiddling by hand with the hyperparameters some more, I'm able to get a good-looking network with 5 hidden neurons:
model = MLPClassifier(
    (5,),
    learning_rate_init=0.05,
    learning_rate=""adaptive"",
    alpha=0,
    max_iter=1000,
    random_state=0,
)
model.fit(X_train, y_train)
print(model.score(X_test, y_test))
#> 0.964 
print(model.coefs_)
#> 
[array([[-5.28193302, -4.71679774, -0.20732829, -0.82536738, -0.14136384],
        [ 5.26312562,  4.70770845,  0.31451317,  1.42101998, -0.21582437]]),
 array([[-16.27265811],
        [-18.1835566 ],
        [  0.1559244 ],
        [ -0.38534808],
        [  0.7400243 ]])]

You can see that the first and second neurons are finding the right idea, while the last three are a bit off; and the output neuron is starting to ignore those three in favor of the first two (with large negative coefficients, the $\delta$ of Sycorax's formula).  More data would probably strengthen the correct relationship, but this already performs well on the out-of-range test data.

Oh, by taking just a Poisson difference above, always $x_2\ge x_1$, which explains why the two important neurons are both firing on something like $x_2-x_1$ rather than one being $x_1-x_2$.  Multiplying p by a random sign, I have a much harder time getting MLPClassifier to train a good model.  By switching away from ReLU to tanh I can, and in fact manage with just a 2-neuron layer:
model = MLPClassifier(
    (2,),
    activation='tanh',
    solver='lbfgs',
    max_iter=1000,
    random_state=0,
)
#> 
[array([[-155.04975387,   62.57832368],
        [ 155.0491934 ,  -62.57812308]]), 
 array([[ 75.66146126],
        [168.25414012]])]

","To investigate how a neural network might represent a specific function, a simple network with two hidden neurons with ReLU activation and an output neuron with sigmoid activation was built. Despite attempts, the network could not learn the desired two-neuron structure. Increasing hidden neurons to 100 resulted in overfitting.

Experimentation led to a network with five hidden neurons with the following hyperparameters:
- Learning rate: 0.05
- Learning rate type: Adaptive
- Regularization: 0
- Maximum iterations: 1000

This network achieved 96.4% accuracy on a test set and exhibited promising behavior, with the first two neurons focusing on the relevant information.

Further exploration revealed that the network's performance was influenced by the choice of activation function and the specific form of the input data. Using tanh activation and re-engineering the input data resulted in a two-neuron network that performed well."
Why is data augmentation classified as a type of regularization?,https://stats.stackexchange.com/questions/295383/why-is-data-augmentation-classified-as-a-type-of-regularization,neural-networks|regularization|data-augmentation,295383,True,295394,9107,3,9,1652519009,1501502383,18,1501506017,"Regularization (traditionally in the context of shrinkage) adds prior knowledge to a model; a prior, literally, is specified for the parameters.  Augmentation is also a form of adding prior knowledge to a model; e.g. images are rotated, which you know does not change the class label.  Increasing training data (as with augmentation) decreases a model's variance. Regularization also decreases a model's variance.  They do so in different ways, but ultimately both decrease regularization error.
Section 5.2.2 of Goodfellow et al's Deep Learning (http://www.deeplearningbook.org/contents/ml.html) proposes a much broader definition:

Regularization is any modiï¬cation we make to a learning algorithm that
is intended to reduce its generalization error but not its training
error.

There is a tendency to asssociate regularization with shrinkage because of the term ""l-p norm regularization""...perhaps ""augmentation regularization"" is equally valid, although it doesn't roll off the tongue.
","**Summary:**

Regularization and augmentation are techniques that incorporate prior knowledge into machine learning models to reduce generalization error.

* **Regularization** assigns priors to model parameters to shrink their values, reducing model variance. Common forms of regularization include l-p norm regularization.
* **Augmentation** manipulates training data (e.g., rotating images) without affecting class labels. This increases data diversity and reduces model overfitting, also decreasing variance.

Both regularization and augmentation ultimately aim to improve model performance by reducing generalization error. While regularization is traditionally associated with shrinkage, augmentation is also a valid form of regularization.

In a broader sense, regularization encompasses any modification to a learning algorithm that reduces generalization error while preserving training error."
Numeric Gradient Checking: How close is close enough?,https://stats.stackexchange.com/questions/184220/numeric-gradient-checking-how-close-is-close-enough,neural-networks|convolutional-neural-network|gradient,184220,True,188724,6265,3,9,1525096443,1448873702,9,1451487645,"The closest I have seen to addressing this was in the Stanford UFLDL tutorial within the softmax regression section (http://ufldl.stanford.edu/wiki/index.php/Exercise:Softmax_Regression#Step_3:_Gradient_checking).  Copying the key statement:

The norm of the difference between the numerical gradient and your analytical gradient should be small, on the order of $10^{-9}$.

In python the code would look something like this:
norm(gradients - numericalGradients)/norm(gradients + numericalGradients)

where gradients are you results from the derivative and numericalGradients are the approximated gradients.
","**Summary:**

To validate gradient calculations in machine learning models, the numerical and analytical gradients should exhibit a small difference in their norms. This difference should be on the order of 10^-9.

In Python code, this validation can be performed by calculating the norm of the difference between the analytical and numerical gradients and dividing it by the norm of their sum. If the quotient is small (close to zero), it indicates that the gradients are consistent."
Are optimal hyperparameters still optimal for a deeper neural net architecture?,https://stats.stackexchange.com/questions/439274/are-optimal-hyperparameters-still-optimal-for-a-deeper-neural-net-architecture,neural-networks|hyperparameter,439274,True,439276,983,1,9,1575461315,1575457621,20,1575458522,"Unfortunately, it doesn't work that way. Hyperparameters cooperate in hard-to-predict ways. For example, a bit extreme to make the point. 
You have no hidden layers, in other words, you are fitting a logistic regression. A logistic regression will usually not really overfit. So you use a relatively big learning rate and a lot of epochs, and find that that works fine, at least, not worse than other hyperparameter configurations. Then you increase the number of layers. You get a complex model, that is now suddenly prone to overfitting. Then the big learning rate and the many epochs that worked fine earlier are no longer optimal. 
Small thing, I would say the number of hidden nodes, or more generally, the whole architecture of the neural network, is also part of the hyperparameters. So your question I read more like, will the same learning rate be optimal if I increase the complexity of the network. 
","Hyperparameter tuning for neural networks is a complex process due to the cooperative and unpredictable relationships between hyperparameters.

For example, a larger learning rate and more training epochs may be suitable for a simple logistic regression model with no hidden layers. However, increasing the model complexity by adding hidden layers can make it prone to overfitting, necessitating adjustments to hyperparameters like learning rate and epochs.

Additionally, the model architecture, including the number of hidden nodes and the overall network structure, also affects hyperparameter optimization. The same hyperparameter settings may not be optimal for different model complexities."
Consistency between two outputs of a neural network,https://stats.stackexchange.com/questions/571053/consistency-between-two-outputs-of-a-neural-network,machine-learning|neural-networks|loss-functions,571053,True,571058,1536,3,9,1649759538,1649592928,4,1649597073,"Another method would be to build two neural networks. The first NN is trained to predict the destination. For the second NN, include the destination predicted by the first NN as an input feature and train the network to predict the class. The second network should then learn to only predict classes that are options for the predicted destination.
Edited in response to @Jivan's comment.
There are more complex methods of multi-label classification, but I'd keep it simple if possible, and try either @Dikran's or my approach first. They are both standard ways of implementing multi-label classification (see this Medium post (https://medium.com/analytics-vidhya/an-introduction-to-multi-label-text-classification-b1bcb7c7364c)). Dikran's method is a Label Powerset and mine is a Classifier Chain. As you've pointed out, there are pros and cons to both these methods. If neither of these produce a good enough result, you could try a variation of the classifier chain, where you build one network to predict one label from the union of destinations and classes. Then train two further networks, one that predicts the destination given a predicted class and the other that predicts the class given a predicted destination. At inference time, you would use the first network to predict either a class or destination, then the appropriate second network predict the other label.
","To address multi-label classification, two neural network (NN) approaches are proposed:

**Approach 1 (Label Powerset):**
* Train a NN to predict the destination.
* Use the predicted destination as an input feature for a second NN to predict the class.
* The second NN learns to only predict classes available for the predicted destination.

**Approach 2 (Classifier Chain):**
* Train a NN to predict either a class or destination from a combined set of destinations and classes.
* Use the predicted label to train two additional NNs, one to predict the destination given a predicted class and one to predict the class given a predicted destination.
* At inference, use the first NN to predict a label, then use the appropriate second NN to predict the other label.

Both approaches are standard methods for multi-label classification, with varying pros and cons. If these methods do not yield satisfactory results, a variation of the Classifier Chain can be employed."
Variance of average of $n$ correlated random variables,https://stats.stackexchange.com/questions/391740/variance-of-average-of-n-correlated-random-variables,machine-learning|deep-learning|bootstrap|regularization|bagging,391740,True,391750,4247,1,9,1577624178,1549810709,19,1549814393,"By definition, we have 
$$\operatorname{var}\left(\sum_{i=1}^n{X_i}\right)=\operatorname{cov}\left(\sum_{i=1}^n{X_i},\sum_{i=1}^n{X_i}\right)=\sum_{i=1}^n{\operatorname{var}(X_i)}+\sum_{i\neq j}\operatorname{cov}(X_i,X_j)$$ 
which is $n \operatorname{var}(X_i)+n(n-1)\operatorname{cov}(X_i,X_j)=n\sigma^2+n(n-1)\rho\sigma^2$, where $i\neq j$. Substituting this into the original equation yields the following:
$$\operatorname{var}\left(\frac{1}{n}\sum_{i=1}^nX_i\right)=\frac{1}{n^2}(n\sigma^2+n(n-1)\rho\sigma^2)=\rho\sigma^2+\frac{1-\rho}{n}\sigma^2$$
Each $X_i$ can be thought of as a single decision mechanism, call it DM, (e.g. regressor). The variance of your decision was $\sigma^2$. By using bootstrap samples and aggregating your DMs' outputs, you end up with a decision variance as above, which is strictly smaller than $\sigma^2$ when $\rho \neq 1$ and $n\neq 1$. DMs will have some degree of correlation of course, because they are trained over bootstrap samples obtained from the same base dataset; however, the correlation between them most probably won't be equal to $1$. Overfitted mechanisms in general have large variance, so by aiming to decrease the variance of your DM, you actually address the problem of overfitting implicitly.
","**Summary:**

The variance of the sum of random variables equals the sum of their variances plus the sum of their covariances. This applies to both unscaled and scaled sums.

Specifically, for the scaled sum of independent random variables, the variance becomes:

```
Var((1/n) * Î£X_i) = Ï * Ï^2 + (1-Ï) * (Ï^2/n)
```

where:

* Ï^2 is the variance of each individual random variable
* Ï is the correlation between the random variables
* n is the number of random variables

This formula shows that the scaled sum's variance is always smaller than the individual variance when Ï is not 1 and n is greater than 1.

The variables being summed can represent decision mechanisms (DMs). By combining multiple DMs through bootstrapping, the resulting combined DM has a lower variance than any individual DM, as it effectively reduces overfitting."
Difference between neural network architectures,https://stats.stackexchange.com/questions/195494/difference-between-neural-network-architectures,neural-networks|deep-learning|convolutional-neural-network|recurrent-neural-network|restricted-boltzmann-machine,195494,True,195500,2153,1,9,1505211991,1455434701,19,1455440266,"To fully answer this question, it would require a lot of pages here. Don't forget, stackexchange is not a textbook from which people read for you.

Multi-layered perceptron (MLP): are the neural networks that (probably) started everything. They are strictly feed-forward (one directional), i.e. a node from one layer can only have connections to a node of the next layer (no crazy stuff here). All layers are fully connected. This is the equivalent to a feed-forward neural network. Both are directed graphs. Backprop is usually used to train these networks. They neurons/nodes in this network perform a dot-product of a weight-vector belonging to this neuron with the input. The output is passed through a sigmoidal function, which later makes it easy to compute gradients and form the backprop algorithms.
Recurrent neural networks (RNNs) are networks which form an undirected cycle, essentially per layer. Meaning that this kind of network has a (fixed) storage capacity of information. It is/was often used on problems that require these specific ""memory buffers"", e.g. handwriting recognition. Training is usually performed by gradient descent (the principle behind backprop). 
Hopfield network: can be seen as an (somewhat unofficial) form of a RNN. It only has one layer, which then (already) provides outputs.. The nodes, however, are interconnected in a special way -- Feedback-Nets (google it). One important point to make is that the neurons/nodes are of binary nature, e.g. they only take 1 or 0 as an input. Training is usually performed by Hebbian learning.
Restricted Boltzman Machines (RBMs) also usually only take binary input. It can be described as a two-layer ""network"" (better: 'graph'). The first layer are visible units, i.e. we observe them. The second layer are hidden (latent) units, i.e. we have to infer them. These nets are trained using contrastive divergence (a mix of gradient descent and Gibbs Sampling). Note that the training procedure does not optimize the exact energy function (I won't explain that here) but rather a different yet related type. In practice this works well. The power of these models lies in the fact that they can be stacked, i.e. one RBM after another. Training is performed separately. Research on RBMs and their development into stacked models was mainly executed by Geoffrey Hinton and his team. It can be categorized as a form of deep learning. 
Recursive neural network: I actually never worked with them, so I probably can't say much about them. I think the main idea is that a neuron can point at itself and therefore enables temporal modeling. These networks can be unrolled and then trained in a regular fashion.
Convolutional neural network: Are usually a special kind of networks in deep learning. Let's first discuss them. 'Deep' here essentially means to have more and more layers in your model. Why didn't we do this before with MLPs? Well, backprob pushes the error the network has produced back to the inputs, i.e. in reverse using the derivatives w.r.t. all parameters. We said before a non-linear transfer function is used in the neurons -- a sigmoidal function. The problem here is, that with many layers, this function causes the gradient to vanish. This is obvious, you put your signal through mutliple sigmoidal functions, which are capped at [0,1] or [-1,1]. They were essentially replaced with rectified linear units (ReLu). These are essentially zero from $-\infty$ to zero and grow linearly from zero to $+\infty$. That solved the issue of the vanishing gradients. Another problem was that it took quite a long time to train such networks on the computers back then. This was resolved by porting the problem to modern GPUs, which can train the most sophisticated nets these days in roughly a week and the more easier ones in less than a day. 
CNN: So what is a convolutional neural network? In its simplest form it is a shallow MLP and the input is, e.g. and most often, an image. Convolutional filters are computed over the image and give input to the next (second) layer. Note: The weights of the convolutional filters are learned as well in the process. These days they are almost always used in deep architectures in combination with pooling layers and other tricks of the trade.

Material for you:

Books to read: 


Neural Networks for Pattern Recognition by Christopher M. Bishop -everybody working with network structures such as the ones you asked for should have read this book. 
The Deep Learning Book by Ian Goodfellow, Yoshua Bengio and Aaron Courville (+ the community), This book is still in progress and hence can be downloaded for free at this point in time: http://www.deeplearningbook.org/ (http://www.deeplearningbook.org/)

Lectures: 


Machine Learning Summer School: http://videolectures.net/mlss09uk_cambridge/?q=Machine%20Learning%20Summer%20School (http://videolectures.net/mlss09uk_cambridge/?q=Machine%20Learning%20Summer%20School) -- a very good summer school and other years are online as well. You should be interested in the talk by Geoffrey Hinton.
Deep Learning summer school: http://videolectures.net/deeplearning2015_montreal/?q=Deep%20Learning%20summer%20school (http://videolectures.net/deeplearning2015_montreal/?q=Deep%20Learning%20summer%20school) -- this one should help you a lot.


These explanations are by far not complete but hopefully correct. If you want to understand this field, you have to read a lot more than this. 
","**Multi-Layered Perceptron (MLP)**

MLP, or feed-forward neural networks, are layers of fully connected nodes that perform dot-product operations on weight vectors. The output is passed through a sigmoid function for easy gradient computation and backpropagation training.

**Recurrent Neural Networks (RNNs)**

RNNs have undirected loops within layers, providing storage capacity. They're used in tasks requiring memory buffers, such as handwriting recognition. Training is typically done through gradient descent.

**Hopfield Network**

Hopfield networks consist of a single interconnected layer with binary nodes. They're trained through Hebbian learning and can be used for optimization problems.

**Restricted Boltzmann Machines (RBMs)**

RBMs are two-layer models with visible and hidden units. Training involves contrastive divergence, and they can be stacked to form deep learning models.

**Convolutional Neural Networks (CNNs)**

CNNs are deep learning models that use convolutional filters to process image data. By replacing sigmoid functions with ReLu units, CNNs overcome the problem of vanishing gradients and are now widely used for image recognition."
How to estimate confidence level for SVM or Random Forest?,https://stats.stackexchange.com/questions/94845/how-to-estimate-confidence-level-for-svm-or-random-forest,classification|svm|neural-networks|random-forest,94845,True,94894,13497,2,9,1398275143,1398251708,7,1398275143,"For random forests you can look at the vote counts instead of just the winning class. Ie did 92% or 52% of the trees in the ensemble vote for class 1. How you do this will depend on the implementation. 
","Random forests, a machine learning technique, allow for the examination of individual tree votes within the ensemble. Unlike traditional models that only provide the predicted class, random forests can reveal the level of consensus among the trees.

Instead of solely focusing on the winning class, practitioners can analyze the distribution of votes for each class. This provides insights into the model's confidence in its predictions. For example, a vote count of 92% for a particular class indicates high confidence, while a count of 52% suggests a more uncertain prediction.

The implementation of this feature varies based on the specific software or library used. Overall, the analysis of vote counts provides additional information about the model's decision-making process, enabling practitioners to assess the reliability of the predictions and potentially identify areas for improvement."
Interpret neural network like the linear regression equation such as how much will Y change if we change X1 and keep the other variables fixed,https://stats.stackexchange.com/questions/589432/interpret-neural-network-like-the-linear-regression-equation-such-as-how-much-wi,r|regression|machine-learning|neural-networks,589432,True,589439,1174,3,9,1663952227,1663691758,8,1663694600,"One of the issues when you introduce nonlinearities and interactions is that the change resulting in a change in a variable of interest depends on the starting value of that variable of interest and of the other variables in the model.
For instance, consider a model like $\hat y = x_1-x_2+x_1^2x_2$. If you want to know by how much $y$ changes upon changing $x_1$ by one unit, take the derivative.
$$
\dfrac{
\partial \hat y
}{
\partial x_1
} = 1 + 2x_1x_2
$$
You cannot answer the question (with a single number) unless you know $x_1$ and $x_2$.
Neural networks are no different. We can take partial derivatives (making use of the chain rule) and interpret those derivatives as slopes just like normal. However, those slopes are likely to depend on the values of all variables (just like above), including the variable of interest.
Consequently, there is no simple interpretation like, âWhen $x_1$ increases one unit, our predicted $y$ increases by $\hat\beta_1$ units.â
If $A$ is an activation function, a simple neural network with two features and two neurons (with activation function $A$) in the hidden layer is:
$$
\hat y = \hat b_{2,1} + \hat w_{2,1}A\bigg(\hat b_{1,1}+\hat w_{1,1}x_1 + \hat b_{1,3}x_2\bigg) + \hat w_{2,2} A\bigg(\hat b_{1,2} + \hat w_{1,2}x_1 + \hat w_{1,4}x_2\bigg)
$$
For a nonlinear activation function $A$, the partial derivatives with respect to $x_1$ and $x_2$ can involve both $x_1$ and $x_2$, meaning that you must know the point where you want to talk about changes in order to talk about changes.
","When dealing with nonlinearities and interactions in mathematical models, the change in a variable depends on its initial value and the values of other variables. This makes it difficult to provide a single answer to questions about changes without specifying the starting point.

Neural networks, with their nonlinear activation functions and complex interactions, exhibit a similar behavior. The slopes or gradients calculated from partial derivatives are dependent on the values of input variables. As a result, it is challenging to provide simple interpretations of how changes in one variable affect the predicted output, as the impact may vary based on the specific values of all variables involved.

For example, in a neural network with two features and two neurons in the hidden layer, the output can be expressed as a function of inputs and weights. The partial derivatives of this function with respect to input variables will involve both input values, indicating that the change in predicted output depends on the specific point at which the change is being considered."
What would be the output distribution of ReLu activation?,https://stats.stackexchange.com/questions/392226/what-would-be-the-output-distribution-of-relu-activation,distributions|neural-networks|data-visualization|loss-functions,392226,True,392230,5527,2,9,1564417218,1550024793,13,1550027004,"Your question seems to boil down to the following:

Suppose $X \sim N(\mu, \sigma^2)$.
  What is the distribution of $Y = \operatorname{ReLU}(X) = \max\{0, X\}$?

Answer.
Let $F_X$ and $F_Y$ denote the cumulative distribution functions of $X$ and $Y$, respectively.
Let $\Phi$ be the standard normal cumulative distribution function:
$$
\Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2 \pi}} e^{-z^2 / 2} \, dz,
$$
so that
$$
F_X(x)
= \Phi\left(\frac{x - \mu}{\sigma}\right)
$$
for all $x \in \mathbb{R}$.
If $y \in \mathbb{R}$, then
$$
\begin{aligned}
F_Y(y)
&= P(Y \leq y) \\
&= P(\max\{0, X\} \leq y) \\
&= P(0 \leq y, X \leq y) &&\text{(*)} \\
&= \begin{cases}
0, & \text{if $y < 0$}, \\
P(X \leq y), & \text{if $y \geq 0$}
\end{cases} \\
&= \begin{cases}
0, & \text{if $y < 0$}, \\
F_X(y), & \text{if $y \geq 0$}
\end{cases} \\
&= \begin{cases}
0, & \text{if $y < 0$}, \\
\Phi\left(\frac{y - \mu}{\sigma}\right), & \text{if $y \geq 0$}
\end{cases}
\end{aligned}
$$
(*) Here we used the fact that $\max\{a, b\} \leq c$ if and only if $a \leq c$ and $b \leq c$ (for any $a, b, c \in \mathbb{R}$).
It's worth emphasizing that $F_Y$ is the cumulative distribution function.
I don't know if this distribution has a name off the top of my head, but knowing the cumulative distribution function allows you to say everything there is to say about the distribution of $Y$.
Visualization
Here is a plot of the cumulative distribution function of $Y$ for various distributions of $X$:
 (https://i.sstatic.net/JM6BO.png)
Note: the distribution of $Y$ is neither discrete nor continuous!
You can see that the distribution of $Y$ is not continuous since continuous distributions have continuous cumulative distribution functions (and $Y$ clearly does not), and $Y$ is not discrete because discrete distributions have piecewise constant cumulative distribution functions (which again $Y$ does not).
In particular, this means that $Y$ does not have a density function.
Effect of Affine Transformations
Suppose your neural network has $p$-dimensional $\mathbf{X} = (X_1, \ldots, X_p) \sim N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ (multivariate normal with mean $\boldsymbol{\mu} \in \mathbb{R}^p$ and covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{p \times p}$).
Suppose that the next layer consists of $q$ units $\mathbf{Y} = (Y_1, \ldots, Y_q) \in \mathbb{R}^q$ given by an affine transformation followed by ReLU:
$$
Y_i = \operatorname{ReLU}\left(b_i + \sum_{j=1}^p w_{i, j} X_j\right).
$$
Let $\mathbf{X}^\prime = (X_1^\prime, \ldots, Y_q^\prime)$ denote the pre-activations:
$$
X_i^\prime = b_i + \sum_{j=1}^p w_{i, j} X_j.
$$
More concisely,
$$
\mathbf{X}^\prime = \mathbf{b} + \mathbf{W} \mathbf{X},
$$
where $\mathbf{b} = (b_1, \ldots, b_q)$ and $\mathbf{W}$ is the matrix of the $w_{i, j}$'s.
Since $\mathbf{X}$ is multivariate normal, so is $\mathbf{X}^\prime$, and we have
$$
\mathbf{X}^\prime
\sim N_q(\mathbf{b} + \mathbf{W}\boldsymbol{\mu}, \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^\top).
$$
In particular, each component $X_i^\prime$ of $\mathbf{X}^\prime$ is itself univariate normal with some mean and variance that can be read off from the joint mean and variance
Then we can apply the argument at the top of this answer to figure out the distribution of each activation $Y_i = \operatorname{ReLU}(X_i^\prime)$.
","**Summary:**

For a random variable $X$ following a normal distribution $N(\mu, \sigma^2)$, the distribution of $Y = \max\{0, X\}$ (ReLU of $X$) is determined by its cumulative distribution function (CDF):

- **CDF of $Y$:**
    - $0$ for $y < 0$.
    - $\Phi\left(\frac{y - \mu}{\sigma}\right)$ for $y \geq 0$.

The graph of the CDF shows that the distribution of $Y$ is neither continuous nor discrete, meaning it lacks a density function.

**Effect on Neural Networks:**

When using ReLU in neural networks, the pre-activation values $\mathbf{X}^\prime$ are normally distributed if the input $\mathbf{X}$ is normally distributed. The distribution of each activation $Y_i = \operatorname{ReLU}(X_i^\prime)$ can be derived using the CDF of $Y$."
Time series forecasting: from ARIMA to LSTM,https://stats.stackexchange.com/questions/478600/time-series-forecasting-from-arima-to-lstm,time-series|neural-networks|forecasting|references|hidden-markov-model,478600,True,478638,706,3,9,1595898884,1595509319,8,1595519591,"There are a couple of good review papers on the topic of deep learning for forecasting:

Neural forecasting: Introduction and literature
overview (https://arxiv.org/pdf/2004.10240.pdf)
Recurrent Neural Networks for Time Series Forecasting: Current Status
and Future Directions (https://arxiv.org/pdf/1909.00590.pdf)
And a very good presentation by the amazon team (http://vldb2018.lncc.br/documents/slides/VLDB2018-tutorial-forecasting-big-time-series.pdf)

A word of warning though: I am a very big fan of LSTM based forecasting and I advocate for it alot in my various roles. But I would be the first to tell you to tread very, very carefully: The number of use cases where LSTM provide an advantage over traditional statistical models is very limited, and Deep Learning is very far from being an established theoretical topic, the way ARIMA or State Space Models are.
","**Summary:**

Deep learning techniques, particularly LSTM-based models, have emerged in the field of time series forecasting. However, while LSTM models hold promise, it is crucial to recognize their limitations. Statistical models, such as ARIMA and State Space Models, remain well-established and have a proven track record. The applicability of LSTM models should be approached cautiously, as their superiority over traditional methods is limited to specific use cases. Despite the potential of LSTM models, deep learning lacks the theoretical foundation of established statistical models."
What is meant by &#39;Black box variational inference&#39;?,https://stats.stackexchange.com/questions/411221/what-is-meant-by-black-box-variational-inference,machine-learning|neural-networks|variational-bayes,411221,True,411361,2103,3,9,1706113825,1559483413,12,1559575506,"So heâs referring the technique introduced in this paper: https://arxiv.org/abs/1401.0118 (https://arxiv.org/abs/1401.0118)
The idea behind black box VI is that normally in VI it takes a significant amount of work to decide on a variational posterior and derive the ELBO and itâs gradients. As such there was room for a more general algorithm that can be easily implemented and doesnât require the practitioner to derive these forms every time. The âblack boxâ part of the name is just from the fact itâs a general algorithm that works and you donât need to think about whatâs going on inside.
Essentially black box VI is a method that yields an estimator for the gradient of the ELBO with respect to the variational parameters with very little constraint on the form of the posterior or the variational distribution. These constraints (the black box criteria you mention) are only that you can evaluate the first derivatives of the log of the variational distribution with respect to its parameters (which you should be able to as you would normally pick a relatively simple distribution for the variational) and that you can evaluate the log of the joint of the data and latent variables (again very standard in the setting of probabilistic modelling).
","Black box variational inference (VI) aims to reduce the computational burden of VI by eliminating the need to derive complex equations manually. Instead, it provides a general algorithm that can estimate the gradient of the evidence lower bound (ELBO) for any variational posterior, given only the ability to evaluate its log-derivatives and the log-joint probability.

This simplification makes VI more accessible and reduces the risk of errors. The algorithm's black box nature refers to its ability to work without requiring detailed understanding of its internal mechanisms. Black box VI thus provides an efficient and reliable method for performing VI with minimal manual effort, enabling practitioners to focus on the broader modeling process."
Multi-label or multi-class...or both?,https://stats.stackexchange.com/questions/319874/multi-label-or-multi-class-or-both,classification|deep-learning|multi-class|multilabel,319874,True,320165,8049,1,9,1514023929,1513859650,15,1514023929,"Definitions.
In  a classification task, your goal is to learn a mapping $h: X\rightarrow Y$ (with your favourite ML algorithm, e.g CNNs). We make two common distinctions:

Binary vs multiclass: In binary classification, $\left|Y\right|=2$ (e.g, a positive category, and a negative category). In multiclass classifcation, $\left|Y\right|=k$ for some $k\in\mathbb{N}$. In other words, this is just a matter of ""how many possible answers are there"". 
Single-label vs multilabel: This refers to how many possible outcomes are possible for a single example $x\in X$. This refers to whether your chosen categories are mutually exclusive, or not. For example, if you are trying to predict the color of an object, then you're probably doing single label classification: a red object can not be a black object at the same time. On the other hand, if you're doing object detection in an image, then since one image can contain multiple objects in it, you're doing multi-label classification.

Effect on network architecture. The first distinction determines the number of output units (i.e, number of neurons in the final layer). The second distinction determines which choice of activation function for the final layer + loss function you should you. For single-label, the standard choice is softmax with categorical cross-entropy; for multi-label, switch to sigmoid activations with binary-cross entropy. See here (https://stats.stackexchange.com/questions/207794/what-loss-function-for-multi-class-multi-label-classification-tasks-in-neural-n) for a more detailed discussion on this question.
Creating ""hybrid"" combinations. I'll describe an example similar to the one in your question. Suppose I'm trying to classify animals, and I'm interested in recognizing the following:

color (black, white, orange)
size (small, medium, large)
type (cat, dog, chimpanzee)

This looks confusing: some of the labels are mutually exclusive (an animal can't be both black and orange) and others aren't (it can be a black dog). In this case, the solution is to perform multi-class classification with $k=3\cdot 3=9$ (or generally, number of categories times the size of the largest category; in this case all categories were of equal length, 3). You just have to define the loss function carefully: You would apply a softmax activation for each group of 3 (each category) and compare that to the true label. I created a little sketch which I think makes it clear:
 (https://i.sstatic.net/Cu0hc.png)
So the final loss is $L(\hat y, y)=CE_{color} + CE_{size}$. The entire idea here is that we exploited information about the structure of the labels (which are mutually exclusive and which aren't) to significantly reduce the number of outputs (from an exponential number  - all combinations, in this case $3^3$ - to a multiplicative number, $3\cdot 3$). 
","**Summary:**

**Classification tasks** aim to learn a mapping between input data (X) and output labels (Y). Two key distinctions in classification tasks are:

**Binary vs. Multiclass:**
* Binary classification: Two possible outputs (e.g., positive or negative).
* Multiclass classification: Multiple possible outputs (e.g., different categories).

**Single-label vs. Multilabel:**
* Single-label: Each input can have only one label (e.g., determining the color of an object).
* Multilabel: Each input can have multiple labels (e.g., detecting objects in an image).

These distinctions influence the neural network architecture:

* Number of output units: Determined by the number of possible outputs (Y).
* Activation function and loss function: Determined by the label exclusivity (single-label vs. multilabel).

**Hybrid Combinations:**

In certain cases, labels may contain both mutually exclusive and non-exclusive categories. To handle this, a hybrid approach can be used:

* Multiclass classification with multiple subcategories (k = n * m), where n is the number of categories and m is the number of subcategories.
* Define a specific loss function that applies a softmax activation to each subcategory and compares it to the true label.
* This approach reduces the number of outputs while preserving the exclusive and non-exclusive label relationships."
Convolutional neural networks: shared weights?,https://stats.stackexchange.com/questions/154860/convolutional-neural-networks-shared-weights,neural-networks|convolutional-neural-network,154860,True,154867,17152,3,9,1496657854,1433090245,9,1433093971,"The main advantage of shared weights, is that you can substantially lower the degrees of freedom of your problem. Take the simplest case, think of a tied autoencoder, where the input weights are $W_{x} \in \mathbb{R}^d$ and the output weights are $W_{x}^T$. You have lowered the parameters of your model by half from $2d \rightarrow d$. You can see some visualizations here: link (http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/algorithms/autoencoders.html). Similar results would be obtained in a Conv Net.
This way you can get the following results:

less parameters to optimize,
which means faster convergence to some
minima,
at the expense of making your model less flexible. It is
interesting to note that, this ""less flexibility"" can work as a
regularizer many times and avoiding overfitting as the weights are shared with some other neurons. 

Therefore, it is a nice tweak to experiment with and I would suggest you to try both. I've seen cases where sharing information (sharing weights), has paved the way to better performance, and others, that made my model become significantly less flexible.
","**Summary:**

Shared weights in neural networks offer a key advantage by reducing model complexity and the number of parameters to optimize. In the case of a tied autoencoder, where input and output weights are transposed copies of each other, the number of parameters is effectively halved. This technique extends to convolutional neural networks (ConvNets).

The reduced parameter count leads to faster model convergence but potentially decreased model flexibility. However, weight sharing can act as a regularizer, preventing overfitting as shared weights control the behavior of multiple neurons.

Experimenting with shared weights is recommended. In some cases, it can enhance performance, while in others, it may limit model adaptability."
Can deep learning determine if two samples of handwriting are by the same person?,https://stats.stackexchange.com/questions/435443/can-deep-learning-determine-if-two-samples-of-handwriting-are-by-the-same-person,neural-networks|optical-character-recognition,435443,True,435446,2309,1,9,1594099724,1573402210,14,1573402604,"This paper seems to do exactly what you want: recognize authorship of handwriting samples, even when the texts don't match.
""DeepWriter: A Multi-Stream Deep CNN for
Text-independent Writer Identification (https://arxiv.org/pdf/1606.06472.pdf)""
Linjie Xing, Yu Qiao. 2016.

Text-independent writer identification is challenging due to the huge variation of written contents and the ambiguous written styles of different writers. This paper proposes DeepWriter, a deep multi-stream CNN to learn deep powerful representation for recognizing writers. DeepWriter takes local handwritten patches as input and is trained with softmax classification loss. The main contributions are: 1) we design and optimize multi-stream structure for writer identification task; 2) we introduce data augmentation learning to enhance the performance of DeepWriter; 3) we introduce a patch scanning strategy to handle text image with different lengths. In addition, we find that different languages such as English and Chinese may share common features for writer identification, and joint training can yield better performance. Experimental results on IAM and HWDB datasets show that our models achieve high identification accuracy: 99.01% on 301 writers and 97.03% on 657 writers with one English sentence input, 93.85% on 300 writers with one Chinese character input, which outperform previous methods with a large margin. Moreover, our models obtain accuracy of 98.01% on 301 writers with only 4 English alphabets as input.

Siamese networks are used to compare things like signatures; it seems reasonable to try and extend this method to handwriting analysis. One challenge would be that whereas signatures are kind of like ""stamps"" in the sense that the writer will want to reproduce the same symbol over and over, two handwriting samples might not be writing the same words and phrases. So the success or failure of the project hinges on whether the neural network can recognize the writing style as distinct from the words.
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sickinger and Roopak Shah. ""Signature Verification using a 'Siamese' Time Delay Neural Network (https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf)."" AT&T Bell Labs. 1994

This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a ""Siamese"" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.

Another approach is to use the triplet-loss (/questions/tagged/triplet-loss) and embedding strategies such as that used in FaceNet. Then you compare the embeddings by some means to decide if two images have the same or a different author. The success on faces taken from different angles and different lightning conditions is promising, and perhaps a better fit for matching handwriting samples.
Florian Schroff, Dmitry Kalenichenko, James Philbin. ""FaceNet: A Unified Embedding for Face Recognition and Clustering (https://arxiv.org/abs/1503.03832)""

Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.
Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.
On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets.
We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.

","**Summary:**

Recent research explores deep learning techniques for text-independent writer identification, aiming to recognize handwriting styles regardless of the written content.

**DeepWriter:**

* A multi-stream CNN architecture designed specifically for writer identification.
* Leverages local handwritten patches as input, trained with softmax classification loss.
* Key contributions: multi-stream structure optimization, data augmentation learning, and patch scanning strategy.
* Demonstrates high identification accuracy on English and Chinese handwriting datasets.

**Siamese Networks:**

* Initially developed for signature verification, where two neural network sub-networks extract features from separate signatures and compare them.
* Challenges arise when applying this approach to handwriting analysis, as samples may not share the same written content.

**Triplet-Loss with Embeddings:**

* An approach inspired by face recognition, where handwritten images are embedded into a Euclidean space and distances between embeddings correspond to similarity.
* Promises improved representational efficiency and performance in identifying handwriting authors.

**Significance:**

* These deep learning methods offer promising new approaches for writer identification, even when dealing with texts that vary in content and length.
* Potential applications include forensic document analysis, authentication, and personalized handwriting recognition."
Constrain a Neural Network to be monotonic?,https://stats.stackexchange.com/questions/342651/constrain-a-neural-network-to-be-monotonic,regression|machine-learning|neural-networks,342651,True,355442,6054,2,9,1617874639,1524652491,11,1531239516,"Here's an example of an early publication in this vein. 
Joseph Sill. ""Monotonic Networks"". California Institute of Technology. 1998.

Monotonicity is a constraint which arises in many application domains. We present a machine learning model, the monotonic network, for which monotonicity can be enforced exactly, i.e., by virtue of functional form. A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differentiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches. 

","**Summary:**

Joseph Sill's 1998 paper introduces ""monotonic networks,"" a machine learning model designed to enforce monotonicity, a common constraint in various applications.

Monotonicity implies that the output of a function always increases or decreases as its input increases. By enforcing monotonicity through functional form, monotonic networks overcome limitations of existing models.

The paper outlines a straightforward method for implementing and training monotonic networks. It also demonstrates their theoretical capability to approximate any continuous, differentiable monotonic function.

To validate their practical utility, the paper applies monotonic networks to predict corporate bond ratings and compares them to other approaches. The results indicate that monotonic networks perform well on this real-world task.

Overall, the paper establishes the concept of monotonic networks, their implementation and training techniques, their theoretical properties, and their potential applications in domains where monotonicity is crucial."
How to correctly use validation and test sets for Neural Network training?,https://stats.stackexchange.com/questions/283631/how-to-correctly-use-validation-and-test-sets-for-neural-network-training,machine-learning|neural-networks|convolutional-neural-network|validation|train,283631,True,330471,12518,1,9,1519580080,1496651952,13,1519580080,"The bottom line is:

As soon as you use a portion of the data to choose which model performs better, you are already biasing your model towards that data.1

Machine learning in general
In general machine learning scenarios you would use cross-validation to find the optimal combination of your hyperparameters, then fix them and train on the whole training set. In the end, you would evaluate on the test set only to get a realistic idea about its performance on new, unseen data.
If you would then train a different model and select the one of them which performs better on the test set, you are already using the test set as part of your model selection loop, so you would need yet a new, independent test set to evaluate the test performance.
Neural networks
Neural networks are a bit specific in the sense that their training is usually very long, thus cross-validation is not used very often (if training would take 1 day, then doing 10 fold cross validation already takes over a week on a single machine). Moreover, one of the important hyperparameters is the number of training epochs. The optimal length of the training varies with different initializations and different training sets, so fixing number of epochs to one number and then training on all training data (training+validation) for this fixed number is not very reliable approach.
Instead, as you mentioned, some form of early stopping is used: Potentially, the model is trained for a long time, saving ""snapshots"" periodically, and eventually the ""snapshot"" with the best performance on some validation set is picked. To enable this, you have to always keep some portion of the validation data aside2. Therefore, you will never train the neural net on all of the samples.
Finally, there are plenty of other hyperparameters, such as the learning rate, weight decay, dropout ratios, but also the network architecture itself (depth, number of units, size of conv. kernels, etc.). You could potentially use the same validation set which you use for early stopping to tune these, but then again, you are overfitting to this set by using it for early stopping, so it does give you a biased estimate. Ideal would be, however, using yet another, separate validation set. Once you fix all the remaining hyperparameters, you could merge this second validation set into your final training set.

To wrap it up:

Split all your data into training + validation 1 + validation 2 + testing
Train network on training, use validation 1 for early stopping
Evaluate on validation 2, change hyperparameters, repeat 2.
Select the best hyperparameter combination from 3., train network on training + validation 2, use validation 1 for early stopping
Evaluate on testing. This is your final (real) model performance.


1 This is exactly the reason why Kaggle challenges have 2 test sets: a public and private one. You can use the public test set to check the performance of your model, but eventually it is the performance on the private test set that matters, and if you overfit to the public test set, you lose.
2 Amari et al. (1997) in their article Asymptotic Statistical Theory of Overtraining and Cross-Validation (https://www.ncbi.nlm.nih.gov/pubmed/18255701) recommend setting the ratio of samples used for early stopping to $1/\sqrt{2N}$, where $N$ is the size of the training set.
","**Summary:**

Machine learning models can become biased if data used for model selection is also used to train the model. Overfitting to the training data can lead to poor performance on unseen data.

For neural networks, which require lengthy training, early stopping is often used to prevent overfitting. This involves periodically saving snapshots of the network and choosing the snapshot with the best performance on a validation set. However, using the same validation set for early stopping and hyperparameter tuning can also introduce bias.

To avoid bias, it is recommended to:

* Split the data into training, validation 1 (for early stopping), validation 2 (for hyperparameter tuning), and testing sets.
* Train the network on the training data and use validation 1 for early stopping.
* Select the best hyperparameter combination using validation 2.
* Retrain the network on the training data combined with validation 2, using validation 1 for early stopping.
* Evaluate the final model performance on the testing set.

This approach ensures that the model is not overfitted to any specific subset of the data and provides a more realistic assessment of its performance on unseen data."
Is it better to avoid ReLu as activation function if input data has plenty of negative values?,https://stats.stackexchange.com/questions/362461/is-it-better-to-avoid-relu-as-activation-function-if-input-data-has-plenty-of-ne,machine-learning|neural-networks|deep-learning,362461,True,362462,9727,2,9,1561991377,1534408837,10,1534409609,"No, because the activation function of choice isn't applied directly on the input data. The earliest it is applied is after the first layer: $a(Wx+b)$, which is when the weights are properly initialized, leads to both positive and negative inputs into the activation.
Edit: I specified proper initialization because it is important. Typically, weights are initialized to small random values distributed symmetrically about $0$, and biases are initialized to exactly $0$. This means that initially, inputs are split roughly evenly between positive and negative, which is a good starting point since we would like to be within the general region in which the nonlinearities of relu can be exploited.
","**Summary:**

Applying an activation function directly to input data is not ideal because it can result in all inputs having the same sign (positive or negative). This limits the effectiveness of the ReLU (rectified linear unit) activation function, which relies on exploiting nonlinearities.

To address this, weights and biases are initialized in such a way that ensures a mix of positive and negative inputs to the activation function. This is done by initializing weights with small random values distributed symmetrically around zero and biases with zero.

With proper initialization, the activation function is applied after the first layer, where the inputs have a balanced distribution between positive and negative values. This allows the ReLU activation to effectively introduce nonlinearities into the model."
Do Autoencoders preserve distances?,https://stats.stackexchange.com/questions/351212/do-autoencoders-preserve-distances,neural-networks|dimensionality-reduction|distance|autoencoders,351212,True,352945,3568,2,9,1529916142,1528911301,10,1529858447,"No, they don't. We basically design them so that they cannot preserve distances. An autoencoder is a neural network which learns a ""meaningful"" representation of the input, preserving its ""semantic"" features. The quoted words (like so many terms in Deep Learning papers) have no rigorous definition, but let's say that, trained on a set of inputs, the autoencoder should learn some common features of these inputs, which allow to reconstruct an unseen input with small error 1. 
The simplest way for the autoencoder to minimize the differences between input and output (reconstructed input) would be to just output the input, i.e., to learn the identity function, which is an isometry, thus it preserves distances. However, we don't want the autoencoder to simply learn the identity map, because otherwise we don't learn ""meaningful"" representation, or, to say it better, we don't learn to ""compress"" the input by learning its basic semantic features and ""throwing away"" the minute details (the noise, in the case of denoising autoencoder). 
To prevent the autoencoder from learning the identity transformation, and forcing it to compress the input, we reduce the number of units in the hidden layers of the autoencoder (bottleneck layer or layers). In other words, we force it to learn a form of nonlinear dimensionality reduction: not for nothing, there is a deep connection between linear autoencoders and PCA (https://arxiv.org/abs/1804.10253), a well-known statistical procedure for linear dimensionality reduction. 
However, this comes to a cost: by forcing the autoencoder to perform some kind of nonlinear dimensionality reduction, we prevent it from preserving distances. As a matter of fact, you can prove that there exists no isometry, i.e., no distance preserving transformation, between two Euclidean spaces $\mathbb{E}^n$ and $\mathbb{E}^m$ if $m < n$ (this is implicitly proven in this proof of another statement (https://math.stackexchange.com/a/2830228/169160)). In other words, a dimension-reducing transformation cannot be an isometry. This is quite intuitive, actually: if the autoencoder must learn to map elements of a high-dimensional vector space $V$, to elements of a lower-dimensional manifold $M$ embedded in $V$, it will have to ""sacrifice"" some directions in $V$, which means that two vectors differing only along these directions will be mapped to the same element of $M$. Thus, their distance, initially nonzero, is not preserved (it becomes 0). 
NOTE: it can be possible to learn a mapping of a finite set of elements of $V$ $S=\{v_1,\dots,v_n\}$, to a finite set of elements $O=\{w_1,\dots,w_n\}\in M$, such that the pairwise distances are conserved. This is what multidimensional scaling attempts to do. However, it's impossible to map all the elements of $V$ to elements of a lower-dimensional space $W$ while preserving distances.

1things gets more complicated when we refer to my favourite flavour of autoencoder, the Variational Autoencoder, but I won't focus on them here.

","Autoencoders are neural networks designed to learn meaningful representations of input data by preserving its semantic features. To prevent them from simply learning the identity function, which would not provide a useful representation, the number of units in hidden layers is reduced, forcing the autoencoder to perform dimensionality reduction.

However, this dimensionality reduction prevents autoencoders from preserving distances in the input data. This is because a transformation that maps a higher-dimensional space to a lower-dimensional space cannot be an isometry, meaning it cannot preserve distances.

Specifically, while it is possible to learn mappings of finite sets of elements while preserving distances (as in multidimensional scaling), it is impossible to maintain distances for all elements of an input space when mapping to a lower-dimensional space."
What is the output of a tf.nn.dynamic_rnn()?,https://stats.stackexchange.com/questions/330176/what-is-the-output-of-a-tf-nn-dynamic-rnn,deep-learning|lstm|tensorflow|recurrent-neural-network|gru,330176,True,330682,17380,2,9,1525265539,1519387461,10,1519679573,"Yes, cell output equals to the hidden state. In case of LSTM, it's the short-term part of the tuple (second element of LSTMStateTuple), as can be seen in this picture:
 (https://i.sstatic.net/3lrvn.png)
But for tf.nn.dynamic_rnn, the returned state may be different when the sequence is shorter (sequence_length argument). Take a look at this example:
n_steps = 2
n_inputs = 3
n_neurons = 5

X = tf.placeholder(dtype=tf.float32, shape=[None, n_steps, n_inputs])
seq_length = tf.placeholder(tf.int32, [None])

basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)
outputs, states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, dtype=tf.float32)

X_batch = np.array([
  # t = 0      t = 1
  [[0, 1, 2], [9, 8, 7]], # instance 0
  [[3, 4, 5], [0, 0, 0]], # instance 1
  [[6, 7, 8], [6, 5, 4]], # instance 2
  [[9, 0, 1], [3, 2, 1]], # instance 3
])
seq_length_batch = np.array([2, 1, 2, 2])

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  outputs_val, states_val = sess.run([outputs, states], 
                                     feed_dict={X: X_batch, seq_length: seq_length_batch})

  print(outputs_val)
  print()
  print(states_val)

Here the input batch contains 4 sequences and one of them is short and padded with zeros. Upon running you should something like this:
[[[ 0.2315362  -0.37939444 -0.625332   -0.80235624  0.2288385 ]
  [ 0.9999524   0.99987394  0.33580178 -0.9981791   0.99975705]]

 [[ 0.97374666  0.8373545  -0.7455188  -0.98751736  0.9658986 ]
  [ 0.          0.          0.          0.          0.        ]]

 [[ 0.9994331   0.9929737  -0.8311569  -0.99928087  0.9990415 ]
  [ 0.9984355   0.9936006   0.3662448  -0.87244385  0.993848  ]]

 [[ 0.9962312   0.99659646  0.98880637  0.99548346  0.9997809 ]
  [ 0.9915743   0.9936939   0.4348318   0.8798458   0.95265496]]]

[[ 0.9999524   0.99987394  0.33580178 -0.9981791   0.99975705]
 [ 0.97374666  0.8373545  -0.7455188  -0.98751736  0.9658986 ]
 [ 0.9984355   0.9936006   0.3662448  -0.87244385  0.993848  ]
 [ 0.9915743   0.9936939   0.4348318   0.8798458   0.95265496]]

... which indeed shows that state == output[1] for full sequences and state == output[0] for the short one. Also output[1] is a zero vector for this sequence. The same holds for LSTM and GRU cells. 
So the state is a convenient tensor that holds the last actual RNN state, ignoring the zeros. The output tensor holds the outputs of all cells, so it doesn't ignore the zeros. That's the reason for returning both of them.
","In LSTM cells, the cell output (short-term memory) is accessible through the hidden state. However, in ""tf.nn.dynamic_rnn"" when a sequence is shorter than specified, the returned state differs.

The state tensor contains the actual RNN state, ignoring any zeros resulting from padding. In contrast, the output tensor includes the outputs of all cells, including zeros from padding. This distinction serves the purpose of providing a concise representation of the finalRNN state (state tensor) while retaining the complete sequence information (output tensor).

For shorter sequences, the state equates to the first output value (output[0]), while for full sequences, it corresponds to the second output value (output[1]). In the case of the second output value, it represents a zero vector for short sequences."
Why does reducing the learning rate quickly reduce the error,https://stats.stackexchange.com/questions/282544/why-does-reducing-the-learning-rate-quickly-reduce-the-error,machine-learning|neural-networks,282544,True,282555,5330,1,9,1496301816,1496152633,12,1496156658,"I think an intuitive answer goes along these lines:
Suppose you want to find a parameter that 'best explains' your data in only one dimension. 'Best explains' means to make an error function small (or a likelihood big, these are usually the same as error = -log(likelihood)). We forget for a moment that we want to talk about neural nets: our model is very simplistic: its just $f(x,\theta) = x + \theta$. $x$ is the input for the function and $\theta$ is the parameter of the model (in the case of NNs its the weights). We want to find the best parameter for explaining the single data point $(x,y) = (1,5)$. Also imagine that we do not use an L2-error but an L1-error, i.e. the error function is
$$l(\hat{y}, y) = |\hat{y} - y| = |x + \theta - y|$$
Of course, the absolute value is not differentiable at $0$ so we just define the differential of the absolute value at $0$ to be $0$ (if you are exactly at the minimal value of the function, then do not move at all). Otherwise the differential is $+1$ if $x + \theta < y$ and $-1$ if $x+\theta > y$. We start with some random $\theta$, for example, $\theta = -1.5$. Then $\hat{y} = x + \theta = 1 - 1.5 = -0.5$ so graphically, the error function looks like this:
 (https://i.sstatic.net/TLSuB.png)
Now the derivative is $+1$ so we move $\theta$ by $+1$ to the right which makes $\hat{y} = +0.5$:
 (https://i.sstatic.net/gQXlC.png)
After that the derivative is $-1$ which makes us set $\theta$ back to $-0.5$ and so on and so on. We always run around the minimal value but since we do not give any weight to our gradient, we circulate around it instead of finding our way inside. This is intuitively what could happens when you set your learning rate to be constantly $1$ instead of some lower value. Of course, in the real world we use an L2-error which already would take the distance to the minimal value into account and everything looks much more complicated... Also the reason is not that we are principally unable to find our way to the minimal value but rather that there are numerical instabilities which makes us over- or underestimate the 'perfect' gradient.
Question 2): The only explanation I can come up with is the following: Suppose we use a constant learning rate and the parameter search algorithm spirals around a local minimum for some time:
 (https://i.sstatic.net/LZZSN.png)
Sue to the fact that the learning rate stays constant it could happen that at some point the algorithm 'spiraled' into a suitable point so that in the next step it actually overshoots so much that it moves out of the complete region it was in all the time and 'accidentally' finds its way into a completely new valley and into the direction of another even lower local minimum that it can spiral around:
 (https://i.sstatic.net/8YD2Q.png)
However, despite all of this I would not pay too much attention to question no. 2 in general: What the error graph does not tell you is that after so many epochs you are probably overfitting your data quite a lot, i.e. the best solution for your problem is mostly not the global minimum of the error function! Question no. 1 indeed is much more important. You will find realizations of the concept of an 'adaptive' learning rate in many different algorithms (for example in Gradient Boosting which is completely unrelated to NNs you have trees and you do not simply add the next tree but you add $\lambda*\text{next tree}$ where $0 < \lambda < 1$).
","**Summary:**

* **Error Function Minimization:** To find a parameter that best explains data, we minimize an error function (-log(likelihood)).
* **L1 Error:** In L1 error, the absolute value is used, making the derivative discontinuous at 0 and requiring a special definition at that point.
* **Constant Learning Rate:** A constant learning rate can lead to oscillations around the minimum, failing to converge due to overshooting.
* **Adaptive Learning Rate:** Conversely, an adaptive learning rate adjusts based on the gradient, helping to avoid overshooting and accelerate convergence.
* **Spiraling Around Local Minima:** With a constant learning rate, the algorithm can spiral around a local minimum and occasionally overshoot into a different region, potentially finding a lower minimum.
* **Overfitting and Local Minima:** However, finding the global minimum may not be the best solution due to overfitting, and adaptive learning rates play a crucial role in many algorithms."
Why can&#39;t this function be used as a loss function?,https://stats.stackexchange.com/questions/271144/why-cant-this-function-be-used-as-a-loss-function,machine-learning|neural-networks|loss-functions,271144,True,271173,12591,3,9,1721735545,1491002493,9,1491025327,"The loss function in the original post seems like a 0-1 loss (https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function) divided by $N$, which is in fact the ultimate goal of most of classification settings. Actually, 0-1 loss divided by $N$ is equivalent to the accuracy (https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers#Single_metrics) metric.
What your friend said is that it is difficult to use 0-1 loss directly for training a model. This is true for many reasons, but mostly because the loss is not differentiable. Many other loss functions used in classification, for example the likelihood of data, can be viewed as approximations of 0-1 loss.
However, since the 0-1 loss is so intuitive and straightforward, the loss function is often measured during the training and assessing models. For me, I would love to print out both likelihood and 0-1 loss.
","**Summary:**

The 0-1 loss function, which measures the accuracy of a classification model, is equivalent to the accuracy metric. While intuitive, it is difficult to use directly for training models due to its non-differentiability. Instead, other loss functions, such as the likelihood of data, are used as approximations of the 0-1 loss.

Despite its limitations for training, the 0-1 loss remains valuable for assessing models' performance. Practitioners often track both the likelihood loss and the 0-1 loss to evaluate a model's effectiveness in classification tasks."
How do I force the L-BFGS-B to not stop early? Projected gradient is zero,https://stats.stackexchange.com/questions/126251/how-do-i-force-the-l-bfgs-b-to-not-stop-early-projected-gradient-is-zero,optimization|python|deep-learning|scipy,126251,True,126254,17407,2,9,1439581820,1417498004,8,1417500098,"I don't know much about the SciPy wrapper, but the underlying L-BFGS-B code gives several options.  The help file for the R interface lists several of them.
Assuming your gradient is just small but isn't actually zero, you have several options that will either increase the size of the gradient or decrease the size that the software will tolerate.

You can rescale the parameters so that a small difference in the parameters produces a more substantial change in your objective function.  The R wrapper has a way to do this automatically, but I don't see one in the SciPy one.  You could also do it manually.
You can rescale your objective function (e.g. by multiplying it by some constant) so that the differences and derivatives are larger (e.g. greater than $10^-5$).
You can adjust the method's tolerances.  The tolerance limit you're bumping up against is for pgtol, which is $10^-5$, by default.  The documentation for L-BFGS-B seems to suggest (at the end of Section 3) that you could safely bring this value down to the ""square root of machine precision"", which is about $10^-8$ on most machines.  The other tolerance limits (absolute and relative) might also become important once you relax pgtol, if your gradients are very small.

Link to the L-BFGS-B documentation (postscript format) (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&ved=0CD4QFjAE&url=http%3A%2F%2Fsepwww.stanford.edu%2Fsep%2Fclaudio%2FResearch%2FPrst_ExpRefl%2FTargwe%2Fsrc%2Fwemva%2Fl-bfgs%2FLbfgsb.2.1%2Fcode.ps&ei=gVR9VK66Bs64ogSt5YGwCg&usg=AFQjCNFq2L-PFtV_QKQHxojlOXYwEOZw0A&sig2=6oOy3arL5-UVv5HIz0On6w)
Link to the R documentation for L-BFGS-B (https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html)
","**Summary:**

When using the SciPy wrapper for L-BFGS-B, it's important to know how to handle small gradients. Here are some options:

**1. Rescale Parameters:**
* Make parameter changes more significant by increasing the scale of the parameters.

**2. Rescale Objective Function:**
* Increase the values of the objective function and its derivatives to exceed potential tolerance limits.

**3. Adjust Tolerances:**
* L-BFGS-B includes various tolerance parameters that affect gradient convergence. By default, the gradient norm tolerance (pgtol) is set to 1e-5.
* The documentation suggests that pgtol can be safely reduced to the square root of machine precision, which is approximately 1e-8 on most machines.

**Additional Considerations:**
* If pgtol is relaxed, other tolerance parameters (absolute and relative) may also need to be adjusted.
* The R wrapper for L-BFGS-B provides a built-in function for parameter rescaling, but this feature is not currently available in the SciPy wrapper."
Sparse Autoencoder [Hyper]parameters,https://stats.stackexchange.com/questions/101237/sparse-autoencoder-hyperparameters,neural-networks|optimization|deep-learning|deep-belief-networks|autoencoders,101237,True,104597,5250,2,9,1654523874,1401947596,10,1403635742,"The autoencoder package is just an implementation of the autoencoder described in Andrew Ng's class notes (https://web.stanford.edu/class/archive/cs/cs294a/cs294a.1104/sparseAutoencoder.pdf), which might be a good starting point for further reading. Now, to tackle your questions

 People sometimes distinguish between *parameters*, which the learning algorithm calculates itself, and *hyperparameters*, which control that learning process and need to be provided to the learning algorithm. **It is important to realise that there are NO MAGIC VALUES** for the hyperparameters. The optimal value will vary, depending on the data you're modeling: you'll have to try them on your data. 
a) Lambda ($\lambda$) controls how the weights are updated during backpropagation. Instead of just updating the weights based on the difference between the model's output and the ground truth), the cost function includes a term which penalizes large weights (actually the squared value of all weights). Lambda controls the relative importance of this penalty term, which tends to drag weights towards zero and helps avoid overfitting.
b) Rho ($\rho)$ and beta $(\beta$) control sparseness. Rho is the expected activation of a hidden unit (averaged across the training set). The representation will become sparser and sparser as it becomes smaller. This sparseness is imposed by adjusting the bias term, and beta controls the size of its updates. (It looks like $\beta$ actually just rescales the overall learning rate $\alpha$.)
c) Epsilon ($\epsilon)$ controls the initial weight values, which are drawn at random from $N(0, \epsilon^2)$.
Your rho values don't seem unreasonable since both are near the bottom of the activation function's range (0 to 1 for logistic, -1 to 1 for tanh). However, this obviously depends on the amount of sparseness you want and the number of hidden units you use too.

LeCunn's major concern with small weights that the error surface becomes very flat near the origin if you're using a symmetric sigmoid. Elsewhere in that paper, he recommends initializing with weights randomly drawn from a normal distribution with zero mean and $m^{-1/2}$ standard deviation, where $m$ is the number of connections each unit receives.

There are lots of ""rules of thumb"" for choosing the number of hidden units. Your initial guess (2x input) seems in line with most of them. That said, these guesstimates are much more guesswork than estimation. Assuming you've got the processing power, I would err on the side of more hidden units, then enforce sparseness with a low rho value.

One obvious use of autoencoders is to generate more compact feature representations for other learning algorithms. A raw image might have millions of pixels, but a (sparse) autoencoder can re-represent that in a much smaller space. [Geoff Hinton][2] (and others) have shown that they generate useful features for subsequent classification. Some of the deep learning work uses autoencoders or similar to pretrain the network. [Vincent et al.][3] use autoencoders directly to perform classification.
The ability to generate succinct feature representations can be used in other contexts as well. Here's a neat little project where autoencoder-produced states are used to guide a reinforcement learning algorithm through Atari games (https://arxiv.org/pdf/1312.5602.pdf).
Finally, one can also use autoencoders to reconstruct noisy or degraded input, like so (https://ieeexplore.ieee.org/document/4564577), which can be a useful end in and of itself.
","**Summary of Autoencoder Hyperparameters:**

Autoencoders are a class of neural networks that learn compact representations of input data. Their performance depends on several hyperparameters, which should be tuned specifically to the data being modeled.

**Lambda (Î»):** Controls weight updates during backpropagation, penalizing large weights to prevent overfitting.

**Rho (Ï) and Beta (Î²):** Control sparsity, the level of activation in hidden units. Rho determines the desired activation, while beta adjusts the bias term to achieve it.

**Epsilon (Îµ):** Determines the initial weight values drawn from a normal distribution with zero mean and standard deviation Îµ.

**Choosing Hyperparameters:**

There are no universal optimal values for hyperparameters. They should be experimented with based on the data. For example, rho values near zero promote a sparse representation.

**Number of Hidden Units:**

Guesstimates can be used as a starting point, such as 2 times the input dimension. However, using more hidden units with a low rho value can enforce sparsity.

**Uses of Autoencoders:**

* Generating compact feature representations for other learning algorithms
* Pre-training deep networks
* Performing classification directly
* Guiding reinforcement learning algorithms
* Reconstructing noisy or degraded input"
Should I normalize all data prior feeding the neural network models?,https://stats.stackexchange.com/questions/458579/should-i-normalize-all-data-prior-feeding-the-neural-network-models,machine-learning|neural-networks|normalization|tensorflow|scales,458579,True,458604,16224,2,9,1662915805,1586090033,7,1586099950,"Yes, normalisation/scaling is typically recommended and sometimes very important. Especially for neural networks, normalisation can be very crucial because when you input unnormalised inputs to activation functions, you can get stuck in a very flat region in the domain and may not learn at all. Or worse, you can end up with numerical issues. 
One very obvious reason is that you need to tune (but you don't) the weight initialisations in the network according to the input range corresponding to that weight, e.g. let $x_1,x_2$ be two distinct features and $w_1,w_2$ be the corresponding weights. Also let the range of the feature be as follows: $x_1\in[0,1000],x_2\in[0,1]$. When you initialise $w_i$ with numbers within $[-1,1]$ for example, it won't mean the same for $x_1$ and $x_2$. Probably, the sum $w_1x_1+w_2x_2$ will be dominated by $w_1x_1$ and you won't see the effect of $w_2x_2$ for some time unless you're very lucky, and learning will be hindered significantly until the network is finally able to learn what $w_1$ should have been in the first place.

Doesn't normalization require that data conforms to the normal parametric distribution?

No, normalisation has nothing to do with normal distribution. One form of normalisation, called standardising, which is subtracting the mean and dividing by the deviation is very common in the literature and typically used for converting a normal RV into standard normal RV. Although the idea may stem from normal distributions, the operation has nothing to do with normal distribution.
","**Summary:**

Normalizing or scaling data is essential, especially for neural networks. Unnormalized inputs can lead to issues in neural network learning, such as getting stuck in flat regions of activation functions or encountering numerical problems.

Normalization helps ensure that the impact of different features on a neural network's output is balanced. Without normalization, features with larger ranges may overshadow features with smaller ranges, hindering learning.

Normalization does not require data to follow a normal distribution. A common form of normalization is standardization, which involves subtracting the mean and dividing by the standard deviation. This operation is commonly used to convert a normal random variable into a standard normal random variable but is not limited to data that follows a normal distribution."
How does the bottleneck z dimension affect the reconstruction loss in VAEs,https://stats.stackexchange.com/questions/450229/how-does-the-bottleneck-z-dimension-affect-the-reconstruction-loss-in-vaes,machine-learning|neural-networks|mathematical-statistics|references|autoencoders,450229,True,450385,5194,2,9,1582130357,1582060587,7,1582130357,"It seems to me that the link you are missing here is to the probabilistic / information theory interpretation of VAEs. When the capacity of your networks is large enough you will reach a point where the solution with a larger latent space does not keep more information than a smaller one. This is possible in VAEs because they produce a noisy representation inside. 
To clarify things: First the bits/dim metric is per dimension of the input. You can read more about this metric in links collected here:
What is bits per dimension (bits/dim) exactly (in pixel CNN papers)? (https://stats.stackexchange.com/questions/423120/what-is-bits-per-dimension-bits-dim-exactly-in-pixel-cnn-papers)
Maybe the limit for infinitely big networks and infinite data is instructive here: VAEs optimize a variational bound on the evidence for the model. This is bounded from above by the true entropy of your data. At or near this point your bits/dim will converge and adding more complexity anywhere will no longer improve performance. With limited data this point will come earlier.
As you appear to think in terms of bottlenecks & auto-encoders: For VAEs the bottleneck is not really the number of dimensions in the latent space, but the noise. Without noise even a single continuous number has infinite capacity. As VAEs are allowed to adjust the noise on their representation they may very well represent the same amount of information in fewer dimensions with less noise. Thus, the number of latent dimensions much less informative about the capacity of the encoding than for classic autoencoders. Indeed VAEs regularly have latent space units who converge towards always being equal to the prior, i.e. not carrying any information about images.
In practice more latent units become harder to train and costly, such that you would avoid using too many latent units which ""die"", but from a theoretical viewpoint many dimensions does not equal an open bottleneck for VAEs.
Thus, overall I would say the dimensionality of z for VAEs is one of many knobs changing the expressiveness/complexity of the encoder/decoder and does not affect the reconstruction loss directly beyond this.
","**Summary:**

VAEs use a probabilistic approach to learn latent representations of data. Unlike classic autoencoders, VAEs introduce noise into their representations, allowing them to encode more information in fewer dimensions.

The capacity of a VAE is not directly proportional to the number of latent dimensions but rather to the amount of noise in the representation. As networks get larger, the optimal latent space dimension may not increase because the noise allows for efficient information storage.

The optimal latent space dimension for a VAE depends on factors such as data size and network complexity. While more latent units can increase expressiveness, they can also make training more difficult.

Therefore, for VAEs, the number of latent dimensions is not a direct measure of capacity, as it interacts with the noise level to determine the expressiveness and capacity of the encoder/decoder."
Why is Permuted MNIST good for evaluating continual learning models?,https://stats.stackexchange.com/questions/411235/why-is-permuted-mnist-good-for-evaluating-continual-learning-models,neural-networks|data-transformation|dataset|domain-adaptation|continual-learning,411235,True,428360,4917,4,9,1614171567,1559497236,4,1569251286,"Actually as stated in this paper https://arxiv.org/abs/1805.09733 (https://arxiv.org/abs/1805.09733) the Permuted MNIST evaluation is not a good method to measure Continual Learning performance. This is because the permuted images differ too much from the original, which is not a good representation of real-world scenarios. Normally a new task with a new dataset could have similar images which could lead to the network making false positive predictions. 
","The Permuted MNIST evaluation is not a reliable metric for assessing Continual Learning performance due to the significant differences between permuted images and real-world scenarios. In practical applications, new tasks often involve similar images to existing ones, which can result in incorrect predictions based on false positives. As a result, the Permuted MNIST evaluation provides an inaccurate representation of the network's ability to handle real-world continual learning challenges."
What is the output of an LSTM,https://stats.stackexchange.com/questions/405163/what-is-the-output-of-an-lstm,machine-learning|mathematical-statistics|deep-learning|lstm,405163,True,405177,18042,1,9,1556268463,1556261767,11,1556268091,"The basic recurrent neural network (RNN) cell (https://stats.stackexchange.com/questions/377091/time-steps-in-keras-lstm/377116#377116) is something that takes as input previous hidden state $h_{t-1}$ and current input $x_t$ and returns current hidden state
$$ h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$
Same applies to LSTM, but it is just a little bit more complicated as described in this great blog post (https://colah.github.io/posts/2015-08-Understanding-LSTMs/). So answering your second question, at each step the RNN cell returns an output that can be used to make predictions. There are two ways of using RNN's, you can either process whole input sequence and look only at the last output state (e.g. process a whole sentence and then classify the sentiment of the sentence), or use the intermediate outcomes (in Keras (https://keras.io/layers/recurrent/) this is the return_sequence=True parameter) and process them further, or make some kind of predictions (e.g. named-entity recognition per each word of a sentence). The only difference in here is that in the first case you simply ignore the intermediate states. If this is too abstract, the following figure (from the blog post referred above) may be helpful.
 (https://i.sstatic.net/FDfkx.png)
As you can see, at each step you have some output $h_t$ that is a function of current input $x_t$ and all the history, as passed through the previous hidden state $h_{t-1}$.
As about shape of the hidden state, this is a matrix algebra, so the shape will depend on the shape of the inputs and weights. If you use some pre-build software, like Keras (https://keras.io/layers/recurrent/), then this is controlled by the parameters of LSTM cell (number of hidden units). If you code it by hand, this will depend on the shape of the weights.
","**Summary:**

A recurrent neural network (RNN) cell processes sequential data by updating its hidden state ($h_t$) based on previous hidden state ($h_{t-1}$) and current input ($x_t$). The updated hidden state represents the cell's memory and is used to make predictions or process further inputs.

RNNs can operate in two ways:

* **Whole sequence processing:** Process an entire sequence and use only the final hidden state for predictions.
* **Intermediate sequence processing:** Use intermediate hidden states for further processing or predictions.

The shape of the hidden state depends on the dimensions of the input and weight matrices. In pre-built packages like Keras, this is controlled by the number of hidden units in the cell."
How well should I expect Adam to work?,https://stats.stackexchange.com/questions/397852/how-well-should-i-expect-adam-to-work,machine-learning|neural-networks|optimization|adam,397852,True,397880,2767,2,9,1552756551,1552743424,5,1552754121,"Actually, one of ADAM's key features is that it is slower and thus more careful. See section 2.1 (https://arxiv.org/pdf/1412.6980.pdf%20%22%20entire%20document) of the paper.
In particular, there are pretty tight upper bounds on the step size. The paper lists 3 upper bounds, the simplest being that no individual parameter steps larger than $\alpha$ during any update, which is recommended to be 0.001. 
With stochastic gradients, especially those with the potential for very large variations sample to sample, this is a very important feature. Your model may currently have near optimal parameter values at some point during optimization, but by bad luck, it hits an outlier shortly before the algorithm terminates, leading to an enormous jump to a very suboptimal set of parameter values. By using an extremely small trust region, as ADAM does, you can greatly reduce the probability of this occurring, as you would need to hit a very large number of outliers in a row to move a far distance away from your current solution.
This trust region aspect is important in the cases when you have a potentially very noisy approximation of the gradient (especially if there are rare cases of extremely inaccurate approximations) and if the second derivative is potentially very unstable. If these conditions do not exist, then the trust region aspect of ADAM are most likely to greatly slow down convergence without much benefit. 
","ADAM, an optimization algorithm, stands out for its cautious approach that prioritizes accuracy over speed. Unlike other algorithms, ADAM imposes stringent limits on the size of parameter adjustments during each update. This conservative approach minimizes the risk of drastic parameter deviations due to occasional extreme gradient approximations.

When gradients exhibit significant sample-to-sample variations, and when the second derivative is unstable, ADAM's trust region aspect becomes crucial. By limiting the step size, ADAM reduces the likelihood of encountering a sequence of highly inaccurate gradients that could lead to significant parameter shifts.

However, in scenarios where gradient approximations are relatively stable, ADAM's cautious approach may slow down convergence without providing substantial benefits. Users should consider the specific characteristics of their optimization problem to determine whether ADAM's trust region aspect is advantageous."
Drop-out as a multiplicative noise in deep neural networks,https://stats.stackexchange.com/questions/364820/drop-out-as-a-multiplicative-noise-in-deep-neural-networks,neural-networks|deep-learning|dropout,364820,True,365055,1634,1,9,1627238684,1535710852,11,1535884566,"The correct answer is actually the opposite of the answer by DuttaA. Dropout works so well exactly because it doesn't allow the ReLU to make the noise insignificant by just making its output larger. Let's see why. To make things simpler, I'll use the same toy architecture used in the book, which is a 2-2-1 fully connected NN (figure 7.7 of the book):
 (https://i.sstatic.net/LAWmv.png)
and I'll follow the same notation, thus $h_i$ is the activation of unit $i$, and $\boldsymbol{\mu}=(\mu_1,\mu_2,\mu_3,\mu_4)$ is the vector of dropout masks, i.e., it's a vector of binary values $[0,1]$. It's a 4-element vector because dropout is only applied to input or hidden units, but not to output units.
Now, how do we add noise to the neural vector using dropout? The noiseless output would be:
$$ y = w_{21}h_1(w_{11}x_1+b_1)+w_{21}h_2(w_{12}x_2+b_2)+b_y $$
Let's consider additive noise: additive noise is usually added only to the input nodes. As a matter of fact, Ian explains in the paragraph just before the one you quoted that one of the advantages of dropout over traditional noise injection techniques is that the traditional methods only add noise at the input layer level, while dropout multiplies noise also at the hidden layers level. However, to simplify the treatment, I'll consider additive noise and dropout only at the hidden layer level (if you look around on GitHub, you'll find plenty of Keras models which don't use dropout for the input layer). Since the input layer is untouched, I'll make the following substitutions:
$$\begin{align}
w_{11}x_1+b_1 &= a_1  \\
w_{12}x_2+b_2 &= a_2 
\end{align}$$
Thus we have:
Additive noise
$$ y = w_{21}(h_1(a_1)+\epsilon_1)+w_{21}(h_2(a_2)+\epsilon_2)+b_y = \mathbf{w}_2(\mathbf{h}+\boldsymbol{\epsilon})+b_y$$
If $\mathbf{h}$ is made of unbounded activation functions (e.g., ReLU), the neural network can get rid of $\boldsymbol{\epsilon}$ just by making $\mathbf{h}$ larger and larger. In this simple example, this could be achieved simply by making $\mathbf{w}_1$ and/or $\mathbf{b}$ huge. This is a pathological solution to the noise problem! We would like the hidden units to learn to perform well, whether other hidden units are in the model or not, i.e., not to rely too much on the presence of all the other hidden units (this is one of the possible interpretations of dropout). Instead, we only told the neural network to use both the hidden units, make their outputs huge and then of course reduce it so that loss is small (i.e., that the predicted $\hat{y}_j$ are close to the correct $y_j$ on the training set), by making $\mathbf{w}_2$ small. This is a classic ""pathological  solution"" found by an optimizer - similar to when we use an optimizer for zero finding (i.e., we look for solutions which make the objective function, or loss, 0), and it finds it by making two additive terms huge, one going to $-\infty$ and the other going to $+\infty$.
Let's see how this is different with multiplicative noise.
Multiplicative noise (dropout)
$$ y = w_{21}(h_1(a_1)\mu_1)+w_{21}(h_2(a_2)\mu_2)+b_y = \mathbf{w}_2(\mathbf{h}\odot \boldsymbol{\mu})+b_y$$
Now, since $\mu_1$ and $\mu_2$ are independent and identically distributed Bernoulli random variables (i.e., their value is either 0 or 1 with a certain probability $p_{drop}$), the NN can't just learn to make $\mathbf{h}$ huge in order to get rid of the noise. If $\mu_1$ is randomly 0, then whatever the (finite) value of $h_1$, its contribution to $y$ during those training steps is exactly 0. If the NN wants to find anyway a solution which yields a decent value of $y$, even in the presence of this pesky multiplicative noise, it will have to learn the fact that it cannot always rely on $h_1$ and $h_2$ both being in the network 1.
PS this part of the book is great! I've never seen dropout explained so well, and the similarities with bagging (as well as the differences with boosting) so clearly underlined.

1of course, dropout would never be applied to such a small network, unless the function to be learned was extremely simple and the training set tiny. With just 2 hidden units, if we drop each units randomly and independently of the other, the neural network has very little capacity to learn anything. Just think that if $p_{drop}=0.5$ (a common choice for hidden units), then in 1 out of every 4 training steps both hidden units will be 0, and during that step the error won't backpropagate at all.
","Dropout is a technique that prevents neural networks from overfitting. Unlike traditional noise injection methods, which only add noise at the input layer, dropout multiplies noise at hidden layers as well.

When additive noise is introduced, neural networks with unbounded activation functions (such as ReLU) can easily overcome the noise by increasing the magnitude of the activations. This is a pathological solution because the network relies excessively on all hidden units being present.

Dropout addresses this issue by introducing multiplicative noise. Since the dropout masks are randomly generated, the network cannot simply rely on large activations to eliminate the noise. Instead, it must learn to make accurate predictions even when certain hidden units are ""dropped out.""

This forces the network to generalize better and prevents it from relying on specific combinations of hidden units. As a result, dropout improves the network's performance and robustness."
Why does Deep Q-Learning have &quot;do nothing&quot; actions?,https://stats.stackexchange.com/questions/351035/why-does-deep-q-learning-have-do-nothing-actions,deep-learning|reinforcement-learning,351035,True,351121,2322,1,9,1528877973,1528826929,11,1528877973,"At the start of every game, a random number of no-op actions are played (with the maximum number of such actions controlled by that parameter) to introduce variety in the initial game states. 
If an agent starts from exactly the same initial state every time it plays the same game, they're afraid that the Reinforcement Learning will simply learn to memorize a good sequence of actions from that initial state, rather than learning to observe the current state and select a good action based on that observation (which is the more interesting thing we're interested in learning). The idea is that by introducing randomness in the state we ""start playing from"", it should become impossible / more difficult for the agent to ""cheat"" and simply memorize a complete sequence of actions from a single specific initial state.
Note that, in 2017, it has been argued in the Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents (https://arxiv.org/abs/1709.06009) paper that these kinds of sequences no-op actions are not as effective at the goal described above as we would like, and an alternative solution is proposed which consists of introducing stochasticity throughout the entire game through ""sticky actions"" (basically sometimes randomly continue with the most recently-selected action rather than a new action selected by the agent).
","To prevent Reinforcement Learning (RL) agents from memorizing specific action sequences, many games introduce randomness at the start via ""no-op"" actions. By varying the initial state, agents are forced to observe current states and select appropriate actions.

However, a study suggests that this method may not be as effective as desired. An alternative approach proposed is to introduce stochasticity throughout the game using ""sticky actions,"" where agents may sometimes continue with the previous action instead of choosing a new one. This approach aims to make it more challenging for agents to rely on memorized sequences and encourages them to adapt to changing game states."
Is manually tuning learning rate during training redundant with optimization methods like Adam?,https://stats.stackexchange.com/questions/286723/is-manually-tuning-learning-rate-during-training-redundant-with-optimization-met,machine-learning|neural-networks|optimization|adam,286723,True,289271,4270,1,9,1499407128,1498136741,11,1499407128,"Yes, it is good practice to tune the learning rate even with Adam.
Most variants of SGD which claim to be ""adaptive"", including optimizers like Adagrad and Adam, adjust the relative learning rates of parameters. 
The update rules for many of these adaptive SGD variants involves an update very similar to the following:
$$\Delta w = \frac{1}{r} \frac{\partial{f}}{\partial w}$$
where $r$ usually is some sort of accumulating average of $\left| \frac{\partial{f}}{\partial w} \right|$ over time. (This is a very rough sketch of how it works!)
Since $r$ is different for every parameter, this means that even if one parameter has a really large gradient and another has a very small gradient, they are updated at the same rate. 
However, if the gradient eventually becomes small, then $1/r$ will become large, which means the $\Delta w$ will stay roughly the same in size. Therefore, it is still necessary to lower the learning rate in order to achieve good results. 
To summarize, the type of adaptivity provided by algorithms like Adam deal with adjusting the relative learning rates of different parameters, not decreasing the learning rate over time, so it's a different type of adaptive.
There are however other algorithms, such as YellowFin, which claim to not need tuning of any parameters at all. https://arxiv.org/abs/1706.03471 (https://arxiv.org/abs/1706.03471)
","**Summary:**

Adaptive optimizers like Adam adjust the relative learning rates of different parameters but don't decrease the overall learning rate over time. This means that while Adam can optimize parameters differently, it's still necessary to tune the learning rate for optimal performance.

The adaptivity provided by algorithms like Adam differs from the need to decrease the learning rate over time. Adjusting relative learning rates optimizes how different parameters are updated, while decreasing the overall learning rate ensures convergence and prevents overfitting.

It's important to note that some algorithms, such as YellowFin, claim to eliminate the need for tuning any parameters. However, the majority of adaptive optimizers require manual tuning of the learning rate for optimal results."
Deep Learning: Use L2 and Dropout Regularization Simultaneously?,https://stats.stackexchange.com/questions/241001/deep-learning-use-l2-and-dropout-regularization-simultaneously,neural-networks|regularization|ridge-regression|dropout,241001,True,241026,5933,1,9,1518794302,1476833604,11,1476841839,"The paper {1} that introduced dropout combined dropout with L2:

We found that dropout combined with max-norm regularization gives the
  lowest generalization error.

 (https://i.sstatic.net/j2F6j.png)


{1} Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. ""Dropout: a simple way to prevent neural networks from overfitting."" Journal of Machine Learning Research 15, no. 1 (2014): 1929-1958. http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf (http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)

","Researchers discovered that combining dropout, a regularization technique that randomly deactivates neurons during training, with L2 regularization (a technique that penalizes large weights) leads to reduced generalization error, indicating improved performance on unseen data. This combination was found to be more effective than using dropout or L2 regularization alone.

By reducing overfitting, the combination of dropout and L2 regularization enhances the ability of neural networks to generalize well to new data. This is particularly beneficial in complex machine learning tasks where overfitting can be a significant issue."
What does mAP mean?,https://stats.stackexchange.com/questions/225897/what-does-map-mean,neural-networks|terminology,225897,True,225919,13183,1,9,1540484037,1469625305,11,1469632963,"mAP probably stands for mean Average Precision. The AP provides a measure of quality across all recall levels for single class classification, it can be seen as the area under the precision-recall curve. Then the mAP is the mean of APs in multi-class classification.
","**Summary:**

Mean Average Precision (mAP) is a measure used to evaluate multi-class classification models. It is derived from Average Precision (AP), which quantifies the quality of a model's predictions across all recall levels.

AP is calculated as the area under the precision-recall curve. Precision measures the proportion of correctly predicted positive instances, while recall measures the proportion of actual positive instances that were correctly predicted.

mAP is the average of the AP values across all classes in a multi-class classification task. It provides a comprehensive measure of the model's performance across different classes and recall levels.

In essence, mAP assesses the model's ability to identify true positive instances while minimizing false positives and false negatives. A higher mAP indicates a better overall performance of the classification model."
The bottleneck of applying deep learning in practice,https://stats.stackexchange.com/questions/184952/the-bottleneck-of-applying-deep-learning-in-practice,machine-learning|deep-learning|deep-belief-networks,184952,True,185155,1136,2,9,1451409803,1449205961,9,1449314007,"True, some details used for improving performance are considered as tricks and you won't always know if these tricks yield the same improvement for your data and your network.
Some things that you will definitely need:

Data, lots of it
GPUs will let you run experiments faster and try out more things in a shorter time span.
Learning curve analysis. In the end it comes down to performance on the test set, but looking at the both train and test metrics you can identify reasons for bad performance. Strong bias? Overfitting from too many hidden nodes?
The activation function. I don't think it counts as a trick to know which kind of activation function you need. ReLU have a critical charactersitic in that they do not saturate like sigmoids and tanh. A neuron with ReLU will longer have probability-like output, but you don't need this for neurons in mid-level layers anyway. The advantag you get is mitigating the vanishing or exploding of gradients and speed up convergence.
Regularization. Might apply as tricks, but if you're using any of the mainstream deep learning libraries you can get off-the-shelf implementations for regularization via dropout.
Data augmentation. You're basically expanding your dataset synthetically without the added cost of manual annotation. The key is to augment the data with transformations that actuall make sense. So that the network gets to see variants of the data it may encounter in the test phase or when it gets deployed into the product. For visual data it horizontal flipping is trivial and adds a lot of gain. Jitter is probably dependent on the type of data and how noisy it is.
Diving into hyperparameter exploration can be frustrating. Start off with small networks and simple training procedures. Smaller networks are faster to train. Add more layers when you see signs of overfitting.
Good initialization. Random intitialization are appropriate for gauging the network's ability to converge but will not necessarily lead t optimal performance. At the same time, just keeping on iterating might lead to the network overfitting to the training data. If possible use a pre-trained network that has already learned a representation and fine tune it to your dataset. Unsupervised pre-training is another way to go and can allow the supervised training procedure to start from a far more promising position in weight space.
Scrutinize tricks. Understand what the trick really does. A paper describing a small detail that was used in improving the performance of a network will focus on that new aspect. The paper could be part of a sequence of projects that the authors have been working on. The context of the trick may not always be clear right away but for the authors it's not a trick but a technique that solves a problem they had. Sometimes a technique comes out and is treated as a trick and later someone will analyze its impact and describe its function. For example that this trick is equivalent to L2 regularization which more people are familiar with. We can the decide if we should try out this new technique or stick with the L2 regularization that we already know about. A lot of these tricks try to solve problems in deep learning, like risk of overfitting, costly computations, over parameterization and highly redundant weights. It's worth taking the time to understand what these tricks really do. By understanding the problem they try to solve we can judge the applicability of different tricks and pick the one that works well with constraints we may have (e.g. little computing power, small dataset)

","**Summary:**

Deep learning performance optimization involves techniques that may be regarded as ""tricks."" Some key aspects for improvement include:

**Essential Elements:**

* Ample data
* GPUs for faster experimentation

**Performance Analysis:**

* Monitor training and test metrics to identify issues (e.g., bias, overfitting)

**Activation Functions:**

* ReLU (Rectified Linear Unit) is advantageous for preventing gradient vanishing/exploding and speeding up convergence

**Regularization:**

* Dropouts, available in most deep learning libraries, reduce overfitting

**Data Augmentation:**

* Expanding datasets synthetically with transformations that enhance network performance (e.g., flipping, jitter)

**Hyperparameter Exploration:**

* Start with small networks and gradually increase layers
* Use pre-trained networks or unsupervised pre-training for improved initialization

**Scrutinizing Tricks:**

* Understand the purpose and impact of optimization techniques
* Weigh the applicability of tricks based on problem-solving capabilities and resource constraints"
How to extract the function being approximated by a neural network?,https://stats.stackexchange.com/questions/145897/how-to-extract-the-function-being-approximated-by-a-neural-network,neural-networks|approximation,145897,True,145910,3263,2,9,1428839433,1428777771,5,1428780566,"Let's denote $f$ the true underlying function and $\hat f$ the function that your machine learning algorithm converges too ($\hat f$ belongs to a family of parametrized functions $F$).
For simplicity, let's also assume that $f$ can be expressed analytically and that $f$ is deterministic.

My question is, are there ways to actually extract this explicit
  function f? Both in practice and in theory?

I assume that by ""practice"", you mean with machine learning (using experimental data) and by ""theory"", you mean modelling mathematically without machine learning (without data).
In practice, if you have enough data and if $F$ contains $f$, then it should be possible to obtain $\hat f$ = $f$ with an appropriate machine learning methodology. 
Theoretically, you may try to model $f$ with physical laws (or other modelling laws). For example is $f(p,s)$ models the time it takes for an object of shape s and weight p to fall from the top of the Eiffel tower, you can use classical mechanics (assuming they are true in the scope/scale of $f$) to model $f$.
For apple and oranges, $f$ is subjective to a particular person (given an ambiguous picture, two persons may disagree). So let's consider your $f$. $f$ is then defined by your brain! So if we assume that there exists an analytical expression of $f$, here are the two ways to find it:

""in practice"" (with machine learning): choose F sufficiently large to model the brain (the brain has more than 80 billions of neurons...), build a big enough dataset and choose a good machine learning algorithm. Ideally the dataset should contain all the possible images of oranges and apples. Then train until you get a null training error AND a null generalisation error.
""in theory"" (modelling): model the network of biological neurons of your brain. The problem is that we do not yet understand how the brain works.

To recap, you can usually find $f$ but it is really hard in both cases:

in practice: you need a good F, enough data and a good enough ML algorithm.
in theory: you need to know all the ""physical laws"" and be sure they are correct in the scope of the function $f$.

","**Summary:**

The goal is to extract the true underlying function, $f$, using a machine learning algorithm that converges to $\hat f$. In practice (using machine learning), it is possible to obtain $\hat f = f$ if $F$ (the family of parametrized functions) contains $f$, provided sufficient data is available. Theoretically (without machine learning), one can model $f$ using physical or logical laws.

However, extracting $f$ explicitly is challenging in both approaches. In practice, it requires a large $F$, sufficient data, and an effective machine learning algorithm. Theoretically, it requires a complete understanding of the underlying laws governing $f$, which is difficult for complex functions like brain activity.

Therefore, while it is generally possible to find $f$, it is a challenging undertaking that requires careful considerations of data availability, model selection, and theoretical understanding."
Cross entropy vs KL divergence: What&#39;s minimized directly in practice?,https://stats.stackexchange.com/questions/476170/cross-entropy-vs-kl-divergence-whats-minimized-directly-in-practice,neural-networks|maximum-likelihood|kullback-leibler|cross-entropy|risk,476170,True,477120,4705,1,9,1594749860,1594224583,10,1594737834,"Let $q$ be the density of your true data-generating process and $f_\theta$ be your model-density.
Then $$KL(q||f_\theta) = \int q(x) log\left(\frac{q(x)}{f_\theta(x)}\right)dx = -\int q(x) \log(f_\theta(x))dx + \int q(x) \log(q(x)) dx$$
The first term is the Cross Entropy $H(q, f_\theta)$ and the second term is the (differential) entropy $H(q)$. Note that the second term does NOT depend on $\theta$ and therefore you cannot influence it anyway. Therfore minimizing either Cross-Entropy or KL-divergence is equivalent.
Without looking at the formula you can understand it the following informal way (if you assume a discrete distribution). The entropy $H(q)$ encodes how many bits you need if you encode the signal that comes from the distribution $q$ in an optimal way. The Cross-Entropy  $H(q, f_\theta)$ encodes how many bits on average you would need when you encoded the singal that comes from a distribution $q$ using the optimal coding scheme for $f_\theta$. This decomposes into the Entropy $H(q)$ +  $KL(q||f_\theta)$. The KL-divergence therefore measures how many additional bits you need if you use an optimal coding scheme for distribution $f_\theta$ (i.e. you assume your data comes from $f_\theta$ while it is actually generated from $q$). This also explains why it has to be positive. You cannot be better than the optimal coding scheme that yields the average bit-length $H(q)$.
This illustrates in an informal way why minimizing KL-divergence is equivalent to minimizing CE: By minimzing how many more bits you need than the optimal coding scheme (on average) you of course also minimize the total amount of bits you need (on average)
The following post illustrates the idea with the optimal coding scheme: Qualitively what is Cross Entropy (https://stats.stackexchange.com/questions/80967/qualitively-what-is-cross-entropy)
","**Summary:**

The Kullback-Leibler (KL) divergence measures the difference between two probability distributions, $q$ (true data) and $f_\theta$ (model). It can be expressed as the Cross Entropy (CE) minus the Entropy $H(q)$.

CE measures the average number of additional bits needed when encoding data from $q$ using the optimal coding scheme for $f_\theta$. Minimizing CE minimizes the number of bits required for encoding.

KL divergence equals CE, as it incorporates the entropy term that does not depend on the model. Thus, minimizing KL divergence or CE is equivalent.

Intuitively, KL divergence measures the difference between the optimal coding schemes for $q$ and $f_\theta$. A positive KL divergence indicates that using the coding scheme for $f_\theta$ requires more bits than the optimal scheme for $q$.

By minimizing KL divergence, models can better approximate the true data-generating process, resulting in more efficient encoding and improved model performance."
Is there a universal approximation theorem for monotone functions?,https://stats.stackexchange.com/questions/376275/is-there-a-universal-approximation-theorem-for-monotone-functions,machine-learning|neural-networks|approximation,376275,True,419306,772,1,9,1564139811,1541813786,10,1564139517,"I am not sure if a single hidden layer is sufficient, but it can be shown that if your input is in $\mathbb{R}^k$, you will need at most $k$ hidden layers. See Theorem 3.1 in https://ieeexplore.ieee.org/document/5443743 (https://ieeexplore.ieee.org/document/5443743)

Theorem 3.1: For any continuous monotone nondecreasing 
  function $f : K \rightarrow \mathbb{R}$, 
  where $K$ is a compact subset of $\mathbb{R}^k$, there exists a feedforward neural network with at most $k$ hidden layers, positive weights, and output $O$ such that $|f(\mathbf{x}) - O_{\mathbf{x}}| < \varepsilon$, for any $\mathbf{x} \in K$ and $\varepsilon > 0$.

","**Summary:**

The mentioned theorem (3.1) from the paper demonstrates that a feedforward neural network with a maximum of **k hidden layers** can approximate any continuous, monotone nondecreasing function defined on a compact subset of $\mathbb{R}^k$. This result mathematically proves that **k hidden layers are sufficient** for this approximation, where k represents the number of input dimensions.

The theorem specifically states that for any function f satisfying these conditions, there exists a neural network with positive weights that outputs O, such that for any input vector x and a given error threshold Îµ, the absolute difference between f(x) and O(x) is **smaller than Îµ**.

This result provides a theoretical guarantee on the ability of neural networks with a limited number of hidden layers to approximate complex functions. It implies that, for a given input dimension k, a neural network with at most k hidden layers can be constructed to achieve a desired accuracy in approximating the function."
Which deep learning model can classify categories which are not mutually exclusive,https://stats.stackexchange.com/questions/272862/which-deep-learning-model-can-classify-categories-which-are-not-mutually-exclusi,machine-learning|deep-learning|natural-language|tensorflow,272862,True,273024,4578,1,9,1585371197,1491808032,10,1491876651,"You can achieve this multi-label classification by replacing the softmax with a sigmoid activation and using binary crossentropy instead of categorical crossentropy as the loss function. Then you just need one network with as many output units/neurons as you have labels.
You need to change the loss to binary crossentropy as the categorical cross entropy only gets the loss from the prediction for the positive targets. To understand this, look at the formula for the categorical crossentropy loss for one example $i$ (class indices are $j$):
$ L_i = - \sum_j{t_{i,j} \log(p_{i,j})}$
In the normal multiclass setting, you use a softmax, so that the prediction for the correct class is directly dependent on the predictions for the other classes. If you replace the softmax by sigmoid this is no longer true, so negative examples (where $t_{i,j}=0$) are no longer used in the training!
That's why you need to change to binary crossentropy, which uses both positive and negative examples:
$L_i=-\sum_j{t_{i,j} \log(p_{i,j})} -\sum_j{(1 - t_{i,j}) \log(1 - p_{i,j})} $
","**Summary:**

Multi-label classification involves assigning multiple labels to a single data point. This can be achieved by replacing the softmax activation function in a neural network with a sigmoid activation and using binary crossentropy instead of categorical crossentropy as the loss function.

The softmax activation, commonly used in multiclass classification, depends on the predictions for all classes. However, sigmoid activation doesn't have this dependence. As a result, negative examples (where no label applies) are ignored during training.

Binary crossentropy addresses this issue by explicitly considering both positive and negative examples in the loss calculation. It ensures that errors from both correct and incorrect predictions contribute to the training process. By incorporating negative examples, the network learns to better distinguish between relevant and irrelevant labels for each data point."
Is Greedy Layer-Wise Training of Deep Networks necessary for successfully training or is stochastic gradient descent enough?,https://stats.stackexchange.com/questions/232616/is-greedy-layer-wise-training-of-deep-networks-necessary-for-successfully-traini,deep-learning|autoencoders|deep-belief-networks|pre-training,232616,True,232708,5477,1,9,1472666071,1472631765,10,1472661839,"Pre-training is no longer necessary (https://www.reddit.com/r/MachineLearning/comments/22u1yt/is_deep_learning_basically_just_neural_networks/cgqgy9w). Its purpose was to find a good initialization for the network weights in order to facilitate convergence when a high number of layers were employed. Nowadays, we have ReLU (http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a/glorot11a.pdf), dropout (http://www.cs.toronto.edu/~fritz/absps/dropout.pdf) and batch normalization (https://arxiv.org/abs/1502.03167), all of which contribute to solve the problem of training deep neural networks. Quoting from the above linked reddit post (by the Galaxy Zoo Kaggle challenge winner):

I would say that the âpre-training eraâ, which started around 2006, ended in the early â10s when people started using rectified linear units (ReLUs), and later dropout, and discovered that pre-training was no longer beneficial for this type of networks.

From the ReLU paper (linked above):

deep rectifier networks
  can reach their best performance without
  requiring any unsupervised pre-training

With that said, it is no longer necessary, but still may improve performance in some cases where there are too many unsupervised (unlabeled) samples, as seen in this paper (http://arxiv.org/pdf/1412.6597.pdf).
","**Summary:**

Pre-training, once a method for initializing neural network weights, is no longer essential due to advances in techniques like ReLU (Rectified Linear Units), dropout, and batch normalization. These techniques enhance the training process of deep neural networks without the need for pre-training.

ReLU eliminates the need for pre-training by enabling deep networks to reach optimal performance without unsupervised data. Dropout further improves training by randomly omitting units during each iteration, while batch normalization stabilizes the training process.

While pre-training is generally not required, it might still provide benefits in cases with ample unsupervised data, as demonstrated in specific research."
Neural networks with complex weights,https://stats.stackexchange.com/questions/192187/neural-networks-with-complex-weights,machine-learning|neural-networks|gradient-descent,192187,True,192503,4587,2,9,1487079790,1453615037,5,1453799892,"Gradient Descent will work in your case. You can use Theano. It supports differentiation with respect to complex variables. You can check this link (http://www.deeplearning.net/software/theano/proposals/complex_gradient.html) if you need more information. One important note is that your error function needs to return a scalar.
I'm not quit sure that other algorithms like Conjugate gradient or quasi-Newton will work fine for complex numbers. If you want to implement other learning algorithms, you will need to verify their proves to make sure that they are possible as well.
","**Summary:**

Gradient Descent is a suitable algorithm for optimization with complex variables. Theano, a machine learning library, supports differentiation of complex functions. Other algorithms like Conjugate Gradient and Quasi-Newton may be incompatible with complex numbers. Users should verify algorithm proofs for compatibility before implementation. The error function must return a scalar value."
"Do inputs to a Neural Network need to be in [-1,1]? ",https://stats.stackexchange.com/questions/31641/do-inputs-to-a-neural-network-need-to-be-in-1-1,neural-networks,31641,True,31644,18003,1,9,1341439005,1341425299,10,1341429735,"You can normalize the values so that you use, for example,  
$$\frac{AP - AP_0}{AP_1-AP_0}$$
where $AP$ is the current air pressure, $AP_0$ is the air pressure value you want sent to $0$, and $AP_1$ is the air pressure value you want sent to $1$. 
It is ok if your inputs occasionally go a bit outside $[-1,1]$.
It is dangerous if an input is usually small, but has some occasional extreme values. Then it might be better to split the input into more than one input value, or to remove the outliers and accept that the neural network has a restricted context of applicability. Rescaling so that the outliers are between $-1$ and $1$ won't fix the problem.
","To normalize data, you can use the formula:
$$(AP - AP_0) / (AP_1 - AP_0)$$

where:
* $AP$ is the current value
* $AP_0$ is the value you want to set to 0
* $AP_1$ is the value you want to set to 1

Usually, the inputs should stay within the range of -1 to 1. However, it is not a problem if the inputs occasionally exceed this range.

The issue arises when an input is typically low but has occasional extreme values. In this case, it's better to split the input into multiple values or remove the outliers. Rescaling the outliers to fit the range of -1 to 1 will not solve the problem."
"What is a good approach to split 3 years of hourly data in a train, validation and test set for an electricity price forecasting neural network?",https://stats.stackexchange.com/questions/645719/what-is-a-good-approach-to-split-3-years-of-hourly-data-in-a-train-validation-a,machine-learning|time-series|neural-networks,645719,True,645723,450,1,9,1713961763,1713950804,9,1713951295,"The seasonality would be different between the validation and the test set. I would not call it a mismatch.
Indeed, this is a very common setup in time series forecasting, and yes, any results need to be treated with caution because you would be evaluating not over a full (yearly) seasonal cycle. But with the amount of data you have, there is quite simply little else you can do. More data is always helpful, and not always available.
One other way to evaluate time series forecasts is rolling origin evaluation (also called time series cross validation (https://otexts.com/fpp3/tscv.html)), but that quite simply won't address the issue present here.
You could complement your analysis by a ""backward evaluation"": turn your time series around in time and fit & forecast backward. Then if your original evaluation used the last six months of data (July to December), then this ""backward evaluation"" would use the first six months (January to June). I would not necessary trust this more than the original evaluation, but it might be enlightening.
Take a specific model (e.g., ETS(A,N,N)), fit to 1/2021 to 12/2022, test on 1-6/2023, refit and validate on 7-12/2023. Then take the same model and let time run backwards, fitting to 12/2023 to 1/2022, test on 12-7/2021, refit and validate on 6-1/2021. Now you have two evaluations of your model, one on July to December and one on January to June. Of course the two evaluations will be on same model fitted with different parameters - but it tells you something about the model form as such, and in production, you would refit it anyway.
","**Summary:**

Forecasting time series data faces the challenge of seasonality differences between validation and test sets. While this is a common setup, caution is advised as results may not represent a full seasonal cycle. To address this, consider the following:

* **Rolling origin evaluation:** Evaluates a model by iteratively repeating the validation process with different starting points, but does not resolve seasonality differences.
* **Backward evaluation:** Flips the time series and performs evaluation in reverse order, providing an alternative perspective on the model's performance during different seasons.

For a specific model (e.g., ETS(A,N,N)), consider the following approach:

* Fit the model to data from January 2021 to December 2022 and test on January to June 2023.
* Refit and validate the model on July to December 2023.
* Flip time and fit the model to data from December 2023 to January 2022, test on December 2021 to July 2021, and refit/validate on June 2021 to January 2021.

This approach provides insights into the model's performance across different seasons and ensures that the parameters are recalibrated to fit the seasonal changes."
Why is it hard for a neural network to learn the identity function?,https://stats.stackexchange.com/questions/424558/why-is-it-hard-for-a-neural-network-to-learn-the-identity-function,machine-learning|neural-networks|keras,424558,True,424560,6038,1,9,1567371567,1567367918,9,1567369418,"For a single example, the network takes a 784-element vector as its input. So rephrasing the problem in OP's post, they wish to learn the function
$$
f(x) = Ix
$$
where $I$ is the $784\times 784$ identity matrix.
Perfect fit is impossible with this model
The 1-layer network probably has an easier time because instead of attempting to ""line up"" four weight matrices through four nonlinearities, it only has to line up one, i.e. it is easier to find an approximation in $W_1, b_1$ for
$$
Ix = g(W_1 x+b_1).
$$
But even the simple expression $Ix = g(W_1 x+b_1)$ should be an obvious warning that attempting to find a perfect fit is a fool's errand, because it is trying to approximate a linear function with a nonlinear function. In particular, because of how ReLUs are defined, any $x<0$ is set to 0, (https://stats.stackexchange.com/questions/362588/how-can-a-network-with-only-relu-nodes-output-negative-values) so this model will never achieve 0 error when any elements of $x$ are negative.
The UAT is an approximation theorem
Indeed, for any choice of nonlinear activation $g$, I can find an $x$ for which the error is positive. So then the interesting question becomes ""Can we fit a model so that the error is at most $\epsilon$ for $x$ in some interval $\mathcal{I}$?"" And this statement of the problem is more-or-less compatible with the caveats of the UAT. It also points us in a more profitable direction: instead of seeking 0 error, we wish to find minimal error when the inputs are in some interval.
In other words, theorems about neural networks don't guarantee that you can achieve 0 error, they guarantee that you can bound error for inputs in some interval (subject to some terms and conditions).
The UAT doesn't comment on whether it's easy to train any particular network.
Actually finding the weights and biases which achieve the minimum error is a very challenging problem. In particular, we don't have much reason to believe that the choice of initialization, optimizer, learning rate and number of epochs, etc. in this code snippet are best for this task.
This optimization problem is hard
A four-layer network with ReLU activations $g(x)=\max\{0, x\}$ is given by
$$
h(x)=g(W_4g(W_3g(W_2g(W_1x+b_1)+b_2)+b_3)+b_4).
$$
So what you seek in your question is solutions $W_i, b_i$ such that
$$
Ix = g(W_4g(W_3g(W_2g(W_1x+b_1)+b_2)+b_3)+b_4)
$$
for all $x$, where $W_i, b_i$ are have appropriate shape.
This doesn't look particularly friendly to try and solve. Indeed, in light of my remarks about the UAT, we will have to restate this to bound the error and focus on an interval of inputs.
Even if we restate the problem in this way, it is still challenging from the perspective of gradient descent because of the dying ReLU phenomenon (https://stats.stackexchange.com/questions/237400/the-reason-that-the-larger-gradient-flowing-through-an-relu-neuron-can-cause-it), the weaknesses of gradient descent (https://stats.stackexchange.com/questions/274336/gradient-descent-doesnt-find-solution-to-ordinary-least-squares-on-this-dataset/274362#274362), and the poor conditioning of the optimization task (https://stats.stackexchange.com/questions/420231/effect-of-rescaling-of-inputs-on-loss-for-a-simple-neural-network/420330#420330) due to the scale of the inputs.
Tuning a neural network is the greater part of using neural networks.
If you don't want to spend a lot of time changing hyper-paremters, then you should use a different model. (http://stats.stackexchange.com/questions/352036/%E2%80%A6)
","**Summary:**

**Approximation with Neural Networks**

Neural networks aim to approximate functions, but perfect fits are often impossible. Even simple models face challenges due to the limitations of non-linear activation functions.

**Universality of Approximation Theorem (UAT)**

The UAT states that neural networks can approximate any function within an error bound for a given input interval. However, achieving this approximation can be challenging.

**Training Neural Networks**

Optimizing neural network weights and biases is difficult due to the complexity of the network structure and optimization algorithms. Factors such as ReLU activations, gradient descent weaknesses, and poor conditioning make tuning hyperparameters crucial for successful training.

**Practical Considerations**

Finding optimal network parameters requires significant effort and tuning. If avoiding extensive hyperparameter optimization is desired, alternative models may be more suitable."
How can we interpret a neural network with sgd from a Bayesian perspective?,https://stats.stackexchange.com/questions/352452/how-can-we-interpret-a-neural-network-with-sgd-from-a-bayesian-perspective,bayesian|neural-networks,352452,True,352547,1180,3,9,1529607262,1529566345,7,1529596780,"I think you would be interested in ""Stochastic Gradient Descent as Approximate Bayesian Inference (https://arxiv.org/pdf/1704.04289.pdf)"" by Stephan Mandt, Matthew D. Hoffman, David M. Blei. 

Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.

Some additional relevant literature:

Radford M. Neal was active in Bayesian inference and neural networks in the ""first wave"" of NN research in the 1990s. He published several articles and the book Bayesian Learning for Neural Networks.
Sergios Theodoridis' Machine Learning: A Bayesian and Optimization Perspective is a truly massive tome (about 1000 pages) and includes a chapter on neural networks and deep learning.
This paper crossed my desk recently: ""Bayesian Neural Networks (https://arxiv.org/abs/1801.07710)"" by Vikram Mullachery, Aniruddh Khera, Amir Husain 

This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems


","Stochastic Gradient Descent (SGD), commonly used in neural network training, can be interpreted as an approximate Bayesian inference algorithm. By understanding SGD as simulating a Markov chain, researchers can derive novel results:

1. Constant SGD can be used to approximate posterior inference by adjusting its parameters to minimize the divergence between its stationary distribution and the desired posterior.

2. Constant SGD leads to a variational EM algorithm for optimizing hyperparameters in complex probabilistic models.

3. SGD with momentum can be used for sampling and its damping coefficient can be adjusted accordingly.

4. The approximation errors of Langevin Dynamics and Stochastic Gradient Fisher Scoring, MCMC algorithms, can be quantified due to finite learning rates.

5. The stochastic process perspective allows for a simplified proof of the optimality of Polyak averaging. This leads to the proposal of the Averaged Stochastic Gradient Sampler, a scalable approximate MCMC algorithm.

The paper also discusses Bayesian Neural Networks (BNNs), a combination of probabilistic models and neural networks. BNNs offer probabilistic guarantees on predictions and provide insights into the distribution of learned parameters. The emergence of probabilistic programming libraries has contributed to the growing interest in BNNs."
Approximate the sine function with shallow neural network,https://stats.stackexchange.com/questions/320289/approximate-the-sine-function-with-shallow-neural-network,neural-networks,320289,True,320824,11843,1,9,1547421762,1514161773,9,1514577688,"So I solved my own problem, and the solution was to use a more advanced optimizer instead of vanilla gradient descent. By using the ""nn"" module from pytorch, you can select from a range of optimizers which incorporate concepts like ""momentum"", regularization, and learning rate decay to update the weights of the network in a way that is more likely to find a local minimum.
UPDATE: 
I have created an interactive tutorial on this problem for those interested. It is a Jupyter notebook containing the minimum code to get this problem running, and leaves room for the user to improve the fit of the model through experimentation with layers, optimizers, etc.: link (https://github.com/rlorigro/shallow_neural_network_example)
This (http://cs231n.github.io/neural-networks-3/#sgd) page has some explanation. More in their cs231n (free) online lectures.
Another (http://ruder.io/optimizing-gradient-descent/) explanation with nice animations.
Training with Adam optimizer, 1000 iterations, loss=L1Loss (Y=loss, X=iter):
 (https://i.sstatic.net/oBCpT.png)
Resulting model (orange=prediction, blue=training data):
 (https://i.sstatic.net/HV95i.png)
zoomed:
 (https://i.sstatic.net/lTcwH.png)
","To resolve a model optimization issue, an advanced optimizer was used instead of vanilla gradient descent. PyTorch's ""nn"" module provides access to optimizers like Momentum, regularization, and learning rate decay. These help the model find local minima more efficiently.

An interactive tutorial is available to demonstrate the problem and provide a platform for experimentation with optimizers, layers, and other model parameters.

External resources provide further explanations on the topic, including a CS231n online lecture and an explanation with animations.

An example using the Adam optimizer shows a reduction in loss over iterations. The resulting model exhibits a good fit to the training data."
What is the relation between belief networks and Bayesian networks?,https://stats.stackexchange.com/questions/231204/what-is-the-relation-between-belief-networks-and-bayesian-networks,machine-learning|neural-networks|bayesian-network|networks,231204,True,231231,3537,2,9,1587573230,1471921475,8,1471941636,"No, Bayesian network and deep belief network are not the same thing.

Bayesian network is a type of probabilistic graphical model (https://en.wikipedia.org/wiki/Graphical_model) where vertexes are random variables and edges are conditional dependencies. For large number of random variables, we use the graphical structure assumptions to decompose the joint distribution in a manageable level. In Bayesian network, there are two major tasks, learning and inference. The ultimate goal of learning is getting the joint distribution of the data, and the goal of inferences is trying to calculate the probability for a given events, assuming you already have the model
As mentioned in the link you provided:


""Neural networks"" is a term usually used to refer to feedforward neural networks. Deep Neural Networks are feedforward Neural Networks with many layers.

You can think about neural network is just a big non-linear function, that it can approximate complicated outcomes. Here the nodes are ""neurons"" and the edges are ""connections"", where they are essentially building blocks for a function.
","**Summary:**

Bayesian networks and deep belief networks (DBNs) are distinct types of probabilistic graphical models.

**Bayesian Networks:**

* Represent relationships between random variables as a graph.
* Used for learning joint distributions and performing inference tasks.
* Learning involves estimating the joint probability distribution.
* Inference involves calculating probabilities based on known values.

**Deep Belief Networks (DBNs):**

* A subtype of neural networks with multiple layers.
* Approximate complex functions by combining non-linear transformations.
* Nodes represent neurons, and edges represent connections."
How to understand the geometric intuition of the inner workings of neural networks?,https://stats.stackexchange.com/questions/196585/how-to-understand-the-geometric-intuition-of-the-inner-workings-of-neural-networ,machine-learning|neural-networks|approximation|intuition|function,196585,True,197032,2524,1,9,1455719301,1455491090,9,1455719301,"There are two great recent articles on some of the geometric properties of deep neural networks with piecewise linear nonlinearities (which would include the ReLU activation):

On the Number of Linear Regions of Deep Neural Networks (http://arxiv.org/abs/1402.1869) by Montufar, Pascanu, Cho and Bengio.
On the number of response regions of deep feed forward networks with piece-wise linear activations (http://arxiv.org/abs/1312.6098) by Pascanu, Montufar and Bengio.

They provide some badly needed theory and rigor when it comes to neural networks. 
Their analysis centers around the idea that:

deep networks are able to separate their input space into exponentially more linear response regions than their shallow counterparts, despite using the same number of computational units.

Thus we may interpret deep neural networks with piecewise linear activations as partitioning the input space into a bunch of regions, and over each region is some linear hypersurface. 
In the graphic you have referenced, notice that the various (x,y)-regions have linear hypersurfaces over them (seemingly either slanted planes or flat planes). So we see the hypothesis from the above two articles in action in your referenced graphics. 
Furthermore they state (emphasis from the co-authors):

deep networks are able to identify an exponential number of input neighborhoods by mapping them to a common output of some intermediary hidden layer. The computations carried out on the activations of this intermediary layer are replicated many times, once in each of the identified neighborhoods. This allows the networks to compute very complex looking functions even when they are defined with relatively few parameters.

Basically this is the mechanism that allows deep networks to have incredibly robust and diverse feature representations despite having a fewer number of parameters than their shallow counterparts. In particular, the deep neural networks can learn an exponential number of these linear regions. Take for example, Theorem 8 from the first referenced paper, which states:

Theorem 8: A maxout network with $L$ layers of width $n_0$ and rank $k$ can compute functions with at least $k^{L-1}k^{n_0}$ linear regions. 

This is again for deep neural networks with piecewise linear activations, like ReLUs for example. If you used sigmoid-like activations, you would have smoother sinusoidal looking hypersurfaces. A lot of researchers now use ReLUs or some variation of ReLUs (leaky ReLUs, PReLUs, ELUs, RReLUs, the list goes on) because their piecewise linear structure allows for better gradient backpropagation vs the sigmoidal-units which can saturate (have very flat/asymptotic regions) and effectively kill gradients. 
This exponentiality result is crucial, otherwise the piecewise linearity might not be able to efficiently represent the types of nonlinear functions we must learn when it comes to computer vision or other hard machine learning tasks. However, we do have this exponentiality result and therefore these deep networks can (in theory) learn all sorts of nonlinearities by approximating them with a huge number of linear regions.
As for your question about the hypersurface: you can absolutely setup a regression problem where your deep net tries to learn the $y = f(x_1, x_2)$ hypersurface. This is tantamount to just using a deep net to setup a regression problem, many deep learning packages can do this, no problem. 
If you want to just test your intuition, there's a lot of great deep learning packages available these days: Theano (Lasagne, No Learn and Keras built on top of it), TensorFlow, a bunch of others I'm sure I'm leaving out. These deep learning packages will compute the backpropagation for you. However, for a smaller scale problem like the one you mentioned it really is a good idea to code up the backpropagation yourself, just to do it once, and learn how to gradient check it. But like I said, if you just want to try it out and visualize it, you can get started pretty quickly with these deep learning packages.
If one is able to properly train the network (we use enough data points, initialize it properly, training goes well, this is its own whole other issue to be frank), then one way to visualize what our network has learned, in this case, a hypersurface, is to just graph our hypersurface over an xy-mesh or grid and visualize it. 
If the above intuition is correct, then using deep nets with ReLUs, our deep net will have learned an exponential number of regions, each region having its own linear hypersurface. Of course, the whole point is that because we have exponentially many, the linear approximations can become so fine and we do not perceive the jagged-ness of it all, given that we used a deep/large enough network. 
","Deep neural networks with piecewise linear nonlinearities (e.g., ReLU activation) possess remarkable geometric properties. These networkspartition the input space into an exponential number of linear regions, with different linear hypersurfaces over each region. This allows them to learn highly complex functions with relatively few parameters.

Theorem 8 states that maxout networks with piecewise linear activations can compute functions with at least $k^{L-1}k^{n_0}$ linear regions, where L is the number of layers, n0 is the width, and k is the rank.

Deep neural networks learn these linear regions by mapping exponentially many input neighborhoods to a common hidden layer output. This allows them to compute diverse feature representations with minimal parameters.

To visualize the hypersurface learned by a deep network, the network can be applied to a regression problem, and the resulting hypersurface can be plotted over an input space mesh. As the network learns more data, the linear regions become increasingly refined, resulting in a smooth hypersurface."
My ReLU network fails to launch,https://stats.stackexchange.com/questions/188040/my-relu-network-fails-to-launch,neural-networks|optimization|deep-learning|backpropagation,188040,True,366651,2425,2,9,1536787224,1450883182,6,1536787224,"
The problem is that output units will sometimes get pushed into a regime where they are ""dead"" - that is, not responsive to any input. Once they're dead, they're dead - all gradients from that unit become zero, and therefore they can not learn to be useful again.

This is a well-known problem with ReLU units. As work-arounds, some folks have designed alternative activation functions which are largely similar, but do not have a flat gradient. The Leaky ReLU function $L$ is probably the simplest
$$
L(x) = 
\begin{cases}
x &\text{if}~x > 0\\
\alpha x &\text{otherwise}
\end{cases}
$$
where $0 < \alpha< 1$ is some constant chosen by the user, usually something like $\alpha=0.2$. This always has positive gradient, so the weights can always update.

They can more easily be pushed into this regime when the learning rate is higher.

This thread addresses your observation about larger learning rates.

the reason that the larger gradient flowing through an ReLU neuron can cause it to die (https://stats.stackexchange.com/questions/237400/the-reason-that-the-larger-gradient-flowing-through-an-relu-neuron-can-cause-it?s=2|48.8374)

","**Summary:**

ReLU activation units can encounter a problem known as ""dying,"" where they become unresponsive to input. This occurs due to their flat gradient, causing all gradients from that unit to become zero and preventing them from learning.

As a workaround, alternative activation functions like the Leaky ReLU have been developed. Leaky ReLU introduces a non-zero gradient for negative inputs, ensuring that weights can always update.

Higher learning rates can increase the likelihood of neurons dying. Larger gradients flowing through an ReLU neuron can push it into a regime where it becomes unresponsive, leading to the inability to learn effectively."
Reframing a HMM problem as an RNN,https://stats.stackexchange.com/questions/588236/reframing-a-hmm-problem-as-an-rnn,machine-learning|neural-networks|recurrent-neural-network|hidden-markov-model,588236,True,588246,589,2,9,1662720558,1662696952,5,1662705875,"The hidden nodes (states) in an HMM are random variables, while in an RNN only the input nodes could be considered random variables, all the other nodes are just deterministic nonlinear functions.
Thus, it is difficult to formulate an HMM with an RNN. However, some attempts have been made to combine the ideas of dynamic Bayesian networks (DBNs), of which HMMs would be examples, and neural networks, e.g. the VRRN (https://arxiv.org/abs/1506.02216), alpha-nets (https://www.sciencedirect.com/science/article/abs/pii/016763939090049F), or GenHMM (https://arxiv.org/abs/1910.05744). How much those still resemble vanilla RNNs or DBNs is up for discussion.
","**Summary:**

Hidden Markov Models (HMMs) feature random variables representing hidden states, while Recurrent Neural Networks (RNNs) generally treat all nodes (except input nodes) as deterministic functions. This distinction makes it challenging to integrate HMMs and RNNs.

Despite the challenge, attempts have been made to combine them, resulting in models such as VRRN, alpha-nets, and GenHMM. These models explore the integration of dynamic Bayesian networks (DBNs) and neural networks. However, the extent to which they retain characteristics of both HMMs and RNNs remains a subject of discussion."
Question with Matrix Derivative: Why do I have to transpose?,https://stats.stackexchange.com/questions/234024/question-with-matrix-derivative-why-do-i-have-to-transpose,neural-networks|optimization|backpropagation|recurrent-neural-network|gradient,234024,True,234071,3975,1,9,1575211719,1473355142,8,1473373587,"As noted in the comments, it is best to write out the matrix equations and then apply the standard derivative rules. After a bit of experience with small cases, where you expand out all the terms, you can try just writing out the equations using summations and subscripts.
In terms of mathematical rules, for completeness I will note that there is a pretty good reference called The Matrix Cookbook (https://www.google.com/search?q=%22the%20matrix%20cookbook%22%20filetype:pdf)*, that documents a large number of rules for calculus on matrix equations. (*I am not sure if there is a stable home for this document, so I am linking a Google search, which has always reliably found (many) copies, in my experience!)
That said, I find it difficult to follow these usually, so I end up using summation/subscript forms when I need to compute these sorts of derivatives. (Or sometimes I will use index notation (https://en.wikipedia.org/wiki/Einstein_notation).)
EDIT: For your specific problem, I think you may be getting stuck on the large equations. Here I would suggest that intermediate variables can be your friend, helping to break down the structure (similar to functions in modular programming).
EDIT the second: When I first looked at your problem, I thought the lowercase $h$ variables were vectors. Now I realize they are matrices, which changes things. Given this, it does not make sense to me that the result would be a matrix, rather than a higher-order tensor. The order of a tensor is the number of indices. If you take a derivative of one tensor $U$ with respect to another tensor $V$, the order of the resulting tensor $W$ will be the sum of the two, i.e. 
$$W_{ij,pq}=\frac{\partial U_{ij}}{\partial V_{pq}}$$
For instance the gradient of a scalar function is a vector ($0+1=1$), and the Jacobian of a vector function is a matrix ($1+1=2$). For an example of a physically-meaningful $4_{th}$ order tensor, a simple example is the ""spring constant"" for the continuum-mechanics generalization of Hooke's Law (https://en.wikipedia.org/wiki/Hooke%27s_law#Thermodynamic_basis) (the last equation on that page is essentially the same as the one I give above).
When I work through your case, I find that the result is sparse, so is effectively $3_{rd}$ order, but it is not so sparse as to be $2_{nd}$ order $\ldots$ unless I made a mistake. (More precisely: I find $W_{ij,pq}=0$ for $p\neq i$, so we can effectively work with $\hat{W}_{ij,k}\equiv W_{ij,ik}$ as an order-3 tensor.)
EDIT the third: As the OP requested clarification, here is a simple example to demonstrate the issue. Consider the matrix equation
$$U=VA\implies U_{ij}=\sum_kV_{ik}A_{kj}\implies \frac{\partial U_{ij}}{\partial V_{pq}}=\sum_k\frac{\partial V_{ik}}{\partial V_{pq}}A_{kj}$$
To proceed, we introduce the Kronecker delta (https://en.wikipedia.org/wiki/Kronecker_delta) symbol (essentially the identity matrix):
$$\delta_{ij} =
    \begin{cases}
            1 &         i=j\\
            0 &         i\neq j
    \end{cases}$$
Then the derivative within the summation can be expressed as
$$\frac{\partial V_{ik}}{\partial V_{pq}}=\delta_{ip}\delta_{kq}$$
i.e. it is one if the both indices match and zero otherwise.
Noting the ""index replacement"" property of a summed tensor-$\delta$ product
$$\sum_k\delta_{kq}A_{kj}=A_{qj}$$
we then have
$$\frac{\partial U_{ij}}{\partial V_{pq}}=\delta_{ip}A_{qj}$$
So finally
$$W_{ij,pq}=
    \begin{cases}
            A_{qj} &         p=i\\
            0 &         p\neq i
    \end{cases}$$
(and $\hat{W}_{ij,k}=A_{kj}$)
","The paragraph presents a concise summary of matrix calculus. It begins by suggesting an efficient approach to matrix calculus, which involves writing out matrix equations and applying standard derivative rules. Alternatively, one can use summations and subscripts.

The paragraph heavily emphasizes the complexity of matrix equations, making it difficult to use existing rules for calculus. As a result, the author uses summation/subscript forms or index notation when computing derivatives.

For a specific problem involving matrix equations, the author suggests using intermediate variables to simplify the structure. However, they later clarify that the original problem should have resulted in a higher-order tensor (4th order) rather than a matrix. This order depends on the number of indices in the derivative.

To demonstrate, the author provides an example of a matrix equation and the subsequent derivative, which results in a Kronecker delta symbol. Substituting the Kronecker delta into the derivative, they arrive at the final result, confirming the order of the derivative tensor."
Difference between Time delayed neural networks and Recurrent neural networks,https://stats.stackexchange.com/questions/160070/difference-between-time-delayed-neural-networks-and-recurrent-neural-networks,time-series|forecasting|neural-networks|finance,160070,True,176153,14460,2,9,1512741285,1436148260,6,1444362065,"I have never worked with recurrent networks, but from what I know, in practice, some RNN and TDNN can be used for the same purpose that you want: Predict time series values. However, they work different.
It is possible with TDNN:

Predict process' values
Find a relationship between two processes.

Some RNN, like NARX (https://en.wikipedia.org/wiki/Nonlinear_autoregressive_exogenous_model) also allow you to do that, and it is also used to predict financial time series, usually better than TDNN.
A TDNN looks more like a feedforward network, because time aspect is only inserted through its inputs, unlike NARX that also needs the predicted/real future value as input. This characteristic makes TDNN less robust than NARX for predicting values, but requires less processing and is easier to train.
If you are trying to find a relationship between a process $X(t)$ and a process $Y(t)$, NARX requires you to have past values of $Y$, while TDNN does not.
I recommend reading Simon Haykin's Neural Networks: A Comprehensive Foundation (2nd Edition) and this FAQ (ftp://ftp.sas.com/pub/neural/FAQ.html). There are lots of neural networks architectures and variations. Sometimes they have many names or there is no consensus about their classification.
","**Summary:**

Recurrent Neural Networks (RNNs) and Time Delay Neural Networks (TDNNs) are both suitable for time series prediction, but operate differently.

**TDNNs:**

* Resemble feedforward networks, incorporating time information only through inputs.
* Capable of predicting process values and identifying process relationships.
* Less robust than RNNs for value prediction but require less processing and training effort.
* Do not require past values of dependent variables for relationship modeling.

**NARX (a type of RNN):**

* Uses feedback loops to incorporate past values into predictions.
* Typically used in financial time series forecasting, often outperforming TDNNs.
* Requires past values of both input and output variables for process relationship modeling.

When choosing between TDNNs and NARXs, consider factors such as:

* Robustness of value predictions (NARXs recommended for critical applications).
* Computational efficiency and ease of training (TDNNs preferred for less demanding scenarios).
* Requirement for past dependent variable values for relationship modeling (NARXs required, TDNNs not)."
What can Deep Neural Networks do that Support Vector Machines can&#39;t?,https://stats.stackexchange.com/questions/366581/what-can-deep-neural-networks-do-that-support-vector-machines-cant,neural-networks|deep-learning|svm,366581,True,366599,1713,1,9,1536767910,1536764497,7,1536767910,"I will list a few areas where I am fairly confident DNNs perform better than SVMs, and it's not just the ""hype"". I'm sure there are more, just as I'm sure there are places where SVMs would do better. In particular, I've found that lots of people who ask questions like these are thinking only about Fully Connected networks (a.k.a. feed forward networks, or multi-layer perceptrons, or ANNs, or ...), typically being applied to more standard ""tabular"" style data. In these cases, I have not seen incredible results from DNNs, and if that is your only experience it may be easy to believe it is just hype.
Where DNNs really shine is with Convolutional Neural Networks, but also their ability to handle sequential data, their ability to generate data, and reinforcement learning (e.g. learning to play Go or Atari games). I'll go into detail on a few below.
Images
DNNS, in particular Convolutional Neural Networks (CNNs), are the clear state of the art on almost every image processing task. I'm not sure if anyone is seriously suggesting that SVMs reach comparable performance on classification datasets like ImageNet, Cifar10, or even MNIST.
This goes doubly for ""dense"" image predictions, e.g. given a 500x500 image of a CT scan classify exactly which pixels are a tumor (see U-Net: Convolutional Networks for Biomedical Image Segmentation by Olaf Ronneberger, Philipp Fischer, Thomas Brox (https://arxiv.org/abs/1505.04597) for one of the earlier works in this area). I'm not sure what it would even look like for an SVM to do that.
Note that while images are the poster child project for CNNs, they can also be readily applied to other tasks where signal processing may historically have been used, and in my personal experience have reached better performance levels.
Data Generation
There are a number of data generation tasks that are currently areas of research. For example, using Generative Adversarial Networks to generate new images (in practice, though, it is typically just used as unsupervised feature learning). There is also interesting work being done to try and generate art/music (https://magenta.tensorflow.org/ (https://magenta.tensorflow.org/)). Similar to dense image predictions, I'm not sure what it would look like for SVMs to do this. Maybe there are people doing fascinating work in this area with SVMs, I'm not going to claim to be an expert on that, but my impression is that is not happening.
I will note that data generation is not just purely academic, there are uses in voice synthesis (think Siri, Cortana, or Google Assistant), and likely other areas.
Reinforcement Learning
Researchers have been able to train DNNs to learn how to play Atari games using only the raw pixel data and the scores as input (https://deepmind.com/research/dqn/ (https://deepmind.com/research/dqn/)). Perhaps this is just my inexperience, but this is a feat I would struggle to achieve with SVMs.
","DNNs (Deep Neural Networks) generally excel over SVMs (Support Vector Machines) in specific areas:

1. **Image Processing:** Convolutional Neural Networks (CNNs), a type of DNN, dominate image processing tasks, including classification and dense pixel predictions, outperforming SVMs on datasets like ImageNet and MNIST.

2. **Sequential Data Handling:** DNNs are adept at handling sequential data, enabling them to process time-series data, natural language, and other sequential formats.

3. **Data Generation:** DNNs, particularly Generative Adversarial Networks (GANs), can generate data, including images, art, and music. This capability is not evident in SVMs.

4. **Reinforcement Learning:** DNNs have been used to train agents to play Atari games solely based on pixel data and score feedback, a task that SVMs may struggle with.

However, it's important to note that SVMs may be more suitable for different tasks, such as tabular data classification, where DNNs have not consistently demonstrated superior performance."
Understanding weight distribution in neural network,https://stats.stackexchange.com/questions/198762/understanding-weight-distribution-in-neural-network,neural-networks,198762,True,227169,7979,2,9,1658252065,1456509554,7,1470261843,"This is expected. Weights in a CNN form feature detectors, so that a certain pattern in an image is connected to strong weights, but the rest of the image pixels should not cause any activations in the next layer neurons. 
Only a small fraction of neurons in a layer is activated every time an image is shown, and a small fraction of weights is needed to be large to activate (or suppress) any particular neuron. Moreover, the number of patterns a network needs to detect is fairly small, especially in the early layers. Therefore, overall connectivity for the network is usually very sparse. 
The same reasoning applies to regulatization methods, such as L2/L1 - forcing the weights to be small makes the network more robust to noise in the data, and forces the network to learn only the features present in many images. 
","**Summary:**

In Convolutional Neural Networks (CNNs), the weights act as feature detectors, connecting particular image patterns to strong weights. However, only a small proportion of neurons and weights are activated for any given image, as the network focuses on detecting a limited number of patterns, especially in early layers.

Connectivity in CNNs tends to be sparse, meaning that most weights are small and only a few are large enough to activate or suppress neurons. Regularization methods like L2/L1 enforce this sparsity, making the network more robust to noise and promoting the detection of features that are prevalent in multiple images."
What are the differences between filters learned in autoencoder and convolutional neural network?,https://stats.stackexchange.com/questions/140698/what-are-the-differences-between-filters-learned-in-autoencoder-and-convolutiona,machine-learning|neural-networks|convolutional-neural-network|autoencoders,140698,True,140709,1411,1,9,1471723791,1425667401,7,1425674881,"In case of CNN filters are applied to small patches of an image at each possible location (which also makes them translation invariant).
Autoencoder's hidden layers get whole image (output of the previous layer) as their input, which doesn't look like a good idea for images: usually only spatially local features correlate, whereas more distant ones are less correlated. Also, these hidden neurons are not translation invariant.
Thus, CNNs are like usual ANNs with a special kind of regularization, which zeros out most of weights to make use of locality.  
","**Summary:**

Convolutional Neural Networks (CNNs) and Autoencoders are two types of neural networks used in image processing.

**CNNs:**

* Apply filters to small patches of an image, making them translation invariant (not affected by image shifts).
* Only consider spatially local features, as distant features are less correlated.
* Use regularization to set most weights to zero, ensuring locality.

**Autoencoders:**

* Feed the entire image output of the previous layer into hidden layers.
* This approach is less suitable for images, as distant features are not strongly correlated.
* Hidden neurons are not translation invariant.

Therefore, CNNs are better suited for image processing due to their use of local features and translation invariance."
Expected value notation in GAN loss,https://stats.stackexchange.com/questions/401484/expected-value-notation-in-gan-loss,neural-networks|expected-value|notation|gan,401484,True,401485,2240,1,9,1640269563,1554542496,6,1554543007,"$E_{x\sim p(x)}[f(X)]$ means the expected value of $f(X)$ if its assumed to be distributed wrt $p(x)$, e.g. for a continuous distribution we have:
$$E_{x\sim p(x)}[f(X)]=\int f(x)p(x)dx$$
It's used when the distribution of $x$ subject to change in an optimization problem. Specifically, in the paper, authors have two distributions (in page 5) $p_g$ and $p_{data}$.
Edit: And, the $x$ in the subscript of the expected value notation is not a realization. It's the random variable; or more specifically, in the paper it is the random vector, $\mathbf{x}$ (It's also in bold in Page 5).
","The expected value notation $E_{x\sim p(x)}[f(X)]$ represents the expected value of the function $f(X)$ when the random variable $x$ is assumed to follow the probability distribution $p(x)$. In particular, for a continuous distribution, this value is calculated using the integral $\int f(x)p(x)dx$.

In an optimization problem, the distribution of $x$ may vary. This notation is used to emphasize that the expected value is calculated under the assumption of the specific distribution $p(x)$ being used.

In the mentioned paper, two distributions are considered: $p_g$ and $p_{data}$. The random variable $x$ is a random vector denoted as $\mathbf{x}$, as specified in Page 5 of the paper."
What is a sensible order for parameter tuning in neural networks?,https://stats.stackexchange.com/questions/369104/what-is-a-sensible-order-for-parameter-tuning-in-neural-networks,neural-networks|optimization|hyperparameter,369104,True,369315,5653,1,9,1540787829,1538112553,6,1538212387,"
There are so many aspects one could possibly change in a deep neural network that it is generally not feasible to do a grid search over all of them.

True. But an alternative to that, i.e. random search is feasible in many cases. Please have a look at this post for an interesting explanation (https://stats.stackexchange.com/questions/160479/practical-hyperparameter-optimization-random-vs-grid-search). 

Hence, when tuning the various components of a neural network by hand, what is considered to be a sensible order?

The hyper-parameters interact but for practical purposes they can be tuned independently, as these interactions have no apparent structure. So, the order in which the hyper-parameters need to be tuned is largely subjective. But one of the recommendations from this paper (https://arxiv.org/abs/1503.04069), which I do follow is, one should tune the learning rate first. That saves a lot of experimentation. For illustration of the importance of the learning rate have a look at the image taken from the linked paper. They have experimented with different variants of LSTM over three datasets and have presented the performance over the test set. The chart shows what fraction of the test set performance variance can be attributed to different hyper-parameters
They also show that optimal value of learning rate is dependent on the dataset. 
 (https://i.sstatic.net/iraKo.png)
So, if I have to answer the order that I follow for training neural networks I would stick with this:
a) Optimizer
b) Learning rate 
c) batch size 
d) Input noise
e) Network design -number of hidden layers and number of neurons
f) Regularizers - (L1, L2, dropout etc.)
But, again, every dataset is different and hyper-parameters will surely be dependent on that. So, for every problem one approach won't do. Plotting the error will give the feel for the dataset and help in finding the 'optimal' hyper-parameters.
Some posts that might be useful:
a) In what order should we tune hyperparameters in Neural Networks?
 (https://stackoverflow.com/questions/37467647/in-what-order-should-we-tune-hyperparameters-in-neural-networks)
b) Hyperparameter tuning for machine learning models. (https://www.jeremyjordan.me/hyperparameter-tuning/)
c) A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning (https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)
d) Hyper parameters tuning: Random search vs Bayesian optimization (https://stats.stackexchange.com/questions/302891/hyper-parameters-tuning-random-search-vs-bayesian-optimization)
e) hyperparameter tuning in neural networks (https://stats.stackexchange.com/questions/342462/hyperparameter-tuning-in-neural-networks)
","Tuning deep neural networks involves adjusting numerous hyperparameters, which can be overwhelming. Grid search is usually impractical due to the vast number of combinations. Random search is recommended as an alternative.

For manual tuning, the recommended order is:
1. Optimizer
2. Learning rate
3. Batch size
4. Input noise
5. Network design (hidden layers and neurons)
6. Regularizers (L1, L2, dropout, etc.)

However, the optimal order may vary based on the dataset and problem at hand. The learning rate is particularly important and should be adjusted first, as it can significantly impact performance.

Plotting the error metric can provide insights into the dataset's behavior and help in identifying optimal hyperparameters. Referenced materials provide additional guidance on hyperparameter tuning."
One-shot object detection with Deep Learning,https://stats.stackexchange.com/questions/336313/one-shot-object-detection-with-deep-learning,neural-networks|deep-learning|image-processing|computer-vision|object-detection,336313,True,379731,2284,1,9,1543641866,1521821584,6,1543641546,"As it turns out, just training a ordinary object detection network with a bunch of data augmentation will get you some decent results. 
I took the ""coca cola"" logo from your post, and performed some random augmentations on it. Then I downloaded 10000 random images from flickr and randomly pasted the logo onto these images. I also added random red regions to the images so the network wouldn't learn that any red blob was a valid object. Some samples from my training data:
 (https://i.sstatic.net/EF6atm.jpg)
 (https://i.sstatic.net/E08Pqm.jpg)
 (https://i.sstatic.net/O4gMOm.jpg)
 (https://i.sstatic.net/N6L3L.jpg)
I then trained an RCNN model on this dataset. Here are some test-set images I found on google images, and the model seems to do pretty ok. 
 (https://i.sstatic.net/XQevD.png)
 (https://i.sstatic.net/fgjGL.png)
 (https://i.sstatic.net/xaJSb.png)
The results aren't perfect, but I slapped this together in about 2 hours. I expect with a bit more care spent with data generation and with training the model, you could get far better results.
I think ideas from papers such as Learning to Model the Tail (https://papers.nips.cc/paper/7278-learning-to-model-the-tail) could be used to allow learning of new object categories with just one or a few examples, instead of needing to generate a bunch of data like I did, but I'm not aware of them doing any experiments with object detection.
","**Summary:**

Using data augmentation techniques, ordinary object detection networks can achieve satisfactory results. By randomly generating images with augmented versions of a target object (e.g., the Coca-Cola logo), a network can be trained to detect the object in different contexts.

A sample dataset was created by pasting the logo onto 10,000 randomly selected Flickr images and adding noise to prevent the model from recognizing generic red blobs. An RCNN model trained on this dataset effectively detected the logo in test images.

With further improvements in data generation and model training, more accurate results are possible. Additionally, methods like ""Learning to Model the Tail"" hold promise for training object detectors on limited data, potentially eliminating the need for extensive data augmentation."
Neural networks - Switching loss function during training for better gradients,https://stats.stackexchange.com/questions/232845/neural-networks-switching-loss-function-during-training-for-better-gradients,neural-networks|deep-learning|loss-functions,232845,True,238010,3827,1,9,1475404549,1472734040,6,1475404549,"
yes you can do it. It's your network, anything you can code, you can do to it. As was mentioned in the comments, it's just getting a starting point for different optimization problem. Your errors having the same minimum, doesn't matter that much, because the whole problem is nonconvex and you might get stuck in different local minimals/plateaus. 
did you hear about pre-training? Although I don't know about any published network that would use this method to get faster convergence, it's common to pre-train the network in some way before you actually train it with your final loss function. But usually it is done in unsupervised way. 
You just answered yourself. It's getting you a better gradients, so it's worth it with respect to gradients. The question is, if you can get better results by doing something else. E.g: using momentum, changing your alpha on the way or as was mentioned before, to use some pre-training methods

","**Summary:**

The process of minimizing errors in nonconvex optimization problems can lead to multiple local minima. To improve convergence, one can:

* **Pre-train the network:** Initialize the network with a starting point that is closer to the desired solution. This can be done in an unsupervised manner.

* **Use momentum or adaptive learning rate:** Adjust the update direction and learning rate during training to overcome local minima and plateaus.

The main advantages of using a pre-trained network include:

* **Improved gradients:** Pre-training provides better gradients for the final optimization problem, leading to faster convergence.

* **Potential for better results:** Pre-training can result in improved performance compared to training the network directly with the final loss function."
Dealing with small batch size in SGD training,https://stats.stackexchange.com/questions/201775/dealing-with-small-batch-size-in-sgd-training,machine-learning|neural-networks|deep-learning|gradient-descent|stochastic-gradient-descent,201775,True,201836,3751,2,9,1540361322,1458031696,5,1458054518,"With a small batchsize, I believe the SGD descent direction becomes a very noisy estimate of the ""true"" descent direction (i.e. if we evaluated it on the entire training set). With a small batchsize, I am not sure how much increasing the momentum would help as it would be accumulating momentum in very noisy directions. But I could be wrong, maybe your optimization problem is well-posed enough where this could work. 
If you aren't gunning for ""state of the art"" results, one option you have for natural image data is to resize the images. I actually think that modulo chasing down elite benchmarking performance, natural images have a lot of scale invariant properties and a lot of their semantic features are fairly robust under reasonable scaling transformations. This would alleviate some of the GPU memory and allow you to increase your batchsize and your SGD descent directions would be better estimates of the descent directions. 
If you are dealing with a separable loss function like negative log likelihood, we can exploit the fact that the gradient of a large batch is merely the sum/average of the gradients of its constituent sub-batches. For example if our batchsize is $B$, we can compute gradients of a super batchsize $BK$ by iterating through the batches as usual, computing each batch gradient, but instead of updating the weights, we cache each gradient into a running sum or average. If we average appropriately, we will be computing the exact gradient for the $BK$ sized super batch. We then perform the weight update after each $K$-th batch has been processed. 
We will be exactly computing the $BK$ batch gradients by serializing the computation as described above. There is minimal extra computational or memory overhead, one only needs to modify the minibatch iterator to include the super batch serialization and gradient cache. 
","**Summary:**

Using a small batch size in Stochastic Gradient Descent (SGD) can result in noisy descent directions. Increasing momentum might not help in such cases.

To address this, consider:

* **Reducing image size:** Resizing natural images can alleviate GPU memory constraints and allow for larger batch sizes, resulting in more accurate descent directions.
* **Batching for separable loss functions:** For separable loss functions (e.g., negative log likelihood), gradients of large batch sizes can be computed by accumulating gradients from smaller sub-batches, reducing noise.
* **Gradient accumulation:** Gradients can be cached and accumulated over multiple batches before performing weight updates, effectively enlarging the batch size and reducing noise.

This approach offers minimal computational overhead and can significantly improve the accuracy of SGD descent directions."
Effect of rescaling of inputs on loss for a simple neural network,https://stats.stackexchange.com/questions/420231/effect-of-rescaling-of-inputs-on-loss-for-a-simple-neural-network,neural-networks|normalization,420231,True,420330,2294,1,9,1665170888,1564688465,5,1564748629,"Modern initialization methods are designed with strong assumptions about the scale of the input data, usually inputs have 0 mean and unit variance or that inputs are in the unit interval.
If we apply scaling so that inputs are $X_{ij}\in [0,1]$, then activations for the first layer during the first iteration are
$$
X\theta^{(1)} + \beta^{(1)}
$$
and at convergence are
$$
X\theta^{(n)} + \beta^{(n)},
$$
where the weights are $\theta$, the bias is $\beta$.
Network initialization draws values from some specific distribution, usually concentrated in a narrow interval around 0 (https://stats.stackexchange.com/questions/309231/xavier-initialization-formula-clarification/362846#362846). If you don't apply scaling, then activations for the first layer during the first iteration are
$$
255\cdot X\theta^{(1)} + \beta^{(1)}
$$
So the effect of multiplying by the weights is obviously 255 times as large. At convergence, the model will arrive at the same loss as the scaled case; however, it will take longer to get there since the non-scaled model has initial weights that are 255 times too large. Larger weights are close to saturating the softmax function, where the gradient is not very steep, so it will take a long time for the weights to update enough to compensate for the lack of scale.

The video is talking about scaling in the context of features that have different scales. That poses a different kind of conditioning problem for the optimizer. See: In Machine learning, how does normalization help in convergence of gradient descent? (https://stats.stackexchange.com/questions/437840/in-machine-learning-how-does-normalization-help-in-convergence-of-gradient-desc/437848#437848)
","Network initialization methods often assume input data is within a specific scale, such as 0 mean and unit variance. However, when input data is not scaled, the activation values during the first layer's first iteration can be drastically larger than if scaling was applied. This is because the weights are multiplied by the input data, and un-scaled inputs have values significantly larger than scaled ones.

Consequently, the model takes longer to converge because the initial weights are too large and close to saturating the softmax function, resulting in a less steep gradient. Scaling the data ensures that the activation values are within a reasonable range, allowing the model to converge more efficiently.

In summary, scaling input data before network initialization is crucial for efficient convergence, especially when features have different scales. Scaling normalizes the data, preventing excessively large activations and enabling the optimizer to adjust weights more effectively."
Why do saddle points become &quot;attractive&quot; in Newtonian dynamics?,https://stats.stackexchange.com/questions/319440/why-do-saddle-points-become-attractive-in-newtonian-dynamics,machine-learning|neural-networks|optimization|deep-learning|gradient-descent,319440,True,362772,3065,1,9,1537283479,1513621808,5,1534563768,"
There's not really a more intuitive way to think about this. Suppose that you have the eigendecomposition of the Hessian for $P$ an orthonormal matrix of eigenvectors and $D$ diagonal matrix of eigenvalues.
$$
\begin{align}
\nabla^2 f(x) &= PDP^\top \\
\left[\nabla^2 f(x)\right]^{-1} &= PD^{-1}P^\top
\end{align}
$$
This is relevant to Newton's method because the update is given by
$$
\begin{align}
x^{(t+1)} &= x^{(t)}-\left[\nabla^2 f(x^{(t)})\right]^{-1}\nabla f(x^{(t)}) \\
&= x^{(t)}-PD^{-1}P^\top\nabla f(x^{(t)})
\end{align}
$$
Saddle points have gradient 0, and Newton's method seeks points with gradient 0. If the problem is non-convex, then depending on the starting point, you may find yourself in the ""basin of attraction"" of the saddle point.

I also think that this post is of interest:

Gradient descent on non-convex functions (https://stats.stackexchange.com/questions/327251/gradient-descent-on-non-convex-functions/328500#328500)
Why is Newton's method not widely used in machine learning? (https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning?noredirect=1&lq=1)

","**Summary:**

Newton's method finds the minima of convex functions by iteratively updating the current estimate. For non-convex functions, Newton's method may converge to saddle points instead of minima.

The eigendecomposition of the Hessian matrix (the matrix of second partial derivatives) provides insights into Newton's method. The update rule involves the matrix inverse of the diagonal matrix of eigenvalues and the orthonormal matrix of eigenvectors of the Hessian.

At saddle points, where the gradient is zero, Newton's method may become trapped in the ""basin of attraction"" of the saddle point, depending on the starting point. This is a challenge in using Newton's method for non-convex functions.

Additional resources provide further information:

- Gradient descent on non-convex functions: https://stats.stackexchange.com/questions/327251/gradient-descent-on-non-convex-functions/328500#328500
- Newton's method in machine learning: https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning?noredirect=1&lq=1"
Why is sqrt(6) used to calculate epsilon for random initialisation of neural networks?,https://stats.stackexchange.com/questions/291777/why-is-sqrt6-used-to-calculate-epsilon-for-random-initialisation-of-neural-net,machine-learning|neural-networks|random-generation,291777,True,291810,1300,1,9,1500819190,1500182028,5,1500210646,"I believe this is Xavier normalized initialization (implemented in several deep learning frameworks eg Keras, Cafe, ...)
 from Understanding the difficulty of training deep feedforward neural networks (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by Xavier Glorot & Yoshua Bengio.
See equations 12, 15 and 16 in the paper linked: they aim to satisfy equation 12: 
$$\text{Var}[W_i] = \frac{2}{n_i + n_{i+1}}$$
and the variance of a uniform RV in $[-\epsilon,\epsilon]$ is $\epsilon^2/3$ (mean is zero, pdf = $1/(2\epsilon)$ so variance $=\int_{-\epsilon}^{\epsilon}x^2 \frac{1}{2\epsilon}dx$
","Xavier initialization is a widely used method to initialize weights in deep neural networks. It aims to preserve the variance of inputs and outputs across layers, preventing vanishing or exploding gradients. The variance of the initialized weights is set to 2/($n_i$ + $n_{i+1}$), where $n_i$ and $n_{i+1}$ are the number of units in the current and subsequent layers, respectively. This formula ensures that the variance of activations remains approximately constant as they propagate through the network. Xavier initialization is implemented in popular deep learning frameworks like Keras and Caffe."
"In reinforcement learning, what is the correct definition of &quot;value function&quot;?",https://stats.stackexchange.com/questions/501034/in-reinforcement-learning-what-is-the-correct-definition-of-value-function,machine-learning|neural-networks|markov-process|reinforcement-learning|definition,501034,True,501041,855,1,9,1610060665,1608073974,3,1608078889,"
Which one is correct and why is there such a seemingly large amount of definitions?

Let me answer the second question first: Why is there such a (seemingly) large amount of definitions?
I would guess that there are two different reasons for that:

(probably prominently) Lack of knowledge of many mathematical things: What is a random variable actually? What is a Markov process actually mathematically? What are sigma algebras and measures? Further, people do often not know that we must assume or show that random variables have densities, that the word 'density' itself does not make sense but it always has to be a density with respect to a 'natural', alternative measure on the target space, etc etc (could continue here for a while because I have seen all sorts of wild stuff out there, Suttons book is an example of this: My attempt to really understand the 'proof' of the Bellman equation in that book amounted to my answer here: Deriving Bellman's Equation in Reinforcement Learning (https://stats.stackexchange.com/questions/243384/deriving-bellmans-equation-in-reinforcement-learning/391113#391113))
Different setups in which one can do Markov Processes / Reinforcement Learning. For example: do we only allow deterministic policies. Given a current state $s$, do we regard the policy as a function $\pi(s)$ that returns one single action or do we rather regard the policy as something indeterministic that gives rise to a probability distribution over the actions?

Now let's look at the 'one true' definition of the value function. Let me first comment why (in my personal perspective) all of the ones that you name are 'flawed'/make special assumptions:

does mathematically not make sense. An expected condition looks either like $E[X|Y]$ where $X, Y$ are both random variables and it itself is then a new random variable or like $E[X|Y=\cdot]$ and is actually a function in $y=\cdot$. Hence, 1. is not defined. Ng is introducing a new notion here which is (mathematically speaking) simply not defined (also not by him I guess). Apart from that: What exactly is $R(s_t)$? In the Markovian process thare are random variables $R_t, S_t, A_t$ but the symbol $R(s_t)$ is just not yet defined (and probably will never be).

is mathematically senseful but makes certain assumptions on the Markovian process: stationarity (i.e. that the whole setup does not depend on the time). However, one can also view the whole process when policies depend on the time (i.e. you do not have $\pi$ but potentially infinitely many $\pi_t$). But in general, this seems to be the 'correct one' (see below).

Is a perfect example of the 'law of equal amount of work'. Let's say you have a complicated theorem that states that $A=B$ and you are a lecturer of a course and you do not want to go into the details of the proof of $A=B$. Since you are a really clever guy, you simply define $A:=B$, then the proof that $A=B$ is easy, right? It is true by definition. Unfortunately, this universe is a mean one. In 100% of the observed cases (again, just personal observation) you will need some property of $A$ that is only valid when it is defined as original $A$ and not as $B$. No matter which way you go, you cannot get around the work of understanding the proof of $A=B$! For example: Why is this thing in 4. even called value function? It is supposed to give the value of a certain policy given that we start at a certain state... how are these things related? Why is it nevertheless the same as the other things (up to all the things that make mathematically no sense)? Because of the Bellman equation. But this Bellman equation is a complicated proof that you cannot avoid :-)

seems to assume that the policy is a deterministic function. However, in many mathematical constructions (for example, finding the best policy using value iteration) one needs the policies to be probabilistic and not deterministic.

Is just very weird. The left hand side depends on $s_t$ (also unclear what influence $t$ has on the whole thing...) but the right hand side does not... ?


What is the true, most general definition? With regards to all the questions in the area of the foundations of RL and Markov Processes, etc I can only recommend one single source of mathematical truth: Puterman, Markov Decision Processes (https://rads.stackoverflow.com/amzn/click/com/0471727822).
First some basic rules: A Markov process is a touple of random variables $(R_t, S_t, A_t)$ that satisfy certain properties. Let us assume that we look only at stationary processes (that is some additional condition that assure that the time $t$ is irrelevant). How does the policy go in here? Well, that is related to the difference between Markov Automata and Markov Processes. What we fix is the transitions $p(s_{t+1}|s_t, a_t)$ and the rewards $p(r_t|s_{t+1}, a_t, s_t)$ as functions independently of time. However, a Markov process is only uniquely defined if we also specify $p(s_0)$ and $p(a_t|s_t)$. On the other hand, given those two further ingredients, we can actually give rise to this unique Markov Process using a somewhat involved but straightforward construction similar to here (https://mathoverflow.net/questions/292942/markov-processes-construction-of-the-state-variables).
Summarized: Fix some $\gamma \in (0,1)$. Given the transition probabilities and the rewards that only defines a Markov automata (that is something completely different than random variables). For each choice of probability distributions for the initial state $s_0$ and a policy we can give rise to a Markov process (i.e. now, secretly, implicitly all the random variables $R_t, A_t, S_t$ depend on $\pi$ and the distribution of $s_0$ but if we referred to this all the time then our notation would become really clumsy, that's why we leave it blank and only remind ourselves that the variables itself depend on $\pi$ when we write $V^\pi$ instead of just $V$). Then
$$R_t + \gamma R_{t+1} + \gamma^2 R_{t+1} + ...$$
converges almost everywhere and
$$V^{t, \pi}(s) = E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+1} + ... | S_t=s]$$
is actually independent of $t$. With this value function one can show many nice things. For example, if certain conditions on the state and action set are met then there is a provably best, deterministic policy (see Thm. 6.2.9  in the book of Puterman). In particular, these conditions are met when the state and action space are finite. That means that all common board games are actually boring because somewhere out in this universe, there is a best strategy to play this game and both players should actually just follow this strategy to maximize their (discounted) reward.
","**Summary**

**Definitions of Value Function for Markov Decision Processes (MDPs)**

The value function for an MDP varies widely due to:

* Lack of understanding of underlying mathematical concepts (e.g., random variables, sigma algebras)
* Different contexts in which MDPs are used (e.g., deterministic vs. probabilistic policies)

**Common Definitions and Their Limitations:**

* **E[R(s_t)]** (Sutton and Barto): Mathematically undefined; R(s_t) is not defined in an MDP.
* **E[â_i Î³^i R_i | S_0=s]** (Sutton and Barto): Assumes stationarity, which may not always be true.
* **V(**Ï**) = A** (Sutton and Barto): Circular definition; avoids the need for proof, but doesn't explain the relationship between A and the value function.
* **V(s, a) = E[R_t + Î³V(s', a') | S_t=s, A_t=a]** (Puterman): Assumes a deterministic policy.
* **Value of state s = Reward of s + Expected value of next state** (Kaelbling et al.): Left and right sides of the equation have different dependencies.

**True Definition:**

The value function is defined as follows:

* **V(**Ï**, s) = E[R_t + Î³R_{t+1} + Î³Â²R_{t+2} + ... | S_t=s]**

This definition applies to stationary MDPs with arbitrary policies, deterministic or probabilistic. It is independent of time (t) and relies on the Bellman equation for its validity."
RNN learning sine waves of different frequencies,https://stats.stackexchange.com/questions/220307/rnn-learning-sine-waves-of-different-frequencies,regression|time-series|neural-networks|deep-learning|recurrent-neural-network,220307,True,220339,2505,1,9,1466702102,1466686897,3,1466698935,"Your data basically cannot be learned with an RNN trained that way.
Your input is $\sin(t)$ is $2\pi$-periodic $\sin(t) = \sin(t+2\pi)$
but your target $\sin(t/2)$ is $4\pi$-periodic and $\sin(t/2) = -\sin(t+2\pi)$
Therefore, in your dataset you'll have pairs of identical inputs with opposite outputs.
In terms of Mean Squared Error, it means that the optimal solution is a null function.
These are two slices of your plot where you can see identical inputs but opposite targets
 (https://i.sstatic.net/xgAdf.png)
","**Summary:**

The effectiveness of RNNs (Recurrent Neural Networks) for time series forecasting is limited when there are identical inputs with different outputs. Specifically, if the input is periodic (e.g., $\sin(t)$) and the target is also periodic but with a different frequency (e.g., $\sin(t/2)$), the optimal solution for the RNN is a null function (i.e., it predicts the same value for all inputs).

This occurs because the RNN's objective function (e.g., Mean Squared Error) considers identical inputs with opposite outputs as an error. As a result, the RNN learns to predict an average value for all inputs in order to minimize this error, which is not the desired behavior for time series forecasting.

In the case of $\sin(t)$ and $\sin(t/2)$, the input is $2\pi$-periodic, while the target is $4\pi$-periodic and has an opposite sign when shifted by $2\pi$. This creates a situation where identical inputs have opposite outputs, leading to the optimal solution being a null function."
Confused about the notion of overfitting and noisy target function,https://stats.stackexchange.com/questions/625557/confused-about-the-notion-of-overfitting-and-noisy-target-function,regression|machine-learning|neural-networks|overfitting|generalization,625557,True,625559,1169,6,8,1694171444,1693713317,13,1693716578,"As they mention in the video, the classic mathematical definition of a function you typically hear about in algebra or calculus is that a function has a given domain (inputs) and range (outputs), wherein the inputs can only have one unique output. One commonly taught way to test this is the ""vertical line test"", where you draw a vertical line through a plot of $(x,y)$ values and see if you hit a line twice. An example below:
 (https://i.sstatic.net/iFWCD.png)
A sin curve is an example of this function if we have that exact function. A normal sin curve spans the range of $[-1,1]$. This passes the vertical line test.
 (https://i.sstatic.net/9lFl9.png)
 (https://i.sstatic.net/WiJBt.png)
The real world is rarely so convenient. Suppose we are trying to find the actual function, $\sin(x)$, but we have all this noise:
 (https://i.sstatic.net/g4Lsm.png)
We have to find the real function via approximation. However, if we simply ""chase the noise"" by overfitting, we can get something like this, which fits a line to the data really well, but is not really approximating the underlying function. This is the essence of overfitting.
 (https://i.sstatic.net/sqMYH.png)
By trying not to chase the noise, we can get a better approximation of the true function by not trying to fit every point on the graph. Here I have fit a spline with 10 degrees of freedom (red) and the actual $\sin(x)$ function (blue). Even here we are not doing an amazing job, but were are doing better than before (the last spline used 200 degrees of freedom by comparison).
 (https://i.sstatic.net/C7Fhr.png)
By doing this we have let the data speak for us rather than the other way around. So to answer your questions:

Most of the real-world target functions are noisy.

This is true, as shown by nearly any linear regression that is performed in the real world. In fact, this is the essence of the typical simple linear regression formula:
$$
y = \beta_0 + \beta_1 \cdot x + \epsilon
$$
where our $\epsilon$ here, the error, is the ""leftover"" information we failed to capture. Having nearly zero error is often suspect and warrants concern, for reasons outlined earlier.

We are trying to learn target functions (in the real world).

We are trying to learn target functions, but rarely will you ever get an exact function unless it is already known.

Overfitting causes the model to fit onto the noise.

As demonstrated above, we can wildly overestimate where our function is by chasing noise. If we fit this to a regression, our predictions will be terrible, as the regression will be overly confident in where the data should be and thus anything that ranges outside of the ""guesses"" will misfire by a lot.
Edit
For clarification since OP asked further questions, this particular comment from @num_39 sums it up:

There is no contradiction. We are trying to learn a target function
that is a) deterministic + b) noise. There is nothing to be learned
from noise. The noise in one sample is unrelated to the noise in the
next sample. Anything we fit to noise in a particular sample will not
tell us anything about the target function.

You are trying to accomplish two goals...you try to fit data to a noisy function (and approximate it through things like residual standard error, etc.) but also try not to fit the regression so precisely that it doesn't generalize. What the book is getting at is you need some kind of ""sweet spot"" between finding what the signal is without letting the noise do all the talking.
Lets say we fit a line very close to the points in one sample using R with a similar function as before.
#### Library and Seed ####
library(splines)
set.seed(123)

#### Sample 1 ####
x <- runif(1000,-10,10)
y <- sin(x) + rnorm(n=1000,sd=.5)
plot(x,y)
sp <- smooth.spline(x,y,df=200)
lines(sp,col=""red"")

 (https://i.sstatic.net/GTwbX.png)
We then find a new sample which more closely resembles a sin wave. We fit our previously crafted regression to see if it predicts this second dataset's distribution.
#### Sample 2 ####
x2 <- runif(1000,-10,10)
y2 <- sin(x2) + rnorm(n=1000,sd=.1) 
plot(x2,y2)
lines(sp,col=""red"")

We find now that our data points are far above and below where we predicted they should be. It also still doesn't show the general trend well, as its quite erratic compared to the actual function that generated this data. So in short, our original model did a poor job because it also needs to generalize to other samples, where here it did not because its overfit.
 (https://i.sstatic.net/9FXY6.png)
","**Summary:**

A mathematical function typically has a defined domain and range, with each input having only one unique output. This is tested by the vertical line test, which checks if a vertical line intersects the graph of the function more than once.

In the real world, target functions are often noisy. This noise can lead to overfitting, where a model fits the noise instead of the underlying function. This can result in poor predictions, as the model cannot generalize to new data.

To avoid overfitting, it is important to strike a balance between fitting the data and allowing for some noise. This can be achieved by using techniques like regularization, which penalizes models for fitting too closely to the data.

By letting the data speak for itself and avoiding chasing the noise, we can obtain better approximations of the true function."
What are the most popular artificial neural network algorithms for recognising the content of images?,https://stats.stackexchange.com/questions/112595/what-are-the-most-popular-artificial-neural-network-algorithms-for-recognising-t,neural-networks|image-processing|backpropagation|artificial-intelligence|open-source,112595,True,112630,11412,3,8,1505214736,1408531415,15,1408544208,"According to Wikipedia (https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms), there are 4 main types of artificial neural network (https://en.wikipedia.org/wiki/Artificial_neural_network) learning algorithms: supervised, unsupervised, reinforcement and deep learning.
Unsupervised learning (https://en.wikipedia.org/wiki/Unsupervised_learning) algorithms:

Perceptron (https://en.wikipedia.org/wiki/Perceptron),
Self-organizing map (https://en.wikipedia.org/wiki/Self-organizing_map),
Radial basis function network (https://en.wikipedia.org/wiki/Radial_basis_function_network)

Supervised learning (https://en.wikipedia.org/wiki/Supervised_learning) algorithms:

Backpropagation (https://en.wikipedia.org/wiki/Backpropagation),
Autoencoders (https://en.wikipedia.org/wiki/Autoencoder),
Hopfield networks (https://en.wikipedia.org/wiki/Hopfield_network),
Boltzmann machines (https://en.wikipedia.org/wiki/Boltzmann_machine),
Restricted Boltzmann Machines (https://en.wikipedia.org/wiki/Restricted_Boltzmann_Machine),
Spiking neural networks (https://en.wikipedia.org/wiki/Spiking_neural_network)

Reinforcement learning algorithms:

Temporal difference learning (https://en.wikipedia.org/wiki/Temporal_difference_learning),
Q-learning (https://en.wikipedia.org/wiki/Q-learning),
Learning Automata (https://en.wikipedia.org/wiki/Learning_Automata),
Monte Carlo Method (https://en.wikipedia.org/wiki/Monte_Carlo_Method),
SARSA (https://en.wikipedia.org/wiki/SARSA)

Deep learning algorithms:

Deep belief networks (https://en.wikipedia.org/wiki/Deep_belief_network),
Deep Boltzmann machines (https://en.wikipedia.org/wiki/Boltzmann_machine),
Deep Convolutional neural networks (https://en.wikipedia.org/wiki/Convolutional_neural_network),
Deep Recurrent neural networks (https://en.wikipedia.org/wiki/Recurrent_neural_network),
Hierarchical temporal memory (https://en.wikipedia.org/wiki/Hierarchical_temporal_memory)

And other (e.g. Data Pre-processing (https://en.wikipedia.org/wiki/Data_Pre-processing)).
So basically you need a good algorithm for pattern recognition (https://en.wikipedia.org/wiki/Pattern_recognition) for your computer vision (https://en.wikipedia.org/wiki/Computer_vision) analysis.
Object recognition methods in computer vision

Object recognition is a process for identifying a specific object in a
  digital image or video. Object recognition algorithms rely on matching
  or learning algorithms using appearance-based or feature-based
  techniques. Common techniques include edges, gradients, Histogram of
  Oriented Gradients (HOG), Haar wavelets, and linear binary patterns.
  Object recognition is useful in applications such as video
  stabilization, automated vehicle parking systems, and cell counting in
  bioimaging.

You can recognize objects using a variety of models, including:

Extracted features and boosted learning algorithms,
Bag-of-words models with features such as SURF and MSER,
Gradient-based and derivative-based matching approaches,
Viola-Jones algorithm,
Template matching,
Image segmentation and blob analysis,
Fuzzy Membership Rules,
etc.

So there are plenty of different approaches and it's difficult to choose the most efficient or popular, because it really depends on the needs. And the list is increasing every year.

There are also independent algorithms in computer vision, in example:

In Chinese University of Hong Kong the guys developed a face recognition algorithm called GaussianFace that outperforms humans for the first time. Read more: The Face Recognition Algorithm That Finally Outperforms Humans (https://medium.com/the-physics-arxiv-blog/the-face-recognition-algorithm-that-finally-outperforms-humans-2c567adbf7fc),
The BYU image algorithm is highly accurate system learns to decipher images on its own. Read more: BYU's smart object recognition algorithm doesn't need humans (http://news.byu.edu/archive14-jan-objectrecognition.aspx), A smart-object recognition algorithm that doesnât need humans (http://www.kurzweilai.net/a-smart-object-recognition-algorithm-that-doesnt-need-humans) (research paper (http://www.sciencedirect.com/science/article/pii/S0031320313002549)),
Googleâs New Street View Image Recognition Algorithm Can Beat Most CAPTCHAs or can find and read street numbers in Street View, and correlates those numbers with existing addresses to pinpoint their exact location on Google Maps. Read more: Street View and reCAPTCHA technology just got smarter (http://googleonlinesecurity.blogspot.co.uk/2014/04/street-view-and-recaptcha-technology.html) (research paper (http://arxiv.org/abs/1312.6082)),
In 2012 Google built high-level features using large scale unsupervised learning and the system achieved 81.7 percent accuracy in detecting human faces, 76.7 percent accuracy when identifying human body parts and 74.8 percent accuracy when identifying cats. Read more: Google brain simulator identifies cats on YouTube (http://www.wired.co.uk/news/archive/2012-06/26/google-brain-recognises-cats) (research paper (http://research.google.com/archive/unsupervised_icml2012.html)),
DARPA Visual Media Reasoning program have developed vision system development tools for the automated evaluation of vision algorithm performance and for combining computer graphics and machine vision technology (Read more: ""Software aims to characterize algorithm performance,"" Vision Systems Design, December 2013 (http://www.vision-systems.com/articles/print/volume-18/issue-11/departments/technology-trends/imaging-software-software-aims-to-characterize-algorithm-performance.html))
etc.

Read more:

Developers look to open sources for vision algorithms (http://www.vision-systems.com/articles/print/volume-19/issue-3/features/developers-look-to-open-sources-for-vision-algorithms.html) (Article)


Popular open source computer vision software:

OpenCV (http://opencv.org/) (Open Source Computer Vision Library),

Open source computer vision and machine learning software library. It
  has C++, C, Python and Java interfaces and supports Windows, Linux,
  Android and Mac OS.
OpenCV is released under a BSD license, it is free for both academic
  and commercial use. It has C++, C, Python and soon Java interfaces
  running on Windows, Linux, Android and Mac. The library has >2500
  optimized algorithms (see figure below). It is used around the world,
  has >2.5M downloads and >40K people in the user group.
  New algorithms continue to be added to the Open CV library.

SimpleCV (http://www.simplecv.org/)

SimpleCV is a Python interface to several powerful open source
  computer vision libraries in a single convenient package.
It allow access to the high-level feature detection, filtering, and
  pattern recognition algorithms found in Open CV without the need to
  understanding of bit depth, file format, or buffer management
  techniques.

Accord.NET Framework (http://accord-framework.net/)

The Accord.NET framework provides machine learning, mathematics,
  statistics, computer vision, computer audition, and several scientific
  computing related methods and techniques to .NET. This project extends
  the popular AForge.NET Framework providing a more complete scientific
  computing environment.

MATLAB (http://www.mathworks.co.uk/products/matlab/) from MathWorks (http://www.mathworks.com/)

An open-source platform-independent
  C++ framework for machine learning and computer vision research
  framework. Working with Open CV, the framework contains MATLAB
  wrappers for core components of the library and an experimental
  graphical user interface for developing and visualizing machine
  learning data flows.
  Using MATLAB, you can analyze data, develop algorithms, and create models and applications.

ROVIS Machine Vision System (http://rovis.unitbv.ro/about-rovis-lab/)

An open source software application under development by the ROVIS Research Group.

Open Vision Control (http://openvisionc.sourceforge.net/)

A software package for object motion detection.


Such open source frameworks are also available for the Android operating system. These include Cuckoo, an Android framework. 

Conclusion
So everything is about the needs, requirements, scalability, time and money involved. Therefore I would start to experiment with OpenCV library (which has over 2500 optimized algorithms) and learning algorithms written in Matlab, Octave or Python.
On-line courses:

Neural Networks for Machine Learning (https://www.coursera.org/course/neuralnets) at coursera
University of Washington: Machine Learning (https://www.coursera.org/course/machlearning) at coursera
Stanford University: Machine Learning (https://www.coursera.org/course/ml) at coursera
Other: Computer Science: Artificial Intelligence courses (https://www.coursera.org/courses?cats=cs-ai) at coursera

Links:

Neural Networks and Deep Learning (http://neuralnetworksanddeeplearning.com/) (free online book)
OpenCV Computer Vision with Python (http://books.google.co.uk/books?id=OQm3gNQ7xGcC) by Joseph Howse

","**Summary of Machine Learning Algorithms and Object Recognition Techniques**

**Machine Learning Algorithms**

There are four main types of artificial neural network learning algorithms:

* **Unsupervised:** Perceptron, Self-organizing map, Radial basis function network
* **Supervised:** Backpropagation, Autoencoders, Hopfield networks, Boltzmann machines, Spiking neural networks
* **Reinforcement:** Temporal difference learning, Q-learning, Monte Carlo Method, SARSA
* **Deep:** Deep belief networks, Deep Boltzmann machines, Deep Convolutional neural networks, Deep Recurrent neural networks, Hierarchical temporal memory

**Object Recognition Techniques**

* Appearance-based: Edges, gradients, Histogram of Oriented Gradients (HOG), Haar wavelets, linear binary patterns
* Feature-based: Extracted features and boosted learning algorithms, Bag-of-words models, Gradient-based and derivative-based matching approaches, Viola-Jones algorithm, Template matching, Image segmentation and blob analysis

**Independent Algorithms**

* GaussianFace (face recognition)
* BYU image algorithm (object recognition)
* Google's Street View Image Recognition Algorithm
* Google's high-level feature detection algorithm
* DARPA Visual Media Reasoning program

**Popular Open Source Computer Vision Software**

* OpenCV
* SimpleCV
* Accord.NET Framework
* MATLAB
* ROVIS Machine Vision System
* Open Vision Control
* Cuckoo (Android)

**Conclusion**

The choice of algorithms and techniques for machine learning and object recognition depends on the specific requirements and constraints of the task. OpenCV is a popular open-source library with a wide range of algorithms, while Python, Matlab, and Octave offer powerful frameworks for developing and implementing machine learning models."
"Classification in time series: SVMs, Neural Networks, Random Forests or non parametric models",https://stats.stackexchange.com/questions/60939/classification-in-time-series-svms-neural-networks-random-forests-or-non-para,time-series|classification|svm|neural-networks|random-forest,60939,True,60956,15565,2,8,1370442223,1370426997,11,1370438011,"These decisions IMHO can only be made in a sensible way with intimate knowledge about the problem and the data at hand (search terms: no free lunch theorem for pattern recognition/classification). So all we can tell you here are very general rules of thumb.

The more statistically independent cases you have for training, the more complex models you can afford. Very restrictive models (e.g. linear) are very often chosen because more complex models cannot be afforded with the given amount of data and less about really being convinced of having actually linear class boundaries.
See bias variance tradeoff and model complexity e.g. in The Elements of Statistical Learning (http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
knowledge about the nature of your problem and data may also suggest sensible ways of feature generation.
If you don't have terribly many samples, but absolutely need nonlinear boundaries and therefore get unstable models, then ensemble models (like the random Forest) can help. You can aggregate not only decision trees but all other kinds of models as well.  
There are rumours* that for the final quality of the model the choice of model often matters less than the experience the user has with the chosen type of model. I try to collect some evidence about this rumour in this question (https://stats.stackexchange.com/questions/49536/predictive-performance-depends-more-on-expertise-of-data-analyst-than-on-method).
The conclusion would be to look for someone to consult who has experience with the classifiers you consider or, even better, with classification of your type of data (that would need a more detailed description than just saying it is time series).

Note: the first three can also be set up to output posterior probabilities. 
*I don't know any scientific study that reports this, but have heard numerous people reporting this observation, and there is a number of descriptions of the differences between types of models that at the end conclude that the theoretical differences in practice hardly ever matter. 
","Choosing an appropriate classification model requires intimate knowledge of the problem and data. Statistical independence of training cases determines model complexity, with linear models often chosen for data limitations rather than linearity assumptions.

For nonlinear boundaries with limited data, ensemble models like random forests can provide stability. The choice of model may be less important than the user's expertise with it. Consider seeking consultation from individuals experienced with the specific classifier or data type.

Certain models (e.g., decision trees, support vector machines, and logistic regression) can additionally provide posterior probabilities. While theoretical differences between model types exist, practical impact is often minimal, according to anecdotal observations."
Approximating leaky ReLU with a differentiable function,https://stats.stackexchange.com/questions/329776/approximating-leaky-relu-with-a-differentiable-function,neural-networks,329776,True,329803,4727,3,8,1676996158,1519202206,16,1519211691,"The softplus function is commonly described as a smooth approximation of the standard ReLU:
$$s(x) = \log(1 + e^x)$$
 (https://i.sstatic.net/NvK0U.png)
The leaky ReLU (with leak coefficient $\alpha$) is:
$$r_L(x) = \max \{ \alpha x, x\}$$
We can also write this as:
$$r_L(x) = \alpha x + (1-\alpha) \max\{0, x\}$$
Note that $\max\{0, x\}$ is the standard ReLU. So, we can construct a smooth approximation to the leaky ReLU by substituting in the softplus function:
$$s_L(x) = \alpha x + (1-\alpha) \log(1 + e^x)$$
 (https://i.sstatic.net/OJMK1.png)
","**Summary:**

The softplus function is a smooth alternative to the standard ReLU (Rectified Linear Unit) activation function. It is defined as the natural logarithm of one plus the exponential of its input.

The leaky ReLU is a variation of the standard ReLU that introduces a small slope for negative inputs, preventing complete deactivation. It can be expressed as a weighted sum of the input and the standard ReLU, with a coefficient $\alpha$ determining the slope of the negative region.

By replacing the standard ReLU in the leaky ReLU with the softplus function, we obtain a smooth approximation to the leaky ReLU, called the soft leaky ReLU. This function combines the benefits of the leaky ReLU (avoiding vanishing gradients) with the smoothness of the softplus function, offering a more stable and continuous activation function."
Dealing with LSTM overfitting,https://stats.stackexchange.com/questions/351741/dealing-with-lstm-overfitting,neural-networks|overfitting|lstm|recurrent-neural-network|model-evaluation,351741,True,474012,14633,2,8,1624570423,1529200346,9,1593095480,"Your NN is not necessarily overfitting. Usually, when it overfits, validation loss goes up as the NN memorizes the train set, your graph is definitely not doing that. The mere difference between train and validation loss could just mean that the validation set is harder or has a different distribution (unseen data). Also, I don't know what the error means, but maybe 0.15 is not a big difference, and it is just a matter of scaling.
As a suggestion, you could try a few things that worked for me:

Add a small dropout to your NN (start with 0.1, for example);
You can add dropout to your RNN, but it is trickier, you have to use the same mask for every step, instead of a random mask for each step;
You could experiment with NN size, maybe the answer is not making it smaller, but actually bigger, so your NN can learn more complex functions. To know if it is underfitting or overfitting, try to plot predict vs real;
You could do feature selection/engineering -- try to add more features or remove the ones that you might think that are just adding noise;
If your NN is simply input -> rnn layers -> output, try adding a few fully connected layers before/after the rNN, and use MISH as an activation function, instead of ReLU;
For the optimizer, instead of Adam, try using Ranger.
The problem could be the loss function. Maybe your labels are very sparse (a lot of zeros), and the model learns to predict all zeros (sudden drop in the beginning) and cant progress further after that. To solve situations like that you can try different metric, like pos_weight on BCE, dice loss, focal loss, etc.

Good luck!
","The provided paragraph suggests that overfitting is unlikely in the given neural network (NN) as the validation loss is not increasing. Instead, the difference between train and validation loss may indicate a more challenging or differently distributed validation set.

To improve NN performance, several suggestions are offered:

* Add dropout to the NN to prevent overfitting.
* Experiment with NN size, considering both smaller and larger models to optimize function learning complexity.
* Perform feature selection or engineering to remove noisy features or add additional ones.
* Modify the NN architecture by adding fully connected layers and using a MISH activation function.
* Consider using Ranger as an optimizer instead of Adam.
* Adjust the loss function if it is not suited for sparse labels, such as by using BCE with pos_weight, dice loss, or focal loss."
How does the second derivative inform an update step in Gradient Descent?,https://stats.stackexchange.com/questions/315215/how-does-the-second-derivative-inform-an-update-step-in-gradient-descent,neural-networks|optimization|deep-learning|gradient-descent|hessian,315215,True,315217,8451,3,8,1601469302,1511393706,12,1511396109,"I agree with your distaste for the writing. It seems as though you have an understanding of what is going on, but I will attempt to clarify why the second derivative is important. Consider a two-dimensional orthogonal system. Since they are orthogonal we can look at them independently, and together. This need not be the case, but I use the orthogonal system to avoid the linear algebra which may muddy the intuition . 
In the $x_1$ dimension, the objective, f, varies roughly as $f=x_1^2$. In the $x_2$ dimension, the objective varies as $f = .00001x_2^2$. The minima is f(0,0) = 0. 
This is the gradient descent update in each dimension:

$x_{1,k+1} = x_{1,k} - 2\alpha x_{1,k}  $
$x_{2,k+1} = x_{2,k} - .00002\alpha x_{2,k}  $

Where $\alpha$ is the learning rate. That is, According to a gradient descent update, if you start at about (1,1) then after a few iterations you will be at $\approx (0,1)$ because the gradient in the $x_2$ direction is already very near zero. True, we may have predicted this based on the fact that the gradient at every point in the $x_2$ direction is near zero, but it is still undesirable -- I think that is the point they were trying to make in the bolded sentence.
Now we note that $\frac{\partial^2 f}{\partial x_2^2} = .00002$. $\frac{\partial^2 f}{\partial x_1^2} = 2$. Dividing by this amounts to accounting for the curvature (or lack thereof) in that dimension.  Now let's solve these two 1-d functions using second order information. Recall that the form is $x_{k+1} = x_k - \alpha \frac{\partial^2 f}{\partial x_k^2}^{-1} \frac{\partial f}{\partial x_k}$:

$x_{1,k+1} = x_{1,k} - \alpha x_{1,k}  $
$x_{2,k+1} = x_{2,k} - \alpha x_{2,k}  $

That is, they are converging at the same rate, exactly as we would hope they would!
","**Summary:**

Gradient descent updates typically adjust step sizes based on the gradient at the current position. In multi-dimensional optimization, different dimensions may have different curvatures (second derivatives). Ignoring curvature can lead to suboptimal convergence, especially when one dimension has a much lower curvature than the others.

To account for curvature, the second derivative can be incorporated into the gradient descent update. This effectively scales the step size in each dimension to mitigate the effect of different curvatures. As a result, the convergence rates in all dimensions are equalized, leading to more efficient optimization.

In the example provided, the gradient in one dimension is initially close to zero, leading to slower convergence. However, incorporating the second derivative ensures that the step size in that dimension is smaller, compensating for its low curvature and resulting in equal convergence rates for both dimensions."
Is there anything called Shallow Learning?,https://stats.stackexchange.com/questions/617772/is-there-anything-called-shallow-learning,neural-networks,617772,True,617777,1596,2,8,1685948762,1685809487,15,1685809959,"Yes, for instance multi-layer perceptron (MLP) neural networks (with two layers of modifiable weights), support vector machines, Radial Basis Function (RBF) neural networks, Kernel Logistic Regression, Gaussian Processes.  There are a lot of ""shallow"" learning methods, and they are still very useful even if not very fashionable at the moment.  Linear regression and standard logistic regression are also ""shallow learning"".
Oddly enough, shallow learning methods can out-perform deep learning methods in some applications, but I suspect publication bias means that this isn't as widely appreciated as it should be.
","**Summary:**

Shallow learning methods, such as multilayer perceptron neural networks, support vector machines, and linear regression, remain valuable despite being less popular than deep learning approaches. These methods involve fewer layers and modifiable weights, making them less complex than deep learning models.

Despite their simplicity, shallow learning methods can outperform deep learning in certain applications. However, publication bias may lead to underappreciation of this phenomenon. Shallow learning algorithms offer advantages such as faster training times, lower computational requirements, and improved interpretability compared to deep learning models.

Therefore, shallow learning methods should not be dismissed as outdated, and their potential for effective performance in various applications should be acknowledged."
k-fold CV of forecasting financial time series -- is performance on last fold more relevant?,https://stats.stackexchange.com/questions/14197/k-fold-cv-of-forecasting-financial-time-series-is-performance-on-last-fold-mo,time-series|cross-validation|neural-networks|finance,14197,True,14252,2742,3,8,1674468711,1313180667,13,1313365190,"With time series, you cannot test a forecasting model via cross-validation in the normal way because you are then using future observations to predict the past. You must use only past observations to predict the future. The time series equivalent of LOO CV is to use a rolling forecast origin instead. I've written about it in this blog post (http://robjhyndman.com/researchtips/crossvalidation/). I'm not sure if k-fold CV has a direct time series equivalent.
","**Summary:**

Time series data presents unique challenges in evaluating forecasting models. Traditional cross-validation methods, which divide data into subsets and test models on unseen data, cannot be directly applied to time series because it would use future observations to predict the past.

To address this issue, a rolling forecast origin approach is used as the time series equivalent of leave-one-out cross-validation (LOO CV). This method involves splitting the data into consecutive subsets, using only past observations to forecast future values. The model is then evaluated on the remaining portion of the data.

The rolling forecast origin approach ensures that the model is tested on data that was not used in the training process, providing a more reliable evaluation of its forecasting accuracy. However, the direct equivalent of k-fold cross-validation, where the data is randomly shuffled and divided into multiple subsets, is not directly applicable to time series data."
"In neural network literature, which one is activation?",https://stats.stackexchange.com/questions/272035/in-neural-network-literature-which-one-is-activation,neural-networks,272035,True,272051,3555,3,8,1491556537,1491398061,8,1491400792,"A 'layer' does not have an activation. Each individual neuron has an activation.
The state of a neuron is it's bias + all incoming connections (weight * activation from source neuron). So that's $z_2$.
The activation is the state of a neuron passed through an activation function (https://en.wikipedia.org/wiki/Activation_function). So that's $a_2$. As $\sigma()$ is the activation function.
Related: [1] (https://datascience.stackexchange.com/questions/11059/what-does-the-activation-of-a-neuron-mean)

The answer below seems contradictory, however (logically) it makes sense to call the activation of a neuron the value that is received from the activation function. References to support my claim:

As you see, the value neuron y is getting from x1 is called the 'activation of neuron1', meaning its output - thus the value received after the activation function. [Source] (https://books.google.nl/books?id=jJTN8RPgyXgC&pg=PA23&dq=activation+neuron+neural+network&hl=nl&sa=X&redir_esc=y#v=onepage&q=activation%20neuron%20neural%20network&f=false)
","**Summary:**

In a neural network, the activation of a neuron refers to the output value of the neuron that has been processed through an activation function. An activation function is a mathematical operation that transforms the weighted sum of the neuron's inputs (known as its state) into an output.

The state of a neuron represents the linear combination of its inputs, which includes the bias term and the weighted activations of the source neurons. Once the state is calculated, it is passed through an activation function to obtain the neuron's activation.

It's important to note that the term ""activation"" is not associated with the layer itself but with individual neurons within the layer. Each neuron has its own activation value, which is the output it produces after processing its inputs through the activation function."
Does ReLU layer work well for a shallow network?,https://stats.stackexchange.com/questions/225748/does-relu-layer-work-well-for-a-shallow-network,neural-networks,225748,True,354649,2976,4,8,1544245415,1469554134,6,1530807118,"Changing the activation function interacts with all of the other configuration choices that you've made, from the initialization method to the regularization parameters. You'll have to tune the network again.
","**Summary:**

Choosing an activation function affects the network configuration. It interacts with initialization methods, regularization parameters, and other settings. As a result, changing the activation function typically requires fine-tuning the network again.

**Key Points:**

* Activation function selection influences the behavior and performance of the network.
* Network parameters must be adjusted after changing the activation function to optimize performance.
* Initialization methods, regularization parameters, and other configuration choices are intertwined with the choice of activation function."
matrix-calculus - Understanding numerator/denominator layouts,https://stats.stackexchange.com/questions/553883/matrix-calculus-understanding-numerator-denominator-layouts,machine-learning|neural-networks|gradient-descent|derivative|matrix-calculus,553883,True,553939,1917,2,8,1683304388,1638022448,7,1638052050,"If you think of $L$ as a column vector, then I think both your sources agree that  $\frac{dJ}{dL}$ should be a row vector.
But what if you really want $L$ as a row vector. Surely, the math shouldn't ""care"" about how you arrange your collection of numbers. One way to clarify this is by designating dimensions of your objects as ""covariant"" or ""contravariant"".
Many things are contravariant, meaning they change opposite to a change in basis (if you go from a bigger unit, ""hours"" to a smaller unit ""seconds"", your measurements become bigger). On the other hand, a derivative, like ""m/hour"" becomes smaller when you change the units to ""m/second"", hence ""co"".
Things which are ""co"" can be multiplied with things which are ""contra"", e.g. 5 m/second * 10 seconds = 50m. Yet it makes much less sense to multiply two ""contra"" or two ""co"" together (admittedly, second^2 or m^2/second^2 are sometimes useful units, but this is not always the case).
So yes, you could say that $\frac{dJ}{dL}$ is a ""column"" covector with size $m$, and $\frac{dL}{da}$ is a matrix with shape (contra-$m$, co-$m$). We could write $\left(\frac{dJ}{dL}\right)^i = \frac{\partial J}{\partial L_i}$, and $\left(\frac{dL}{da}\right)_i^j = \frac{\partial L_i}{ \partial a_j}$ (we give superscripts to ""co"" dimensions, and subscripts to ""contra"", to make things clear). Then, following our rule that co can only be multipled by contra, we see that
$$\left(\frac{dJ}{da}\right)^j = \sum_{i=1}^m \left(\frac{dJ}{dL}\right)^i \left(\frac{dL}{da}\right)_i^j = \left(\frac{dJ}{dL}^T \frac{dL}{da} \right)^j$$
So even if you ""force"" $\frac{dJ}{dL}$ into a column, if you want to respect our new multiplication rule, you need to transpose before applying matrix mult.
To take this a step further, let's say we are interested in $\frac{da}{dX}$, which has shape (contra-$m$, co-$(n,m)$): $\left( \frac{da}{dX} \right)_j^{u,v} = \frac{\partial a_j}{\partial X_{u,v}}$. Then we have
$$\left(\frac{dJ}{dX}\right)^{u,v} = \sum_{j=1}^m \left(\frac{dJ}{da}\right)^j \left(\frac{da}{dX}\right)_j^{u,v}$$

To translate this back to ""numerator layout"" matrix calculus terms, you could say that column vectors are always contravariant, row vectors are always covariant or ""covectors"", gradients are covariant, hence always row vectors. An $m$ by $n$ Jacobian matrix is contra-$m$, co-$n$. This works nicely because if you think of a column vector as a (contra-$n$, co-1) matrix or a row vector as a (contra-1, co-$m$) matrix, notice that by following the ordinary rules of matrix mutliplcation, you'll never accidentally multiply two contra / two co together, and the product of two objects will always be in a (contra, co) form. On the other hand, ""denominator layout"" has everything in (co, contra) form, which is just as fine and accomplishes the same thing.
However, if you start working with less standard objects, like the derivative of a matrix with respect to a vector, or the derivative of a row vector with respect to a column vector (as in our example above), then you'll need to keep track for yourself what is covariant and what is contravariant.
","The text discusses the use of covariant and contravariant objects in mathematics, with a focus on how they relate to differentiation.

Covariant objects change in the same direction as a change in basis, while contravariant objects change in the opposite direction. Derivatives are covariant, while matrices are contravariant.

The multiplication rule states that covariant objects can only be multiplied by contravariant objects. When a covariant object (e.g., a derivative) is represented as a column vector, it must be transposed before multiplying it by a contravariant object (e.g., a matrix).

A Jacobian matrix is contravariant in the row dimension and covariant in the column dimension.

It is important to keep track of the covariance/contravariance of different objects when working with less standard operations, such as the derivative of a matrix or the derivative of a row vector with respect to a column vector."
Why does torchvision.models.resnet18 not use softmax?,https://stats.stackexchange.com/questions/542054/why-does-torchvision-models-resnet18-not-use-softmax,neural-networks|classification|image-processing|softmax|torch,542054,True,542055,4576,1,8,1630769268,1630420306,13,1630422017,"Whether you need a softmax layer to train a neural network in PyTorch will depend on what loss function you use. If you use the torch.nn.CrossEntropyLoss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), then the softmax is computed as part of the loss. From the link:

The loss can be described as:
$$
\text{loss}(x,class) = â\log\left(\frac{\expâ¡(x[class])}{\sum_j \exp(x[j])}\right)
$$

This loss is just the concatenation of a torch.nn.LogSoftmax (https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) followed by the torch.nn.NLLLoss (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss. From the documentation of torch.nn.CrossEntropyLoss:

This criterion combines LogSoftmax and NLLLoss in one single class.

and from the documentation of torch.nn.NLLLoss:

Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.

It seems that the developers of these pretrained models had the torch.nn.CrossEntropyLoss in mind when they were creating them.
","Whether a neural network in PyTorch requires a softmax layer depends on the loss function used.

When using the torch.nn.CrossEntropyLoss, the softmax is included as part of the loss function, combining torch.nn.LogSoftmax and torch.nn.NLLLoss.

The documentation suggests that using torch.nn.CrossEntropyLoss is preferable to adding a separate softmax layer, as it simplifies the network structure.

Therefore, if you are using torch.nn.CrossEntropyLoss, you do not need to explicitly add a softmax layer to your network."
Is it allowed to refer to Artificial Neural Networks as Statistical learning?,https://stats.stackexchange.com/questions/524205/is-it-allowed-to-refer-to-artificial-neural-networks-as-statistical-learning,machine-learning|neural-networks|terminology|academia,524205,True,524249,969,2,8,1621023600,1621003492,8,1621019218,"The classic The Elements of
Statistical Learning (https://web.stanford.edu/%7Ehastie/ElemStatLearn/) handbook by Hastie et al discusses neural networks among other algorithms, so it needs to be a âstatistical learningâ algorithm.
Depending whom youâd ask, neural networks are either statistics, statistical learning, pattern recognition, machine learning, deep learning, or artificial intelligence. Thereâs no single, agreed category used by everybody to describe them.
","**Summary:**

The categorisation of neural networks is not universally agreed upon. While the handbook ""The Elements of Statistical Learning"" considers neural networks as ""statistical learning"" algorithms, others classify them as statistics, pattern recognition, machine learning, deep learning, or artificial intelligence. This lack of consensus reflects the broad applicability and interdisciplinary nature of neural networks, which draw upon concepts from various fields."
Why do attention models need to choose a maximum sentence length?,https://stats.stackexchange.com/questions/411736/why-do-attention-models-need-to-choose-a-maximum-sentence-length,neural-networks|natural-language|recurrent-neural-network|attention,411736,True,412156,9028,2,8,1560057092,1559782965,3,1560057092,"A ""typical"" attention mechanism might assign the weight $w_i$ to one of the source vectors as $w_i \propto \exp(u_i^Tv)$ where $u_i$ is the $i$th ""source"" vector and $v$ is the query vector. The attention mechanism described in OP from ""Pointer Networks"" opts for something slightly more involved: $w_i \propto \exp(q^T \tanh(W_1u_i + W_2v))$, but the main ideas are the same -- you can read my answer here (https://stats.stackexchange.com/a/345441/26948) for a more comprehensive exploration of different attention mechanisms. 

The tutorial mentioned in the question appears to have the peculiar mechanism 
$$w_i \propto \exp(a_i^Tv)$$
Where $a_i$ is the $i$th row of a learned weight matrix $A$. I say that it is peculiar because the weight on the $i$th input element does not actually depend on any of the $u_i$ at all! In fact we can view this mechanism as attention over word slots -- how much attention to put to the first word, the second word, third word etc, which does not pay any attention to which words are occupying which slots.
Since $A$, a learned weight matrix, must be fixed in size, then the number of word slots must also be fixed, which means the input sequence length must be constant (shorter inputs can be padded). Of couse this peculiar attention mechanism doesn't really make sense at all, so I wouldn't read too much into it.

Regarding length limitations in general: the only limitation to attention mechanisms is a soft one: longer sequences require more memory, and memory usage scales quadratically with sequence length (compare this to linear memory usage for vanilla RNNs). 
I skimmed the ""Effective Approaches to Attention-based Neural Machine Translation"" paper mentioned in the question, and from what I can tell they propose a two-stage attention mechanism: in the decoder, the network selects a fixed sized window of the input of the encoder outputs to focus on. Then, attention is applied across only those source vectors within the fixed sized window. This is more efficient than typical ""global"" attention mechanisms.
","Attention mechanisms assign weights to input elements based on their relevance to a query. A typical mechanism uses an exponential function to calculate weights based on the dot product of the query vector and source vector.

The tutorial's peculiar attention mechanism assigns weights based on learned weights independent of source vectors. This is equivalent to attention over fixed-size word slots, not considering the actual words. This mechanism is inefficient and impractical.

Attention mechanisms generally require more memory compared to vanilla RNNs, with memory usage increasing quadratically with sequence length. To address this, the ""Effective Approaches to Attention-based Neural Machine Translation"" paper proposes a two-stage attention mechanism that focuses on a fixed-size window of input, improving efficiency."
Difference between strided and non-strided convolution,https://stats.stackexchange.com/questions/360899/difference-between-strided-and-non-strided-convolution,neural-networks|convolutional-neural-network|tensorflow,360899,True,360959,15563,5,8,1613848981,1533544433,12,1533567343,"Stride is the distance between spatial locations where the convolution kernel is applied. In the default scenario, the distance is 1 in each dimension. This is also the default value in Tensor Flow, as @Axel Vanraes (https://stats.stackexchange.com/questions/360899/difference-between-strided-and-non-strided-convolution#360943) mentions.
I suppose this is sometimes referred to as non-strided convolution, although that is incorrect: the stride is one. When the stride is larger than one, one usually talks about strided convolution to make the difference explicit.
To visualize the difference:

Stride-1 convolution (""non-strided""):



Stride-2 convolution (""strided""):



Images from https://github.com/vdumoulin/conv_arithmetic (https://github.com/vdumoulin/conv_arithmetic)
","**Summary:**

In image processing, convolution is a mathematical operation that applies a filter (kernel) to an input image to extract specific features. The stride of a convolution refers to the distance between the filter's application locations.

By default, the stride is set to one in both dimensions. This is known as ""non-strided"" convolution, though this term is technically incorrect as the stride is still one. When the stride is greater than one, it is typically referred to as ""strided"" convolution.

In non-strided convolution, the filter is applied at every pixel in the input image. In strided convolution, the filter is applied at every second, third, or other specified interval.

By varying the stride, the output size of the convolution operation can be adjusted. Strided convolution is often used to reduce the dimensionality of the output feature map while retaining essential information."
Classification with a neural network when one class has disproportionately many entries,https://stats.stackexchange.com/questions/193887/classification-with-a-neural-network-when-one-class-has-disproportionately-many,classification|neural-networks|unbalanced-classes,193887,True,193960,7103,2,8,1483397520,1454525210,12,1454570739,"You are coping with an imbalanced dataset.
Lucky for you, you are not alone. 
This is a common problem.
For surveys on the topic see Editorial: Special Issue on Learning from Imbalanced Data
Sets (6 pages) (https://www3.nd.edu/~dial/papers/ACMSIGKDD04.pdf) and Learning from Imbalanced Data (22 pages) (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.6205&rep=rep1&type=pdf)
The method I like best is the following:
The method is based of the boosting algorithm Robert E. Schapire presented at ""The strength of weak learnability (http://rob.schapire.net/papers/strengthofweak.pdf)"" (Machine Learning, 5(2):197â227, 1990. The Strength of Weak Learnability ).
In this paper, Schapire presented a boosting algorithm based on combining triplets of 3 weak learners recursively. By the way, this was the first boosting algorithm.
We can use the first step of the algorithm (even without the recursion) to cope with the lack of balance.
The algorithm trains the first learner, L1, one the original data set.
The second learner, L2, is trained on a set on which L1 has 50% chance to be correct (by sampling from the original distribution).
The third learner, L3, is trained on the cases on which L1 and L2 disagree.
As output, return the majority of the classifiers.
See the paper to see why it improves the classification.
Now, for the application of the method of an imbalanced set:
Assume the concept is binary and the majority of the samples are classified as true. 
Let L1 return always true.
L2 is being trained were L1 has 50% chance to be right. Since L1 is just true, L2 is being trained on a balanced data set.
L3 is being trained when L1 and L2 disagree, that is, when L2 predicts false.
The ensemble predicts by majority vote; hence, it predicts false only when both L2 and L3 predict false.
I used this method in practice many times, and it is very useful.
It also has a theoretical justification so all fronts are covered.
","Dealing with imbalanced datasets, where one class significantly outnumbers others, is a common challenge. To address this, consider a method inspired by Schapire's boosting algorithm.

The algorithm involves training three weak learners (L1, L2, L3) recursively. In the first step, L1 is trained on the original dataset. L2 is trained on a balanced dataset where L1 has a 50% chance of being correct. L3 is trained on cases where L1 and L2 disagree.

In the context of an imbalanced dataset with binary classes, where the majority is classified as true, L1 can always predict true. L2 is trained on a balanced dataset, making it more accurate for the minority class. L3 is trained to capture cases where L2 predicts false.

The output is determined by majority vote, so the ensemble predicts false only when both L2 and L3 predict false. This approach improves classification by leveraging the complementary strengths of the different learners. It has been effectively used in practice and has a theoretical basis for its effectiveness."
Understand the idea of margin in contrastive loss for siamese networks,https://stats.stackexchange.com/questions/555954/understand-the-idea-of-margin-in-contrastive-loss-for-siamese-networks,neural-networks|loss-functions|siamese,555954,True,559537,5273,3,8,1695379145,1639413880,2,1641545246,"If you look at the Loss it is composed of two parts, 1st the loss when they are similar and second the loss when they are dissimilar.

If model works very well, observation which are similar should have very small distance. Using Sqaure distance in first ensure that model is penalized if its gives high distance for similar observation (classes)

For Dissimilar observation, if two class are seprated well enough defined by Margin then the error contirbution is zero. Lets look at the example below :
Suppose we choose margin to be 10, and we have two observation where distance between them is 19,1 lets see how loss looks like
    Error Observation 1 = max(0,10-19)^2   = 0  - No Contribution to error
    Error Observation 2 = max(0,10-1)^2    = 81  - High Contribution to error



As objective is to minimize loss, Observation 2 which is difficult to embed would be contibuting to loss so model can optimise for that
So margin ensure observation which have was well separated i.e. Distance Greater than Margin their contribution to error is zero. So optimisation algorithm can concentrate on seprating difficult Data Point in above example Observation 2. Thus Margin Helps on optimisation to embed difficult to seprate points
","The loss function in machine learning consists of two components: similarity loss and dissimilarity loss. Similarity loss penalizes the model for assigning high distances to similar data points, ensuring accurate representation of similar data classes.

Dissimilarity loss, on the other hand, is zero for data points sufficiently separated by a margin. This mechanism ensures that the model focuses on optimizing the embedding of difficult-to-separate data points, rather than wasting effort on well-separated data.

The margin parameter defines the separation threshold, ensuring that data points with distances exceeding the margin have no contribution to the loss. This optimization strategy allows the model to prioritize the embedding of difficult data points, improving overall accuracy."
"When the data set size is not a multiple of the mini-batch size, should the last mini-batch be smaller, or contain samples from other batches?",https://stats.stackexchange.com/questions/187591/when-the-data-set-size-is-not-a-multiple-of-the-mini-batch-size-should-the-last,neural-networks|deep-learning|gradient-descent,187591,True,188044,6461,1,8,1450885686,1450579645,12,1450885686,"Same number, otherwise you're putting more weight on the samples in the final minibatch (unless you scale down the learning weight to match the smaller size).  
Adding random samples from the training set should be fine too (as long as your sampling pool includes the runt minibatch), since each sample has an equal chance of being seen twice in an epoch.
Or just do a modulo and grab samples from the beginning again.  
In practice, it probably doesn't matter much.  
","**Summary:**

When dividing a dataset into minibatches, it's important to ensure that each sample has an equal chance of being seen in an epoch. This can be achieved by:

* Maintaining the same number of samples in each minibatch.
* Randomly sampling from the training set, as long as the sampling pool includes the smallest minibatch.
* Using a modulo operation to wrap around and start sampling from the beginning again.

In practice, the method used to handle minibatches with different sizes is not generally critical for model performance."
Interpreting the neural network output in R?,https://stats.stackexchange.com/questions/44331/interpreting-the-neural-network-output-in-r,r|neural-networks,44331,True,44624,13471,2,8,1405628110,1353769222,7,1354124888,"Your interpretation looks correct.  You can check it yourself by calling predict on some data and comparing your calculations to predict.  I first did this in a spreadsheet, and then I calculated an R neural network using metaprogramming (https://heuristically.wordpress.com/2012/10/04/nnet2sas-supports-centering-and-scaling/).
By the way, the R package neuralnet draws nice diagrams, but apparently it supports only regression (not classification?).
","**Summary:**

The author presented a custom interpretation of a machine learning model and suggested verifying its accuracy by comparing its predictions to the predictions made by the original model. The author utilized a spreadsheet and an R neural network to calculate and compare the predictions.

Additionally, the author mentioned the R package ""neuralnet,"" which can create visualizations of the neural network model but is limited to regression tasks and does not support classification."
Machine learning on non-fixed-length sequential data?,https://stats.stackexchange.com/questions/326222/machine-learning-on-non-fixed-length-sequential-data,machine-learning|neural-networks|sequential-pattern-mining,326222,True,326227,4745,2,8,1517480944,1517478799,6,1517480944,"It seems you are asking two questions here:

How to deal with the situation where different samples have different numbers of features, i.e. when some features are either not applicable to some samples or are not available
How to perform supervised classification on time-series data

With regards to question 1, it depends. Each sample does need to have the same number of features. Some models, i.e. decision-tree based ones, can explicitly deal with missing/NA data. Others, like logistic regression, need ordinal features and cannot deal with categorical features. In this case, it may be worth introducing additional binary features (representing whether feature X is present/applicable), and choosing some appropriate value for feature X in case it is missing / not applicable. A good choice would depend on the specific problem.
Question 2: you have a choice of manually engineering features, or trying a model that can attempt to deal with the temporal structure of your data automatically. Most models assume that each sample is independent of the others; ideally, you would apply some feature engineering to make your time series stationary and use your domain knowledge to decide what historical data is important for each sample and how it should be represented. Z-scores, moving averages, variances etc. could all be useful here. If you have a lot of data, you may attempt to use RNNs, but in my experience it is only worth it if you have a lot of data and you otherwise have no intuition about which features may be useful.
Regardless of which model you choose to use, setting up appropriate validation and testing frameworks is absolutely crucial. With time series you need to be extra careful. E.g. you need to decide if using data from the future to train your model is appropriate, whether you need to throw some data around your training set away etc. Do not just blindly randomly sample data into validation/test sets, this will likely give you wildly biased estimates that will not be useful.
I would also recommend researching each question independently, both have been addressed on this stackexchange before. Good luck!
","**Summary:**

1. **Handling Samples with Different Features:**
   - Some models can handle missing data (e.g., decision trees), while others require all features to be present (e.g., logistic regression).
   - Consider creating binary features to indicate feature presence and imputing missing values with appropriate constants.

2. **Supervised Classification on Time-Series Data:**
   - Feature engineering is recommended to make time series stationary and identify relevant historical information.
   - Z-scores, moving averages, and variances can be useful feature engineering techniques.
   - RNNs can be considered if there is a large amount of data and limited domain knowledge about useful features.

3. **Validation and Testing:**
   - Setting up appropriate validation and testing frameworks is crucial for time series data.
   - Avoid randomly sampling data for testing, as it can lead to biased estimates.
   - Consider the appropriateness of using future data for training and the need to discard data around the training set."
What&#39;s the receptive field of a stack of dilated convolutions?,https://stats.stackexchange.com/questions/265462/whats-the-receptive-field-of-a-stack-of-dilated-convolutions,machine-learning|neural-networks|deep-learning|convolution,265462,True,267752,5134,2,8,1660554619,1488663985,8,1489621169,"I think it should be 1024*3.
After the first block, the indices of the receptive fields of the outputs should be 1-1024, 2-1025, 3-1026, etc. (assuming no padding, but receptive field size should be same with padding anyways).
When you make the second block with a receptive field size of 1024, the first output of that block will ""see"" the outputs that had receptive field indices 1-1024, 2-1025, ... 1024-2048. So its receptive field covers 1-2048. So each block just adds 1024 to the overall receptive field size I think.
In general, I think the formula for the receptive field size s of a layer l  should be:
$s_{l_0} = 1$
$s_{l_i}=s_{l_i} + (kernel size - 1) \cdot dilationfactor$
If this is correct, their kernel size seems to be 2 (to arrive at 1024 receptive field size), which is a bit surprising, I hope it is not due to some fault of my logic :)
Stacking of the blocks might be also more useful to refine outputs at a more finegrained level after having processed larger receptive fields in the previous block, rather than just maximally increasing receptive field size.
","**Summary:**

The passage discusses the receptive field size of a neural network architecture, particularly for a set of stacked blocks. The author proposes a formula for calculating the receptive field size ($s_l$) of layer $l$:

- $s_{l_0} = 1$
- $s_{l_i}=s_{l_i} + (kernel size - 1) \times dilationfactor$

The author suggests that the receptive field size increases by the kernel size minus 1, multiplied by the dilation factor, for each subsequent layer. They also note that the kernel size in the proposed architecture appears to be 2, which differs from expectations.

The author further suggests that stacking blocks could serve to refine outputs at a more detailed level, rather than solely expanding the receptive field size. This additional refinement could enhance the model's ability to handle complex tasks."
Neural network over-fitting,https://stats.stackexchange.com/questions/131233/neural-network-over-fitting,neural-networks|overfitting,131233,True,131234,8735,1,8,1420447566,1420446949,11,1420447566,"Overfitting is not only when test error increases with iterations. We say that there is overfitting when the performance on test set is much lower than the performance on train set (because the model fits too much to seen data, and do not generalize well). 
In your second plot we can see that performances on test sets are almost 10 times lower than performances on train sets, which can be considered as overfitting.
It's almost always the case that a model performs better on the training set than on test set, since the model has already seen the data. However, a good model should be able to generalize well on unseen data, and then to reduce the gap between performances on train and test sets.
Your first example of overfitting can be solved by early stopping for example. Your second example can be solved by regularization, by corrupting input, etc.
","Overfitting occurs when a model's performance on unseen data (test set) is significantly lower than its performance on data it has been trained on (train set). This happens when the model learns specific details of the training data too closely, leading to poor generalization to new data.

There are two main types of overfitting:

* **Moderate overfitting:** The performance on the test set is worse than on the train set, but it remains within a reasonable range. This can often be solved with techniques like early stopping, which stops training when the model starts overfitting.
* **Severe overfitting:** The performance on the test set is dramatically lower than on the train set, often by orders of magnitude. This requires more drastic measures like regularization or data augmentation to address.

To avoid overfitting, models should be able to perform well on both the train and test sets, with a relatively small gap between the two."
"Differences between Multi-layer NN, Hopfield, Helmholtz and Boltzmann machines",https://stats.stackexchange.com/questions/82844/differences-between-multi-layer-nn-hopfield-helmholtz-and-boltzmann-machines,neural-networks,82844,True,82848,4589,1,8,1392916720,1390238173,11,1390240950,"Multilayer NN (MLP) and Hopfield networks are deterministic networks. Concretely, the first can be shown to estimate the conditional average on the target data. For details you may have a look at Bishop's book (https://my315344.sapbydesign.com/sap/public/byd/runtime?logoff=1) on neural networks.
The Hopfield is a deterministic recurrent neural network. Deterministic because once the initial state is given, its dynamics evolves following a Lyapunov function (http://en.wikipedia.org/wiki/Lyapunov_function). See papers by Hopfield and Tank (http://www.ncbi.nlm.nih.gov/pubmed/4027280). It has been shown that it can solve combinatorial problems and learn time series.
Helmholtz and Boltzmann machines are stochastic networks, meaning that given an input, the state of the network does not converge to a unique state, but to an ensemble distribution. A probability distribution of the state of the neural network. They are the stochastic equivalent of the Hopfield network.
One can actually prove that in the limit of absolute zero, $T \rightarrow 0$, the Boltzmann machine reduces to the Hopfield model.
You may look at the early papers by Hinton on the topic to see the basic differences, and the new ones to understand how to make them work.
Also, the Boltzmann and Helmholtz machines are strongly related to Markov Random Fields and Conditional Random Fields, as explained here (http://www.learning.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf) and here (http://www.gatsby.ucl.ac.uk/~dayan/papers/d2000a.pdf). This leads to development of algorithms for inference that can be applied to both kinds of models, as for example fractional belief propagation (http://books.nips.cc/papers/files/nips15/LT16.pdf).
","**Summary:**

Multilayer Neural Networks (MLPs) and Hopfield networks are deterministic, meaning they produce a consistent output for the same input. MLPs approximate conditional averages, while Hopfield networks solve combinatorial problems and learn time series through a deterministic dynamic process.

Helmholtz and Boltzmann machines, in contrast, are stochastic, meaning they produce a probability distribution of states rather than a single state. They are equivalent to Hopfield networks at absolute zero.

Boltzmann and Helmholtz machines are related to Markov Random Fields and Conditional Random Fields, which have led to the development of inference algorithms like fractional belief propagation that can be applied to both types of models."
On what tasks does neuroevolution outperform basic application of neural networks or genetic algorithms?,https://stats.stackexchange.com/questions/23486/on-what-tasks-does-neuroevolution-outperform-basic-application-of-neural-network,neural-networks|genetic-algorithms,23486,True,23487,1184,1,8,1328774681,1328743058,11,1328774681,"This has been researched for 20 years or so, and there are many papers claiming to outperform backpropagation. Xin Yao did a lot of work on this in the 1990s, and Kenneth Stanley created one of the currently most active frameworks, NEAT (NeuroEvolution of Augmenting Topologies (see http://www.cs.ucf.edu/~kstanley/neat.html (http://www.cs.ucf.edu/~kstanley/neat.html) and http://tech.groups.yahoo.com/group/neat/ (http://tech.groups.yahoo.com/group/neat/)).
There's a lot of published material on different neuroevolutionary techniques, but these references may be useful in getting a feel for progress over the years:

Azzini, A., Tettamanzi, A. (2008) 'Evolving Neural Networks for
Static Single-Position Automated Trading', Journal of Artiï¬cial
Evolution and Applications, Volume 2008, Article ID 184286
Hintz, K.J., Spofford, J.J. (1990) 'Evolving a Neural Network',
Proceedings, 5th IEEE International Symposium on Intelligent
Control, pp. 479-484
Miller, G.F., Todd, P.M., Hedge, S.U. (1989) 'Designing neural
networks using genetic algorithms', Proceedings of the Third
International Conference on Genetic Algorithms
Montana, D.J. (1995) 'Neural Network Weight Selection Using Genetic
Algorithms', Intelligent Hybrid Systems
Yao, X. (1993) 'Evolutionary artificial neural networks',
International Journal of Neural Systems, Vol. 4, No. 3, pp. 203-222

","**Summary:**

Neuroevolution is a field of artificial intelligence that employs evolutionary algorithms to train neural networks. It has been studied for over two decades, with various techniques developed to surpass the popular backpropagation method.

Xin Yao and Kenneth Stanley have made significant contributions to the field in the 1990s, with Stanley's NEAT (NeuroEvolution of Augmenting Topologies) framework being a prominent tool today.

Extensive research has been conducted on neuroevolution techniques, with published material available to trace its progress. Some key references include:

* Azzini and Tettamanzi (2008): Evolving neural networks for automated trading
* Hintz and Spofford (1990): Evolving a neural network
* Miller et al. (1989): Using genetic algorithms to design neural networks
* Montana (1995): Genetic algorithms for neural network weight selection
* Yao (1993): Evolutionary artificial neural networks"

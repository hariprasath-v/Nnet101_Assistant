q_title,q_link,q_tags,q_question_id,q_is_answered,q_accepted_answer_id,q_view_count,q_answer_count,q_score,q_last_activity_date,q_creation_date,a_score,a_creation_date,a_answer,llm_answer_summary
How to choose the number of hidden layers and nodes in a feedforward neural network?,https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw,"['model-selection', 'neural-networks']",181,True,1097,1145532,10,820,1661947755,1279584902,671,1280715630,"I realize this question has been answered, but I don't think the extant answer really engages the question beyond pointing to a link generally related to the question's subject matter. In particular, the link describes one technique for programmatic network configuration, but that is not a ""[a] standard and accepted method"" for network configuration.
By following a small set of clear rules, one can programmatically set a competent network architecture (i.e., the number and type of neuronal layers and the number of neurons comprising each layer). Following this schema will give you a competent architecture but probably not an optimal one.
But once this network is initialized, you can iteratively tune the configuration during training using a number of ancillary algorithms; one family of these works by pruning nodes based on (small) values of the weight vector after a certain number of training epochs--in other words, eliminating unnecessary/redundant nodes (more on this below).
So every NN has three types of layers: input, hidden, and output.
Creating the NN architecture, therefore, means coming up with values for the number of layers of each type and the number of nodes in each of these layers.
The Input Layer
Simple--every NN has exactly one of them--no exceptions that I'm aware of.
With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term.
The Output Layer
Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.
Is your NN going to run in Machine Mode or Regression Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing)? Machine mode: returns a class label (e.g., ""Premium Account""/""Basic Account""). Regression Mode returns a value (e.g., price).
If the NN is a regressor, then the output layer has a single node.
If the NN is a classifier, then it also has a single node unless softmax is used
in which case the output layer has one node per class label in your model.
The Hidden Layers
So those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leaves the hidden layers.
How many hidden layers? Well, if your data is linearly separable (which you often know by the time you begin coding a NN), then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job.
Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ (http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) for an excellent summary (http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.
So what about the size of the hidden layer(s)--how many neurons? There are some empirically derived rules of thumb; of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, the author of Introduction to Neural Networks in Java (https://www.heatonresearch.com/book/), offers a few more.
In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) the number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers. 
Optimization of the Network Configuration
Pruning describes a set of techniques to trim network size (by nodes, not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look at weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training, then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on network architecture, err on the side of more neurons, if you add a pruning step.
Put another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single ""up-front"" (such as a genetic-algorithm-based algorithm), I don't know, though I do know that for now, this two-step optimization is more common.
","**Network Configuration in Neural Networks**

Neural networks require network configuration, which involves determining the number and types of layers and the number of neurons within each layer.

**Standard Method:**

* Initialize a competent network architecture using a set of rules that determine the number and size of input, hidden, and output layers.

**Optimization:**

* Once initialized, the network configuration can be iteratively tuned during training using pruning techniques.
* Pruning eliminates unnecessary nodes based on their low weight values.

**Layer Configuration:**

* Input layer: Number of neurons determined by the number of features in the training data.
* Output layer: Number of neurons determined by the model configuration (classifier vs. regressor).
* Hidden layers: Typically one hidden layer is sufficient, and its size can be estimated as the mean of the input and output layer sizes.

**Optimization Process:**

* Initialize with a larger network configuration to allow for pruning.
* Apply a pruning algorithm during training to identify and remove redundant nodes.
* This two-step optimization approach is commonly used to approach optimal network configuration."
What should I do when my neural network doesn&#39;t learn?,https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn,"['neural-networks', 'faq']",352036,True,352037,365347,9,368,1701358003,1529367960,455,1529367960,"1.  Verify that your code is bug free
There's a saying among writers that ""All writing is re-writing"" -- that is, the greater part of writing is revising. For programmers (or at least data scientists) the expression could be re-phrased as ""All coding is debugging.""
Any time you're writing code, you need to verify that it works as intended. The best method I've ever found for verifying correctness is to break your code into small segments, and verify that each segment works. This can be done by comparing the segment output to what you know to be the correct answer. This is called unit testing (https://en.wikipedia.org/wiki/Unit_testing). Writing good unit tests is a key piece of becoming a good statistician/data scientist/machine learning expert/neural network practitioner. There is simply no substitute.
You have to check that your code is free of bugs before you can tune network performance! Otherwise, you might as well be re-arranging deck chairs on the RMS Titanic.
There are two features of neural networks that make verification even more important than for other types of machine learning or statistical models.

Neural networks are not ""off-the-shelf"" algorithms in the way that random forest or logistic regression are. Even for simple, feed-forward networks, the onus is largely on the user to make numerous decisions about how the network is configured, connected, initialized and optimized. This means writing code, and writing code means debugging.

Even when a neural network code executes without raising an exception, the network can still have bugs! These bugs might even be the insidious kind for which the network will train, but get stuck at a sub-optimal solution, or the resulting network does not have the desired architecture. (This is an example of the difference between a syntactic and semantic error (https://web.archive.org/web/20161201151434/https://wci.llnl.gov/codes/basis/manual/node53.html).)


This Medium post, ""How to unit test machine learning code (https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765),"" by Chase Roberts discusses unit-testing for machine learning models in more detail. I borrowed this example of buggy code from the article:
def make_convnet(input_image):
    net = slim.conv2d(input_image, 32, [11, 11], scope=""conv1_11x11"")
    net = slim.conv2d(input_image, 64, [5, 5], scope=""conv2_5x5"")
    net = slim.max_pool2d(net, [4, 4], stride=4, scope='pool1')
    net = slim.conv2d(input_image, 64, [5, 5], scope=""conv3_5x5"")
    net = slim.conv2d(input_image, 128, [3, 3], scope=""conv4_3x3"")
    net = slim.max_pool2d(net, [2, 2], scope='pool2')
    net = slim.conv2d(input_image, 128, [3, 3], scope=""conv5_3x3"")
    net = slim.max_pool2d(net, [2, 2], scope='pool3')
    net = slim.conv2d(input_image, 32, [1, 1], scope=""conv6_1x1"")
    return net

Do you see the error? Many of the different operations are not actually used because previous results are over-written with new variables. Using this block of code in a network will still train and the weights will update and the loss might even decrease -- but the code definitely isn't doing what was intended.  (The author is also inconsistent about using single- or double-quotes but that's purely stylistic.)
The most common programming errors pertaining to neural networks are

Variables are created but never used (usually because of copy-paste errors);
Expressions for gradient updates are incorrect;
Weight updates are not applied;
Loss functions are not measured on the correct scale (https://stats.stackexchange.com/questions/544337/same-loss-and-accuracy-on-epochs/544381#544381) (for example, cross-entropy loss can be expressed in terms of probability or logits)
The loss is not appropriate for the task (for example, using categorical cross-entropy loss for a regression task).
Dropout is used during testing, instead of only being used for training. (https://stats.stackexchange.com/questions/551144/why-am-i-getting-different-results-on-a-prediction-using-the-same-keras-model-an/551158#551158)
Make sure you're minimizing the loss function $L(x)$, instead of minimizing $-L(x)$ (https://stats.stackexchange.com/questions/562374/implementing-a-vae-in-pytorch-extremely-negative-training-loss/562402#562402).
Make sure your loss is computed correctly (https://stats.stackexchange.com/questions/565956/confused-with-binary-crossentrophy-vs-categorical-crossentropy/566066#566066).

Unit testing is not just limited to the neural network itself. You need to test all of the steps that produce or transform data and feed into the network. Some common mistakes here are

NA or NaN or Inf values in your data creating NA or NaN or Inf values in the output, and therefore in the loss function.
Shuffling the labels independently from the samples (for instance, creating train/test splits for  the labels and samples separately);
Accidentally assigning the training data as the testing data;
When using a train/test split, the model references the original, non-split data instead of the training partition or the testing partition.
Forgetting to scale the testing data;
Scaling the testing data using the statistics of the test partition instead of the train partition;
Forgetting to un-scale the predictions (e.g. pixel values are in [0,1] instead of [0, 255]).
Here's an example of a question where the problem appears to be one of model configuration or hyperparameter choice, but actually the problem was a subtle bug in how gradients were computed. Is this drop in training accuracy due to a statistical or programming error? (https://stats.stackexchange.com/questions/527045/drop-in-training-accuracy#comment970055_527045)

2. For the love of all that is good, scale your data
The scale of the data can make an enormous difference on training. Sometimes, networks simply won't reduce the loss if the data isn't scaled. Other networks will decrease the loss, but only very slowly. Scaling the inputs (and certain times, the targets) can dramatically improve the network's training.

Prior to presenting data to a neural network, standardizing the data to have 0 mean and unit variance, or to lie in a small interval like $[-0.5, 0.5]$ can improve training. This amounts to pre-conditioning, and removes the effect that a choice in units has on network weights. For example, length in millimeters and length in kilometers both represent the same concept, but are on different scales. The exact details of how to standardize the data depend on what your data look like.


Data normalization and standardization in neural networks (https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks)

Why does $[0,1]$ scaling dramatically increase training time for feed forward ANN (1 hidden layer)? (https://stats.stackexchange.com/questions/364735/why-does-0-1-scaling-dramatically-increase-training-time-for-feed-forward-an/364776#364776)




Batch or Layer normalization can improve network training. Both seek to improve the network by keeping a running mean and standard deviation for neurons' activations as the network trains. It is not well-understood why this helps training, and remains an active area of research.

""Understanding Batch Normalization (https://arxiv.org/abs/1806.02375v1)"" by Johan Bjorck, Carla Gomes, Bart Selman
""Towards a Theoretical Understanding of Batch Normalization (https://arxiv.org/abs/1805.10694v1)"" by Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, Thomas Hofmann
""How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) (https://arxiv.org/abs/1805.11604v2)"" by Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry



3. Crawl Before You Walk; Walk Before You Run
Wide and deep neural networks, and neural networks with exotic wiring, are the Hot Thing right now in machine learning. But these networks didn't spring fully-formed into existence; their designers built up to them from smaller units. First, build a small network with a single hidden layer and verify that it works correctly. Then incrementally add additional model complexity, and verify that each of those works as well.

Too few neurons in a layer can restrict the representation that the network learns, causing under-fitting. Too many neurons can cause over-fitting because the network will ""memorize"" the training data.
Even if you can prove that there is, mathematically, only a small number of neurons necessary to model a problem, it is often the case that having ""a few more"" neurons makes it easier for the optimizer to find a ""good"" configuration. (But I don't think anyone fully understands why this is the case.) I provide an example of this in the context of the XOR problem here: Aren't my iterations needed to train NN for XOR with MSE < 0.001 too high? (https://stats.stackexchange.com/questions/351216/arent-my-iterations-needed-to-train-nn-for-xor-with-mse-0-001-too-high/351713#351713).

Choosing the number of hidden layers lets the network learn an abstraction from the raw data. Deep learning is all the rage these days, and networks with a large number of layers have shown impressive results. But adding too many hidden layers can make risk overfitting or make it very hard to optimize the network.

Choosing a clever network wiring can do a lot of the work for you. Is your data source amenable to specialized network architectures? Convolutional neural networks can achieve impressive results on ""structured"" data sources, image or audio data. Recurrent neural networks can do well on sequential data types, such as natural language or time series data. Residual connections can improve deep feed-forward networks.


4. Neural Network Training Is Like Lock Picking
To achieve state of the art, or even merely good, results, you have to set up all of the parts configured to work well together. Setting up a neural network configuration that actually learns is a lot like picking a lock: all of the pieces have to be lined up just right. Just as it is not sufficient to have a single tumbler in the right place, neither is it sufficient to have only the architecture, or only the optimizer, set up correctly.
Tuning configuration choices is not really as simple as saying that one kind of configuration choice (e.g. learning rate) is more or less important than another (e.g. number of units), since all of these choices interact with all of the other choices, so one choice can do well in combination with another choice made elsewhere.
This is a non-exhaustive list of the configuration options which are not also regularization options or numerical optimization options.
All of these topics are active areas of research.

The network initialization is often overlooked as a source of neural network bugs. Initialization over too-large an interval can set initial weights too large, meaning that single neurons have an outsize influence over the network behavior.

The key difference between a neural network and a regression model is that a neural network is a composition of many nonlinear functions, called activation functions. (See: What is the essential difference between neural network and linear regression (https://stats.stackexchange.com/questions/259950/what-is-the-essential-difference-between-neural-network-and-linear-regression))
Classical neural network results focused on sigmoidal activation functions (logistic or $\tanh$ functions). A recent result has found that ReLU (or similar) units tend to work better because the have steeper gradients, so updates can be applied quickly. (See: Why do we use ReLU in neural networks and how do we use it? (https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)) One caution about ReLUs is the ""dead neuron"" phenomenon, which can stymie learning; leaky relus and similar variants avoid this problem. See

Why can't a single ReLU learn a ReLU? (https://stats.stackexchange.com/questions/379884/why-cant-a-single-relu-learn-a-relu)

My ReLU network fails to launch (https://stats.stackexchange.com/questions/188040/my-relu-network-fails-to-launch/)


There are a number of other options. See: Comprehensive list of activation functions in neural networks with pros/cons (https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)

Residual connections are a neat development that can make it easier to train neural networks. ""Deep Residual Learning for Image Recognition"" (https://arxiv.org/abs/1512.03385)
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun In: CVPR. (2016). Additionally, changing the order of operations within the residual block can further improve the resulting network. ""Identity Mappings in Deep Residual Networks (https://arxiv.org/pdf/1603.05027v3.pdf)"" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

5. Non-convex optimization is hard
The objective function of a neural network is only convex when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem.
In all other cases, the optimization problem is non-convex, and non-convex optimization is hard. The challenges of training neural networks are well-known (see: Why is it hard to train deep neural networks? (https://stats.stackexchange.com/questions/262750/why-is-it-hard-to-train-deep-neural-networks)). Additionally, neural networks have a very large number of parameters, which restricts us to solely first-order methods (see: Why is Newton's method not widely used in machine learning? (https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning)). This is a very active area of research.

Setting the learning rate too large will cause the optimization to diverge, because you will leap from one side of the ""canyon"" to the other. Setting this too small will prevent you from making any real progress, and possibly allow the noise inherent in SGD to overwhelm your gradient estimates. See:

How can change in cost function be positive? (https://stats.stackexchange.com/questions/364360/how-can-change-in-cost-function-be-positive/364366#364366)


Gradient clipping re-scales the norm of the gradient if it's above some threshold. I used to think that this was a set-and-forget parameter, typically at 1.0, but I found that I could make an LSTM language model dramatically better by setting it to 0.25. I don't know why that is.

Learning rate scheduling can decrease the learning rate over the course of training. In my experience, trying to use scheduling is a lot like regex (https://blog.codinghorror.com/regular-expressions-now-you-have-two-problems/): it replaces one problem (""How do I get learning to continue after a certain epoch?"") with two problems (""How do I get learning to continue after a certain epoch?"" and ""How do I choose a good schedule?""). Other people insist that scheduling is essential. I'll let you decide.

Choosing a good minibatch size can influence the learning process indirectly, since a larger mini-batch will tend to have a smaller variance (law-of-large-numbers (/questions/tagged/law-of-large-numbers)) than a smaller mini-batch. You want the mini-batch to be large enough to be informative about the direction of the gradient, but small enough that SGD can regularize your network.

There are a number of variants on stochastic gradient descent which use momentum, adaptive learning rates, Nesterov updates and so on to improve upon vanilla SGD. Designing a better optimizer is very much an active area of research. Some examples:

No change in accuracy using Adam Optimizer when SGD works fine (https://stats.stackexchange.com/questions/313278/no-change-in-accuracy-using-adam-optimizer-when-sgd-works-fine)
How does the Adam method of stochastic gradient descent work? (https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work/220563#comment661981_220563)
Why does momentum escape from a saddle point in this famous image? (https://stats.stackexchange.com/questions/308835/why-does-momentum-escape-from-a-saddle-point-in-this-famous-image)


When it first came out, the Adam optimizer generated a lot of interest. But some recent research has found that SGD with momentum can out-perform adaptive gradient methods for neural networks. ""The Marginal Value of Adaptive Gradient Methods in Machine Learning (https://arxiv.org/abs/1705.08292)"" by Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht

But on the other hand, this very recent paper proposes a new adaptive learning-rate optimizer which supposedly closes the gap between adaptive-rate methods and SGD with momentum. ""Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks (https://arxiv.org/abs/1806.06763v1)"" by Jinghui Chen, Quanquan Gu

Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes ""over adapted"". We design a new algorithm, called Partially adaptive momentum estimation method (Padam), which unifies the Adam/Amsgrad with SGD to achieve the best from both worlds. Experiments on standard benchmarks show that Padam can maintain fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.


Specifically for triplet-loss (/questions/tagged/triplet-loss) models, there are a number of tricks which can improve training time and generalization. See: In training a triplet network, I first have a solid drop in loss, but eventually the loss slowly but consistently increases. What could cause this? (https://stats.stackexchange.com/questions/475655/in-training-i-first-have-a-solid-drop-in-loss-but-eventually-the-loss-slowly-b)


6. Regularization
Choosing and tuning network regularization is a key part of building a model that generalizes well (that is, a model that is not overfit to the training data). However, at the time that your network is struggling to decrease the loss on the training data -- when the network is not learning -- regularization can obscure what the problem is.
When my network doesn't learn, I turn off all regularization and verify that the non-regularized network works correctly. Then I add each regularization piece back, and verify that each of those works along the way.
This tactic can pinpoint where some regularization might be poorly set. Some examples are

$L^2$ regularization (aka weight decay) or $L^1$ regularization is set too large, so the weights can't move.

Two parts of regularization are in conflict. For example, it's widely observed that layer normalization and dropout are difficult to use together. Since either on its own is very useful, understanding how to use both is an active area of research.

""Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift (https://arxiv.org/abs/1801.05134v1)"" by Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang
""Adjusting for Dropout Variance in Batch Normalization and Weight Initialization (https://arxiv.org/abs/1607.02488v2)"" by Dan Hendrycks, Kevin Gimpel.
""Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515)"" by Günter Klambauer, Thomas Unterthiner, Andreas Mayr and Sepp Hochreiter



7. Keep a Logbook of Experiments
When I set up a neural network, I don't hard-code any parameter settings. Instead, I do that in a configuration file (e.g., JSON) that is read and used to populate network configuration details at runtime. I keep all of these configuration files. If I make any parameter modification, I make a new configuration file. Finally, I append as comments all of the per-epoch losses for training and validation.
The reason that I'm so obsessive about retaining old results is that this makes it very easy to go back and review previous experiments. It also hedges against mistakenly repeating the same dead-end experiment. Psychologically, it also lets you look back and observe ""Well, the project might not be where I want it to be today, but I am making progress compared to where I was $k$ weeks ago.""
As an example, I wanted to learn about LSTM language models, so I decided to make a Twitter bot that writes new tweets in response to other Twitter users. I worked on this in my free time, between grad school and my job. It took about a year, and I iterated over about 150 different models before getting to a model that did what I wanted: generate new English-language text that (sort of) makes sense. (One key sticking point, and part of the reason that it took so many attempts, is that it was not sufficient to simply get a low out-of-sample loss, since early low-loss models had managed to memorize the training data, so it was just reproducing germane blocks of text verbatim in reply to prompts -- it took some tweaking to make the model more spontaneous and still have low loss.)
","**Summary:**

Building neural networks requires rigorous debugging and verification.

**1. Unit Testing:**

* Unit testing is essential for detecting bugs in small code segments.
* Common neural network bugs include variable misuse, incorrect gradient updates, and incorrect loss function implementation.
* Test all data transformations and model components.

**2. Data Scaling:**

* Standardizing data to zero mean and unit variance or a small interval improves training.
* Batch or Layer normalization may also enhance network performance.

**3. Incremental Network Design:**

* Start with simple networks and gradually increase complexity.
* Too few or too many neurons can hinder learning.
* Choose an appropriate number of hidden layers and network architecture.

**4. Configuration Optimization:**

* Neural network training involves a delicate balance between many configuration choices.
* These choices interact and impact the overall performance.
* Initialize weights carefully, select appropriate activation functions, and explore residual connections.

**5. Non-Convex Optimization Challenges:**

* Neural network optimization is non-convex and difficult.
* Gradient clipping and learning rate scheduling can improve convergence.
* Various stochastic gradient descent variants may aid in finding optimal solutions.

**6. Regularization:**

* Regularization techniques help prevent overfitting.
* $L^2$ and $L^1$ regularization, layer normalization, and dropout are common methods.
* Disable regularization during debugging to isolate issues.

**7. Experimentation and Logging:**

* Use configuration files to keep track of parameter settings and results.
* Iterate through multiple models and record losses.
* This allows for easy comparison and identifies areas for improvement."
"What exactly are keys, queries, and values in attention mechanisms?",https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms,"['neural-networks', 'natural-language', 'attention', 'machine-translation']",421935,True,424127,260772,11,309,1708928023,1565686855,281,1567068576,"The key/value/query formulation of attention is from the paper Attention Is All You Need (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).

How should one understand the queries, keys, and values

The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).
The attention operation can be thought of as a retrieval process as well.
As mentioned in the paper you referenced (Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473)), attention by definition is just a weighted average of values,
$$c=\sum_{j}\alpha_jh_j$$
where $\sum \alpha_j=1$.
If we restrict $\alpha$ to be a one-hot vector, this operation becomes the same as retrieving from a set of elements $h$ with index $\alpha$. With the restriction removed, the attention operation can be thought of as doing ""proportional retrieval"" according to the probability vector $\alpha$.
It should be clear that $h$ in this context is the value. The difference between the two papers lies in how the probability vector $\alpha$ is calculated. The first paper (Bahdanau et al. 2015) computes the score through a neural network $$e_{ij}=a(s_i,h_j), \qquad \alpha_{i,j}=\frac{\exp(e_{ij})}{\sum_k\exp(e_{ik})}$$
where $h_j$ is from the encoder sequence, and $s_i$ is from the decoder sequence. One problem of this approach is, say the encoder sequence is of length $m$ and the decoding sequence is of length $n$, we have to go through the network $m*n$ times to acquire all the attention scores $e_{ij}$.
A more efficient model would be to first project $s$ and $h$ onto a common space, then choose a similarity measure (e.g. dot product) as the attention score, like
$$e_{ij}=f(s_i)g(h_j)^T$$
so we only have to compute $g(h_j)$ $m$ times and $f(s_i)$ $n$ times to get the projection vectors and $e_{ij}$ can be computed efficiently by matrix multiplication.
This is essentially the approach proposed by the second paper (Vaswani et al. 2017), where the two projection vectors are called query (for decoder) and key (for encoder), which is well aligned with the concepts in retrieval systems. (There are later techniques to further reduce the computational complexity, for example Reformer (https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), Linformer (https://arxiv.org/pdf/2006.04768.pdf), FlashAttention (https://arxiv.org/abs/2307.08691).)

How are the queries, keys, and values obtained

The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.

$$
\begin{align}\text{MultiHead($Q$, $K$, $V$)} & = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^{O} \\
\text{where head$_i$} &  = \text{Attention($QW_i^Q$, $KW_i^K$, $VW_i^V$)}
\end{align}$$
Where the projections are parameter matrices:
$$
\begin{align}
W_i^Q & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^K & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^V & \in \mathbb{R}^{d_\text{model} \times d_v}, \\
W_i^O & \in \mathbb{R}^{hd_v \times d_{\text{model}}}.
\end{align}$$

For unsupervised language model training like GPT (https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), $Q, K, V$ are usually from the same source, so such operation is also called self-attention.
For the machine translation task in the second paper, it first applies self-attention separately to source and target sequences, then on top of that it applies another attention where $Q$ is from the target sequence and $K, V$ are from the source sequence.
For recommendation systems, $Q$ can be from the target items, $K, V$ can be from the user profile and history.
","Attention is a retrieval process that involves weighted averages of ""values"" based on a probability vector. In the key/value/query formulation:

* **Query (Q):** Represents the search or input information.
* **Key (K):** Associated with candidate items, analogous to video titles in a search engine.
* **Value (V):** The actual item to be retrieved, e.g., videos in a search result.

In the original model, scores are calculated through neural networks, but the Vaswani et al. approach proposed a more efficient method using projection vectors and a similarity measure. This aligns with the retrieval system concept, where ""K"" and ""V"" are projected into a common space for efficient attention score computation.

The sources of queries, keys, and values vary depending on the application:

* **Self-attention:** Q, K, V from the same source (e.g., language model training).
* **Encoder-decoder attention:** Q from decoder, K, V from encoder (e.g., machine translation).
* **Recommendation systems:** Q from target items, K, V from user profiles and history."
What is batch size in neural network?,https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network,"['neural-networks', 'python', 'terminology', 'keras']",153531,True,153535,730947,6,305,1650529048,1432286121,421,1432288067,"The batch size defines the number of samples that will be propagated through the network.
For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.
Advantages of using a batch size < number of all samples:

It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.
Typically networks train faster with mini-batches. That's because we update the weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated our network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.

Disadvantages of using a batch size < number of all samples:

The smaller the batch the less accurate the estimate of the gradient will be. In the figure below, you can see that the direction of the mini-batch gradient (green color) fluctuates much more in comparison to the direction of the full batch gradient (blue color).

 (https://i.sstatic.net/lU3sx.png)
Stochastic is just a mini-batch with batch_size equal to 1. In that case, the gradient changes its direction even more often than a mini-batch gradient.
","**Batch Size: Optimization in Deep Learning**

The batch size determines the number of samples used for network training. Using a batch size less than the total number of samples offers advantages:

* **Reduced memory requirements:** Fewer samples reduce memory consumption, making it feasible to train with large datasets that cannot fit in memory.
* **Faster training:** Updating weights after each batch propagation typically speeds up training.

However, smaller batch sizes compromise the accuracy of the gradient estimate, as the direction of the gradient fluctuates more. Stochastic gradient descent (SGD) uses a batch size of 1, resulting in the most gradient direction changes.

The optimal batch size depends on the dataset and network architecture, and a balance must be struck between memory requirements, training speed, and gradient accuracy."
What are the advantages of ReLU over sigmoid function in deep neural networks?,https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks,"['machine-learning', 'neural-networks', 'sigmoid-curve', 'relu']",126238,True,126362,290838,9,234,1723495231,1417486429,205,1417567286,"Two additional major benefits of ReLUs are sparsity and a reduced likelihood of vanishing gradient. But first recall the definition of a ReLU is $h = \max(0, a)$ where $a = Wx + b$.
One major benefit is the reduced likelihood of the gradient to vanish. This arises when $a > 0$. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.
The other benefit of ReLUs is sparsity. Sparsity arises when $a \le 0$. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations.
","ReLU (Rectified Linear Unit) functions, defined as h = max(0, a), offer distinct advantages over other activation functions like Sigmoids:

**Reduced Vanishing Gradient:** When the input (a) is positive, ReLUs produce a constant gradient, unlike Sigmoids whose gradient diminishes with increasing |x|. This constant gradient accelerates learning.

**Sparsity:** When the input (a) is negative, ReLUs produce zero output. This sparsity, where many units within a layer have zero activation, is advantageous. It promotes efficient use of computational resources and leads to more effective representations compared to dense representations produced by Sigmoids."
What does 1x1 convolution mean in a neural network?,https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network,"['neural-networks', 'deep-learning', 'convolution', 'conv-neural-network']",194142,True,194450,218202,8,214,1722919050,1454643197,189,1454863550,"Suppose that I have a conv layer which outputs an $(N, F, H, W)$ shaped tensor where:

$N$ is the batch size
$F$ is the number of convolutional filters
$H, W$ are the spatial dimensions

Suppose the input is fed into a conv layer with $F_1$ 1x1 filters, zero padding and stride 1. Then the output of this 1x1 conv layer will have shape $(N, F_1, H , W)$.
So 1x1 conv filters can be used to change the dimensionality in the filter space. If $F_1 > F$ then we are increasing dimensionality, if $F_1 < F$ we are decreasing dimensionality, in the filter dimension.
Indeed, in the Google Inception article Going Deeper with Convolutions (http://arxiv.org/abs/1409.4842), they state (bold is mine, not by original authors):

One big problem with the above modules, at least in this naive form, is that even a modest number of 5x5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters.
This leads to the second idea of the proposed architecture:  judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch...1x1
convolutions are used to compute reductions before the expensive 3x3 and 5x5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose.

So in the Inception architecture, we use the 1x1 convolutional filters to reduce dimensionality in the filter dimension. As I explained above, these 1x1 conv layers can be used in general to change the filter space dimensionality (either increase or decrease) and in the Inception architecture we see how effective these 1x1 filters can be for dimensionality reduction, explicitly in the filter dimension space, not the spatial dimension space.
Perhaps there are other interpretations of 1x1 conv filters, but I prefer this explanation, especially in the context of the Google Inception architecture.
","1x1 convolutional filters are used to adjust dimensionality in the filter space, altering the number of filters rather than the spatial dimensions. They enable flexible dimensionality changes, either increasing or decreasing the number of filters.

The Google Inception architecture employs 1x1 filters to reduce dimensionality prior to larger 3x3 and 5x5 convolutions. The use of 1x1 filters as dimensionality reduction precedes the computationally expensive larger convolutions and enhances their effectiveness by optimizing filter space.

Overall, 1x1 convolutions serve as a versatile tool for dimensionality adjustment in the filter space, expanding their applications beyond traditional spatial convolutions. They provide a means to judiciously control the computational requirements of complex convolutional architectures while maintaining or enhancing their performance."
What does the hidden layer in a neural network compute?,https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute,"['machine-learning', 'neural-networks', 'nonlinear-regression']",63152,True,63163,202673,4,212,1518901377,1372780747,257,1372790462,"Three sentence version:

Each layer can apply any function you want to the previous layer (usually a linear transformation followed by a squashing nonlinearity). 
The hidden layers' job is to transform the inputs into something that the output layer can use.
The output layer transforms the hidden layer activations into whatever scale you wanted your output to be on.

Like you're 5:
If you want a computer to tell you if there's a bus in a picture, the computer might have an easier time if it had the right tools.
So your bus detector might be made of a wheel detector (to help tell you it's a vehicle) and a box detector (since the bus is shaped like a big box) and a size detector (to tell you it's too big to be a car).  These are the three elements of your hidden layer: they're not part of the raw image, they're tools you designed to help you identify busses.
If all three of those detectors turn on (or perhaps if they're especially active), then there's a good chance you have a bus in front of you.
Neural nets are useful because there are good tools (like backpropagation) for building lots of detectors and putting them together.

Like you're an adult
A feed-forward neural network applies a series of functions to the data.  The exact functions will depend on the neural network you're using: most frequently, these functions each compute a linear transformation of the previous layer, followed by a squashing nonlinearity.  Sometimes the functions will do something else (like computing logical functions in your examples, or averaging over adjacent pixels in an image).  So the roles of the different layers could depend on what functions are being computed, but I'll try to be very general.
Let's call the input vector $x$, the hidden layer activations $h$, and the output activation $y$.  You have some function $f$ that maps from $x$ to $h$ and another function $g$ that maps from $h$ to $y$.  
So the hidden layer's activation is $f(x)$ and the output of the network is $g(f(x))$.
Why have two functions ($f$ and $g$) instead of just one?
If the level of complexity per function is limited, then $g(f(x))$ can compute things that $f$ and $g$ can't do individually.  

An example with logical functions:
For example, if we only allow $f$ and $g$ to be simple logical operators like ""AND"", ""OR"", and ""NAND"", then you can't compute other functions like ""XOR"" with just one of them.  On the other hand, we could compute ""XOR"" if we were willing to layer these functions on top of each other: 
First layer functions:

Make sure that at least one element is ""TRUE"" (using OR)
Make sure that they're not all ""TRUE"" (using NAND)

Second layer function:

Make sure that both of the first-layer criteria are satisfied (using AND)

The network's output is just the result of this second function.  The first layer transforms the inputs into something that the second layer can use so that the whole network can perform XOR.

An example with images:
Slide 61 from this talk (http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/CVPR2012-Tutorial_lee.pdf)--also available here (http://youqianhaozhe.com/deep%20learning.jpg) as a single image--shows (one way to visualize) what the different hidden layers in a particular neural network are looking for.
The first layer looks for short pieces of edges in the image: these are very easy to find from raw pixel data, but they're not very useful by themselves for telling you if you're looking at a face or a bus or an elephant.
The next layer composes the edges: if the edges from the bottom hidden layer fit together in a certain way, then one of the eye-detectors in the middle of left-most column might turn on.  It would be hard to make a single layer that was so good at finding something so specific from the raw pixels: eye detectors are much easier to build out of edge detectors than out of raw pixels.
The next layer up composes the eye detectors and the nose detectors into faces.  In other words, these will light up when the eye detectors and nose detectors from the previous layer turn on with the right patterns.  These are very good at looking for particular kinds of faces: if one or more of them lights up, then your output layer should report that a face is present.
This is useful because face detectors are easy to build out of eye detectors and nose detectors, but really hard to build out of pixel intensities.
So each layer gets you farther and farther from the raw pixels and closer to your ultimate goal (e.g. face detection or bus detection).

Answers to assorted other questions
""Why are some layers in the input layer connected to the hidden layer and some are not?""
The disconnected nodes in the network are called ""bias"" nodes. There's a really nice explanation here (https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks). The short answer is that they're like intercept terms in regression.
""Where do the ""eye detector"" pictures in the image example come from?""
I haven't double-checked the specific images I linked to, but in general, these visualizations show the set of pixels in the input layer that maximize the activity of the corresponding neuron.  So if we think of the neuron as an eye detector, this is the image that the neuron considers to be most eye-like.  Folks usually find these pixel sets with an optimization (hill-climbing) procedure.
In this paper (https://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf) by some Google folks with one of the world's largest neural nets, they show a ""face detector"" neuron and a ""cat detector"" neuron this way, as well as a second way: They also show the actual images that activate the neuron most strongly (figure 3, figure 16).  The second approach is nice because it shows how flexible and nonlinear the network is--these high-level ""detectors"" are sensitive to all these images, even though they don't particularly look similar at the pixel level.

Let me know if anything here is unclear or if you have any more questions.
","Feed-forward neural networks apply functions to transform data sequentially. Each layer performs a linear transformation followed by a non-linearity. The hidden layers modify inputs to assist the output layer in generating the desired output. The functions used vary depending on the network, but commonly include logical operators or image processing functions.

Each layer's role is determined by the functions it computes. The output of a layer is the result of applying multiple functions to the input. This allows networks to represent complex relationships that cannot be expressed by a single function.

In practical applications, hidden layers are designed to detect specific features in data (e.g., edges in images). These features are then combined by subsequent layers to form more complex representations (e.g., eyes in an image). This hierarchical approach makes it easier to identify high-level patterns and perform complex tasks."
"What is the difference between a neural network and a deep neural network, and why do the deep ones work better?",https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w,"['neural-networks', 'deep-learning']",182734,True,184921,141560,11,178,1588573172,1448018730,236,1449182141,"Let's start with a triviliaty: Deep neural network is simply a feedforward network with many hidden layers.
This is more or less all there is to say about the definition. Neural networks can be recurrent or feedforward; feedforward ones do not have any loops in their graph and can be organized in layers. If there are ""many"" layers, then we say that the network is deep.
How many layers does a network have to have in order to qualify as deep? There is no definite answer to this  (it's a bit like asking how many grains make a heap (https://en.wikipedia.org/wiki/Sorites_paradox)), but usually having two or more hidden layers counts as deep. In contrast, a network with only a single hidden layer is conventionally called ""shallow"". I suspect that there will be some inflation going on here, and in ten years people might think that anything with less than, say, ten layers is shallow and suitable only for kindergarten exercises. Informally, ""deep"" suggests that the network is tough to handle.
Here is an illustration, adapted from here (http://neuralnetworksanddeeplearning.com/chap5.html):
 (https://i.sstatic.net/OH3gI.png)
But the real question you are asking is, of course, Why would having many layers be beneficial?
I think that the somewhat astonishing answer is that nobody really knows. There are some common explanations that I will briefly review below, but none of them has been convincingly demonstrated to be true, and one cannot even be sure that having many layers is really beneficial.
I say that this is astonishing, because deep learning is massively popular, is breaking all the records (from image recognition, to playing Go, to automatic translation, etc.) every year, is getting used by the industry, etc. etc. And we are still not quite sure why it works so well.
I base my discussion on the Deep Learning (http://www.deeplearningbook.org/) book by Goodfellow, Bengio, and Courville which went out in 2017 and is widely considered to be the book on deep learning. (It's freely available online.) The relevant section is 6.4.1 Universal Approximation Properties and Depth.
You wrote that 

10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a neural network is able to represent [...]

You must be referring to the so called Universal approximation theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem), proved by Cybenko in 1989 and generalized by various people in the 1990s. It basically says that a shallow neural network (with 1 hidden layer) can approximate any function, i.e. can in principle learn anything. This is true for various nonlinear activation functions, including rectified linear units that most neural networks are using today (the textbook references Leshno et al. 1993 (http://www2.math.technion.ac.il/~pinkus/papers/neural.pdf) for this result).
If so, then why is everybody using deep nets?
Well, a naive answer is that because they work better. Here is a figure from the Deep Learning book showing that it helps to have more layers in one particular task, but the same phenomenon is often observed across various tasks and domains:
 (https://i.sstatic.net/trj4L.png)
We know that a shallow network could perform as good as the deeper ones. But it does not; and they usually do not. The question is --- why? Possible answers:

Maybe a shallow network would need more neurons then the deep one?
Maybe a shallow network is more difficult to train with our current algorithms (e.g. it has more nasty local minima, or the convergence rate is slower, or whatever)?
Maybe a shallow architecture does not fit to the kind of problems we are usually trying to solve (e.g. object recognition is a quintessential ""deep"", hierarchical process)?
Something else?

The Deep Learning book argues for bullet points #1 and #3. First, it argues that the number of units in a shallow network grows exponentially with task complexity. So in order to be useful a shallow network might need to be very big; possibly much bigger than a deep network. This is based on a number of papers proving that shallow networks would in some cases need exponentially many neurons; but whether e.g. MNIST classification or Go playing are such cases is not really clear. Second, the book says this:

Choosing a deep model encodes a very general belief that the function we
  want to learn should involve composition of several simpler functions. This can be
  interpreted from a representation learning point of view as saying that we believe
  the learning problem consists of discovering a set of underlying factors of variation
  that can in turn be described in terms of other, simpler underlying factors of
  variation.

I think the current ""consensus"" is that it's a combination of bullet points #1 and #3: for real-world tasks deep architecture are often beneficial and shallow architecture would be inefficient and require a lot more neurons for the same performance.
But it's far from proven. Consider e.g. Zagoruyko and  Komodakis, 2016, Wide Residual Networks (https://arxiv.org/abs/1605.07146). Residual networks with 150+ layers appeared in 2015 (https://arxiv.org/abs/1512.03385) and won various image recognition contests. This was a big success and looked like a compelling argument in favour of deepness; here is one figure from a presentation (http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf) by the first author on the residual network paper (note that the time confusingly goes to the left here):
 (https://i.sstatic.net/iVURh.png)
But the paper linked above shows that a ""wide"" residual network with ""only"" 16 layers can outperform ""deep"" ones with 150+ layers. If this is true, then the whole point of the above figure breaks down.
Or consider Ba and Caruana, 2014, Do Deep Nets Really Need to be Deep? (https://arxiv.org/abs/1312.6184):

In this paper we provide empirical evidence that shallow nets are capable of learning the same
  function as deep nets, and in some cases with the same number of parameters as the deep nets. We
  do this by first training a state-of-the-art deep model, and then training a shallow model to mimic the
  deep model. The mimic model is trained using the model compression scheme described in the next
  section. Remarkably, with model compression we are able to train shallow nets to be as accurate
  as some deep models, even though we are not able to train these shallow nets to be as accurate as
  the deep nets when the shallow nets are trained directly on the original labeled training data. If a
  shallow net with the same number of parameters as a deep net can learn to mimic a deep net with
  high fidelity, then it is clear that the function learned by that deep net does not really have to be deep.

If true, this would mean that the correct explanation is rather my bullet #2, and not #1 or #3.
As I said --- nobody really knows for sure yet.

Concluding remarks
The amount of progress achieved in the deep learning over the last ~10 years is truly amazing, but most of this progress was achieved by trial and error, and we still lack very basic understanding about what exactly makes deep nets to work so well. Even the list of things that people consider to be crucial for setting up an effective deep network seems to change every couple of years.
The deep learning renaissance started in 2006 when Geoffrey Hinton (who had been working on neural networks for 20+ years without much interest from anybody) published a couple of breakthrough papers offering an effective way to train deep networks (Science paper (https://www.cs.toronto.edu/~hinton/science.pdf), Neural computation paper (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)). The trick was to use unsupervised pre-training before starting the gradient descent. These papers revolutionized the field, and for a couple of years people thought that unsupervised pre-training was the key. 
Then in 2010 Martens showed that deep neural networks can be trained with second-order methods (so called Hessian-free methods) and can outperform networks trained with pre-training: Deep learning via Hessian-free optimization (http://www.cs.toronto.edu/~asamir/cifar/HFO_James.pdf). Then in 2013 Sutskever et al. showed that  stochastic gradient descent with some very clever tricks can outperform  Hessian-free methods: On the importance of initialization and momentum in deep learning (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf). Also, around 2010 people realized that using rectified linear units instead of sigmoid units makes a huge difference for gradient descent. Dropout appeared in 2014. Residual networks appeared in 2015. People keep coming up with more and more effective ways to train deep networks and what seemed like a key insight 10 years ago is often considered a nuisance today. All of that is largely driven by trial and error and there is little understanding of what makes some things work so well and some other things not. Training deep networks is like a big bag of tricks. Successful tricks are usually rationalized post factum.
We don't even know why deep networks reach a performance plateau; just 10 years people used to blame local minima, but the current thinking is that this is not the point (when the perfomance plateaus, the gradients tend to stay large). This is such a basic question about deep networks, and we don't even know this.
Update: This is more or less the subject of Ali Rahimi's NIPS 2017 talk on machine learning as alchemy: https://www.youtube.com/watch?v=Qi1Yry33TQE (https://www.youtube.com/watch?v=Qi1Yry33TQE).

[This answer was entirely re-written in April 2017, so some of the comments below do not apply anymore.]
","Deep neural networks, characterized by multiple hidden layers, have revolutionized machine learning. While there is no consensus on the number of layers that define ""deep,"" networks with two or more hidden layers are generally considered deep. The benefits of depth remain somewhat elusive, but potential explanations include:

* **Increased Efficiency:** Deep architectures may require fewer neurons than shallow ones to achieve similar performance.
* **Hierarchical Representation:** Deep networks can learn hierarchical representations, breaking down complex tasks into simpler components.
* **Unsolved Mysteries:** Despite their widespread success, the exact mechanisms behind deep nets' effectiveness remain unknown.

**Historical Context:**

* Shallow networks were initially believed to be equivalent in representing functions, but experimental evidence suggests that deep architectures often perform better.
* The development of deep learning was driven by advancements in training methods (e.g., unsupervised pre-training, Hessian-free optimization, stochastic gradient descent).
* Continuous innovations in training techniques and network architectures have emerged, with each breakthrough leading to improved performance.

**Outstanding Questions:**

* Why do deep networks reach a performance plateau?
* Can shallow networks outperform deep ones with the same training methods?
* What are the fundamental limits of deep learning's capabilities?

The lack of theoretical understanding highlights the empirical nature of deep learning's progress, relying heavily on trial and error."
"What is the difference between a neural network and a deep neural network, and why do the deep ones work better?",https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w,"['neural-networks', 'deep-learning']",182734,True,184921,141560,11,178,1588573172,1448018730,236,1449182141,"Let's start with a triviliaty: Deep neural network is simply a feedforward network with many hidden layers.
This is more or less all there is to say about the definition. Neural networks can be recurrent or feedforward; feedforward ones do not have any loops in their graph and can be organized in layers. If there are ""many"" layers, then we say that the network is deep.
How many layers does a network have to have in order to qualify as deep? There is no definite answer to this  (it's a bit like asking how many grains make a heap (https://en.wikipedia.org/wiki/Sorites_paradox)), but usually having two or more hidden layers counts as deep. In contrast, a network with only a single hidden layer is conventionally called ""shallow"". I suspect that there will be some inflation going on here, and in ten years people might think that anything with less than, say, ten layers is shallow and suitable only for kindergarten exercises. Informally, ""deep"" suggests that the network is tough to handle.
Here is an illustration, adapted from here (http://neuralnetworksanddeeplearning.com/chap5.html):
 (https://i.sstatic.net/OH3gI.png)
But the real question you are asking is, of course, Why would having many layers be beneficial?
I think that the somewhat astonishing answer is that nobody really knows. There are some common explanations that I will briefly review below, but none of them has been convincingly demonstrated to be true, and one cannot even be sure that having many layers is really beneficial.
I say that this is astonishing, because deep learning is massively popular, is breaking all the records (from image recognition, to playing Go, to automatic translation, etc.) every year, is getting used by the industry, etc. etc. And we are still not quite sure why it works so well.
I base my discussion on the Deep Learning (http://www.deeplearningbook.org/) book by Goodfellow, Bengio, and Courville which went out in 2017 and is widely considered to be the book on deep learning. (It's freely available online.) The relevant section is 6.4.1 Universal Approximation Properties and Depth.
You wrote that 

10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a neural network is able to represent [...]

You must be referring to the so called Universal approximation theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem), proved by Cybenko in 1989 and generalized by various people in the 1990s. It basically says that a shallow neural network (with 1 hidden layer) can approximate any function, i.e. can in principle learn anything. This is true for various nonlinear activation functions, including rectified linear units that most neural networks are using today (the textbook references Leshno et al. 1993 (http://www2.math.technion.ac.il/~pinkus/papers/neural.pdf) for this result).
If so, then why is everybody using deep nets?
Well, a naive answer is that because they work better. Here is a figure from the Deep Learning book showing that it helps to have more layers in one particular task, but the same phenomenon is often observed across various tasks and domains:
 (https://i.sstatic.net/trj4L.png)
We know that a shallow network could perform as good as the deeper ones. But it does not; and they usually do not. The question is --- why? Possible answers:

Maybe a shallow network would need more neurons then the deep one?
Maybe a shallow network is more difficult to train with our current algorithms (e.g. it has more nasty local minima, or the convergence rate is slower, or whatever)?
Maybe a shallow architecture does not fit to the kind of problems we are usually trying to solve (e.g. object recognition is a quintessential ""deep"", hierarchical process)?
Something else?

The Deep Learning book argues for bullet points #1 and #3. First, it argues that the number of units in a shallow network grows exponentially with task complexity. So in order to be useful a shallow network might need to be very big; possibly much bigger than a deep network. This is based on a number of papers proving that shallow networks would in some cases need exponentially many neurons; but whether e.g. MNIST classification or Go playing are such cases is not really clear. Second, the book says this:

Choosing a deep model encodes a very general belief that the function we
  want to learn should involve composition of several simpler functions. This can be
  interpreted from a representation learning point of view as saying that we believe
  the learning problem consists of discovering a set of underlying factors of variation
  that can in turn be described in terms of other, simpler underlying factors of
  variation.

I think the current ""consensus"" is that it's a combination of bullet points #1 and #3: for real-world tasks deep architecture are often beneficial and shallow architecture would be inefficient and require a lot more neurons for the same performance.
But it's far from proven. Consider e.g. Zagoruyko and  Komodakis, 2016, Wide Residual Networks (https://arxiv.org/abs/1605.07146). Residual networks with 150+ layers appeared in 2015 (https://arxiv.org/abs/1512.03385) and won various image recognition contests. This was a big success and looked like a compelling argument in favour of deepness; here is one figure from a presentation (http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf) by the first author on the residual network paper (note that the time confusingly goes to the left here):
 (https://i.sstatic.net/iVURh.png)
But the paper linked above shows that a ""wide"" residual network with ""only"" 16 layers can outperform ""deep"" ones with 150+ layers. If this is true, then the whole point of the above figure breaks down.
Or consider Ba and Caruana, 2014, Do Deep Nets Really Need to be Deep? (https://arxiv.org/abs/1312.6184):

In this paper we provide empirical evidence that shallow nets are capable of learning the same
  function as deep nets, and in some cases with the same number of parameters as the deep nets. We
  do this by first training a state-of-the-art deep model, and then training a shallow model to mimic the
  deep model. The mimic model is trained using the model compression scheme described in the next
  section. Remarkably, with model compression we are able to train shallow nets to be as accurate
  as some deep models, even though we are not able to train these shallow nets to be as accurate as
  the deep nets when the shallow nets are trained directly on the original labeled training data. If a
  shallow net with the same number of parameters as a deep net can learn to mimic a deep net with
  high fidelity, then it is clear that the function learned by that deep net does not really have to be deep.

If true, this would mean that the correct explanation is rather my bullet #2, and not #1 or #3.
As I said --- nobody really knows for sure yet.

Concluding remarks
The amount of progress achieved in the deep learning over the last ~10 years is truly amazing, but most of this progress was achieved by trial and error, and we still lack very basic understanding about what exactly makes deep nets to work so well. Even the list of things that people consider to be crucial for setting up an effective deep network seems to change every couple of years.
The deep learning renaissance started in 2006 when Geoffrey Hinton (who had been working on neural networks for 20+ years without much interest from anybody) published a couple of breakthrough papers offering an effective way to train deep networks (Science paper (https://www.cs.toronto.edu/~hinton/science.pdf), Neural computation paper (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)). The trick was to use unsupervised pre-training before starting the gradient descent. These papers revolutionized the field, and for a couple of years people thought that unsupervised pre-training was the key. 
Then in 2010 Martens showed that deep neural networks can be trained with second-order methods (so called Hessian-free methods) and can outperform networks trained with pre-training: Deep learning via Hessian-free optimization (http://www.cs.toronto.edu/~asamir/cifar/HFO_James.pdf). Then in 2013 Sutskever et al. showed that  stochastic gradient descent with some very clever tricks can outperform  Hessian-free methods: On the importance of initialization and momentum in deep learning (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf). Also, around 2010 people realized that using rectified linear units instead of sigmoid units makes a huge difference for gradient descent. Dropout appeared in 2014. Residual networks appeared in 2015. People keep coming up with more and more effective ways to train deep networks and what seemed like a key insight 10 years ago is often considered a nuisance today. All of that is largely driven by trial and error and there is little understanding of what makes some things work so well and some other things not. Training deep networks is like a big bag of tricks. Successful tricks are usually rationalized post factum.
We don't even know why deep networks reach a performance plateau; just 10 years people used to blame local minima, but the current thinking is that this is not the point (when the perfomance plateaus, the gradients tend to stay large). This is such a basic question about deep networks, and we don't even know this.
Update: This is more or less the subject of Ali Rahimi's NIPS 2017 talk on machine learning as alchemy: https://www.youtube.com/watch?v=Qi1Yry33TQE (https://www.youtube.com/watch?v=Qi1Yry33TQE).

[This answer was entirely re-written in April 2017, so some of the comments below do not apply anymore.]
","**Deep Neural Networks**

Deep neural networks (DNNs) are feedforward neural networks with multiple hidden layers, typically consisting of several layers of rectified linear units. While there is no definitive answer to the number of layers that constitute a DNN, it is generally considered ""deep"" when it has more than one hidden layer.

**Benefits of Depth**

The reason for the popularity of DNNs despite their uncertain benefits is their superior performance across various tasks, including image recognition, natural language processing, and game playing.

**Possible Explanations for Enhanced Performance**

* **Composition of Simpler Functions:** DNNs enable the composition of multiple simpler functions, facilitating the discovery of hierarchical representations.
* **Efficient Utilization of Neurons:** DNNs can achieve similar or better performance with fewer neurons compared to shallow networks.

However, the exact reason for the superiority of DNNs remains uncertain, with alternative explanations including:

* **Increased Training Difficulty:** Shallow networks may be harder to train and prone to local minima.
* **Inherent Suitability for Complex Tasks:** DNNs may naturally align with the hierarchical nature of real-world problems.

**Current Understanding**

The consensus suggests that deep architectures are advantageous for many practical tasks, but shallow networks may also be effective in specific scenarios. The field of deep learning is still evolving, and the precise mechanisms underlying DNN performance are still the subject of ongoing research."
"Why normalize images by subtracting dataset&#39;s image mean, instead of the current image mean in deep learning?",https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current,"['deep-learning', 'image-processing']",211436,True,220970,224675,5,158,1610822271,1462705915,116,1467098699,"Subtracting the dataset mean serves to ""center"" the data. Additionally, you ideally would like to divide by the sttdev of that feature or pixel as well if you want to normalize each feature value to a z-score. 
The reason we do both of those things is because in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. 
We'd like in this process for each feature to have a similar range so that our gradients don't go out of control (and that we only need one global learning rate multiplier). 
Another way you can think about it is deep learning networks traditionally share many parameters - if you didn't scale your inputs in a way that resulted in similarly-ranged feature values (ie: over the whole dataset by subtracting mean) sharing wouldn't happen very easily because to one part of the image weight w is a lot and to another it's too small.
You will see in some CNN models that per-image whitening is used, which is more along the lines of your thinking.  
","To optimize neural network training, data should undergo both mean subtraction and division by standard deviation. Mean subtraction centers the data, while division by standard deviation normalizes feature values. This ensures similar feature ranges, preventing uncontrolled gradients and enabling the use of a single global learning rate. Normalization also facilitates parameter sharing, as features with comparable values have similar impacts on the network's calculations. Note that some CNN models may employ per-image whitening instead of global mean subtraction."
"Why are neural networks becoming deeper, but not wider?",https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider,"['machine-learning', 'classification', 'neural-networks', 'deep-learning', 'conv-neural-network']",222883,True,223637,91159,6,152,1629227205,1468046112,172,1468440687,"As a disclaimer, I work on neural nets in my research, but I generally use relatively small, shallow neural nets rather than the really deep networks at the cutting edge of research you cite in your question.  I am not an expert on the quirks and peculiarities of very deep networks and I will defer to someone who is.
First, in principle, there is no reason you need deep neural nets at all.  A sufficiently wide neural network with just a single hidden layer can approximate any (reasonable) function given enough training data.  There are, however, a few difficulties with using an extremely wide, shallow network.  The main issue is that these very wide, shallow networks are very good at memorization, but not so good at generalization.  So, if you train the network with every possible input value, a super wide network could eventually memorize the corresponding output value that you want.  But that's not useful because for any practical application you won't have every possible input value to train with.
The advantage of multiple layers is that they can learn features at various levels of abstraction.  For example, if you train a deep convolutional neural network to classify images, you will find that the first layer will train itself to recognize very basic things like edges, the next layer will train itself to recognize collections of edges such as shapes, the next layer will train itself to recognize collections of shapes like eyes or noses, and the next layer will learn even higher-order features like faces.  Multiple layers are much better at generalizing because they learn all the intermediate features between the raw data and the high-level classification.
So that explains why you might use a deep network rather than a very wide but shallow network.  But why not a very deep, very wide network?  I think the answer there is that you want your network to be as small as possible to produce good results.  As you increase the size of the network, you're really just introducing more parameters that your network needs to learn, and hence increasing the chances of overfitting.  If you build a very wide, very deep network, you run the chance of each layer just memorizing what you want the output to be, and you end up with a neural network that fails to generalize to new data.
Aside from the specter of overfitting, the wider your network, the longer it will take to train.  Deep networks already can be very computationally expensive to train, so there's a strong incentive to make them wide enough that they work well, but no wider.
","Deep neural networks are advantageous over shallow networks for image classification due to their ability to learn hierarchical features. Shallow networks, while capable of approximation, tend towards memorization rather than generalization.

Adding layers to a network allows for abstraction, with each layer learning features at increasing levels of complexity. This facilitates generalization by recognizing intermediate features between the raw data and the final classification.

Excessive network size (in terms of depth and width) increases the risk of overfitting and computational expense. Optimal network design aims to minimize size while achieving desired results. Therefore, deep networks are preferred over excessively wide and deep networks."
Difference between neural net weight decay and learning rate,https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate,"['neural-networks', 'terminology']",29130,True,31334,184309,4,146,1608146542,1337923047,225,1340942319,"The learning rate is a parameter that determines how much an updating step influences the current value of the weights. While weight decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled.
So let's say that we have a cost or error function $E(\mathbf{w})$ that we want to minimize. Gradient descent tells us to modify the weights $\mathbf{w}$ in the direction of steepest descent in $E$:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i},
\end{equation}
where $\eta$ is the learning rate, and if it's large you will have a correspondingly large modification of the weights $w_i$ (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).
In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function. An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to $\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2}\mathbf{w}^2$. In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter $\lambda$ determines how you trade off the original cost $E$ with the large weights penalization.
Applying gradient descent to this new cost function we obtain:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}-\eta\lambda w_i.
\end{equation}
The new term $-\eta\lambda w_i$ coming from the regularization causes the weight to decay in proportion to its size.
","The learning rate **determines the magnitude of weight updates** during gradient descent. Weight decay **exponentially reduces weights** towards zero.

Regularization aims to **limit model complexity**, often by adding a penalty term to the cost function for large weights. Weight decay can be implemented by adding a **quadratic regularization term to the cost function**. Gradient descent with weight decay updates weights by **subtracting a decay term proportional to the weight's magnitude**. This **curbs weight growth and reduces overfitting**.

The **regularization parameter** balances the trade-off between minimizing the original cost function and penalizing large weights."
"What is the difference between convolutional neural networks, restricted Boltzmann machines, and auto-encoders?",https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma,"['neural-networks', 'deep-learning', 'conv-neural-network', 'autoencoders', 'restricted-boltzmann-machine']",114385,True,117188,118425,4,140,1505214911,1409863944,252,1411998350,"Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. E.g. in a network like this:

output[i] has edge back to input[i] for every i. Typically, number of hidden units is much less then number of visible (input/output) ones. As a result, when you pass data through such a network, it first compresses (encodes) input vector to ""fit"" in a smaller representation, and then tries to reconstruct (decode) it back. The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation (encoding) for input data. 
RBM shares similar idea, but uses stochastic approach. Instead of deterministic (e.g. logistic or ReLU) it uses stochastic units with particular (usually binary of Gaussian) distribution. Learning procedure consists of several steps of Gibbs sampling (propagate: sample hiddens given visibles; reconstruct: sample visibles given hiddens; repeat) and adjusting the weights to minimize reconstruction error. 

Intuition behind RBMs is that there are some visible random variables (e.g. film reviews from different users) and some hidden variables (like film genres or other internal features), and the task of training is to find out how these two sets of variables are actually connected to each other (more on this example may be found here (http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)). 
Convolutional Neural Networks are somewhat similar to these two, but instead of learning single global weight matrix between two layers, they aim to find a set of locally connected neurons. CNNs are mostly used in image recognition. Their name comes from ""convolution"" operator (http://en.wikipedia.org/wiki/Convolution) or simply ""filter"". In short, filters are an easy way to perform complex operation by means of simple change of a convolution kernel. Apply Gaussian blur kernel and you'll get it smoothed. Apply Canny kernel and you'll see all edges. Apply Gabor kernel to get gradient features. 

(image from here (https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html))
The goal of convolutional neural networks is not to use one of predefined kernels, but instead to learn data-specific kernels.  The idea is the same as with autoencoders or RBMs - translate many low-level features (e.g. user reviews or image pixels) to the compressed high-level representation (e.g. film genres or edges) - but now weights are learned only from neurons that are spatially close to each other. 
 
All three models have their use cases, pros and cons, but probably the most important properties are: 

Autoencoders are simplest ones. They are intuitively understandable, easy to implement and to reason about (e.g. it's much easier to find good meta-parameters for them than for RBMs).
RBMs are generative. That is, unlike autoencoders that only discriminate some data vectors in favour of others, RBMs can also generate new data with given joined distribution. They are also considered more feature-rich and flexible. 
CNNs are very specific model that is mostly used for very specific task (though pretty popular task). Most of the top-level algorithms in image recognition are somehow based on CNNs today, but outside that niche they are hardly applicable (e.g. what's the reason to use convolution for film review analysis?). 

UPD. 
Dimensionality reduction
When we represent some object as a vector of $n$ elements, we say that this is a vector in $n$-dimensional space. Thus, dimensionality reduction refers to a process of refining data in such a way, that each data vector $x$ is translated into another vector $x'$ in an $m$-dimensional space (vector with $m$ elements), where $m < n$. Probably the most common way of doing this is PCA (http://en.wikipedia.org/wiki/Principal_component_analysis). Roughly speaking, PCA finds ""internal axes"" of a dataset (called ""components"") and sorts them by their importance. First $m$ most important components are then used as new basis. Each of these components may be thought of as a high-level feature, describing data vectors better than original axes. 
Both - autoencoders and RBMs - do the same thing. Taking a vector in $n$-dimensional space they translate it into an $m$-dimensional one, trying to keep as much important information as possible and, at the same time, remove noise. If training of autoencoder/RBM was successful, each element of resulting vector (i.e. each hidden unit) represents something important about the object - shape of an eyebrow in an image, genre of a film, field of study in scientific article, etc. You take lots of noisy data as an input and produce much less data in a much more efficient representation. 
Deep architectures
So, if we already had PCA, why the hell did we come up with autoencoders and RBMs? It turns out that PCA only allows linear transformation of a data vectors. That is, having $m$ principal components $c_1..c_m$, you can represent only vectors $x=\sum_{i=1}^{m}w_ic_i$. This is pretty good already, but not always enough. No matter, how many times you will apply PCA to a data - relationship will always stay linear. 
Autoencoders and RBMs, on other hand, are non-linear by the nature, and thus, they can learn more complicated relations between visible and hidden units. Moreover, they can be stacked, which makes them even more powerful. E.g. you train RBM with $n$ visible and $m$ hidden units, then you put another RBM with $m$ visible and $k$ hidden units on top of the first one and train it too, etc. And exactly the same way with autoencoders. 
But you don't just add new layers. On each layer you try to learn best possible representation for a data from the previous one: 

On the image above there's an example of such a deep network. We start with ordinary pixels, proceed with simple filters, then with face elements and finally end up with entire faces! This is the essence of deep learning. 
Now note, that at this example we worked with image data and sequentially took
larger and larger areas of spatially close pixels. Doesn't it sound similar? Yes, because it's an example of deep convolutional network. Be it based on autoencoders or RBMs, it uses convolution to stress importance of locality. That's why CNNs are somewhat distinct from autoencoders and RBMs. 
Classification
None of models mentioned here work as classification algorithms per se. Instead, they are used for pretraining - learning transformations from low-level and hard-to-consume representation (like pixels) to a high-level one. Once deep (or maybe not that deep) network is pretrained, input vectors are transformed to a better representation and resulting vectors are finally passed to real classifier (such as SVM or logistic regression). In an image above it means that at the very bottom there's one more component that actually does classification. 
","Autoencoders, Restricted Boltzmann Machines (RBMs), and Convolutional Neural Networks (CNNs) are neural networks used for dimensionality reduction, feature extraction, and image recognition.

**Autoencoders** encode input data into a smaller representation and then reconstruct it, aiming to learn efficient compact representations.

**RBMs** use stochastic units to represent hidden variables and are generative, meaning they can create new data.

**CNNs** employ locally connected neurons to process spatial information, such as in image recognition, where they excel.

All three models facilitate feature extraction by translating low-level input data into high-level, compressed representations. Autoencoders are simple and interpretable, while RBMs are more feature-rich and generative. CNNs are highly specialized for image processing.

Additionally, these models can be stacked to create deep architectures, allowing for nonlinear transformations and the learning of complex relationships. However, they are often used for data pretraining before the application of actual classification algorithms."
How is it possible that validation loss is increasing while validation accuracy is increasing as well,https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy,"['neural-networks', 'deep-learning', 'conv-neural-network', 'overfitting']",282160,True,341054,207641,6,140,1711361645,1495980810,91,1523972701,"Other answers explain well how accuracy and loss are not necessarily exactly (inversely) correlated, as loss measures a difference between raw output (float) and a class (0 or 1 in the case of binary classification), while accuracy measures the difference between thresholded output (0 or 1) and class. So if raw outputs change, loss changes but accuracy is more ""resilient"" as outputs need to go over/under a threshold to actually change accuracy.
However, accuracy and loss intuitively seem to be somewhat (inversely) correlated, as better predictions should lead to lower loss and higher accuracy, and the case of higher loss and higher accuracy shown by OP is surprising. I have myself encountered this case several times, and I present here my conclusions based on the analysis I had conducted at the time. I stress that this answer is therefore purely based on experimental data I encountered, and there may be other reasons for OP's case.

Let's consider the case of binary classification, where the task is to predict whether an image is a cat or a dog, and the output of the network is a sigmoid (outputting a float between 0 and 1), where we train the network to output 1 if the image is one of a cat and 0 otherwise. In this case, two phenomenons can happen at the same time :
 (https://i.sstatic.net/gppxn.png)

Some images with borderline predictions get predicted better and so their output class changes to the correct one (image C in the figure). This is the classic ""loss decreases while accuracy increases"" behavior that we expect when training is going well.

Some images with very bad predictions keep getting worse (image D). This leads to a less classic ""loss increases while accuracy stays the same"".
You might think that this effect should be ""countered"" by the inverse phenomenon of images with very good predictions getting better (images A and B, note that their output has increased the same amount  than the one of image D has decreased (0.02)). However, when using cross-entropy loss for classification (as it is usually done), bad predictions are penalized much more strongly than good predictions are rewarded. For a cat image    (ground truth : 1), the loss is $-log(output)$, so slightly better predictions for images already close to 1 don't change the mean loss much (as the difference between log(<number close to 1>) and log(<number slightly closer to 1>) is low), while a single misclassified cat image getting worse can ""blow up"" your mean loss (as the difference between log(<number close to 0>) and log(<number slightly closer to 0>) is high). See this answer (https://stats.stackexchange.com/a/448033/201218) for further illustration of this phenomenon.
(Getting increasing loss and stable accuracy could also be caused by good predictions being classified a little worse, but I find it less likely because of this loss ""asymmetry"").


So I think that when both accuracy and loss are increasing, the network is starting to overfit, and both phenomena are happening at the same time. The network is starting to learn patterns only relevant for the training set and not great for generalization, leading to phenomenon 2 where some images from the validation set get predicted really wrong (image D), with an effect amplified by the ""loss asymmetry"". However, it is at the same time still learning some patterns which are useful for generalization (phenomenon one, ""good learning"") as more and more images are being correctly classified (images A, B, C).
I sadly have no answer for whether or not this ""overfitting"" is a bad thing in this case: should we stop the learning once the network is starting to learn spurious patterns, even though it's continuing to learn useful ones along the way?
Finally, I think this effect can be further obscured in the case of multi-class classification, where the network at a given epoch might be severely overfit on some classes but still learning on others.
","Accuracy and loss in machine learning are often inversely correlated, indicating better predictions lead to lower loss and higher accuracy. However, sometimes accuracy can increase while loss also increases. This can occur during overfitting, where the network learns irrelevant patterns in the training set.

Two phenomenons contribute to this behavior:

1. Some incorrect predictions improve, increasing accuracy.
2. Some correct predictions worsen, amplifying the loss due to the asymmetry of cross-entropy loss, which penalizes incorrect predictions more severely than it rewards correct ones.

As overfitting increases, the network learns both useful and spurious patterns, leading to simultaneous accuracy and loss increases. This effect can be more apparent in multi-class classification, where overfitting may impact some classes while learning continues on others."
How is it possible that validation loss is increasing while validation accuracy is increasing as well,https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy,"['neural-networks', 'deep-learning', 'conv-neural-network', 'overfitting']",282160,True,341054,207641,6,140,1711361645,1495980810,91,1523972701,"Other answers explain well how accuracy and loss are not necessarily exactly (inversely) correlated, as loss measures a difference between raw output (float) and a class (0 or 1 in the case of binary classification), while accuracy measures the difference between thresholded output (0 or 1) and class. So if raw outputs change, loss changes but accuracy is more ""resilient"" as outputs need to go over/under a threshold to actually change accuracy.
However, accuracy and loss intuitively seem to be somewhat (inversely) correlated, as better predictions should lead to lower loss and higher accuracy, and the case of higher loss and higher accuracy shown by OP is surprising. I have myself encountered this case several times, and I present here my conclusions based on the analysis I had conducted at the time. I stress that this answer is therefore purely based on experimental data I encountered, and there may be other reasons for OP's case.

Let's consider the case of binary classification, where the task is to predict whether an image is a cat or a dog, and the output of the network is a sigmoid (outputting a float between 0 and 1), where we train the network to output 1 if the image is one of a cat and 0 otherwise. In this case, two phenomenons can happen at the same time :
 (https://i.sstatic.net/gppxn.png)

Some images with borderline predictions get predicted better and so their output class changes to the correct one (image C in the figure). This is the classic ""loss decreases while accuracy increases"" behavior that we expect when training is going well.

Some images with very bad predictions keep getting worse (image D). This leads to a less classic ""loss increases while accuracy stays the same"".
You might think that this effect should be ""countered"" by the inverse phenomenon of images with very good predictions getting better (images A and B, note that their output has increased the same amount  than the one of image D has decreased (0.02)). However, when using cross-entropy loss for classification (as it is usually done), bad predictions are penalized much more strongly than good predictions are rewarded. For a cat image    (ground truth : 1), the loss is $-log(output)$, so slightly better predictions for images already close to 1 don't change the mean loss much (as the difference between log(<number close to 1>) and log(<number slightly closer to 1>) is low), while a single misclassified cat image getting worse can ""blow up"" your mean loss (as the difference between log(<number close to 0>) and log(<number slightly closer to 0>) is high). See this answer (https://stats.stackexchange.com/a/448033/201218) for further illustration of this phenomenon.
(Getting increasing loss and stable accuracy could also be caused by good predictions being classified a little worse, but I find it less likely because of this loss ""asymmetry"").


So I think that when both accuracy and loss are increasing, the network is starting to overfit, and both phenomena are happening at the same time. The network is starting to learn patterns only relevant for the training set and not great for generalization, leading to phenomenon 2 where some images from the validation set get predicted really wrong (image D), with an effect amplified by the ""loss asymmetry"". However, it is at the same time still learning some patterns which are useful for generalization (phenomenon one, ""good learning"") as more and more images are being correctly classified (images A, B, C).
I sadly have no answer for whether or not this ""overfitting"" is a bad thing in this case: should we stop the learning once the network is starting to learn spurious patterns, even though it's continuing to learn useful ones along the way?
Finally, I think this effect can be further obscured in the case of multi-class classification, where the network at a given epoch might be severely overfit on some classes but still learning on others.
","While accuracy and loss are generally inversely correlated, they can increase simultaneously. This occurs when:

1. Borderline images are correctly classified, increasing accuracy (image C).
2. Misclassified images worsen, increasing loss (image D).
3. Good predictions slightly improve, which doesn't significantly lower loss.

This behavior suggests that the network is overfitting, learning both useful and spurious patterns. The ""loss asymmetry"" of cross-entropy loss amplifies the effect of misclassified images, further increasing loss while accuracy remains stable. In multi-class classification, overfitting can be obscured as the network may overfit on some classes while still learning on others."
tanh activation function vs sigmoid activation function,https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function,"['machine-learning', 'neural-networks', 'optimization', 'sigmoid-curve']",101560,True,101563,157125,3,123,1723522466,1402207884,120,1402210058,"Yes it matters for technical reasons. Basically for optimization. It is worth reading Efficient Backprop (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by LeCun et al.
There are two reasons for that choice (assuming you have normalized your data, and this is very important):

Having stronger gradients: since data is centered around 0, the derivatives are higher. To see this, calculate the derivative of the tanh function and notice that its range (output values) is [0,1].

The range of the tanh function is [-1,1] and that of the sigmoid function is [0,1]

Avoiding bias in the gradients. This is explained very well in the paper, and it is worth reading it to understand these issues.

","Using the tanh activation function instead of the sigmoid function is beneficial for optimization in neural networks due to two reasons:

1. **Enhanced Gradients:** Tanh's output range [-1, 1] results in higher derivatives, facilitating gradient-based optimization algorithms to learn faster.

2. **Reduced Gradient Bias:** Unlike the sigmoid function with a range [0, 1], tanh has a symmetrical range around 0, eliminating any bias in the gradients. This improves the stability and accuracy of backpropagation algorithms.

These technical advantages justify the preference of the tanh function over the sigmoid function in neural network architectures."
Validation Error less than training error?,https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error,"['machine-learning', 'mathematical-statistics', 'neural-networks', 'cross-validation']",187335,True,187404,216297,10,121,1614730827,1450389872,118,1450446690,"It is difficult to be certain without knowing your actual methodology (e.g. cross-validation method, performance metric, data splitting method, etc.).
Generally speaking though, training error will almost always underestimate your validation error.  However it is possible for the validation error to be less than the training.  You can think of it two ways:

Your training set had many 'hard' cases to learn
Your validation set had mostly 'easy' cases to predict

That is why it is important that you really evaluate your model training methodology.  If you don't split your data for training properly your results will lead to confusing, if not simply incorrect, conclusions.
I think of model evaluation in four different categories:

Underfitting – Validation and training error high
Overfitting – Validation error is high, training error low
Good fit – Validation error low, slightly higher than the training error
Unknown fit - Validation error low, training error 'high'

I say 'unknown' fit because the result is counter intuitive to how machine learning works.  The essence of ML is to predict the unknown.  If you are better at predicting the unknown than what you have 'learned', AFAIK the data between training and validation must be different in some way.  This could mean you either need to reevaluate your data splitting method, adding more data, or possibly changing your performance metric (are you actually measuring the performance you want?).
EDIT
To address the OP's reference to a previous python lasagne question (https://stats.stackexchange.com/questions/178371/python-lasagne-tutorial-validation-error-lower-than-training-error?lq=1).
This suggests that you have sufficient data to not require cross-validation and simply have your training, validation, and testing data subsets.  Now, if you look at the lasagne tutorial (http://lasagne.readthedocs.org/en/latest/user/tutorial.html) you can see that the same behavior is seen at the top of the page.  I would find it hard to believe the authors would post such results if it was strange but instead of just assuming they are correct let's look further.  The section of most interest to us here is in the training loop section, just above the bottom you will see how the loss parameters are calculated.
The training loss is calculated over the entire training dataset.  Likewise, the validation loss is calculated over the entire validation dataset.  The training set is typically at least 4 times as large as the validation (80-20).  Given that the error is calculated over all samples, you could expect up to approximately 4X the loss measure of the validation set.  You will notice, however, that the training loss and validation loss are approaching one another as training continues.  This is intentional as if your training error begins to get lower than your validation error you would be beginning to overfit your model!!!
I hope this clarifies these errors.  
","Training error often underestimates the validation error due to ""hard"" cases in the training set or ""easy"" cases in the validation set. Proper data splitting is crucial for accurate model evaluation.

Model evaluation can be categorized into four types:

* **Underfitting:** High validation and training errors
* **Overfitting:** High validation error, low training error
* **Good fit:** Low validation error, slightly higher than training error
* **Unknown fit:** Low validation error, high training error (indicates different data distributions in training and validation sets)

In the case of the Lasagne tutorial, the larger training set size (80%) and the calculation of loss over the entire datasets can explain why the validation error is initially lower than the training error. However, as training progresses, the training error decreases and approaches the validation error to prevent overfitting."
Is it possible to train a neural network without backpropagation?,https://stats.stackexchange.com/questions/235862/is-it-possible-to-train-a-neural-network-without-backpropagation,"['machine-learning', 'neural-networks', 'optimization', 'backpropagation']",235862,True,235868,54440,6,121,1537898940,1474336101,94,1474338646,"The first two algorithms you mention (Nelder-Mead and Simulated Annealing) are generally considered pretty much obsolete in optimization circles, as there are much better alternatives which are both more reliable and less costly. Genetic algorithms covers a wide range, and some of these can be reasonable.
However, in the broader class of derivative-free optimization (DFO) algorithms, there are many which are significantly better than these ""classics"", as this has been an active area of research in recent decades. So, might some of these newer approaches be reasonable for deep learning?
A relatively recent paper comparing the state of the art is the following:

Rios, L. M., & Sahinidis, N. V. (2013) Derivative-free optimization: a review of algorithms and comparison of software implementations. (https://scholar.google.com/scholar?cluster=13996631775177561404) Journal of Global Optimization.

This is a nice paper which has many interesting insights into recent techniques. For example, the results clearly show that the best local optimizers are all ""model-based"", using different forms of sequential quadratic programming (https://en.wikipedia.org/wiki/Sequential_quadratic_programming) (SQP). 
However, as noted in their abstract ""We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size."" To give an idea of the numbers, for all problems the solvers were given a budget of 2500 function evaluations, and problem sizes were a maximum of ~300 parameters to optimize. Beyond O[10] parameters, very few of these optimizers performed very well, and even the best ones showed a noticable decay in performance as problem size was increased.
So for very high dimensional problems, DFO algorithms just are not competitive with derivative based ones. To give some perspective, PDE (partial differential equation)-based optimization (https://scholar.google.com/scholar?q=pde%20based%20optimization) is another area with very high dimensional problems (e.g. several parameter for each cell of a large 3D finite element grid). In this realm, the ""adjoint method (https://scicomp.stackexchange.com/questions/14259/understanding-the-cost-of-adjoint-method-for-pde-constrained-optimization)"" is one of the most used methods. This is also a gradient-descent optimizer based on automatic differentiation of a forward model code.
The closest to a high-dimensional DFO optimizer is perhaps the Ensemble Kalman Filter (https://en.wikipedia.org/wiki/Ensemble_Kalman_filter), used for assimilating data into complex PDE simulations, e.g. weather models. Interestingly, this is essentially an SQP approach, but with a Bayesian-Gaussian interpretation (so the quadratic model is positive definite, i.e. no saddle points). But I do not think that the number of parameters or observations in these applications is comparable to what is seen in deep learning.
Side note (local minima): From the little I have read on deep learning, I think the consensus is that it is saddle points rather than local minima, which are most problematic for high dimensional NN-parameter spaces.
For example, the recent review (https://scholar.google.com/scholar?cluster=5362332738201102290) in Nature says ""Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general. Instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the remainder.""
A related concern is about local vs. global optimization (for example this question (https://stats.stackexchange.com/questions/207450/in-neural-nets-why-use-gradient-methods-rather-than-other-metaheuristics) pointed out in the comments). While I do not do deep learning, in my experience overfitting is definitely a valid concern. In my opinion, global optimization methods are most suited for engineering design problems that do not strongly depend on ""natural"" data. In data assimilation problems, any current global minima could easily change upon addition of new data (caveat: My experience is concentrated in geoscience problems, where data is generally ""sparse"" relative to model capacity).
An interesting perspective is perhaps

O. Bousquet & L. Bottou (2008) The tradeoffs of large scale learning. (https://scholar.google.com/scholar?cluster=1338734004801715686) NIPS.

which provides semi-theoretical arguments on why and when approximate optimization may be preferable in practice.
End note (meta-optimization): While gradient based techniques seem likely to be dominant for training networks, there may be a role for DFO in associated meta-optimization tasks.
One example would be hyper-parameter tuning. (Interestingly, the successful model-based DFO optimizers from Rios & Sahinidis (https://scholar.google.com/scholar?cluster=13996631775177561404) could be seen as essentially solving a sequence of design-of-experiments/response-surface (https://en.wikipedia.org/wiki/Response_surface_methodology) problems.)
Another example might be designing architectures, in terms of the set-up of layers (e.g. number, type, sequence, nodes/layer). In this discrete-optimization context genetic-style algorithms may be more appropriate. Note that here I am thinking of the case where connectivity is determined implicitly by these factors (e.g. fully-connected layers, convolutional layers, etc.). In other words the $\mathrm{O}[N^2]$ connectivity is $not$ meta-optimized explicitly. (The connection strength would fall under training, where e.g. sparsity could be promoted by $L_1$ regularization and/or ReLU activations ... these choices could be meta-optimized however.)
","Traditional optimization algorithms like Nelder-Mead and Simulated Annealing are outdated, while Genetic Algorithms have some potential. Newer derivative-free optimization (DFO) techniques, such as sequential quadratic programming (SQP), excel as local optimizers but struggle with high-dimensional problems.

PDE-based optimization uses gradient descent with automatic differentiation, while the Ensemble Kalman Filter provides a Bayesian-Gaussian interpretation of SQP. Despite its applicability in data assimilation, the number of parameters it handles is limited compared to deep learning.

In deep learning, saddle points rather than local minima are considered problematic. Gradient-based methods remain dominant for neural network training, but DFO may be useful in meta-optimization tasks like hyper-parameter tuning and architecture design, where genetic-style algorithms may be appropriate for discrete optimization."
What is an embedding layer in a neural network?,https://stats.stackexchange.com/questions/182775/what-is-an-embedding-layer-in-a-neural-network,"['machine-learning', 'neural-networks', 'python', 'word-embeddings']",182775,True,188603,110598,2,116,1726967059,1448037792,82,1451405367,"Relation to Word2Vec

Word2Vec in a simple picture:
 (https://i.sstatic.net/MfyBJ.png)
(source: netdna-ssl.com (https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Word2Vec-Training-Models-768x468.png))
More in-depth explanation:
I believe it's related to the recent Word2Vec innovation in natural language processing. Roughly, Word2Vec means our vocabulary is discrete and we will learn an map which will embed each word into a continuous vector space. Using this vector space representation will allow us to have a continuous, distributed representation of our vocabulary words. If for example our dataset consists of n-grams, we may now use our continuous word features to create a distributed representation of our n-grams. In the process of training a language model we will learn this word embedding map. The hope is that by using a continuous representation, our embedding will map similar words to similar regions. For example in the landmark paper Distributed Representations of Words and Phrases
and their Compositionality (https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf), observe in Tables 6 and 7 that certain phrases have very good nearest neighbour phrases from a semantic point of view. Transforming into this continuous space allows us to use continuous metric notions of similarity to evaluate the semantic quality of our embedding.
Explanation using Lasagne code
Let's break down the Lasagne code snippet:
x = T.imatrix()

x is a matrix of integers. Okay, no problem. Each word in the vocabulary can be represented an integer, or a 1-hot sparse encoding. So if x is 2x2, we have two datapoints, each being a 2-gram.
l_in = InputLayer((3, ))

The input layer. The 3 represents the size of our vocabulary. So we have words $w_0, w_1, w_2$ for example.
W = np.arange(3*5).reshape((3, 5)).astype('float32')

This is our word embedding matrix. It is a 3 row by 5 column matrix with entries 0 to 14.
Up until now we have the following interpretation. Our vocabulary has 3 words and we will embed our words into a 5 dimensional vector space. For example, we may represent one word $w_0 = (1,0,0)$, and another word $w_1 = (0, 1, 0)$ and the other word $w_2 = (0, 0, 1)$, e.g. as hot sparse encodings.  We can view the $W$ matrix as embedding these words via matrix multiplication. Therefore the first word $w_0 \rightarrow w_0W = [0, 1, 2, 3, 4].$ Similarly, $w_1 \rightarrow w_1W = [5, 6, 7, 8, 9]$.
It should be noted, due to the one-hot sparse encoding we are using, you also see this referred to as table lookups.
l1 = EmbeddingLayer(l_in, input_size=3, output_size=5, W=W)

The embedding layer
output = get_output(l1, x)

Symbolic Theano expression for the embedding.
f = theano.function([x], output)

Theano function which computes the embedding.
x_test = np.array([[0, 2], [1, 2]]).astype('int32')

It's worth pausing here to discuss what exactly x_test means. First notice that all of x_test entries are in $\{0, 1, 2\}$, i.e. range(3). x_test has 2 datapoints. The first datapoint [0, 2] represents the 2-gram $(w_0, w_2)$ and the second datapoint represents the 2-gram $(w_1, w_2)$.
We wish to embed our 2-grams using our word embedding layer now. Before we do that, let's make sure we're clear about what should be returned by our embedding function f. The 2 gram $(w_0, w_2)$ is equivalent to a [[1, 0, 0], [0, 0, 1]] matrix. Applying our embedding matrix $W$ to this sparse matrix should yield: [[0, 1, 2, 3, 4], [10, 11, 12, 13, 14]]. Note in order to have the matrix multiplication work out, we have to apply the word embedding matrix $W$ via right multiplication to the sparse matrix representation of our 2-gram.
f(x_test) 

returns:
array([[[  0.,   1.,   2.,   3.,   4.],
        [ 10.,  11.,  12.,  13.,  14.]],
       [[  5.,   6.,   7.,   8.,   9.],
        [ 10.,  11.,  12.,  13.,  14.]]], dtype=float32)

To convince you that the 3 does indeed represent the vocabulary size, try inputting a matrix x_test = [[5, 0], [1, 2]]. You will see that it raises a matrix mis-match error.
","Word2Vec, a natural language processing technique, represents vocabulary as continuous vectors, allowing for semantic similarity evaluation. By using a continuous word embedding map, similar words are mapped to similar vector regions.

In the provided example, Lasagne code demonstrates the process of word embedding using a 3x5 matrix W. This matrix maps the three vocabulary words ($w_0, w_1, w_2$) to a 5-dimensional vector space. The example considers 2-grams, such as (w0, w2), and shows the embedding result as a matrix multiplication between the sparse 2-gram encoding and the word embedding matrix.

By embedding n-grams into a continuous vector space, this Word2Vec approach enables the application of continuous similarity metrics to assess the semantic quality of these representations. It allows for more accurate evaluation of word and n-gram relationships, leading to improved natural language processing tasks."
What&#39;re the differences between PCA and autoencoder?,https://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder,"['machine-learning', 'pca', 'neural-networks', 'autoencoders']",120080,True,120096,50976,4,91,1587820574,1413358015,81,1413363301,"PCA is restricted to a linear map, while auto encoders can have nonlinear enoder/decoders.
A single layer auto encoder with linear transfer function is nearly equivalent to PCA, where nearly means that the $W$ found by AE and PCA won't necessarily be the same - but the subspace spanned by the respective $W$'s will.
","**Summary:**

Principal Component Analysis (PCA) and Autoencoders (AEs) are both dimensionality reduction techniques. PCA operates within a linear space, while AEs allow for nonlinear transformations.

However, when an AE has a single hidden layer with a linear transfer function, it closely approximates PCA. While the exact weights found by AE and PCA may differ, the subspaces spanned by their respective weighting matrices are essentially the same.

Therefore, in situations where linear transformations are appropriate, AEs and PCA can be considered equivalent. AEs, however, provide the flexibility to handle nonlinear relationships when necessary."
How to apply Neural Network to time series forecasting?,https://stats.stackexchange.com/questions/10162/how-to-apply-neural-network-to-time-series-forecasting,"['time-series', 'forecasting', 'neural-networks']",10162,True,10196,88115,1,91,1531649411,1304122279,110,1304231803,"Here is a simple recipe that may help you get started writing code and testing ideas...
Let's assume you have monthly data recorded over several years, so you have 36 values. Let's also assume that you only care about predicting one month (value) in advance.

Exploratory data analysis: Apply some of the traditional time series analysis methods to estimate the lag dependence in the data (e.g. auto-correlation and partial auto-correlation plots, transformations, differencing).
Let's say that you find a given month's value is correlated with the past three month's data but not much so beyond that.
Partition your data into training and validation sets: Take the first 24 points as your training values and the remaining points as the validation set.
Create the neural network layout: You'll take the past three month's values as inputs and you want to predict the next month's value. So, you need a neural network with an input layer containing three nodes and an output layer containing one node. You should probably have a hidden layer with at least a couple of nodes. Unfortunately, picking the number of hidden layers, and their respective number of nodes, is not something for which there are clear guidelines. I'd start small, like 3:2:1.
Create the training patterns: Each training pattern will be four values, with the first three corresponding to the input nodes and the last one defining what the correct value is for the output node. For example, if your training data are values $$x_1,x_2\dots,x_{24}$$ then $$pattern 1:  x_1,x_2,x_3,x_4$$ $$pattern 2: x_2,x_3,x_4,x_5$$ $$\dots$$ $$pattern 21: x_{21},x_{22},x_{23},x_{24}$$ 
Train the neural network on these patterns
Test the network on the validation set (months 25-36): Here you will pass in the three values the neural network needs for the input layer and see what the output node gets set to. So, to see how well the trained neural network can predict month 32's value you'll pass in values for months 29, 30, and 31

This recipe is obviously high level and you may scratch your head at first when trying to map your context into different software libraries/programs. But, hopefully this sketches out the main point: you need to create training patterns that reasonably contain the correlation structure of the series you are trying to forecast. And whether you do the forecasting with a neural network or an ARIMA model, the exploratory work to determine what that structure is is often the most time consuming and difficult part.
In my experience, neural networks can provide great classification and forecasting functionality but setting them up can be time consuming. In the example above, you may find that 21 training patterns is not enough; different input data transformations lead to a better/worse forecasts; varying the number of hidden layers and hidden layer nodes greatly affects forecasts; etc.   
I highly recommend looking at the neural_forecasting (http://www.neural-forecasting-competition.com/index.htm) website, which contains tons of information on neural network forecasting competitions. The Motivations (http://www.neural-forecasting-competition.com/motivation.htm) page is especially useful.
","This recipe provides a structured approach to time series forecasting using neural networks. It emphasizes the importance of exploratory data analysis to identify correlation structures within the data. Using this information, a feedforward neural network with an input layer, a hidden layer, and an output layer is designed to predict values one month in advance.

Training patterns are created by grouping the input and output values. The neural network is trained on these patterns and tested on a validation set of data. The validation results help assess the network's prediction accuracy and guide adjustments to the network parameters (number of hidden layers, hidden layer nodes, etc.).

While neural networks offer powerful forecasting capabilities, their setup can be time-consuming. The neural_forecasting website provides resources and competitions related to neural network forecasting, further aiding practitioners in their endeavors."
ImageNet: what is top-1 and top-5 error rate?,https://stats.stackexchange.com/questions/156471/imagenet-what-is-top-1-and-top-5-error-rate,"['classification', 'neural-networks', 'error', 'measurement-error', 'image-processing']",156471,True,156515,104505,2,90,1630208443,1434021990,103,1434036122,"
[...] where the top-5 error rate is the fraction of test images for which
the correct label is not among the five labels considered most
probable by the mode.

First, you make a prediction using the CNN and obtain the predicted class multinomial distribution ($\sum p_{class} = 1$).
Now, in the case of the top-1 score, you check if the top class (the one with the highest probability) is the same as the target label.
In the case of the top-5 score, you check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).
In both cases, the top score is computed as the number of times a predicted label matched the target label, divided by the number of data points evaluated.
Finally, when 5-CNNs are used, you first average their predictions and follow the same procedure for calculating the top-1 and top-5 scores.
","**Summary:**

In image classification, top-1 and top-5 error rates measure the accuracy of a model's predictions. The top-1 error rate is the percentage of test images where the model's most probable prediction does not match the true label. The top-5 error rate is similar, but allows for a more forgiving margin of error by considering the five most probable predictions.

To calculate these error rates, a Convolutional Neural Network (CNN) predicts the probability distribution of classes for each test image. The top-1 score is calculated by comparing the most probable class with the true label. The top-5 score is calculated by checking if the true label is among the five most probable classes.

When multiple CNNs are employed, their predictions are averaged before calculating the error rates. This approach helps improve model accuracy and robustness."
What is global max pooling layer and what is its advantage over maxpooling layer?,https://stats.stackexchange.com/questions/257321/what-is-global-max-pooling-layer-and-what-is-its-advantage-over-maxpooling-layer,"['neural-networks', 'conv-neural-network', 'pooling']",257321,True,257325,95078,2,86,1510308459,1484931313,99,1484932035,"Global max pooling  =  ordinary max pooling layer with pool size equals to the size of the input (minus filter size + 1, to be precise). You can see that MaxPooling1D takes a pool_length argument, whereas GlobalMaxPooling1D does not.
For example, if the input of the max pooling layer  is $0,1,2,2,5,1,2$, global max pooling outputs $5$, whereas  ordinary max pooling layer with pool size equals to 3 outputs $2,2,5,5,5$ (assuming stride=1).
This can be seen in the code (https://github.com/fchollet/keras/blob/3d176e926f848c5aacd036d6095ab015a2f8cc83/keras/layers/pooling.py#L433):
class GlobalMaxPooling1D(_GlobalPooling1D):
    """"""Global max pooling operation for temporal data.
    # Input shape
        3D tensor with shape: `(samples, steps, features)`.
    # Output shape
        2D tensor with shape: `(samples, features)`.
    """"""

    def call(self, x, mask=None):
        return K.max(x, axis=1)

In some domains, such as natural language processing, it is common to use global max pooling. In some other domains, such as computer vision, it is common to use a max pooling that isn't global.
","Global max pooling is a max pooling operation where the pool size equals the input size. It outputs the maximum value for each feature across the input's temporal dimension. Ordinary max pooling, in contrast, takes a specified pool size and outputs maximum values within that window.

In Keras, the `GlobalMaxPooling1D` layer performs global max pooling on 1D temporal data. It converts a 3D tensor (samples, steps, features) to a 2D tensor (samples, features).

Global max pooling is commonly used in domains like natural language processing, while ordinary max pooling is more prevalent in domains like computer vision."
Why do neural network researchers care about epochs?,https://stats.stackexchange.com/questions/242004/why-do-neural-network-researchers-care-about-epochs,"['neural-networks', 'gradient-descent']",242004,True,242125,15252,3,84,1639229321,1477277099,75,1477331809,"In addition to Franck's answer about practicalities, and David's answer about looking at small subgroups – both of which are important points – there are in fact some theoretical reasons to prefer sampling without replacement. The reason is perhaps related to David's point (which is essentially the coupon collector's problem (https://en.wikipedia.org/wiki/Coupon_collector%27s_problem)).
In 2009, Léon Bottou compared the convergence performance on a particular text classification problem ($n = 781,265$).

Bottou (2009). Curiously Fast Convergence of some
  Stochastic Gradient Descent Algorithms. Proceedings of the
  symposium on learning and data science. (author's pdf (http://leon.bottou.org/publications/pdf/slds-2009.pdf))

He trained a support vector machine via SGD with three approaches:

Random: draw random samples from the full dataset at each iteration.
Cycle: shuffle the dataset before beginning the learning process, then walk over it sequentially, so that in each epoch you see the examples in the same order.
Shuffle: reshuffle the dataset before each epoch, so that each epoch goes in a different order.

He empirically examined the convergence $\mathbb E[ C(\theta_t) - \min_\theta C(\theta) ]$, where $C$ is the cost function, $\theta_t$ the parameters at step $t$ of optimization, and the expectation is over the shuffling of assigned batches.

For Random, convergence was approximately on the order of $t^{-1}$ (as expected by existing theory at that point).
Cycle obtained convergence on the order of $t^{-\alpha}$ (with $\alpha > 1$ but varying depending on the permutation, for example $\alpha \approx 1.8$ for his Figure 1).
Shuffle was more chaotic, but the best-fit line gave $t^{-2}$, much faster than Random.

This is his Figure 1 illustrating that:
 (https://i.sstatic.net/bl3eQ.png)
This was later theoretically confirmed by the paper:

Gürbüzbalaban, Ozdaglar, and Parrilo (2015). Why Random Reshuffling Beats Stochastic Gradient Descent. arXiv:1510.08560 (https://arxiv.org/abs/1510.08560). (video of invited talk at NIPS 2015 (https://www.microsoft.com/en-us/research/video/invited-talk-incremental-methods-for-additive-cost-convex-optimization/))

Their proof only applies to the case where the loss function is strongly convex, i.e. not to neural networks. It's reasonable to expect, though, that similar reasoning might apply to the neural network case (which is much harder to analyze).
","Sampling without replacement is advantageous for theoretical reasons related to convergence performance. By eliminating the possibility of selecting the same elements multiple times, it mimics the coupon collector's problem.

In 2009, Léon Bottou empirically demonstrated faster convergence rates using SGD with shuffled (without replacement) data compared to random or cyclic sampling. This finding was later confirmed theoretically for strongly convex loss functions. Although the proof doesn't extend to neural networks, similar reasoning likely applies, suggesting potential benefits for neural network training as well."
"How can an artificial neural network ANN, be used for unsupervised clustering?",https://stats.stackexchange.com/questions/140148/how-can-an-artificial-neural-network-ann-be-used-for-unsupervised-clustering,"['clustering', 'neural-networks', 'unsupervised-learning', 'self-organizing-maps']",140148,True,140165,137948,3,83,1713284764,1425399661,79,1425405711,"Neural networks are widely used in unsupervised learning in order to learn better representations of the input data. For example, given a set of text documents, NN can learn a mapping from document to real-valued vector in such a way that resulting vectors are similar for documents with similar content, i.e. distance preserving. This can be achieved using, for example, auto-encoders - a model that is trained to reconstruct the original vector from a smaller representation (hidden layer activations) with reconstruction error (distance from the ID function) as cost function. This process doesn't give you clusters, but  it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.
Clustering: There are a number of different NN architectures specifically designed for clustering. The most widely known is probably self organizing maps. A SOM is a NN that has a set of neurons connected to form a topological grid (usually rectangular). When some pattern is presented to an SOM, the neuron with closest weight vector is considered a winner and its weights are adapted to the pattern, as well as the weights of its neighbourhood. In this way an SOM naturally finds data clusters. A somewhat related algorithm is growing neural gas (it is not limited to predefined number of neurons). 
Another approach is Adaptive Resonance Theory where we have two layers: ""comparison field"" and ""recognition field"". Recognition field also determines the best match (neuron) to the vector transferred from the comparison field and also have lateral inhibitory connections. Implementation details and exact equations can readily found by googling the names of these models, so I won't put them here.
","Neural Networks (NNs) excel in unsupervised learning for data representation enhancement. Auto-encoders, a type of NN, can create distance-preserving vectors from input data, allowing for content-based similarity identification. These representations can be further analyzed through clustering algorithms.

NNs also offer specific architectures for clustering, such as Self-Organizing Maps (SOMs), which utilize topological grids to identify clusters through competitive weight adjustments. Growing Neural Gas algorithms are similar, but dynamically adjust the number of neurons based on data.

Adaptive Resonance Theory (ART) employs a two-layer architecture, with the comparison field identifying the closest match to input vectors and the recognition field inhibiting nearby neurons. These models offer a variety of approaches to effectively uncover cluster structures in data."
What are good initial weights in a neural network?,https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network,"['neural-networks', 'normalization']",47590,True,47604,113708,7,82,1681489004,1358025999,59,1358033749,"I assume you are using logistic neurons, and that you are training by gradient descent/back-propagation. 
The logistic function is close to flat for large positive or negative inputs. The derivative at an input of $2$ is about $1/10$, but at $10$ the derivative is about $1/22000$ . This means that if the input of a logistic neuron is $10$ then, for a given training signal, the neuron will learn about $2200$ times slower that if the input was $2$. 
If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range $[-4,4]$. 
The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean $0$ and standard deviation $1$, then a random sum of $d$ terms with weights uniform on $(\frac{-1}{\sqrt{d}},\frac{1}{\sqrt{d}})$ will have mean $0$ and variance $\frac{1}{3}$, independent of $d$. The probability that you get a sum outside of $[-4,4]$ is small. That means as you increase $d$, you are not causing the neurons to start out saturated so that they don't learn. 
With inputs which are not normalized, those weights may not be effective at avoiding saturation. 
","Training logistic neurons with gradient descent can be challenging due to the diminishing derivative of the logistic function for large inputs. To overcome this, the initial weights should be adjusted to keep inputs within a range where the derivative is significant, typically [-4, 4]. This ensures faster learning. For unnormalized inputs, a random sum of weights uniform within a specific range can effectively avoid saturation, regardless of the number of input features. However, if inputs are normalized, the same approach may not be effective, requiring alternative strategies to prevent neuron saturation."
How and why do normalization and feature scaling work?,https://stats.stackexchange.com/questions/41704/how-and-why-do-normalization-and-feature-scaling-work,"['machine-learning', 'neural-networks', 'covariance', 'normalization']",41704,True,41926,119995,8,81,1554361072,1351801248,25,1352130475,"It's simply a case of getting all your data on the same scale: if the scales for different features are wildly different, this can have a knock-on effect on your ability to learn (depending on what methods you're using to do it). Ensuring standardised feature values implicitly weights all features equally in their representation.
","**Summary:**

Feature scaling is crucial for data analysis as it places all features on a uniform scale. This eliminates bias that may arise from vastly different feature scales. By standardizing feature values, all features receive equal importance in model representation, ensuring accurate and effective learning outcomes. This process becomes particularly important when employing specific machine learning methods that are sensitive to scale variations."
Which activation function for output layer?,https://stats.stackexchange.com/questions/218542/which-activation-function-for-output-layer,['neural-networks'],218542,True,218589,188142,7,75,1646063416,1465742531,72,1465773585,"
Regression: linear (because values are unbounded)
Classification: softmax (simple sigmoid works too but softmax works better)

Use simple sigmoid only if your output admits multiple ""true"" answers, for instance, a network that checks for the presence of various objects in an image. In other words, the output is not a probability distribution (does not need to sum to 1).
","In regression tasks, where the output range is unbounded, a linear activation function is recommended. For classification tasks, a softmax activation function is often preferred, as it ensures the output represents a probability distribution. However, in scenarios where multiple classes can be true simultaneously (e.g., image object detection), a simple sigmoid activation function may be sufficient, allowing outputs that do not sum to one and represent the presence or absence of multiple objects."
What&#39;s the difference between feed-forward and recurrent neural networks?,https://stats.stackexchange.com/questions/2213/whats-the-difference-between-feed-forward-and-recurrent-neural-networks,"['machine-learning', 'neural-networks', 'terminology', 'recurrent-neural-network', 'topologies']",2213,True,2218,106811,3,75,1578419125,1283182408,70,1283192604,"Feed-forward (http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.multil.jpg) ANNs allow signals to travel one way only: from input to output. There are no feedback (loops); i.e., the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straightforward networks that associate inputs with outputs. They are extensively used in pattern recognition. This type of organisation is also referred to as bottom-up or top-down.

Feedback (http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.neural2.jpg) (or recurrent or interactive) networks can have signals traveling in both directions by introducing loops in the network. Feedback networks are powerful and can get extremely complicated. Computations derived from earlier input are fed back into the network, which gives them a kind of memory. Feedback networks are dynamic; their 'state' is changing continuously until they reach an equilibrium point. They remain at the equilibrium point until the input changes and a new equilibrium needs to be found.

Feedforward neural networks are ideally suitable for modeling relationships between a set of predictor or input variables and one or more response or output variables. In other words, they are appropriate for any functional mapping problem where we want to know how a number of input variables affect the output variable. The multilayer feedforward neural networks, also called multi-layer perceptrons (http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP), are the most widely studied and used neural network model in practice.
As an example of feedback network, I can recall Hopfield’s network (http://en.wikipedia.org/wiki/Hopfield_net). The main use of Hopfield’s network is as associative memory. An associative memory is a device which accepts an input pattern and generates an output as the stored pattern which is most closely associated with the input. The function of the associate memory is to recall the corresponding stored pattern, and then produce a clear version of the pattern at the output. Hopfield networks are typically used for those problems with binary pattern vectors and the input pattern may be a noisy version of one of the stored patterns. In the Hopfield network, the stored patterns are encoded as the weights of the network.
Kohonen’s self-organizing maps (SOM) represent another neural network type that is markedly different from the feedforward multilayer networks. Unlike training in the feedforward MLP, the SOM training or learning is often called unsupervised because there are no known target outputs associated with each input pattern in SOM and during the training process, the SOM processes the input patterns and learns to cluster or segment the data through adjustment of weights (that makes it an important neural network model for dimension reduction and data clustering). A two-dimensional map is typically created in such a way that the orders of the interrelationships among inputs are preserved. The number and composition of clusters can be visually determined based on the output distribution generated by the training process. With only input variables in the training sample, SOM aims to learn or discover the underlying structure of the data.
(The diagrams are from Dana Vrajitoru's C463 / B551 Artificial Intelligence web site (http://www.cs.iusb.edu/~danav/teach/c463/12_nn.html).)
","Neural networks are computational models inspired by the human brain.

**Feed-forward ANNs:**
- Signals move only from input to output.
- No feedback loops and output does not influence the same layer.
- Suitable for pattern recognition and functional mapping.
- Multilayer feedforward networks are the most commonly used.

**Feedback (recurrent) ANNs:**
- Signals can travel in both directions through loops.
- Powerful and can become complex.
- Computation results are fed back into the network, providing memory.
- Dynamic and continuously changing state.
- Examples include Hopfield's network (associative memory) and Kohonen's self-organizing maps (unsupervised learning).

In summary, feed-forward ANNs are suitable for static relationships between inputs and outputs, while feedback ANNs are useful for dynamic systems and memory-based tasks."
Why is tanh almost always better than sigmoid as an activation function?,https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function,"['machine-learning', 'neural-networks', 'backpropagation', 'sigmoid-curve']",330559,True,330885,82074,7,75,1691573034,1519634703,45,1519760687,"Yan LeCun and others argue in Efficient BackProp (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) that

Convergence is usually faster if the average of each input variable over the training set is close to zero. To see this, consider the extreme case where all the inputs are positive. Weights to a particular node in the first weight layer are updated by an amount proportional to $\delta x$ where $\delta$ is the (scalar) error at that node and $x$ is the input vector (see equations (5) and (10)). When all of the components of an input vector are positive, all of the updates of weights that feed into a node will have the same sign (i.e. sign($\delta$)). As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow.

This is why you should normalize your inputs so that the average is zero.
The same logic applies to middle layers:

This heuristic should be applied at all layers which means that we want the average of the outputs of a node to be close to zero because these outputs are the inputs to the next layer.

Postscript @craq makes the point that this quote doesn't make sense for ReLU(x)=max(0,x) which has become a widely popular activation function. While ReLU does avoid the first zigzag problem mentioned by LeCun, it doesn't solve this second point by LeCun who says it is important to push the average to zero. I would love to know what LeCun has to say about this. In any case, there is a paper called Batch Normalization (https://arxiv.org/pdf/1502.03167.pdf), which builds on top of the work of LeCun and offers a way to address this issue:

It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.


By the way, this video by Siraj (https://www.youtube.com/watch?v=-7scQpJT7uo) explains a lot about activation functions in 10 fun minutes.

@elkout says ""The real reason that tanh is preferred compared to sigmoid (...) is that the derivatives of the tanh are larger than the derivatives of the sigmoid.""
I think this is a non-issue. I never seen this being a problem in the literature. If it bothers you that one derivative is smaller than another, you can just scale it.
The logistic function has the shape $\sigma(x)=\frac{1}{1+e^{-kx}}$. Usually, we use $k=1$, but nothing forbids you from using another value for $k$ to make your derivatives wider, if that was your problem.

Nitpick: tanh is also a sigmoid function. Any function with a S shape is a sigmoid. What you guys are calling sigmoid is the logistic function. The reason why the logistic function is more popular is historical reasons. It has been used for a longer time by statisticians. Besides, some feel that it is more biologically plausible.
","Input normalization plays a crucial role in optimizing neural network training. When input variables have non-zero average, weight updates become biased in one direction, resulting in inefficient and zigzagging convergence. To address this, inputs should be normalized to have zero average.

This principle extends to intermediate layers, where the average output of a node (input to the next layer) should also be close to zero. While ReLU activation functions mitigate the first zigzagging problem, they don't eliminate the need for pushing the average to zero.

Batch normalization provides a technique to achieve this by transforming inputs to have zero mean and unit variance after each layer. This ensures that the network benefits from the faster convergence associated with normalized inputs at all layers."
Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions?,https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary,"['machine-learning', 'neural-networks', 'loss-functions', 'tensorflow', 'cross-entropy']",260505,True,260537,148359,4,74,1692118738,1486479736,82,1486487846,"Bernoulli$^*$ cross-entropy loss is a special case of categorical cross-entropy loss for $m=2$.
$$
\begin{align}
\mathcal{L}(\theta)
&= -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^m y_{ij}\log(p_{ij}) \\
&= -\frac{1}{n}\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right] 
\end{align}
$$
Where $i$ indexes samples/observations and $j$ indexes classes, and $y$ is the sample label (binary for LSH, one-hot vector on the RHS) and $p_{ij}\in(0,1):\sum_{j} p_{ij} =1\forall i,j$ is the prediction for a sample.

I write ""Bernoulli cross-entropy"" because this loss arises from a Bernoulli probability model. There is not a ""binary distribution."" A ""binary cross-entropy"" doesn't tell us if the thing that is binary is the one-hot vector of $k \ge 2$ labels, or if the author is using binary encoding for each trial (success or failure). This isn't a general convention, but it makes clear that these formulae arise from particular probability models. Conventional jargon is not clear in that way.
","Bernoulli cross-entropy loss is a specific instance of categorical cross-entropy loss used in binary classification problems. It calculates the average negative log-likelihood of correctly predicting the binary class label ($y$) given the model's prediction ($p$). This loss function assumes a Bernoulli probability model, where $y$ is a binary variable and $p$ is the probability of the event occurring. The loss is minimized when the predicted probabilities closely match the true labels. Despite its name, this loss does not arise from a ""binary distribution"" but rather from a Bernoulli probability model with binary outcomes."
Data normalization and standardization in neural networks,https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks,"['machine-learning', 'neural-networks', 'normalization', 'standardization']",7757,True,7759,152503,7,73,1605011737,1299005584,54,1299011251,"A standard approach is to scale the inputs to have mean 0 and a variance of 1. Also linear decorrelation/whitening/pca helps a lot.
If you are interested in the tricks of the trade, I can recommend LeCun's efficient backprop paper. (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
","To improve the performance of neural networks, it is recommended to standardize input data by scaling it to have a mean of 0 and a variance of 1. This process, combined with decorrelation or Principal Component Analysis (PCA), can enhance the training process. For further insights, Yann LeCun's paper ""Efficient Backprop"" provides detailed technical guidance on these methods and other optimization techniques for neural network models."
Why is logistic regression a linear classifier?,https://stats.stackexchange.com/questions/93569/why-is-logistic-regression-a-linear-classifier,"['logistic', 'classification', 'neural-networks']",93569,True,93570,84453,3,72,1667922882,1397331269,61,1397331930,"Logistic regression is linear in the sense that the predictions can be written as
$$ \hat{p} = \frac{1}{1 + e^{-\hat{\mu}}}, \text{ where } \hat{\mu} = \hat{\theta} \cdot x. $$
Thus, the prediction can be written in terms of $\hat{\mu}$, which is a linear function of $x$. (More precisely, the predicted log-odds is a linear function of $x$.)
Conversely, there is no way to summarize the output of a neural network in terms of a linear function of $x$, and that is why neural networks are called non-linear.
Also, for logistic regression, the decision boundary $\{x:\hat{p} = 0.5\}$ is linear: it's the solution to $\hat{\theta} \cdot x = 0$. The decision boundary of a neural network is in general not linear.
","Logistic regression is considered linear because its predictions can be expressed as a linear combination of input features, represented by the linear function $\hat{\mu}$. This linearity allows for straightforward interpretation of the model's coefficients.

Conversely, neural networks are non-linear as their predictions cannot be captured by linear functions. This added complexity enables neural networks to model complex relationships and create intricate decision boundaries that are not linearly separable. The decision boundaries of logistic regression are linear, defined by $\hat{\mu} = 0$, while the decision boundaries of neural networks are generally non-linear and more flexible in shape.

In essence, the linearity of logistic regression simplifies its interpretation and prediction, while the non-linearity of neural networks enhances their modeling capabilities and decision-making complexity."
What is the definition of a &quot;feature map&quot; (aka &quot;activation map&quot;) in a convolutional neural network?,https://stats.stackexchange.com/questions/291820/what-is-the-definition-of-a-feature-map-aka-activation-map-in-a-convolutio,"['neural-networks', 'deep-learning', 'conv-neural-network']",291820,True,292064,106523,4,71,1622891571,1500214560,62,1500338311,"A feature map, or activation map, is the output activations for a given filter (a1 in your case) and the definition is the same regardless of what layer you are on. 
Feature map and activation map mean exactly the same thing. It is called an activation map because it is a mapping that corresponds to the activation of different parts of the image, and also a feature map because it is also a mapping of where a certain kind of feature is found in the image. A high activation means a certain feature was found.  
A ""rectified feature map"" is just a feature map that was created using Relu. You could possibly see the term ""feature map"" used for the result of the dot products (z1) because this is also really a map of where certain features are in the image, but that is not common to see. 
","Feature maps, also known as activation maps, represent the output activations for a specific filter in a neural network layer. They indicate the location and level of activation of certain features within an image.

A ""feature map"" refers to a map of where a particular feature is identified in an image, while an ""activation map"" highlights the correspondence between feature activation and different image regions. High activation values signify the presence of a particular feature.

""Rectified feature maps"" are created using the rectified linear unit (ReLU) activation function, preserving only positive activation values. The term ""feature map"" may occasionally be used to refer to the result of dot product operations, but this usage is less common."
What should I do when my neural network doesn&#39;t generalize well?,https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well,"['neural-networks', 'overfitting', 'faq']",365778,True,365806,64304,5,70,1681575887,1536311529,86,1536318458,"First of all, let's mention what does ""my neural network doesn't generalize well"" mean and what's the difference with saying ""my neural network doesn't perform well"".
When training a Neural Network, you are constantly evaluating it on a set of labelled data called the training set. If your model isn't working properly and doesn't appear to learn from the training set, you don't have a generalization issue yet, instead please refer to this post (https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn). However, if your model is achieving a satisfactory performance on the training set, but cannot perform well on previously unseen data (e.g. validation/test sets), then you do have a generalization problem.
Why is your model not generalizing properly?
The most important part is understanding why your network doesn't generalize well. High-capacity Machine Learning models have the ability to memorize the training set, which can lead to overfitting.
Overfitting is the state where an estimator has begun to learn the training set so well that it has started to model the noise in the training samples (besides all useful relationships).
For example, in the image below we can see how the blue on the right line has clearly overfit.

But why is this bad?
When attempting to evaluate our model on new, previously unseen data (i.e. validation/test set), the model's performance will be much worse than what we expect.
How to prevent overfitting?
In the beginning of the post I implied that the complexity of your model is what is actually causing the overfitting, as it is allowing the model to extract unnecessary relationships from the training set, that map its inherent noise. The easiest way to reduce overfitting is to essentially limit the capacity of your model. These techniques are called regularization techniques.

Parameter norm penalties. These add an extra term to the weight update function of each model, that is dependent on the norm of the parameters. This term's purpose is to counter the actual update (i.e. limit how much each weight can be updated). This makes the models more robust to outliers and noise. Examples of such regularizations are L1 and L2 (http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/) regularizations, which can be found on the Lasso (https://en.wikipedia.org/wiki/Lasso_(statistics)), Ridge (https://en.wikipedia.org/wiki/Tikhonov_regularization) and Elastic Net (https://en.wikipedia.org/wiki/Elastic_net_regularization) regressors.
Since each (fully connected) layer in a neural network functions much like a simple linear regression, these are used in Neural Networks. The most common use is to regularize each layer individually.
keras implementation (https://keras.io/regularizers/).

Early stopping. This technique attempts to stop an estimator's training phase prematurely, at the point where it has learned to extract all meaningful relationships from the data, before beginning to model its noise. This is done by monitoring the validation loss (or a validation metric of your choosing) and terminating the training phase when this metric stops improving. This way we give the estimator enough time to learn the useful information but not enough to learn from the noise.
keras implementation (https://keras.io/callbacks/#earlystopping).




Neural Network specific regularizations. Some examples are:

Dropout. Dropout (http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) is an interesting technique that works surprisingly well. Dropout is applied between two successive layers in a network. At each iteration a specified percentage of the connections (selected randomly), connecting the two layers, are dropped. This causes the subsequent layer rely on all of its connections to the previous layer.
keras implementation (https://keras.io/layers/core/#dropout)
Transfer learning. This is especially used in Deep Learning. This is done by initializing the weights of your network to the ones of another network with the same architecture pre-trained on a large, generic dataset.
Other things that may limit overfitting in Deep Neural Networks are: Batch Normalization (https://en.wikipedia.org/wiki/Batch_normalization), which can act as a regulizer and in some cases (e.g. inception modules) works as well as dropout; relatively small sized batches (https://arxiv.org/pdf/1705.08741.pdf) in SGD, which can also prevent overfitting; adding small random noise to weights (https://pdfs.semanticscholar.org/1023/043d2a5a76a07388c3e17c1284018937dbfc.pdf) in hidden layers.



Another way of preventing overfitting, besides limiting the model's capacity, is by improving the quality of your data. The most obvious choice would be outlier/noise removal, however in practice their usefulness is limited. A more common way (especially in image-related tasks) is data augmentation. Here we attempt randomly transform the training examples so that while they appear to the model to be different, they convey the same semantic information (e.g. left-right flipping on images).
Data augmentation overview (http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)
Practical suggestions:

By far the most effective regularization technique is dropout, meaning that it should be the first you should use. However, you don't need to (and probably shouldn't) place dropout everywhere! The most prone layers to overfitting are the Fully Connected (FC) layers, because they contain the most parameters. Dropout should be applied to these layers (impacting their connections to the next layer).
Batch normalization, besides having a regularization effect aids your model in several other ways (e.g. speeds up convergence, allows for the use of higher learning rates). It too should be used in FC layers.
As mentioned previously it also may be beneficial to stop your model earlier in the training phase than scheduled. The problem with early stopping is that there is no guarantee that, at any given point, the model won't start improving again. A more practical approach than early stopping is storing the weights of the model that achieve the best performance on the validation set. Be cautious, however, as this is not an unbiased estimate of the performance of your model (just better than the training set). You can also overfit on the validation set. More on that later.
keras implementation (https://keras.io/callbacks/#modelcheckpoint)
In some applications (e.g. image related tasks), it is highly recommended to follow an already established architecture (e.g. VGG, ResNet, Inception), that you can find ImageNet weights for. The generic nature of this dataset, allows the features to be in turn generic enough to be used for any image related task. Besides being robust to overfitting this will greatly reduce the training time.
Another use of the similar concept is the following: if your task doesn't have much data, but you can find another similar task that does, you can use transfer learning to reduce overfitting. First train your network for the task that has the larger dataset and then attempt to fine-tune the model to the one you initially wanted. The initial training will, in most cases, make your model more robust to overfitting.
Data augmentation. While it always helps to have a larger dataset, data augmentation techniques do have their shortcomings. More specifically, you have to be careful not to augment too strongly, as this might ruin the semantic content of the data. For example in image augmentation if you translate/shift/scale or adjust the brighness/contrast the image too much you'll lose much of the information it contains. Furthermore, augmentation schemes need to be implemented for each task in an ad-hoc fashion (e.g. in handwritten digit recognition the digits are usually aligned and shouldn't be rotated too much; also they shouldn't be flipped in any direction, as they aren't horizontally/vertically symetric. Same goes for medical images).
In short be careful not to produce non realistic images through data augmentation. Moreover, an increased dataset size will require a longer training time. Personally, I start considering using data augmentation when I see that my model is reaching near $0$ loss on the training set.

","Generalization issues arise when a neural network performs well on the training set but struggles with unseen data, indicating overfitting. This occurs when the model memorizes the training set's noise instead of learning meaningful relationships. To mitigate overfitting, regularization techniques are employed to limit model capacity.

Parameter norm penalties, such as L1 and L2 regularization, add constraints to the weight updates to prevent excessive learning from outliers and noise. Early stopping terminates training when the model ceases to improve on the validation set, allowing it to capture relevant patterns without overfitting. Neural Network-specific regularizations include dropout, which randomly drops connections between layers to encourage reliance on all connections, and transfer learning, which initializes weights using pre-trained models.

Improving data quality can also prevent overfitting. Outlier removal is limited in practice, but data augmentation transforms training examples to present different appearances while retaining semantic information. Dropout is the most effective regularization technique and should be applied to fully connected layers. Batch normalization aids in convergence and regularization. Early stopping should be used cautiously, and storing the best-performing weights on the validation set allows for performance evaluation without overfitting. Established architectures and transfer learning can reduce overfitting and training time. Data augmentation should be carefully applied to avoid compromising data integrity."
Proper way of using recurrent neural network for time series analysis,https://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis,"['time-series', 'machine-learning', 'neural-networks']",8000,True,8014,48326,3,70,1615808611,1299568561,52,1299588689,"What you describe is in fact a ""sliding time window"" approach and is different to recurrent networks. You can use this technique with any regression algorithm. There is a huge limitation to this approach: events in the inputs can only be correlatd with other inputs/outputs which lie at most t timesteps apart, where t is the size of the window.
E.g. you can think of a Markov chain of order t. RNNs don't suffer from this in theory, however in practice learning is difficult.
It is best to illustrate an RNN in contrast to a feedfoward network. Consider the (very) simple feedforward network $y = Wx$ where $y$ is the output, $W$ is the weight matrix, and $x$ is the input.
Now, we use a recurrent network. Now we have a sequence of inputs, so we will denote the inputs by $x^{i}$ for the ith input. The corresponding ith output is then calculated via $y^{i} = Wx^i + W_ry^{i-1}$.
Thus, we have another weight matrix $W_r$ which incorporates the output at the previous step linearly into the current output.
This is of course a simple architecture. Most common is an architecture where you have a hidden layer which is recurrently connected to itself. Let $h^i$ denote the hidden layer at timestep i. The formulas are then:
$$h^0 = 0$$
$$h^i = \sigma(W_1x^i + W_rh^{i-1})$$
$$y^i = W_2h^i$$
Where $\sigma$ is a suitable non-linearity/transfer function like the sigmoid. $W_1$ and $W_2$ are the connecting weights between the input and the hidden and the hidden and the output layer. $W_r$ represents the recurrent weights.
Here is a diagram of the structure: 

","**Sliding Time Window Approach:**

This approach correlates inputs and outputs within a fixed time window of size t. Its limitation lies in the limited correlation range, making it unsuitable for capturing long-term dependencies.

**Recurrent Neural Networks (RNNs):**

RNNs overcome the limitations of sliding time windows by incorporating a hidden layer that stores information from previous time steps. Inputs are sequentially processed, with each hidden state influencing the subsequent output.

**Recurrent Architecture:**

In a simple RNN architecture, the hidden state h^i is computed as a function of the current input x^i and the previous hidden state h^{i-1}. This hidden state is then used to generate the output y^i. More complex architectures typically involve multiple hidden layers."
What is the difference between a neural network and a deep belief network?,https://stats.stackexchange.com/questions/51273/what-is-the-difference-between-a-neural-network-and-a-deep-belief-network,"['machine-learning', 'neural-networks', 'deep-learning', 'deep-belief-networks']",51273,True,59854,54192,2,70,1514911542,1362370722,63,1369325939,"""Neural networks"" is a term usually used to refer to feedforward neural networks. Deep Neural Networks are feedforward Neural Networks with many layers. 
A Deep belief network is not the same as a Deep Neural Network.
As you have pointed out a deep belief network has undirected connections between some layers. This means that the topology of the DNN and DBN is different by definition.
The undirected layers in the DBN are called Restricted Boltzmann Machines. This layers can be trained using an unsupervised learning algorithm (Contrastive Divergence) that is very fast (Here's a link (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)! with details).
Some more comments:
The solutions obtained with deeper neural networks correspond to solutions that perform worse than the solutions obtained for networks with 1 or 2 hidden layers. As the architecture gets deeper, it becomes more difficult to obtain good generalization using a Deep NN.
In 2006 Hinton  discovered that much better results could be achieved in deeper architectures when each layer (RBM) is pre-trained with an unsupervised learning algorithm (Contrastive Divergence). Then the Network can be trained in a supervised way using backpropagation in order to ""fine-tune"" the weights.
","Deep Neural Networks (DNNs) are feedforward neural networks with multiple layers. However, Deep Belief Networks (DBNs) differ from DNNs in having undirected connections between some layers, known as Restricted Boltzmann Machines (RBMs).

Unlike DNNs, DBNs can be trained unsupervisedly using fast learning algorithms, making them effective for complex data. Despite their potential, deeper DNNs often lead to poor generalization, a challenge that prompted Hinton's discovery that pre-training each RBM layer unsupervisedly followed by fine-tuning with supervised backpropagation yields significantly better results in deep architectures."
What is the difference between a neural network and a deep belief network?,https://stats.stackexchange.com/questions/51273/what-is-the-difference-between-a-neural-network-and-a-deep-belief-network,"['machine-learning', 'neural-networks', 'deep-learning', 'deep-belief-networks']",51273,True,59854,54192,2,70,1514911542,1362370722,63,1369325939,"""Neural networks"" is a term usually used to refer to feedforward neural networks. Deep Neural Networks are feedforward Neural Networks with many layers. 
A Deep belief network is not the same as a Deep Neural Network.
As you have pointed out a deep belief network has undirected connections between some layers. This means that the topology of the DNN and DBN is different by definition.
The undirected layers in the DBN are called Restricted Boltzmann Machines. This layers can be trained using an unsupervised learning algorithm (Contrastive Divergence) that is very fast (Here's a link (http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)! with details).
Some more comments:
The solutions obtained with deeper neural networks correspond to solutions that perform worse than the solutions obtained for networks with 1 or 2 hidden layers. As the architecture gets deeper, it becomes more difficult to obtain good generalization using a Deep NN.
In 2006 Hinton  discovered that much better results could be achieved in deeper architectures when each layer (RBM) is pre-trained with an unsupervised learning algorithm (Contrastive Divergence). Then the Network can be trained in a supervised way using backpropagation in order to ""fine-tune"" the weights.
","Deep Neural Networks (DNNs) are feedforward neural networks with multiple layers, while Deep Belief Networks (DBNs) feature undirected connections between some layers (Restricted Boltzmann Machines). DBNs can be trained faster using unsupervised learning (Contrastive Divergence). However, DNNs with deeper architectures often lead to worse performance and difficulty in achieving good generalization. In 2006, Hinton's discovery of pre-training each layer of a DNN with Contrastive Divergence, followed by supervised fine-tuning, significantly improved performance in deeper architectures."
Cost function of neural network is non-convex?,https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex,"['machine-learning', 'neural-networks', 'loss-functions']",106334,True,106343,60570,7,69,1660505215,1404914378,50,1404916975,"The cost function of a neural network is in general neither convex nor concave. This means that the matrix of all second partial derivatives (the Hessian) is neither positive semidefinite, nor negative semidefinite. Since the second derivative is a matrix, it's possible that it's neither one or the other.
To make this analogous to one-variable functions, one could say that the cost function is neither shaped like the graph of $x^2$ nor like the graph of $-x^2$. Another example of a non-convex, non-concave function is $\sin(x)$ on $\mathbb{R}$. One of the most striking differences is that $\pm x^2$ has only one extremum, whereas $\sin$ has infinitely many maxima and minima.
How does this relate to our neural network? A cost function $J(W,b)$ has also a number of local maxima and minima, as you can see in this picture (http://www.holehouse.org/mlclass/09_Neural_Networks_Learning_files/Image%20%5B35%5D.png), for example.
The fact that $J$ has multiple minima can also be interpreted in a nice way. In each layer, you use multiple nodes which are assigned different parameters to make the cost function small. Except for the values of the parameters, these nodes are the same. So you could exchange the parameters of the first node in one layer with those of the second node in the same layer, and accounting for this change in the subsequent layers. You'd end up with a different set of parameters, but the value of the cost function can't be distinguished by (basically you just moved a node, to another place, but kept all the inputs/outputs the same).
","Neural network cost functions are often non-convex, meaning they have multiple local minima and maxima. This differs from convex and concave functions, which have a single minimum or maximum, respectively. This non-convexity implies that the cost function can have a complex landscape with many potential optima.

Analogous to one-variable functions, $\sin(x)$ on $\mathbb{R}$ is also non-convex and non-concave, exhibiting many oscillations rather than a single extremum.

In a neural network, multiple minima of the cost function correspond to different configurations of weights and biases that yield similar performance. This reflects the redundancy of using multiple nodes with different parameters to minimize the cost in each layer. Exchanging the parameters of nodes does not change the cost function value, highlighting the non-uniqueness of optimal parameter configurations."
Cross-Entropy or Log Likelihood in Output layer,https://stats.stackexchange.com/questions/198038/cross-entropy-or-log-likelihood-in-output-layer,"['neural-networks', 'maximum-likelihood', 'softmax']",198038,True,198047,96771,3,68,1664304875,1456205823,90,1456209438,"The negative log likelihood (eq.80) is also known as the multiclass cross-entropy (ref: Pattern Recognition and Machine Learning Section 4.3.4), as they are in fact two different interpretations of the same formula.
eq.57 is the negative log likelihood of the Bernoulli distribution, whereas eq.80 is the negative log likelihood of the multinomial distribution with one observation (a multiclass version of Bernoulli). 
For binary classification problems, the softmax function outputs two values (between 0 and 1 and sum to 1) to give the prediction of each class. While the sigmoid function outputs one value (between 0 and 1) to give the prediction of one class (so the other class is 1-p).  
So eq.80 can't be directly applied to the sigmoid output, though it is essentially the same loss as eq.57.
Also see this answer (https://stats.stackexchange.com/a/224491/95569).

Following is a simple illustration of the connection between (sigmoid + binary cross-entropy) and (softmax + multiclass cross-entropy) for binary classification problems. 
Say we take $0.5$ as the split point of the two categories, for sigmoid
output it follows,
$$\sigma(wx+b)=0.5$$
$$wx+b=0$$
which is the decision boundary in the feature space.
For softmax output it follows
$$\frac{e^{w_1x+b_1}}{e^{w_1x+b_1}+e^{w_2x+b_2}}=0.5$$
$$e^{w_1x+b_1}=e^{w_2x+b_2}$$
$$w_1x+b_1=w_2x+b_2$$
$$(w_1-w_2)x+(b_1-b_2)=0$$
so it remains the same model although there are twice as many parameters.
The followings show the decision boundaries obtained using theses two methods, which are almost identical.


","**Summary (350 words):**

Negative log likelihood and multiclass cross-entropy are equivalent concepts, representing different interpretations of the same formula. For binary classification:

* Negative log likelihood (eq.57) applies to the Bernoulli distribution (sigmoid output), while eq.80 applies to the multinomial distribution (softmax output).
* The two interpretations arise from interpreting the same output probabilities differently.
* The softmax function outputs probabilities for all classes, while the sigmoid function outputs a single probability for one class.
* Despite the output difference, the underlying model and decision boundary remain the same for softmax and sigmoid.
* Using a split point of 0.5, the decision boundary equations for both functions are equivalent, indicating the same classification behavior.
* The comparison illustrates the connection between binary cross-entropy with sigmoid and multiclass cross-entropy with softmax, highlighting their underlying similarities in binary classification."
Why do neural networks need so many training examples to perform?,https://stats.stackexchange.com/questions/394118/why-do-neural-networks-need-so-many-training-examples-to-perform,"['neural-networks', 'neuroscience']",394118,True,394128,18826,12,66,1551634625,1551017231,103,1551023084,"I caution against expecting strong resemblance between biological and artificial neural networks. I think the name ""neural networks"" is a bit dangerous, because it tricks people into expecting that neurological processes and machine learning should be the same. The differences between biological and artificial neural networks outweigh the similarities.
As an example of how this can go awry, you can also turn the reasoning in the original post on its head. You can train a neural network to learn to recognize cars in an afternoon, provided you have a reasonably fast computer and some amount of training data. You can make this a binary task (car/not car) or a multi-class task (car/tram/bike/airplane/boat) and still be confident in a high level of success. 
By contrast, I wouldn't expect a child to be able to pick out a car the day - or even the week - after it's born, even after it has seen ""so many training examples."" Something is obviously different between a two-year-old and an infant that accounts for the difference in learning ability, whereas a vanilla image classification neural network is perfectly capable of picking up object classification immediately after ""birth."" I think that there are two important differences: (1) the relative volumes of training data available and (2) a self-teaching mechanism that develops over time because of abundant training data.

The original post exposes two questions. The title and body of the question ask why neural networks need ""so many examples."" Relative to a child's experience, neural networks trained using common image benchmarks have comparatively little data.
I will re-phrases the question in the title to 
""How does training a neural network for a common image benchmark compare & contrast to the learning experience of a child?""
For the sake of comparison I'll consider the CIFAR-10 data because it is a common image benchmark. The labeled portion is composed of 10 classes of images with 6000 images per class. Each image is 32x32 pixels. If you somehow stacked the labeled images from CIFAR-10 and made a standard 48 fps video, you'd have about 20 minutes of footage.
A child of 2 years who observes the world for 12 hours daily has roughly 263000 minutes (more than 4000 hours) of direct observations of the world, including feedback from adults (labels). (These are just ballpark figures -- I don't know how many minutes a typical two-year-old has spent observing the world.) Moreover, the child will have exposure to many, many objects beyond the 10 classes that comprise CIFAR-10.
So there are a few things at play. One is that the child has exposure to more data overall and a more diverse source of data than the CIFAR-10 model has. Data diversity and data volume are well-recognized as pre-requisites for robust models in general. In this light, it doesn't seem surprising that a neural network is worse at this task than the child, because a neural network trained on CIFAR-10 is positively starved for training data compared to the two-year-old. The image resolution available to a child is better than the 32x32 CIFAR-10 images, so the child is able to learn information about the fine details of objects.
The CIFAR-10 to two-year-old comparison is not perfect because the CIFAR-10 model will likely be trained with multiple passes over the same static images, while the child will see, using binocular vision, how objects are arranged in a three-dimensional world while moving about and with different lighting conditions and perspectives on the same objects.
The anecdote about OP's child implies a second question, 
""How can neural networks become self-teaching?""
A child is endowed with some talent for self-teaching, so that new categories of objects can be added over time without having to start over from scratch. 

OP's remark about transfer-learning (/questions/tagged/transfer-learning) names one kind of model adaptation in the machine learning context.
In comments, other users have pointed out that one- and few-shot learning* is another machine learning research area.
Additionally, reinforcement-learning (/questions/tagged/reinforcement-learning) addresses self-teaching models from a different perspective, essentially allowing robots to undertake trial-and-error experimentation to find optimal strategies for solving specific problems (e.g. playing chess).

It's probably true that all three of these machine learning paradigms are germane to improving how machines adapt to new computer vision tasks. Quickly adapting machine learning models to new tasks is an active area of research. However, because the practical goals of these projects (identify new instances of malware, recognize imposters in passport photos, index the internet) and criteria for success differ from the goals of a child learning about the world, and the fact that one is done in a computer using math and the other is done in organic material using chemistry, direct comparisons between the two will remain fraught.

As an aside, it would be interesting to study how to flip the CIFAR-10 problem around and train a neural network to recognize 6000 objects from 10 examples of each. But even this wouldn't be a fair comparison to 2-year-old, because there would still be a large discrepancy in the total volume, diversity and resolution of the training data.
*We don't presently have a tags for one-shot learning or few-shot learning.
","**Summary:**

Artificial neural networks (ANNs) differ significantly from biological neural networks. ANNs may require fewer training examples for simple tasks due to limited training data and lack of a self-teaching mechanism compared to children.

CIFAR-10, a common image benchmark, has far less training data and lower resolution than the vast and diverse experiences of a 2-year-old child. This may explain ANNs' inferior performance in object recognition tasks.

Machine learning research areas like transfer-learning, one- and few-shot learning, and reinforcement-learning explore methods for self-teaching and adaptation in ANNs. However, direct comparisons to children's learning processes remain challenging due to the fundamental differences between biological and artificial systems."
What are alternatives of Gradient Descent?,https://stats.stackexchange.com/questions/97014/what-are-alternatives-of-gradient-descent,"['machine-learning', 'svm', 'neural-networks']",97014,True,97026,46525,7,64,1677401546,1399620098,50,1399628964,"This is more a problem to do with the function being minimized than the method used, if finding the true global minimum is important, then use a method such a simulated annealing (http://en.wikipedia.org/wiki/Simulated_annealing).  This will be able to find the global minimum, but may take a very long time to do so.
In the case of neural nets, local minima are not necessarily that much of a problem.  Some of the local minima are due to the fact that you can get a functionally identical model by permuting the hidden layer units, or negating the inputs and output weights of the network etc.  Also if the local minima is only slightly non-optimal, then the difference in performance will be minimal and so it won't really matter.  Lastly, and this is an important point, the key problem in fitting a neural network is over-fitting, so aggressively searching for the global minima of the cost function is likely to result in overfitting and a model that performs poorly.
Adding a regularisation term, e.g. weight decay, can help to smooth out the cost function, which can reduce the problem of local minima a little, and is something I would recommend anyway as a means of avoiding overfitting.
The best method however of avoiding local minima in neural networks is to use a Gaussian Process model (or a Radial Basis Function neural network), which have fewer problems with local minima.
","Finding the global minimum of a cost function is crucial for certain optimization tasks. However, in the case of neural networks, local minima may not be significant because functionally equivalent models can be obtained through unit permutations or input/output weight adjustments. Slightly non-optimal local minima may also have minimal impact on performance.

Furthermore, overfitting is a key concern when fitting neural networks. Aggressively searching for the global minimum can lead to overfitting and poor performance. Regularization techniques, such as weight decay, can mitigate local minima while also reducing overfitting.

Gaussian Process models or Radial Basis Function neural networks offer an alternative approach with reduced local minima issues. These models are recommended for applications where avoiding local minima is crucial."
Why sigmoid function instead of anything else?,https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else,"['logistic', 'neural-networks', 'least-squares']",162988,True,163152,54442,4,63,1650293613,1437736470,31,1437840026,"Quoting myself from this answer (https://stats.stackexchange.com/a/156422/17023) to a different question:

In section 4.2 of Pattern Recognition and Machine Learning (http://research.microsoft.com/en-us/um/people/cmbishop/PRML/) (Springer 2006), Bishop shows that the logit arises naturally as the form of the posterior probability distribution in a Bayesian treatment of two-class classification. He then goes on to show that the same holds for discretely distributed features, as well as a subset of the family of exponential distributions. For multi-class classification the logit generalizes to the normalized exponential or softmax function.

This explains why this sigmoid is used in logistic regression.
Regarding neural networks, this blog post (http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/) explains how different nonlinearities including the logit / softmax and the probit used in neural networks can be given a statistical interpretation and thereby a motivation. The underlying idea is that a multi-layered neural network can be regarded as a hierarchy of generalized linear models; according to this, activation functions are link functions, which in turn correspond to different distributional assumptions.
","In a Bayesian approach to binary classification, the logit emerges naturally as the posterior probability distribution. This extends to discrete features and certain exponential distributions. For multiple classes, the logit generalizes to the softmax function. This explains the use of the sigmoid in logistic regression.

In neural networks, nonlinearities like the logit/softmax and probit can be interpreted statistically. A neural network can be viewed as a hierarchy of generalized linear models, where activation functions are link functions corresponding to specific distributional assumptions. This provides a statistical motivation for various nonlinearities used in neural networks."
Why do Convolutional Neural Networks not use a Support Vector Machine to classify?,https://stats.stackexchange.com/questions/168064/why-do-convolutional-neural-networks-not-use-a-support-vector-machine-to-classif,"['machine-learning', 'neural-networks', 'svm', 'deep-learning', 'conv-neural-network']",168064,True,168073,25788,3,63,1635380791,1440081828,73,1440086453,"What is an SVM, anyway?
I think the answer for most purposes is “the solution to the following optimization problem”:
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal H} \frac{1}{n} \sum_{i=1}^n \ell_\mathit{hinge}(f(x_i), y_i) \, + \lambda \lVert f \rVert_{\mathcal H}^2
\\ \ell_\mathit{hinge}(t, y) = \max(0, 1 - t y)
,\end{split}
\tag{SVM}
$$
where $\mathcal H$ is a reproducing kernel Hilbert space, $y$ is a label in $\{-1, 1\}$, and $t = f(x) \in \mathbb R$ is a “decision value”; our final prediction will be $\operatorname{sign}(t)$. In the simplest case, $\mathcal H$ could be the space of affine functions $f(x) = w \cdot x + b$, and $\lVert f \rVert_{\mathcal H}^2 = \lVert w \rVert^2 + b^2$. (Handling of the offset $b$ varies depending on exactly what you’re doing, but that’s not important for our purposes.)
In the ‘90s through the early ‘10s, there was a lot of work on solving this particular optimization problem in various smart ways, and indeed that’s what LIBSVM / LIBLINEAR / SVMlight / ThunderSVM / ... do. But I don’t think that any of these particular algorithms are fundamental to “being an SVM,” really.
Now, how do we train a deep network? Well, we try to solve something like, say,
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal F} \frac1n \sum_{i=1}^n \ell_\mathit{CE}(f(x_i), y) + R(f)
\\
\ell_\mathit{CE}(p, y) = - y \log(p) - (1-y) \log(1 - p)
,\end{split}
   \tag{$\star$}
$$
where now $\mathcal F$ is the set of deep nets we consider, which output probabilities $p = f(x) \in [0, 1]$. The explicit regularizer $R(f)$ might be an L2 penalty on the weights in the network, or we might just use $R(f) = 0$. Although we could solve (SVM) up to machine precision if we really wanted, we usually can’t do that for $(\star)$ when $\mathcal F$ is more than one layer; instead we use stochastic gradient descent to attempt at an approximate solution.
If we take $\mathcal F$ as a reproducing kernel Hilbert space and $R(f) = \lambda \lVert f \rVert_{\mathcal F}^2$, then $(\star)$ becomes very similar to (SVM), just with cross-entropy loss instead of hinge loss: this is also called kernel logistic regression. My understanding is that the reason SVMs took off in a way kernel logistic regression didn’t is largely due to a slight computational advantage of the former (more amenable to these fancy algorithms), and/or historical accident; there isn’t really a huge difference between the two as a whole, as far as I know. (There is sometimes a big difference between an SVM with a fancy kernel and a plain linear logistic regression, but that’s comparing apples to oranges.)
So, what does a deep network using an SVM to classify look like? Well, that could mean some other things, but I think the most natural interpretation is just using $\ell_\mathit{hinge}$ in $(\star)$.
One minor issue is that $\ell_\mathit{hinge}$ isn’t differentiable at $\hat y = y$; we could instead use $\ell_\mathit{hinge}^2$, if we want. (Doing this in (SVM) is sometimes called “L2-SVM” or similar names.) Or we can just ignore the non-differentiability; the ReLU activation isn’t differentiable at 0 either, and this usually doesn’t matter. This can be justified via subgradients (https://en.wikipedia.org/wiki/Subderivative), although note that the correctness here is actually quite subtle (https://proceedings.neurips.cc/paper/2020/hash/4aaa76178f8567e05c8e8295c96171d8-Abstract.html) when dealing with deep networks.
An ICML workshop paper – Tang, Deep Learning using Linear Support Vector Machines (http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf), ICML 2013 workshop Challenges in Representation Learning – found using $\ell_\mathit{hinge}^2$ gave small but consistent improvements over $\ell_\mathit{CE}$ on the problems they considered. I’m sure others have tried (squared) hinge loss since in deep networks, but it certainly hasn’t taken off widely.
(You have to modify both $\ell_\mathit{CE}$ as I’ve written it and $\ell_\mathit{hinge}$ to support multi-class classification, but in the one-vs-rest scheme used by Tang, both are easy to do.)

Another thing that’s sometimes done is to train CNNs in the typical way, but then take the output of a late layer as ""features"" and train a separate SVM on that. This was common in early days of transfer learning with deep features, but is I think less common now.

Something like this is also done sometimes in other contexts, e.g. in meta-learning by Lee et al., Meta-Learning with Differentiable Convex Optimization (https://arxiv.org/abs/1904.03758), CVPR 2019, who actually solved (SVM) on deep network features and backpropped through the whole thing. (They didn't, but you can even do this with a nonlinear kernel in $\mathcal H$; this is also done in some other ""deep kernels"" contexts.) It’s a very cool approach – one that I've also worked on – and in certain domains this type of approach makes a ton of sense, but there are some pitfalls, and I don’t think it’s very applicable to a typical ""plain classification"" problem.
","Support Vector Machines (SVMs) seek to minimize a hinge loss function penalizing predictions that deviate from the true labels. This optimization problem is typically solved within a reproducing kernel Hilbert space, with the regularization term controlling the complexity of the model.

Similarly, deep networks can be trained by minimizing a cross-entropy loss function regularized by an L2 penalty. When the network's function space is defined as a reproducing kernel Hilbert space, the optimization problem resembles SVM, using cross-entropy loss instead of hinge loss (known as kernel logistic regression).

Using SVMs for classification in deep networks can involve employing hinge loss in the optimization objective. The non-differentiability of hinge loss can be addressed by using its squared form or ignoring it, as with the ReLU activation.

While studies have shown that squared hinge loss may offer slight improvements over cross-entropy loss in deep networks, it remains uncommon. Additionally, SVM training can be applied to features extracted from deep networks, or used in meta-learning contexts. However, these approaches may have limitations in standard classification problems."
How does LSTM prevent the vanishing gradient problem?,https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem,"['neural-networks', 'lstm']",185639,True,263956,59318,4,63,1564498983,1449565307,35,1488017915,"The vanishing gradient is best explained in the one-dimensional case. The multi-dimensional is more complicated but essentially analogous. You can review it in this excellent paper [1].
Assume we have a hidden state $h_t$ at time step $t$. If we make things simple and remove biases and inputs, we have
$$h_t = \sigma(w h_{t-1}).$$
Then you can show that 
\begin{align}
\frac{\partial h_{t'}}{\partial h_t} 
&= \prod_{k=1}^{t' - t} w \sigma'(w h_{t'-k})\\
&= \underbrace{w^{t' - t}}_{!!!}\prod_{k=1}^{t' - t} \sigma'(w h_{t'-k})
\end{align}
The factored marked with !!! is the crucial one. If the weight is not equal to 1, it will either decay to zero exponentially fast in $t'-t$, or grow exponentially fast.
In LSTMs, you have the cell state $s_t$. The derivative there is of the form
$$\frac{\partial s_{t'}}{\partial s_t} = \prod_{k=1}^{t' - t} \sigma(v_{t+k}).$$
Here $v_t$ is the input to the forget gate. As you can see, there is no exponentially fast decaying factor involved. Consequently, there is at least one path where the gradient does not vanish. For the complete derivation, see [2].
[1] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. ""On the difficulty of training recurrent neural networks."" ICML (3) 28 (2013): 1310-1318.
[2] Bayer, Justin Simon. Learning Sequence Representations. Diss. München, Technische Universität München, Diss., 2015, 2015.
","The vanishing gradient problem occurs in multi-layered neural networks where gradients become exponentially small or large with increasing layer depth, hindering training. In one-dimensional networks, this is caused by a term in the gradient that decays exponentially with layer count.

However, in Long Short-Term Memory (LSTM) networks, a crucial difference lies in the cell state gradient, which does not include this exponentially decaying factor. Instead, it depends on the forget gate input, allowing for at least one non-vanishing gradient path.

This distinction allows LSTMs to avoid the vanishing gradient problem, unlike traditional neural networks, and is a key reason for their success in processing sequential data."
Recurrent vs Recursive Neural Networks: Which is better for NLP?,https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp,"['machine-learning', 'neural-networks', 'deep-learning', 'natural-language']",153599,True,158995,57199,4,62,1586887349,1432317020,54,1435446936,"Recurrent Neural networks are recurring over time. For example if you have a sequence
x = ['h', 'e', 'l', 'l']
This sequence is fed to a single neuron which has a single connection to itself.
At time step 0, the letter 'h' is given as input.At time step 1, 'e' is given as input. The network when unfolded over time will look like this.

A recursive network is just a generalization of a recurrent network. In a recurrent network the weights are shared (and dimensionality remains constant) along the length of the sequence because how would you deal with position-dependent weights when you encounter a sequence at test-time of different length to any you saw at train-time. In a recursive network the weights are shared (and dimensionality remains constant) at every node for the same reason.
This means that all the W_xh weights will be equal(shared) and so will be the W_hh weight. This is simply because it is a single neuron which has been unfolded in time.
This is what a Recursive Neural Network looks like.

It is quite simple to see why it is called a Recursive Neural Network. Each parent node's children are simply a node similar to that node.
The Neural network you want to use depends on your usage. In Karpathy's blog (https://karpathy.github.io/2015/05/21/rnn-effectiveness/), he is generating characters one at a time so a recurrent neural network is good.
But if you want to generate a parse tree, then using a Recursive Neural Network is better because it helps to create better hierarchical representations. 
If you want to do deep learning in c++, then use CUDA. It has a nice user-base, and is fast. I do not know more about that so cannot comment more.
In python, Theano is the best option because it provides automatic differentiation, which means that when you are forming big, awkward NNs, you don't have to find gradients by hand. Theano does it automatically for you. This feature is lacked by Torch7. 
Theano is very fast as it provides C wrappers to python code and can be implemented on GPUs.  It also has an awesome user base, which is very important while learning something new.
","Recurrent Neural Networks (RNNs) process sequences iteratively, allowing them to learn temporal dependencies. They unfold over time, with a single neuron connecting to itself. Recursive Neural Networks (Recursive NNs) generalize RNNs, sharing weights at every node. This allows them to handle variable-length sequences.

The choice between RNNs and Recursive NNs depends on the task. RNNs are suitable for tasks like character generation, while Recursive NNs excel at creating hierarchical representations (e.g., parse trees).

For deep learning in C++, CUDA is recommended due to its speed and user base. In Python, Theano offers automatic differentiation, making it easier to train complex neural networks. It also provides C wrappers for fast execution and has an extensive user base for support."
Recurrent vs Recursive Neural Networks: Which is better for NLP?,https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp,"['machine-learning', 'neural-networks', 'deep-learning', 'natural-language']",153599,True,158995,57199,4,62,1586887349,1432317020,54,1435446936,"Recurrent Neural networks are recurring over time. For example if you have a sequence
x = ['h', 'e', 'l', 'l']
This sequence is fed to a single neuron which has a single connection to itself.
At time step 0, the letter 'h' is given as input.At time step 1, 'e' is given as input. The network when unfolded over time will look like this.

A recursive network is just a generalization of a recurrent network. In a recurrent network the weights are shared (and dimensionality remains constant) along the length of the sequence because how would you deal with position-dependent weights when you encounter a sequence at test-time of different length to any you saw at train-time. In a recursive network the weights are shared (and dimensionality remains constant) at every node for the same reason.
This means that all the W_xh weights will be equal(shared) and so will be the W_hh weight. This is simply because it is a single neuron which has been unfolded in time.
This is what a Recursive Neural Network looks like.

It is quite simple to see why it is called a Recursive Neural Network. Each parent node's children are simply a node similar to that node.
The Neural network you want to use depends on your usage. In Karpathy's blog (https://karpathy.github.io/2015/05/21/rnn-effectiveness/), he is generating characters one at a time so a recurrent neural network is good.
But if you want to generate a parse tree, then using a Recursive Neural Network is better because it helps to create better hierarchical representations. 
If you want to do deep learning in c++, then use CUDA. It has a nice user-base, and is fast. I do not know more about that so cannot comment more.
In python, Theano is the best option because it provides automatic differentiation, which means that when you are forming big, awkward NNs, you don't have to find gradients by hand. Theano does it automatically for you. This feature is lacked by Torch7. 
Theano is very fast as it provides C wrappers to python code and can be implemented on GPUs.  It also has an awesome user base, which is very important while learning something new.
","Recurrent Neural Networks (RNNs) are networks that operate over sequences, allowing them to capture temporal dependencies. In a simplified RNN, a single neuron connects to itself over time, processing sequential data (e.g., ""hell"" as ['h', 'e', 'l', 'l']).

Recursive Neural Networks (RecNNs) generalize RNNs by sharing weights and dimensions at every node, making them suitable for hierarchical representations (e.g., parse trees). The choice between RNNs and RecNNs depends on the application: RNNs excel at generating sequential data, while RecNNs are better for creating structured representations.

For deep learning in Python, Theano is recommended due to its automatic differentiation and support for both CPUs and GPUs. Theano also boasts a large user base, making it an accessible and efficient choice."
Neural networks vs support vector machines: are the second definitely superior?,https://stats.stackexchange.com/questions/30042/neural-networks-vs-support-vector-machines-are-the-second-definitely-superior,"['machine-learning', 'svm', 'neural-networks']",30042,True,30100,57270,5,61,1563481549,1339124379,51,1339185721,"It is a matter of trade-offs. SVMs are in right now, NNs used to be in. You'll find a rising number of papers that claim Random Forests, Probabilistic Graphic Models or Nonparametric Bayesian methods are in. Someone should publish a forecasting model in the Annals of Improbable Research on what models will be considered hip.
Having said that for many famously difficult supervised problems the best performing single models are some type of NN, some type of SVMs or a problem specific stochastic gradient descent method implemented using signal processing methods.

Pros of NN:

They are extremely flexible in the types of data they can support. NNs do a decent job at learning the important features from basically any data structure, without having to manually derive features.



NN still benefit from feature engineering, e.g. you should have an area feature if you have a length and width. The model will perform better for the same computational effort.
Most of supervised machine learning requires you to have your data structured in a observations by features matrix, with the labels as a vector of length observations. This restriction is not necessary with NN. There is fantastic work with structured SVM, but it is unlikely it will ever be as flexible as NNs.



Pros of SVM:

Fewer hyperparameters. Generally SVMs require less grid-searching to get a reasonably accurate model. SVM with a RBF kernel usually performs quite well.
Global optimum guaranteed.


Cons of NN and SVM:

For most purposes they are both black boxes. There is some research on interpreting SVMs, but I doubt it will ever be as intuitive as GLMs. This is a serious problem in some problem domains.



If you're going to accept a black box then you can usually squeeze out quite a bit more accuracy by bagging/stacking/boosting many many models with different trade-offs.

Random forests are attractive because they can produce out-of-bag predictions(leave-one-out predictions) with no extra effort, they are very interpretable, they have an good bias-variance trade-off(great for bagging models) and they are relatively robust to selection bias. Stupidly simple to write a parallel implementation of.
Probabilistic graphical models are attractive because they can incorporate domain-specific-knowledge directly into the model and are interpretable in this regard.
Nonparametric(or really extremely parametric) Bayesian methods are attractive because they produce confidence intervals directly. They perform very well on small sample sizes and very well on large sample sizes. Stupidly simple to write a linear algebra implementation of.



","The choice of supervised machine learning models involves trade-offs. Neural networks (NNs) and support vector machines (SVMs) are currently popular, while other methods like random forests, probabilistic graphical models, and nonparametric Bayesian methods are gaining attention. NNs excel in handling diverse data types and feature extraction, while SVMs have fewer hyperparameters and guarantee global optima. However, both NN and SVM are often considered black boxes, lacking interpretability. Random forests offer interpretability and out-of-bag predictions, probabilistic graphical models incorporate domain knowledge, and nonparametric Bayesian methods provide confidence intervals and robustness. Ultimately, the selection of a model depends on the specific problem, data characteristics, and desired attributes such as interpretability and accuracy."
CNN architectures for regression?,https://stats.stackexchange.com/questions/335836/cnn-architectures-for-regression,"['regression', 'machine-learning', 'neural-networks', 'conv-neural-network', 'tensorflow']",335836,True,336077,90947,1,59,1565009830,1521636786,101,1521727701,"First of all a general suggestion: do a literature search before you start making experiments on a topic you're not familiar with. You'll save yourself a lot of time. 
In this case, looking at existing papers you may have noticed that 

CNNs have been used multiple times for regression: this (https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf) is a classic but it's old (yes, 3 years is old in DL). A more modern paper wouldn't have used AlexNet for this task. This (https://arxiv.org/abs/1708.05628) is more recent, but it's for a vastly more complicated problem (3D rotation), and anyway I'm not familiar with it.
Regression with CNNs is not a trivial problem. Looking again at the first paper, you'll see that they have a problem where they can basically generate infinite data. Their objective is to predict the rotation angle needed to rectify 2D pictures. This means that I can basically take my training set and augment it by rotating each image by arbitrary angles, and I'll obtain a valid, bigger training set. Thus the problem seems relatively simple, as far as Deep Learning problems go. By the way, note the other data augmentation tricks they use: 

We use translations (up to 5% of the image width), brightness
  adjustment in the range [−0.2, 0.2], gamma adjustment with γ ∈ [−0.5,
  0.1] and Gaussian pixel noise with a standard deviation in the range [0, 
  0.02].

I don't know your problem well enough to say if it makes sense to consider 
variations in position, brightness and gamma noise for your pictures, carefully 
shot in a lab. But you can always try, and remove it if it doesn't improve your test set loss. Actually, you should really use a validation set or $k-$fold cross-validation for these kinds of experiments, and don't look at the test set until you have defined your setup, if you want the test set loss to be representative of the generalization error.
Anyway, even in their ideal conditions, the naive approach didn't work that well (section 4.2). They stripped out the output layer (the softmax layer) and substituted it with a layer with two units which would predict the sine $y$ and cosine $x$ of the rotation angle. The actual angle would then be computed as $\alpha=\text{atan2}(y,x)$. The neural network was also pretrained on ImageNet (this is called transfer learning). Of course the training on ImageNet had been for a different task (classification), but still training the neural network from scratch must have given such horrible results that they decided not to publish them. So you had all ingredients to make a good omelette: potentially infinite training data, a pretrained network and an apparently simple regression problem (predict two numbers between -1 and 1). Yet, the best they could get with this approach was a 21° error. It's not clear if this is an RMSE error, a MAD error or what, but still it's not great: since the maximum error you can make is 180°, the average error is $>11\%$ of the maximum possible error. They did slightly better by using two networks in series: the first one would perform  classification (predict whether the angle would be in the $[-180°,-90°],[-90°,0°],[0°,90°]$ or $[90°,180°]$ class), then the image, rotated by the amount predicted by the first network, would be fed to another neural network (for regression, this time), which would predict the final additional rotation in the $[-45°,45°]$ range.
On a much simpler (rotated MNIST) problem, you can get something better (https://it.mathworks.com/help/nnet/examples/train-a-convolutional-neural-network-for-regression.html?requestedDomain=true), but still you don't go below an RMSE error which is $2.6\%$ of the maximum possible error. 

So, what can we learn from this? First of all, that 5000 images is a small data set for your task. The first paper used a network which was pretrained on images similar to that for which they wanted to learn the regression task: not only you need to learn a different task from that for which the architecture was designed (classification), but your training set doesn't look anything at all like the training sets on which these networks are usually trained (CIFAR-10/100 or ImageNet). So you probably won't get any benefits from transfer learning. The MATLAB example had 5000 images, but they were black and white and semantically all very similar (well, this could be your case too). 
Then, how realistic is doing better than 0.3? We must first of all understand what do you mean by 0.3 average loss. Do you mean that the RMSE error is 0.3,
$$\frac{1}{N}\sum_{i=1}^N (h(\mathbf{x}_i)-y_i)^2$$
where $N$ is the size of your training set (thus, $N< 5000$), $h(\mathbf{x}_i)$ is the output of your CNN for image $\mathbf{x}_i$ and $y_i$ is the corresponding concentration of the chemical? Since $y_i\in[80,350]$, then assuming that you clip the predictions of your CNN between 80 and 350 (or you just use a logit to make them fit in that interval), you're getting less than $0.12\%$ error. Seriously, what do you expect? it doesn't seem to me a big error at all. 
Also, just try to compute the number of parameters in your network: I'm in a hurry and I may be making silly mistakes, so by all means double check my computations with some summary function from whatever framework you may be using. However, roughly I would say you have
$$9\times(3\times 32 + 2\times 32\times 32 + 32\times64+2\times64\times64+ 64\times128+2\times128\times128) +128\times128+128\times32+32 \times32\times32=533344$$
(note I skipped the parameters of the batch norm layers, but they're just 4 parameters for layer so they don't make a difference). You have half a million parameters and 5000 examples...what would you expect? Sure, the number of parameters is not a good indicator for the capacity of a neural network (it's a non-identifiable model), but still...I don't think you can do much better than this, but you can try a few things:

normalize all inputs (for example, rescale the RGB intensities of each pixel between -1 and 1, or use standardization) and all outputs. This will especially help if you have convergence issues.
go to grayscale: this would reduce your input channels from 3 to 1. All your images seem (to my highly untrained eye) to be of relatively similar colors. Are you sure it's the color that it's needed to predict $y$, and not the existence of darker or brighter areas? Maybe you're sure (I'm not an expert): in this case skip this suggestion.
data augmentation: since you said that flipping, rotating by an arbitrary angle or mirroring your images should result in the same output, you can increase the size of your data set a lot. Note that with a bigger dataset the error on the training set will go up: what we're looking for here is a smaller gap between training set loss and test set loss. Also, if the training set loss increases a lot, this could be good news: it may mean that you can train a deeper network on this bigger training set without the risk of overfitting. Try adding more layers and see if now you get a smaller training set and test set loss. Finally, you could try also the other data augmentation tricks I quoted above, if they make sense in the context of your application.
use the classification-then-regression trick: a first network only determines if $y$ should be in one of, say, 10 bins, such as $[80,97],[97,124]$,etc. A second network then computes a $[0,27]$ correction: centering and normalizing may help here too. Can't say without trying.
try using a modern architecture (Inception or ResNet) instead than a vintage one. ResNet has actually less parameters than VGG-net. Of course, you want to use the small ResNets here - I don't think ResNet-101 could help on a 5000 images data set. You can augment the data set a lot, though....
Since your output is invariant to rotation, another great idea would be to use either group equivariant CNNs (http://proceedings.mlr.press/v48/cohenc16.html), whose output (when used as classifiers) is invariant to discrete rotations, or steerable CNNs (https://arxiv.org/pdf/1612.08498.pdf) whose output is invariant to continuous rotations. The invariance property would allow you to get good results with much less data augmentation, or ideally none at all (for what it concerns rotations: of course you still need the other types of d. a.). Group equivariant CNNs are more mature than steerable CNNs from an implementation point of view, so I’d try group CNNs first. You can try the classification-then-regression, using the G-CNN for the classification part, or you may experiment with the pure regression approach. Remember to change the top layer accordingly.
experiment with the batch size (yeah, yeah, I know hyperparameters-hacking is not cool, but this is the best I could come with in a limited time frame & for free :-)
finally, there are architectures which have been especially developed to make accurate predictions with small data sets. Most of them used dilated convolutions: one famous example is the mixed-scale dense convolutional neural network (http://www.pnas.org/content/115/2/254). The implementation is not trivial, though.

","**Summary:**

Regression tasks using Convolutional Neural Networks (CNNs) can be challenging, especially with limited data. Despite using a pretrained network and data augmentation, research shows that achieving high accuracy can be difficult. This is because the networks have a large number of parameters compared to the small training set.

To improve accuracy, several suggestions are made: normalize inputs and outputs, consider using grayscale images, and extensively use data augmentation techniques (flipping, rotating, mirroring). Additionally, using a classification-then-regression approach or modern architectures (e.g., ResNet or Inception) can be beneficial.

Novel architectures like group equivariant CNNs or steerable CNNs may be suitable for tasks with rotational invariance. Experimenting with batch size and using architectures designed for small data sets (e.g., mixed-scale dense convolutional neural networks) may also enhance performance."
Why do we use ReLU in neural networks and how do we use it?,https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it,['neural-networks'],226923,True,226927,137154,6,59,1616778699,1470158770,55,1470160787,"The ReLU function is $f(x)=\max(0, x).$ Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like.
One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of $x$). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations.
Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is  has its own problem, called ""dead neurons,"" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU, or PReLU, etc.) can ameliorate this.
$\frac{d}{dx}\text{ReLU}(x)=1\forall x > 0$ . By contrast, the gradient of a sigmoid unit is at most $0.25$; on the other hand, $\tanh$ fares better for inputs in a region near 0 since $0.25 < \frac{d}{dx}\tanh(x) \le 1 \forall x \in [-1.31, 1.31]$ (approximately).
","ReLU (Rectified Linear Unit) function, defined as $f(x) = \max(0, x)$, is a widely used activation function in multi-layer perceptron (MLP) neural networks. It has several advantages:

* **Faster training:** Simple gradient computation and no complex operations speed up training.
* **Improved accuracy:** Positive gradients accelerate weight updates, leading to better performance.

However, ReLU has the drawback of ""dead neurons"" where gradients are zero for negative inputs. Modified ReLU variants like ELU and Leaky ReLU address this issue.

In comparison to other activation functions, ReLU's gradient is constant for positive inputs, while sigmoid has a maximum gradient of 0.25 and tanh has a higher gradient for values near zero."
Adam optimizer with exponential decay,https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay,"['neural-networks', 'deep-learning', 'gradient-descent', 'tensorflow', 'adam']",200063,True,200105,90401,6,59,1572756842,1457166121,46,1457194373,"Empirically speaking: definitely try it out, you may find some very useful training heuristics, in which case, please do share!

Usually people use some kind of decay, for Adam it seems uncommon. Is there any theoretical reason for this? Can it be useful to combine Adam optimizer with decay?

I haven't seen enough people's code using ADAM optimizer to say if this is true or not. If it is true, perhaps it's because ADAM is relatively new and learning rate decay ""best practices"" haven't been established yet. 
I do want to note however that learning rate decay is actually part of the theoretical guarantee for ADAM. Specifically in Theorem 4.1 of their ICLR article (http://arxiv.org/abs/1412.6980), one of their hypotheses is that the learning rate has a square root decay, $\alpha_t = \alpha/\sqrt{t}$. Furthermore, for their logistic regression experiments they use the square root decay as well. 
Simply put: I don't think anything in the theory discourages using learning rate decay rules with ADAM. I have seen people report some good results using ADAM and finding some good training heuristics would be incredibly valuable. 
","Adam, a widely used optimizer in deep learning, does not commonly employ learning rate decay. However, theory suggests that decay is not discouraged in Adam's theoretical foundations. Experiments have shown promising results combining Adam with square root decay, which is an integral part of its theoretical guarantees. 

Overall, integrating learning rate decay rules into Adam is theoretically valid and empirically beneficial, leading to improved training heuristics. Researchers are encouraged to explore and share their findings on this strategy's effectiveness."
What are the main theorems in Machine (Deep) Learning?,https://stats.stackexchange.com/questions/321851/what-are-the-main-theorems-in-machine-deep-learning,"['machine-learning', 'deep-learning', 'mathematical-statistics']",321851,True,322934,9854,6,58,1595866955,1515253072,65,1515856887,"As I wrote in the comments, this question seems too broad to me, but I'll make an attempt to an answer. In order to set some boundaries, I will start with a little math which underlies most of ML, and then concentrate on recent results for DL.

The bias-variance tradeoff is referred to in countless books, courses, MOOCs, blogs, tweets, etc. on ML, so we can't start without mentioning it:
$$\mathbb{E}[(Y-\hat{f}(X))^2|X=x_0]=\sigma_{\epsilon}^2+\left(\mathbb{E}\hat{f}(x_0)-f(x_0)\right)^2+\mathbb{E}\left[\left(\hat{f}(x_0)-\mathbb{E}\hat{f}(x_0)\right)^2\right]=\text{Irreducible error + Bias}^2 \text{ + Variance}$$
Proof here: https://web.stanford.edu/~hastie/ElemStatLearn/ (https://web.stanford.edu/%7Ehastie/ElemStatLearn/)

The Gauss-Markov Theorem (yes, linear regression will remain an important part of Machine Learning, no matter what: deal with it) clarifies that, when the linear model is true and some assumptions on the error term are valid, OLS has the minimum mean squared error (which in the above expression is just $\text{Bias}^2 \text{ + Variance}$) only among the unbiased linear estimators of the linear model. Thus there could well be linear estimators with bias (or nonlinear estimators) which have a better mean square error, and thus a better expected prediction error, than OLS. And this paves the way to all the regularization arsenal (ridge regression, LASSO, weight decay, etc.) which is a workhorse of ML. A proof is given here (and in countless other books):
https://www.amazon.com/Linear-Statistical-Models-James-Stapleton/dp/0470231467 (https://rads.stackoverflow.com/amzn/click/com/0470231467)
Probably more relevant to the explosion of regularization approaches, as noted by Carlos Cinelli in the comments, and definitely more fun to learn about, is the James-Stein theorem. Consider $n$ independent, same variance but not same mean Gaussian random variables:
$$X_i|\mu_i\sim \mathcal{N}(\theta_i,\sigma^2), \quad i=1,\dots,n$$
in other words, we have an $n-$components Gaussian random vector $\mathbf{X}\sim \mathcal{N}(\boldsymbol{\theta},\sigma^2I)$. We have one sample $\mathbf{x}$ from $\mathbf{X}$ and we want to estimate $\boldsymbol{\theta}$. The MLE (and also UMVUE) estimator is obviously $\hat{\boldsymbol{\theta}}_{MLE}=\mathbf{x}$. Consider the James-Stein estimator
$$\hat{\boldsymbol{\theta}}_{JS}= \left(1-\frac{(n-2)\sigma^2}{||\mathbf{x}||^2}\right)\mathbf{x} $$
Clearly, if $(n-2)\sigma^2\leq||\mathbf{x}||^2$, $\hat{\boldsymbol{\theta}}_{JS}$ shrinks the MLE estimate towards zero. The James-Stein theorem states that for $n\geq4$, $\hat{\boldsymbol{\theta}}_{JS}$ strictly dominates $\hat{\boldsymbol{\theta}}_{MLE}$, i.e., it has lower MSE $\forall \ \boldsymbol{\theta}$. Pheraps surprisingly, even if we shrink towards any other constant $\boldsymbol{c}\neq \mathbf{0}$, $\hat{\boldsymbol{\theta}}_{JS}$ still dominates $\hat{\boldsymbol{\theta}}_{MLE}$. Since the $X_i$ are independent, it may seem weird that, when trying to estimate  the height of three unrelated persons, including a sample from the number of apples produced in Spain, may improve our estimate on average. The key point here is ""on average"": the mean square error for the simultaneous estimation of all the components of the parameter vector is smaller, but the square error for one or more components  may well be larger, and indeed it often is, when you have ""extreme"" observations.
Finding out that MLE, which was indeed the ""optimal"" estimator for the univariate estimation case, was dethroned for multivariate estimation, was quite a shock at the time, and led to a great interest in shrinkage, better known as regularization in ML parlance. One could note some similarities with mixed models and the concept of ""borrowing strength"": there is indeed some connection, as discussed here
Unified view on shrinkage: what is the relation (if any) between Stein's paradox, ridge regression, and random effects in mixed models? (https://stats.stackexchange.com/questions/122062/unified-view-on-shrinkage-what-is-the-relation-if-any-between-steins-paradox)
Reference: James, W., Stein, C., Estimation with Quadratic Loss. Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 361--379, University of California Press, Berkeley, Calif., 1961 (https://projecteuclid.org/euclid.bsmsp/1200512173)

Principal Component Analysis is key to the important topic of dimension reduction, and it's based on the Singular Value Decomposition: for each $N\times p$ real matrix $X$ (although the theorem easily generalizes to complex matrices) we can write
$$X=UDV^T$$
where $U$ of size $N \times p$ is orthogonal, $D$ is a $p \times p$ diagonal matrix with nonnegative diagonal elements and $U$ of size $p \times p$ is again orthogonal. For proofs and algorithms on how to compute it see: Golub, G., and Van Loan, C. (1983), Matrix computations, John Hopkins University press, Baltimore.

Mercer's theorem is the founding stone for a lot of different ML methods: thin plate splines, support vector machines, the Kriging estimate of a Gaussian random process, etc. Basically, is one of the two theorems behind the so-called kernel trick. Let $K(x,y):[a,b]\times[a,b]\to\mathbb{R}$ be a symmmetric continuous function or kernel. if $K$ is positive semidefinite, then it admits an orthornormal basis of eigenfunctions corresponding to nonnegative eigenvalues:
$$K(x,y)=\sum_{i=1}^\infty\gamma_i \phi_i(x)\phi_i(y)$$
The importance of this theorem for ML theory is testified by the number of references it gets in famous texts, such as for example Rasmussen & Williams text on Gaussian processes (http://www.gaussianprocess.org/gpml/chapters/RW4.pdf).
Reference: J. Mercer, Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209:415-446, 1909 (http://rsta.royalsocietypublishing.org/content/roypta/209/441-458/415.full.pdf)
There is also a simpler presentation in Konrad Jörgens, Linear integral operators, Pitman, Boston, 1982.

The other theorem which, together with Mercer's theorem, lays out the theoretical foundation of the kernel trick, is the representer theorem. Suppose you have a sample space $\mathcal{X}$ and a symmetric positive semidefinite kernel $K: \mathcal{X} \times \mathcal{X}\to \mathbb{R}$. Also let $\mathcal{H}_K$ be the RKHS associated with $K$. Finally, let $S=\{\mathbb{x}_i,y_i\}_{i=1}^n$ be a training sample. The theorem says that among all functions $f\in \mathcal{H}_K$, which all admit an infinite representation in terms of eigenfunctions of $K$ because of Mercer's theorem, the one that minimizes the regularized risk always has a finite representation in the basis formed by the kernel evaluated at the $n$ training points, i.e.
$$\min_{f \in \mathcal{H}_K} \sum_{i=1}^n L(y_i,f(x_i))+\lambda||f||^2_{\mathcal{H}_K}=\min_{\{c_j\}_1^\infty} \sum_{i=1}^n L(y_i,\sum_j^\infty c_j\phi_j(x_i))+\lambda\sum_j^\infty \frac{c_j^2}{\gamma_j}=\sum_{i=1}^n\alpha_i K(x,x_i)$$
(the theorem is the last equality). References: Wahba, G. 1990, Spline Models for Observational Data, SIAM, Philadelphia.

The universal approximation theorem has been already cited by user Tobias Windisch and is much less relevant to Machine Learning than it is to functional analysis, even if it may not seem so at a first glance. The problem is that the theorem only says that such a network exists, but:

it doesn't give any correlation between the size $N$ of the hidden layer and some measure of complexity of the target function $f(x)$, such as for example Total Variation. If $f(x)=\sin(\omega x):[0,2\pi]\to[-1,1]$ and the $N$ required for a fixed error $\epsilon$ growed exponentially with $\omega$, then single hidden layer neural networks would be useless.
it doesn't say if the network $F(x)$ is learnable. In other words assume that given $f$ and $\epsilon$, we know that a size $N$ NN will approximate $f$ with the required tolerance in the hypercube. Then by using training sets of size $M$  and a learning procedure such as for example back-prop, do we have any guarantee that by increasing $M$ we can recover $F$?
finally, and worse of them all, it doesn't say anything about the prediction error of neural networks. What we're really interested in is an estimate of the prediction error, at least averaged over all training sets of size $M$. The theorem doesn't help in this respect.

A smaller pain point with the Hornik's version of this theorem is that it doesn't hold for ReLU activation functions. However, Bartlett has since proved  an extended version which covers this gap.

Until now, I guess all the theorems I considered were well-known to anybody. So now it's time for the fun stuff :-) Let's see a few Deep Learning theorems:
Assumptions:

the deep neural network $\Phi(X,W)$ (for fixed $W$, $\Phi_W(X)$ is the function which associates the inputs of the neural network with its outputs) and the regularization loss $\Theta(W)$ are both sums of positively homogeneous functions of the same degree
the loss function $L(Y,\Phi(X,W))$ is convex and at least once differentiable in $X$, in a compact set $S$

Then:

any local minimum for $L(Y,\Phi(X,W))+\lambda\Theta(W)$ such that a subnetwork of $\Phi(X,W)$ has zero weights, is a global minimum (Theorem 1)
above a critical network size, local descent will always converge to a global minimum from any initialization (Theorem 2).

This is very interesting: CNNs made only of convolutional layers, ReLU, max-pooling, fully connected ReLU and linear layers are positively homogenous functions, while if we include sigmoid activation functions, this isn't true anymore, which may partly explain the superior performance in some applications of ReLU + max pooling with respect to sigmoids. What's more, the theorems only hold if also $\Theta$ is positively homogeneous in $W$ of the same degree as $\Phi$. Now, the fun fact is that  $l_1$ or $l_2$ regularization, although positively homogeneous, don't have the same degree of $\Phi$ (the degree of $\Phi$, in the simple CNN case mentioned before, increases with the number of layers). Instead, more modern regularization methods such as batch normalization and path-SGD do correspond to a positively homogeneous regularization function of the same degree as $\Phi$, and dropout, while not fitting this framework exactly, holds strong similarities to it. This may explain why, in order to get high accuracy with CNNs, $l_1$ and $l_2$ regularization are not enough, but we need to employ all kinds of devilish tricks, such as dropout and batch normalization! To the best of my knowledge, this is the closest thing to an explanation of the efficacy of batch normalization, which is otherwise very obscure, as correctly noted by Al Rahimi in his talk.
Another observation that some people make, based on Theorem 1, is that it could explain why ReLU work well, even with the problem of dead neurons. According to this intuition, the fact that, during training, some ReLU neurons ""die"" (go to zero activation and then never recover from that, since for $x<0$ the gradient of ReLU is zero) is ""a feature, not a bug"", because if we have reached a minimum and a full subnetwork has died, then we're provably reached a global minimum (under the hypotheses of Theorem 1). I may be missing something, but I think this interpretation is far-fetched. First of all, during training ReLUs can ""die"" well before we have reached a local minimun. Secondly, it has to be proved that when ReLU units ""die"", they always do it over a full subnetwork: the only case where this is trivially true is when you have just one hidden layer, in which case of course each single neuron is a subnetwork. But in general I would be very cautious in seeing ""dead neurons"" as a good thing.
References:
B. Haeffele and R. Vidal, Global optimality in neural network training,
In IEEE Conference on Computer Vision and Pattern Recognition,
2017.
B. Haeffele and R. Vidal. Global optimality in tensor factorization,
deep learning, and beyond, arXiv, abs/1506.07540, 2015.

Image classification requires learning representations which are invariant (or at least robust, i.e., very weakly sensitive) to various transformations such as  location, pose, viewpoint, lighting, expression, etc. which are commonly present in natural images, but do not contain info for the classification task. Same thing for speech recognition: changes in pitch, volume, pace, accent. etc. should not lead to a change in the classification of the word. Operations such as convolution, max pooling, average pooling, etc., used in CNNs, have exactly this goal, so intuitively we expect that they would work for these applications. But do we have theorems to support this intuition? There is a vertical translation invariance theorem, which, notwithstanding the name, has nothing to do with translation in the vertical direction, but it's basically a result which says that features learnt in following layers get more and more invariant, as the number of layers grows. This is opposed to an older horizontal translation invariance theorem which however holds for scattering networks, but not for CNNs.
The theorem is very technical, however:

assume $f$ (your input image) is square-integrable
assume your filter commutes with the translation operator $T_t$, which maps the input image $f$ to a translated copy of itself $T_t f$. A learned convolution kernel (filter) satisfies this hypothesis.
assume all filters, nonlinearities and pooling in your network satisfy a so-called weak admissibility condition, which is basically some sort of weak regularity and boundedness conditions. These conditions are satisfied by learned convolution kernel (as long as some normalization operation is performed on each layer), ReLU, sigmoid, tanh, etc, nonlinearities, and by average pooling, but not by max-pooling. So it covers some (not all) real world CNN architectures.
Assume finally that each layer $n$ has a pooling factor $S_n> 1$, i.e., pooling is applied in each layer and effectively discards information. The condition $S_n\geq 1 $ would also suffice for a weaker version of the theorem.

Indicate with $\Phi^n(f)$ the output of layer $n$ of the CNN, when the input is $f$. Then finally:
$$\lim_{n\to\infty}|||\Phi^n(T_f f)-\Phi^n(f)|||=0$$
(the triple bars are not an error) which basically means that each layer learns features which become more and more invariant, and in the limit of an infinitely deep network we have a perfectly invariant architecture. Since CNNs have a finite number of layers, they're not perfectly translation-invariant, which is something well-known to practitioners.
Reference: T. Wiatowski and H. Bolcskei, A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction, arXiv:1512.06293v3 (https://arxiv.org/pdf/1512.06293.pdf).

To conclude, numerous bounds for the generalization error of a Deep Neural Network based on its Vapnik-Chervonkensis dimension or on the Rademacher complexity grow with the number of parameters (some even exponentially), which means they can't explain why DNNs work so well in practice even when the number of parameters is considerably larger than the number of training samples. As a matter of fact, VC theory is not very useful in Deep Learning.
Conversely, some results from last year bound the generalization error of a DNN classifier with a quantity which is independent of the neural network's depth and size, but depends only on the structure of the training set and the input space. Under some pretty technical assumptions on the learning procedure, and on the training set and input space, but with very little assumptions on the DNN (in particular, CNNs are fully covered), then with probability at least $1-\delta$, we have
$$\text{GE} \leq \sqrt{2\log{2}N_y\frac{\mathcal{N_{\gamma}}}{m}}+\sqrt{\frac{2\log{(1/\delta)}}{m}}$$
where:

$\text{GE}$ is the generalization error, defined as the difference between the expected loss (the average loss of the learned classifier on all possible test points) and the empirical loss (just the good ol' training set error)
$N_y$ is the number of classes
$m$ is the size of the training set
$\mathcal{N_{\gamma}}$ is the covering number of the data, a quantity related to the structure of the input space and to the the minimal separation among points of different classes in the training set. Reference:

J. Sokolic, R. Giryes, G. Sapiro, and M. Rodrigues. Generalization
error of invariant classifiers. In AISTATS, 2017
","**Summary of Mathematical Foundations of Machine Learning and Deep Learning**

Mathematical principles underpin the functioning of machine learning and deep learning algorithms.

**Bias-Variance Tradeoff:** This fundamental concept relates the bias and variance of an estimator to its mean squared error, emphasizing the balance between underfitting and overfitting.

**Shrinkage Methods:** The James-Stein theorem demonstrates that shrinkage estimators can outperform maximum likelihood estimators in multivariate estimation, leading to the development of regularization techniques in machine learning.

**Dimension Reduction:** Principal component analysis relies on singular value decomposition to reduce dimensionality while preserving key information in data.

**Kernel Methods:** Mercer's theorem and the representer theorem form the theoretical basis for the kernel trick, which allows for efficient computation in machine learning applications.

**Universal Approximation Theorem:** Single-layer neural networks can approximate any continuous function, but it does not guarantee learnability or accuracy.

**Deep Learning Theorems:** For homogeneous neural networks with homogeneous regularization, local minima correspond to global minima (Theorem 1) and local descent guarantees convergence to a global minimum (Theorem 2).

**Invariance in Image Classification:** A vertical translation invariance theorem suggests that deep convolutional neural networks learn increasingly invariant features with increasing depth.

**Generalization Error Bounds:** Despite the exponential growth in generalization error bounds with network size predicted by Vapnik-Chervonenkis theory, recent results show that the error can be bounded independently of network size under certain assumptions."
Danger of setting all initial weights to zero in Backpropagation,https://stats.stackexchange.com/questions/27112/danger-of-setting-all-initial-weights-to-zero-in-backpropagation,"['neural-networks', 'backpropagation']",27112,True,27152,65605,8,58,1686547160,1335378073,26,1335404453,"edit see alfa's comment below. I'm not an expert on neural nets, so I'll defer to him. 
My understanding is different from the other answers that have been posted here.
I'm pretty sure that backpropagation involves adding to the existing weights, not multiplying.  The amount that you add is specified by the delta rule (https://en.wikipedia.org/wiki/Delta_rule).  Note that wij doesn't appear on the right-hand-side of the equation.
My understanding is that there are at least two good reasons not to set the initial weights to zero:

First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values.  You can't do that if they all start at zero.
Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another.

","Backpropagation, a key technique in neural networks, involves incrementally adjusting neuron weights based on a delta rule to minimize error. Unlike most machine learning algorithms, neural networks benefit from non-zero initial weights to avoid local minima and promote neuron diversity, ensuring they perform different functions. The delta rule determines the weight adjustment value without directly using existing weights, preventing recursive multiplication."
Can a deep neural network approximate multiplication function?,https://stats.stackexchange.com/questions/217703/can-a-deep-neural-network-approximate-multiplication-function,"['regression', 'machine-learning', 'neural-networks']",217703,True,218659,25356,7,57,1678355826,1465281487,22,1465820516,"A big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have a zero gradient (because of neural network implementation details and limitations). We can use two approaches:

Divide by a constant. We are just dividing everything before the learning and multiply after.

Use log-normalization. It makes multiplication into addition:
\begin{align}
m &= x \cdot y\\
&\Rightarrow  \\
\ln(m) &= \ln(x) + \ln(y)
\end{align}


","Large gradients in multiplication operations within neural networks can lead to ""zero gradients"" in their hidden nodes, hindering training. Two approaches address this:

**Constant Division:**
Multiplying and dividing by a constant before and after learning, respectively, resolves the issue without altering the network's behavior.

**Log-normalization:**
Converting multiplication into addition using the logarithmic function. This simplifies gradient calculations and allows hidden nodes to retain non-zero gradients."
Can a deep neural network approximate multiplication function?,https://stats.stackexchange.com/questions/217703/can-a-deep-neural-network-approximate-multiplication-function,"['regression', 'machine-learning', 'neural-networks']",217703,True,218659,25356,7,57,1678355826,1465281487,22,1465820516,"A big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have a zero gradient (because of neural network implementation details and limitations). We can use two approaches:

Divide by a constant. We are just dividing everything before the learning and multiply after.

Use log-normalization. It makes multiplication into addition:
\begin{align}
m &= x \cdot y\\
&\Rightarrow  \\
\ln(m) &= \ln(x) + \ln(y)
\end{align}


","Due to technical limitations, multiplication operations in neural networks can cause the gradient to vanish, hindering learning. To address this issue, two approaches are presented:

* **Constant Division:** Multiplying both operands by a constant allows for canceling out the multiplication during learning.

* **Log-Normalization:** Converting multiplication to addition via logarithms enables backpropagation by transforming the gradient of multiplication into the sum of individual gradients. This normalization allows the network to learn effectively without encountering the vanishing gradient problem."
What is pre training a neural network?,https://stats.stackexchange.com/questions/193082/what-is-pre-training-a-neural-network,"['neural-networks', 'pre-training']",193082,True,193451,62115,3,57,1538072535,1454073171,58,1454324111,"The usual way of training a network:
You want to train a neural network to perform a task (e.g. classification) on a data set (e.g. a set of images). You start training by initializing the weights randomly. As soon as you start training, the weights are changed in order to perform the task with less mistakes (i.e. optimization).
Once you're satisfied with the training results you save the weights of your network somewhere.
You are now interested in training a network to perform a new task (e.g. object detection) on a different data set (e.g. images too but not the same as the ones you used before). Instead of repeating what you did for the first network and start from training with randomly initialized weights, you can use the weights you saved from the previous network as the initial weight values for your new experiment. Initializing the weights this way is referred to as using a pre-trained network. The first network is your pre-trained network. The second one is the network you are fine-tuning.
The idea behind pre-training is that random initialization is...well...random, the values of the weights have nothing to do with the task you're trying to solve. Why should a set of values be any better than another set? But how else would you initialize the weights? If you knew how to initialize them properly for the task, you might as well set them to the optimal values (slightly exaggerated). No need to train anything. You have the optimal solution to your problem.
Pre-training gives the network a head start. As if it has seen the data before.
What to watch out for when pre-training:
The first task used in pre-training the network can be the same as the fine-tuning stage. The datasets used for pre-training vs. fine-tuning can also be the same, but can also be different. It's really interesting to see how pre-training on a different task and different dataset can still be transferred to a new dataset and new task that are slightly different.
Using a pre-trained network generally makes sense if both tasks or both datasets have something in common. The bigger the gap, the less effective pre-training will be. It makes little sense to pre-train a network for image classification by training it on financial data first. In this case there's too much disconnect between the pre-training and fine-tuning stages.
","Pre-training involves using previously trained network weights as an initialization for a new network performing a different task or using a different dataset. This head start accelerates training by leveraging the knowledge gained from the first network. However, pre-training is effective when the tasks and datasets share commonalities.

When pre-training, the initial task and dataset may differ from the fine-tuning stage. Despite this, knowledge can still be transferred to new tasks and datasets, particularly when there are similarities. Pre-training is less effective when there is a significant disconnect between the two stages."
How to weight KLD loss vs reconstruction loss in variational auto-encoder?,https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder,"['machine-learning', 'neural-networks', 'tensorflow', 'autoencoders', 'variational-bayes']",332179,True,333176,43632,5,57,1693338250,1520417952,34,1520941971,"For anyone stumbling on this post also looking for an answer, this twitter thread (https://twitter.com/memotv/status/973323454350090240) has added a lot of very useful insight.
Namely:
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (https://openreview.net/forum?id=Sy2fzU9gl)
discusses my exact question with a few experiments. Interestingly, it seems their $\beta_{norm}$ (which is similar to my normalised KLD weight) is also centred around 0.1, with higher values giving more structured latent space at the cost of poorer reconstruction, and lower values giving better reconstruction with less structured latent space (though their focus is specifically on learning disentangled representations).
and related reading (where similar issues are discussed)

Semi-Supervised Learning with Deep Generative Models (https://arxiv.org/abs/1406.5298)
https://github.com/dpkingma/nips14-ssl (https://github.com/dpkingma/nips14-ssl)
InfoVAE: Information Maximizing Variational Autoencoders (https://arxiv.org/abs/1706.02262)
Density estimation using Real NVP (https://arxiv.org/abs/1605.08803)
Neural Discrete Representation Learning (https://arxiv.org/abs/1711.00937)

","**Summary:**

Variational Autoencoders (VAEs) strive to balance reconstruction quality with latent space structure. The beta-hyperparameter in the VAE's loss function (KLD weight) plays a crucial role in this balancing act. Higher beta values promote a more structured latent space, allowing for disentangled representations, but at the expense of reconstruction accuracy. Lower beta values prioritize reconstruction, leading to less structured latent spaces.

Experiments by beta-VAE demonstrate this relationship, with optimal beta values around 0.1. Other related works, such as Semi-Supervised Learning with Deep Generative Models, InfoVAE, and Neural Discrete Representation Learning, also explore this trade-off and provide insights into the impact of beta on VAE performance."
What does the term saturating nonlinearities mean?,https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean,"['machine-learning', 'neural-networks', 'terminology', 'conv-neural-network']",174295,True,174438,29316,3,57,1617097774,1443296715,56,1443399709,"Intuition
A saturating activation function squeezes the input.

Definitions

$f$ is non-saturating iff $ (|\lim_{z\to-\infty} f(z)| = +\infty) \vee (|\lim_{z\to+\infty} f(z)| = +\infty) $ 
$f$ is saturating iff $f$ is not non-saturating. 

These definitions are not specific to convolutional neural networks.

Examples
The Rectified Linear Unit (ReLU) activation function, which is defined as $f(x)=max(0,x)$ is non-saturating because $\lim_{z\to+\infty} f(z) = +\infty$:
 (https://i.sstatic.net/liEMr.png)
The sigmoid activation function, which is defined as $f(x) = \frac{1}{1 + e^{-x}}$ is saturating, because it squashes real numbers to range between $[0,1]$: 
 (https://i.sstatic.net/LUJKU.png)
The tanh (hyperbolic tangent) activation function is saturating as it squashes real numbers to range between $[-1,1]$:
 (https://i.sstatic.net/tjl2T.png)
(figures are from CS231n (http://cs231n.github.io/neural-networks-1/),  MIT License)
","**Summary:**

An activation function is a function that transforms the output of a neuron in a neural network. Non-saturating activation functions have an unbounded range, allowing the output to grow to infinity. In contrast, saturating activation functions limit the output to a specific range, ""squeezing"" the input.

Examples of non-saturating activation functions include the Rectified Linear Unit (ReLU), where the output can reach positive infinity. Saturating activation functions include the sigmoid, which restricts the output to the range [0, 1], and the tanh, which limits the output to [-1, 1].

The saturation property of an activation function affects the behavior of the neural network. Non-saturating activation functions allow neurons to have an unlimited influence on the network's output, while saturating activation functions limit their impact."
R libraries for deep learning,https://stats.stackexchange.com/questions/41771/r-libraries-for-deep-learning,"['r', 'neural-networks', 'deep-learning', 'restricted-boltzmann-machine', 'deep-belief-networks']",41771,True,118266,47763,8,56,1487428280,1351877756,19,1412753479,"OpenSource h2o.deepLearning() is package for deeplearning in R from h2o.ai
here's a write up http://www.r-bloggers.com/things-to-try-after-user-part-1-deep-learning-with-h2o/ (http://www.r-bloggers.com/things-to-try-after-user-part-1-deep-learning-with-h2o/)
And code: https://gist.github.com/woobe/3e728e02f6cc03ab86d8#file-link_data-r (https://gist.github.com/woobe/3e728e02f6cc03ab86d8#file-link_data-r)
######## *Convert Breast Cancer data into H2O*
dat <- BreastCancer[, -1]  # remove the ID column
dat_h2o <- as.h2o(localH2O, dat, key = 'dat')

######## *Import MNIST CSV as H2O*
dat_h2o <- h2o.importFile(localH2O, path = "".../mnist_train.csv"")

######## *Using the DNN model for predictions*
h2o_yhat_test <- h2o.predict(model, test_h2o)

######## *Converting H2O format into data frame*
df_yhat_test <- as.data.frame(h2o_yhat_test)

######## Start a local cluster with 2GB RAM
library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE, 
                    Xmx = '2g') 
########Execute deeplearning

model <- h2o.deeplearning( x = 2:785,  # column numbers for predictors
               y = 1,   # column number for label
               data = train_h2o, # data in H2O format
               activation = ""TanhWithDropout"", # or 'Tanh'
               input_dropout_ratio = 0.2, # % of inputs dropout
               hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
               balance_classes = TRUE, 
               hidden = c(50,50,50), # three layers of 50 nodes
               epochs = 100) # max. no. of epochs

","H2O's h2o.deepLearning() package enables deep learning in R. Users can convert data into H2O format, import external datasets, and make predictions using deep neural networks (DNNs). The DNN model can be trained on data stored in H2O, specifying parameters like activation functions, dropout rates, and hidden layer configurations. After training, predictions can be made and converted back into data frames for further analysis. To use H2O, one can initiate a local cluster with specified memory allocation and execute deep learning algorithms on provided datasets."
Understanding &quot;almost all local minimum have very similar function value to the global optimum&quot;,https://stats.stackexchange.com/questions/203288/understanding-almost-all-local-minimum-have-very-similar-function-value-to-the,"['machine-learning', 'neural-networks', 'optimization', 'deep-learning']",203288,True,203300,24995,2,56,1722430669,1458752525,83,1458755872,"A recent paper The Loss Surfaces of Multilayer Networks (http://arxiv.org/abs/1412.0233) offers some possible explanations for this. From their abstract (bold is mine):

""We conjecture that both simulated annealing and SGD converge
to the band of low critical points, and that all critical points found there are local minima of  high  quality  measured  by  the  test  error.
This emphasizes a major difference between large- and small-size networks where for the latter  poor  quality  local  minima  have  non-zero probability of being recovered.  Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.""

A lot of the influential people in deep learning (Yann LeCunn and Yoshua Bengio to name a few) and some researchers coming more from the mathematical angle (Rong Ge and other Sanjeev Arora collaborators) have been discussing and exploring these ideas.
In the above referenced paper, see Figure 3, which shows a banding/concentration phenomenon of the local minima values as the nets have more hidden units. The banding/concentration represents some empirical evidence that for deeper or larger models, a local minima is ""good enough"", since their loss values are roughly similar. And most importantly, they have a loss which is closer to the global minimum as the model gets more complex (in this case wider, but in practice, deeper).
Furthermore, they use a spin-glass model, which they even state is just a model and not necessarily indicative of the true picture, to show that reaching the global minimizer from a local minima may take exponentially long:

""In order to find a further low lying minimum we must pass through a saddle point. Therefore we must go up at  least to the level where there is an equal  amount of saddle points to have a decent chance of finding a
path that might possibly take us to another local minimum. This process takes an exponentially long time so in practice finding the global minimum is not feasible.""

The Rong Ge research is centered around breaking through saddle points. Yoshua Bengio and his collaborators have posed a pretty bold Saddle Point Hypothesis:

Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.

source here: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. (http://arxiv.org/abs/1406.2572)
To some extent, the above two approaches aren't exactly the same (the Saddle Point Hypothesis might question what is really a local minima and what is merely a poorly conditioned saddle point with a very long plateau region?). The idea behind the Saddle Point Hypothesis is that it is possible to design optimization methods to break through saddle points, for example Saddle-Free Newton from the Bengio article, to potentially speed up convergence and maybe even reach the global optimum. The first Multilayer Loss Surface article is not really concerned with reaching the global optimum and actually believes it to have some poor overfitting properties. Curiously, both articles use ideas from statistical physics and spin-glass models.
But they are sort of related in that both articles believe that in order to reach the global minimizer, one must overcome the optimization challenge of saddle points. The first article just believes that local minima are good enough.
It is fair to wonder if momentum methods and other new optimization algorithms, which can estimate some 2nd order curvature properties can escape saddle points. A famous animation by Alec Radford here (https://web.archive.org/web/20160725050440/https://imgur.com/a/Hqolp).
To answer your question: ""where does this belief come from"" I personally think it comes from the fact that it's possible to use different random seeds to learn different weights, but the corresponding nets have similar quantitative performance. For example, if you set two different random seeds for Glorot weight initialization, you will probably learn different weights, but if you train using similar optimization methods, the nets will have similar performance. One common folklore belief is that the optimization landscape is similar to that of an egg carton, another good blog post on this here: No more local minima? (http://blog.terminal.com/no-more-local-minima/) with the egg-carton analogy.
Edit: I just wanted to be clear that the egg carton analogy is not true, otherwise there would be no need for momentum or other more advanced optimization techniques. But it is known that SGD does not perform as well as SGD+Momentum or more modern optimization algorithms, perhaps due to the existence of saddle points.
","**Summary:**

Recent research suggests that deep neural networks exhibit a ""band"" of local minima with similar loss values. This ""band of minima"" hypothesis proposes that local minima are sufficient for practical applications, as they often offer high-quality solutions regardless of variations in network size.

However, the ""Saddle Point Hypothesis"" suggests that saddle points, rather than local minima, may be the main obstacle to finding the global minimum. Saddle points are surrounded by ""plateaus"" that can slow down optimization algorithms, leading to the illusion of local minima.

Both hypotheses agree that overcoming saddle points is crucial for reaching the global minimum. Some optimization methods, such as Saddle-Free Newton, have been developed to address this challenge.

The empirical evidence for these hypotheses includes the observed performance stability of networks initialized with different random weights. Additionally, visualizations suggest that the loss landscape of deep networks resembles an ""egg carton,"" with many local minima of similar quality."
What is a latent space?,https://stats.stackexchange.com/questions/442352/what-is-a-latent-space,"['machine-learning', 'neural-networks', 'definition']",442352,True,442360,31431,3,56,1616964773,1577423495,56,1577431881,"Latent space refers to an abstract multi-dimensional space containing feature values that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.
Just as we, humans, have an understanding of a broad range of topics and the events belonging to those topics, latent space aims to provide a similar understanding to a computer through a quantitative spatial representation/modeling. 
The motivation to learn a latent space (set of hidden topics/ internal representations) over the observed data (set of events) is that large differences in observed space/events could be due to small variations in latent space (for the same topic). Hence, learning a latent space would help the model make better sense of observed data than from observed data itself, which is a very large space to learn from.
Some examples of latent space are:
1) Word Embedding Space - consisting of word vectors where words similar in meaning have vectors that lie close to each other in space (as measured by cosine-similarity or euclidean-distance) and words that are unrelated lie far apart (Tensorflow's Embedding Projector (http://projector.tensorflow.org/) provides a good visualization of word embedding spaces).
2) Image Feature Space - CNNs in the final layers encode higher-level features in the input image that allows it to effectively detect, for example, the presence of a cat in the input image under varying lighting conditions, which is a difficult task in the raw pixel space.
3) Topic Modeling (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) methods such as LDA (https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), PLSA (https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) use statistical approaches to obtain a latent set of topics from an observed set of documents and word distribution. (PyLDAvis (https://medium.com/@sherryqixuan/topic-modeling-and-pyldavis-visualization-86a543e21f58) provides a good visualization of topic models)
4) VAEs & GANs (https://www.youtube.com/watch?v=yFBFl1cLYx8&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=4) aim to obtain a latent space/distribution that closely approximates the real latent space/distribution of the observed data.
In all the above examples, we quantitatively represent the complex observation space with a (relatively simple) multi-dimensional latent space that approximates the real latent space of the observed data.
The terms ""high dimensional"" and ""low dimensional"" help us define how specific or how general the kinds of features we want our latent space to learn and represent. High dimensional latent space is sensitive to more specific features of the input data and can sometimes lead to overfitting when there isn't sufficient training data. Low dimensional latent space aims to capture the most important features/aspects required to learn and represent the input data (a good example is a low-dimensional bottleneck layer in VAEs). 
If this answer helped, please don't forget to up-vote it :)
","Latent space is a multi-dimensional representation of an observation space that captures underlying patterns and features not directly observable. It reduces the dimensionality of complex observations, providing a more manageable representation for models to learn from. Latent space aims to encode meaningful internal representations of events, similar to human understanding of various topics.

Learning a latent space allows models to make better sense of observed data, as it identifies small variations in latent space that correspond to larger differences in observed events. Latent spaces have applications in natural language processing (e.g., word embedding), image analysis (e.g., CNN feature space), document modeling (e.g., topic modeling), and generative models (e.g., VAEs, GANs).

The number of dimensions in the latent space determines its specificity and generality. High-dimensional latent spaces capture detailed features, while low-dimensional spaces focus on essential aspects of the data. Finding an appropriate dimensionality balance is crucial to prevent overfitting and ensure model efficiency."
Understanding LSTM units vs. cells,https://stats.stackexchange.com/questions/241985/understanding-lstm-units-vs-cells,"['neural-networks', 'terminology', 'lstm', 'recurrent-neural-network', 'tensorflow']",241985,True,241996,61869,6,56,1604898021,1477265830,30,1477272608,"The terminology is unfortunately inconsistent. num_units in TensorFlow is the number of hidden states, i.e. the dimension of $h_t$ in the equations you gave.
Also, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md) :

The definition of cell in this package differs from the definition used in the literature. In the literature, cell refers to an object with a single scalar output. The definition in this package refers to a horizontal array of such units.

""LSTM layer"" is probably more explicit, example (http://deeplearning.net/tutorial/code/lstm.py):
def lstm_layer(tparams, state_below, options, prefix='lstm', mask=None):
    nsteps = state_below.shape[0]
    if state_below.ndim == 3:
        n_samples = state_below.shape[1]
    else:
        n_samples = 1

    assert mask is not None
    […]

","**Summary (350 words):**

In TensorFlow, the term ""num_units"" refers to the dimension of hidden states, despite its inconsistent terminology with other sources.

Different definitions of ""cell"" exist: in TensorFlow, it refers to a horizontal array of units, while in literature, it's a single scalar output.

The term ""LSTM layer"" is more precise and refers to an LSTM cell with multiple hidden units. An example of this is provided from the deeplearning.net tutorial code, where ""lstm_layer"" takes inputs, a mask, and various options to define an LSTM layer."
Understanding LSTM units vs. cells,https://stats.stackexchange.com/questions/241985/understanding-lstm-units-vs-cells,"['neural-networks', 'terminology', 'lstm', 'recurrent-neural-network', 'tensorflow']",241985,True,241996,61869,6,56,1604898021,1477265830,30,1477272608,"The terminology is unfortunately inconsistent. num_units in TensorFlow is the number of hidden states, i.e. the dimension of $h_t$ in the equations you gave.
Also, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md) :

The definition of cell in this package differs from the definition used in the literature. In the literature, cell refers to an object with a single scalar output. The definition in this package refers to a horizontal array of such units.

""LSTM layer"" is probably more explicit, example (http://deeplearning.net/tutorial/code/lstm.py):
def lstm_layer(tparams, state_below, options, prefix='lstm', mask=None):
    nsteps = state_below.shape[0]
    if state_below.ndim == 3:
        n_samples = state_below.shape[1]
    else:
        n_samples = 1

    assert mask is not None
    […]

","RNN terminology can be inconsistent, with ""num_units"" referring to hidden state dimension. In TensorFlow's RNNCell class, ""cell"" denotes an array of units, differing from the literature's definition of a single scalar output. To avoid confusion, ""LSTM layer"" is a more explicit alternative when referring to LSTM components.

Examples, such as the ""lstm_layer"" function, illustrate these concepts in code, where ""nsteps"" and ""n_samples"" represent dimensions of the input data, and ""mask"" is a placeholder for a filtering mechanism."
Multivariate linear regression vs neural network?,https://stats.stackexchange.com/questions/41289/multivariate-linear-regression-vs-neural-network,"['regression', 'multiple-regression', 'neural-networks']",41289,True,41290,42484,3,56,1461229848,1351325183,31,1351329963,"Neural networks can in principle model nonlinearities automatically (see the universal approximation theorem (https://en.wikipedia.org/wiki/Universal_approximation_theorem)), which you would need to explicitly model using transformations (splines etc.) in linear regression.
The caveat: the temptation to overfit can be (even) stronger in neural networks than in regression, since adding hidden layers or neurons looks harmless. So be extra careful to look at out-of-sample prediction performance.
","Neural networks excel over linear regression in modeling complex, non-linear relationships without explicit transformations. By adding hidden layers or neurons, they can approximate any non-linearity automatically. However, this power can lead to overfitting if not properly monitored. To mitigate this risk, it is crucial to evaluate out-of-sample prediction performance to ensure that the model generalizes well to unseen data. This approach ensures that neural networks capture underlying patterns without overly fitting to specific training data, leading to more reliable and accurate predictions."
How large should the batch size be for stochastic gradient descent?,https://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent,"['machine-learning', 'neural-networks', 'gradient-descent', 'backpropagation']",140811,True,141265,94450,1,55,1498935327,1425763116,79,1426056940,"The ""sample size"" you're talking about is referred to as batch size, $B$. The batch size parameter is just one of the hyper-parameters you'll be tuning when you train a neural network with mini-batch Stochastic Gradient Descent (SGD) and is data dependent. The most basic method of hyper-parameter search is to do a grid search over the learning rate and batch size to find a pair which makes the network converge.
To understand what the batch size should be, it's important to see the relationship between batch gradient descent, online SGD, and mini-batch SGD. Here's the general formula for the weight update step in mini-batch SGD, which is a generalization of all three types. [2 (http://arxiv.org/abs/1206.5533)]
$$
\theta_{t+1} \leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}
$$

Batch gradient descent, $B = |x|$
Online stochastic gradient descent: $B = 1$
Mini-batch stochastic gradient descent: $B > 1$ but $B < |x|$.

Note that with 1, the loss function is no longer a random variable and is not a stochastic approximation.
SGD converges faster than normal ""batch"" gradient descent because it updates the weights after looking at a randomly selected subset of the training set. Let $x$ be our training set and let $m \subset x$. The batch size $B$ is just the cardinality of $m$: $B = |m|$.
Batch gradient descent updates the weights $\theta$ using the gradients of the entire dataset $x$; whereas SGD updates the weights using an average of the gradients for a mini-batch $m$. (Using the average as opposed to a sum prevents the algorithm from taking steps that are too large if the dataset is very large. Otherwise, you would need to adjust your learning rate based on the size of the dataset.) The expected value of this stochastic approximation of the gradient used in SGD is equal to the deterministic gradient used in batch gradient descent. $\mathbb{E}[\nabla \mathcal{L}_{SGD}(\theta, \textbf{m})] = \nabla \mathcal{L}(\theta, \textbf{x})$.
Each time we take a sample and update our weights it is called a mini-batch. Each time we run through the entire dataset, it's called an epoch.
Let's say that we have some data vector $\textbf{x} : \mathbb{R}^D$, an initial weight vector that parameterizes our neural network, $\theta_0 : \mathbb{R}^{S}$, and a loss function $\mathcal{L}(\theta, \textbf{x}) : \mathbb{R}^{S} \rightarrow \mathbb{R}^{D} \rightarrow \mathbb{R}^S$ that we are trying to minimize. If we have $T$ training examples and a batch size of $B$, then we can split those training examples into C mini-batches:
$$
C = \lceil T / B \rceil
$$
For simplicity we can assume that T is evenly divisible by B. Although, when this is not the case, as it often is not, proper weight should be assigned to each mini-batch as a function of its size.
An iterative algorithm for SGD with $M$ epochs is given below:
\begin{align*}
t &\leftarrow 0 \\
\textrm{while } t &< M \\
\theta_{t+1} &\leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}  \\
t &\leftarrow t + 1
\end{align*}
Note: in real life we're reading these training example data from memory and, due to cache pre-fetching and other memory tricks done by your computer, your algorithm will run faster if the memory accesses are coalesced, i.e. when you read the memory in order and don't jump around randomly. So, most SGD implementations shuffle the dataset and then load the examples into memory in the order that they'll be read.
The major parameters for the vanilla (no momentum) SGD described above are:

Learning Rate: $\epsilon$

I like to think of epsilon as a function from the epoch count to a learning rate. This function is called the learning rate schedule.
$$
    \epsilon(t) : \mathbb{N} \rightarrow \mathbb{R}
$$
If you want to have the learning rate fixed, just define epsilon as a constant function.

Batch Size

Batch size determines how many examples you look at before making a weight update. The lower it is, the noisier the training signal is going to be, the higher it is, the longer it will take to compute the gradient for each step.
Citations & Further Reading:

Introduction to Gradient Based Learning (http://www-labs.iro.umontreal.ca/~bengioy/ift6266/H12/html/gradient_en.html)
Practical recommendations for gradient-based training of deep architectures (http://arxiv.org/abs/1206.5533)
Efficient Mini-batch Training for Stochastic Optimization (http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf)

","**Batch Size in Mini-batch Stochastic Gradient Descent**

In neural network training using mini-batch Stochastic Gradient Descent (SGD), the batch size ($B$) is a hyperparameter that controls the number of training examples used for each weight update. SGD involves randomly selecting subsets (mini-batches) from the training data and updating weights based on the average gradient over those mini-batches.

The relationship between batch size and SGD types is as follows:

* Batch gradient descent: $B = |x|$, where $x$ is the entire training set.
* Online SGD: $B = 1$.
* Mini-batch SGD: $B > 1$ but $B < |x|$.

The optimal batch size is data dependent and can be determined through hyperparameter tuning. Smaller batch sizes result in noisier training signals, while larger batch sizes increase computational time.

The expected value of the stochastic gradient approximation used in mini-batch SGD equals the deterministic gradient in batch gradient descent. Each weight update using a mini-batch is considered an iteration, and running through the entire training set is known as an epoch.

Key parameters for SGD include the learning rate, which can be adjusted via a learning rate schedule, and the batch size, which influences the noise level and computational cost of the training process."
What is the derivative of the ReLU activation function?,https://stats.stackexchange.com/questions/333394/what-is-the-derivative-of-the-relu-activation-function,"['self-study', 'neural-networks']",333394,True,333400,120525,1,54,1521134938,1521016323,63,1521018898,"The derivative is:
$$ f(x)=
\begin{cases} 
0 & \text{if  }  x < 0 \\
1 & \text{if  }  x > 0 \\
\end{cases}
$$
And undefined in $x=0$.
The reason for it being undefined at $x=0$ is that its left- and right derivative (https://en.wikipedia.org/wiki/Semi-differentiability) are not equal.
","The derivative of the piecewise function defined as 0 for negative x and 1 for positive x exhibits undefined behavior at x=0. This is attributed to the lack of equality between the left- and right-hand derivatives at this point, resulting in a discontinuity in the function's differentiability."
Dice-coefficient loss function vs cross-entropy,https://stats.stackexchange.com/questions/321460/dice-coefficient-loss-function-vs-cross-entropy,"['neural-networks', 'loss-functions', 'cross-entropy']",321460,True,344244,75946,3,53,1629643417,1515035521,53,1525361752,"One compelling reason for using cross-entropy over dice-coefficient or the similar IoU metric is that the gradients are nicer.
The gradients of cross-entropy wrt the logits is something like $p - t$, where $p$ is the softmax outputs and $t$ is the target. Meanwhile, if we try to write the dice coefficient in a differentiable form: $\frac{2pt}{p^2+t^2}$ or $\frac{2pt}{p+t}$, then the resulting gradients wrt $p$ are much uglier: $\frac{2t(t^2-p^2)}{(p^2+t^2)^2}$ and $\frac{2t^2}{(p+t)^2}$. It's easy to imagine a case where both $p$ and $t$ are small, and the gradient blows up to some huge value. In general, it seems likely that training will become more unstable.

The main reason that people try to use dice coefficient or IoU directly is that the actual goal is maximization of those metrics, and cross-entropy is just a proxy which is easier to maximize using backpropagation. In addition, Dice coefficient performs better at class imbalanced problems by design: 
However, class imbalance is typically taken care of simply by assigning loss multipliers to each class, such that the network is highly disincentivized to simply ignore a class which appears infrequently, so it's unclear that Dice coefficient is really necessary in these cases. 

I would start with cross-entropy loss, which seems to be the standard loss for training segmentation networks, unless there was a really compelling reason to use Dice coefficient.
","Cross-entropy loss is preferred over Dice coefficient or IoU metrics for training segmentation networks due to its smoother gradients. Cross-entropy gradients are simpler ($p - t$) compared to the complex gradients of Dice coefficient ($2t(t^2 - p^2)/(p^2 + t^2)^2$). These complex gradients can lead to unstable training when both $p$ and $t$ are small.

Dice coefficient is often used as a direct optimization target because it aligns with the ultimate goal of maximizing segmentation metrics. However, class imbalance can be effectively addressed using loss multipliers, making Dice coefficient potentially unnecessary.

Overall, cross-entropy loss remains the standard choice for training segmentation networks unless there is a compelling reason to use Dice coefficient, such as the need to handle extreme class imbalances."
How does the Adam method of stochastic gradient descent work?,https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work,"['neural-networks', 'optimization', 'gradient-descent', 'adam']",220494,True,220563,39795,1,52,1533259432,1466783107,50,1466822956,"The Adam paper says, ""...many objective functions are composed of a sum of subfunctions evaluated at different subsamples of data; in this case optimization can be made more efficient by taking gradient steps w.r.t.  individual subfunctions..."" Here, they just mean that the objective function is a sum of errors over training examples, and training can be done on individual examples or minibatches. This is the same as in stochastic gradient descent (SGD), which is more efficient for large scale problems than batch training because parameter updates are more frequent.
As for why Adam works, it uses a few tricks.
One of these tricks is momentum, which can give faster convergence. Imagine an objective function that's shaped like a long, narrow canyon that gradually slopes toward a minimum. Say we want to minimize this function using gradient descent. If we start from some point on the canyon wall, the negative gradient will point in the direction of steepest descent, i.e. mostly toward the canyon floor. This is because the canyon walls are much steeper than the gradual slope of the canyon toward the minimum. If the learning rate (i.e. step size) is small, we could descend to the canyon floor, then follow it toward the minimum. But, progress would be slow. We could increase the learning rate, but this wouldn't change the direction of the steps. In this case, we'd overshoot the canyon floor and end up on the opposite wall. We would then repeat this pattern, oscillating from wall to wall while making slow progress toward the minimum. Momentum can help in this situation.
Momentum simply means that some fraction of the previous update is added to the current update, so that repeated updates in a particular direction compound; we build up momentum, moving faster and faster in that direction. In the case of the canyon, we'd build up momentum in the direction of the minimum, since all updates have a component in that direction. In contrast, moving back and forth across the canyon walls involves constantly reversing direction, so momentum would help to damp the oscillations in those directions.
Another trick that Adam uses is to adaptively select a separate learning rate for each parameter. Parameters that would ordinarily receive smaller or less frequent updates receive larger updates with Adam (the reverse is also true). This speeds learning in cases where the appropriate learning rates vary across parameters. For example, in deep networks, gradients can become small at early layers, and it make sense to increase learning rates for the corresponding parameters. Another benefit to this approach is that, because learning rates are adjusted automatically, manual tuning becomes less important. Standard SGD requires careful tuning (and possibly online adjustment) of learning rates, but this less true with Adam and related methods. It's still necessary to select hyperparameters, but performance is less sensitive to them than to SGD learning rates.
Related methods:
Momentum is often used with standard SGD. An improved version is called Nesterov momentum or Nesterov accelerated gradient. Other methods that use automatically tuned learning rates for each parameter include: Adagrad, RMSprop, and Adadelta. RMSprop and Adadelta solve a problem with Adagrad that could cause learning to stop. Adam is similar to RMSprop with momentum. Nadam modifies Adam to use Nesterov momentum instead of classical momentum.
References:
Kingma and Ba (2014) (https://arxiv.org/abs/1412.6980). Adam: A Method for Stochastic Optimization.
Goodfellow et  al. (2016) (http://www.deeplearningbook.org/contents/optimization.html). Deep learning, chapter 8.
Slides (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) from Geoff Hinton's course
Dozat (2016) (https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ). Incorporating Nesterov Momentum into Adam.
","**Summary:**

Adam, an optimization algorithm, improves efficiency for large-scale problems by utilizing stochastic gradient descent (SGD). It leverages momentum, a technique that accelerates convergence by compounding updates in a particular direction. This is particularly effective in situations where the objective function presents a narrow canyon-like structure.

Adam also employs adaptive learning rates, selecting different step sizes for each parameter. This strategy enhances learning in scenarios where appropriate rates vary across parameters, such as in deep networks where early layers typically have smaller gradients. The automatic tuning of learning rates simplifies hyperparameter selection and reduces sensitivity to learning rates compared to traditional SGD.

Additional approaches related to Adam include momentum (and Nesterov momentum), Adagrad, RMSprop, Adadelta, and Nadam (which combines Adam with Nesterov momentum). These methods address various issues, such as stopping learning in Adagrad, and offer modifications to improve performance."
How does the Adam method of stochastic gradient descent work?,https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work,"['neural-networks', 'optimization', 'gradient-descent', 'adam']",220494,True,220563,39795,1,52,1533259432,1466783107,50,1466822956,"The Adam paper says, ""...many objective functions are composed of a sum of subfunctions evaluated at different subsamples of data; in this case optimization can be made more efficient by taking gradient steps w.r.t.  individual subfunctions..."" Here, they just mean that the objective function is a sum of errors over training examples, and training can be done on individual examples or minibatches. This is the same as in stochastic gradient descent (SGD), which is more efficient for large scale problems than batch training because parameter updates are more frequent.
As for why Adam works, it uses a few tricks.
One of these tricks is momentum, which can give faster convergence. Imagine an objective function that's shaped like a long, narrow canyon that gradually slopes toward a minimum. Say we want to minimize this function using gradient descent. If we start from some point on the canyon wall, the negative gradient will point in the direction of steepest descent, i.e. mostly toward the canyon floor. This is because the canyon walls are much steeper than the gradual slope of the canyon toward the minimum. If the learning rate (i.e. step size) is small, we could descend to the canyon floor, then follow it toward the minimum. But, progress would be slow. We could increase the learning rate, but this wouldn't change the direction of the steps. In this case, we'd overshoot the canyon floor and end up on the opposite wall. We would then repeat this pattern, oscillating from wall to wall while making slow progress toward the minimum. Momentum can help in this situation.
Momentum simply means that some fraction of the previous update is added to the current update, so that repeated updates in a particular direction compound; we build up momentum, moving faster and faster in that direction. In the case of the canyon, we'd build up momentum in the direction of the minimum, since all updates have a component in that direction. In contrast, moving back and forth across the canyon walls involves constantly reversing direction, so momentum would help to damp the oscillations in those directions.
Another trick that Adam uses is to adaptively select a separate learning rate for each parameter. Parameters that would ordinarily receive smaller or less frequent updates receive larger updates with Adam (the reverse is also true). This speeds learning in cases where the appropriate learning rates vary across parameters. For example, in deep networks, gradients can become small at early layers, and it make sense to increase learning rates for the corresponding parameters. Another benefit to this approach is that, because learning rates are adjusted automatically, manual tuning becomes less important. Standard SGD requires careful tuning (and possibly online adjustment) of learning rates, but this less true with Adam and related methods. It's still necessary to select hyperparameters, but performance is less sensitive to them than to SGD learning rates.
Related methods:
Momentum is often used with standard SGD. An improved version is called Nesterov momentum or Nesterov accelerated gradient. Other methods that use automatically tuned learning rates for each parameter include: Adagrad, RMSprop, and Adadelta. RMSprop and Adadelta solve a problem with Adagrad that could cause learning to stop. Adam is similar to RMSprop with momentum. Nadam modifies Adam to use Nesterov momentum instead of classical momentum.
References:
Kingma and Ba (2014) (https://arxiv.org/abs/1412.6980). Adam: A Method for Stochastic Optimization.
Goodfellow et  al. (2016) (http://www.deeplearningbook.org/contents/optimization.html). Deep learning, chapter 8.
Slides (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) from Geoff Hinton's course
Dozat (2016) (https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ). Incorporating Nesterov Momentum into Adam.
","Adam, an optimization algorithm for deep learning, leverages two techniques to enhance efficiency. One is stochastic gradient descent, where the objective function is broken down into minibatches or individual examples to facilitate more frequent parameter updates. Another is adaptive learning rate optimization, where separate learning rates are assigned to each parameter, allowing for faster learning in cases of varying parameter sensitivities.

To mitigate convergence issues, Adam employs momentum, a technique that accumulates previous updates in the direction of the minimum, preventing oscillations across multiple directions. This approach differs from standard SGD, which requires careful tuning of learning rates. Adam's automated adjustment of learning rates reduces the need for manual intervention while maintaining the benefits of momentum.

Similar algorithms include Adagrad, RMSprop, and Adadelta, which also use adaptive learning rates. Nadam, a variant of Adam, incorporates Nesterov momentum for improved performance."
Difference between GradientDescentOptimizer and AdamOptimizer (TensorFlow)?,https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow,"['machine-learning', 'neural-networks', 'error', 'gradient-descent', 'supervised-learning']",184448,True,184497,63787,1,50,1467878619,1448977698,85,1448994447,"The tf.train.AdamOptimizer (http://www.tensorflow.org/api_docs/python/train.html#AdamOptimizer) uses Kingma and Ba's Adam algorithm (http://arxiv.org/pdf/1412.6980v8.pdf) to control the learning rate. Adam offers several advantages over the simple tf.train.GradientDescentOptimizer. Foremost is that it uses moving averages of the parameters (momentum); Bengio discusses the reasons for why this is beneficial in Section 3.1.1 of this paper (http://arxiv.org/pdf/1206.5533.pdf). Simply put, this enables Adam to use a larger effective step size, and the algorithm will converge to this step size without fine tuning.
The main down side of the algorithm is that Adam requires more computation to be performed for each parameter in each training step (to maintain the moving averages and variance, and calculate the scaled gradient); and more state to be retained for each parameter (approximately tripling the size of the model to store the average and variance for each parameter). A simple tf.train.GradientDescentOptimizer could equally be used in your MLP, but would require more hyperparameter tuning before it would converge as quickly.
","AdamOptimizer (Adam) in TensorFlow utilizes the Adam algorithm for learning rate management. Its advantages include momentum, allowing for larger effective step sizes without manual adjustments. Unlike GradientDescentOptimizer, which requires meticulous hyperparameter tuning for convergence, Adam automates this process.

However, Adam's downsides include increased computational overhead per parameter in each training step, and a larger model state due to maintaining moving averages and variances. Despite these drawbacks, Adam's automation of step size optimization can outperform GradientDescentOptimizer, especially when hyperparameter tuning is challenging."
Importance of local response normalization in CNN,https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn,"['deep-learning', 'convolution', 'conv-neural-network']",145768,True,191203,47970,7,50,1661008030,1428710705,25,1453120120,"It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods. This is what is written in the lecture notes for the Stanford Course CS321n (http://cs231n.github.io/convolutional-networks/) on ConvNets: 

Normalization Layer
Many types of normalization layers have been proposed for use in
  ConvNet architectures, sometimes with the intentions of implementing
  inhibition schemes observed in the biological brain. However, these
  layers have recently fallen out of favor because in practice their
  contribution has been shown to be minimal, if any. For various types
  of normalizations, see the discussion in Alex Krizhevsky's
  cuda-convnet library API.

","**Summary:**

Normalization layers, once common in convolutional neural networks (ConvNets), have diminished in popularity due to their minimal impact. Advancements in regularization techniques (dropout and batch normalization) and initialization and training methods have rendered normalization layers largely obsolete.

While normalization layers were initially inspired by inhibition schemes in the brain, practical experience has shown that their contribution is often insignificant. This has led to their replacement by more effective methods that address regularization and initialization challenges more effectively."
How are kernels applied to feature maps to produce other feature maps?,https://stats.stackexchange.com/questions/85767/how-are-kernels-applied-to-feature-maps-to-produce-other-feature-maps,"['machine-learning', 'neural-networks', 'deep-learning', 'conv-neural-network']",85767,True,86879,26286,4,50,1538069749,1391785265,21,1392654106,"The kernels are 3-dimensional, where width and height can be chosen, while the depth is equal to the number of maps in the input layer - in general. 
They are certainly not 2-dimensional and replicated across the input feature maps at the same 2D location! That would mean a kernel wouldn't be able to distinguish between its input features at a given location, since it would use one and the same weight across the input feature maps!
","**Summary (320 words):**

Convolutional neural networks (CNNs) employ kernels, which are multi-dimensional. The width and height of kernels are adjustable, while the depth corresponds to the number of input layer maps.

Unlike traditional kernels, which are replicated across input feature maps, CNN kernels maintain distinct weights for each input feature, allowing them to identify specific patterns within the input.

This multi-dimensional structure distinguishes CNN kernels from 2D kernels, as the latter would apply the same weight to all input features at a given location, limiting their ability to detect nuanced variations. The 3D nature of CNN kernels enhances their feature extraction capabilities and enables them to recognize complex patterns across different input channels."
How to set up neural network to output ordinal data?,https://stats.stackexchange.com/questions/140061/how-to-set-up-neural-network-to-output-ordinal-data,"['neural-networks', 'ordinal-data', 'softmax']",140061,True,494965,23026,2,49,1716202430,1425347148,17,1604444248,"I think the approach to only encode the ordinal labels as

class 1 is represented as [0 0 0 0 ...]

class 2 is represented as [1 0 0 0 ...]

class 3 is represented as [1 1 0 0 ...]


and use binary cross-entropy as the loss function is suboptimal. As mentioned in the comments, it might happen that the predicted vector is for example [1 0 1 0 ...]. This is undesirable for making predictions.
The paper Rank-consistent ordinal regression for neural networks (https://arxiv.org/pdf/1901.07884.pdf) describes how to restrict the neural network to make rank-consistent predictions. You have to make sure that the last layer shares its weights, but should have different biases. You can implement this in Tensorflow by adding the following as the last part of the network (credits for https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers (https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers)):
class BiasLayer(tf.keras.layers.Layer):
    def __init__(self, units, *args, **kwargs):
        super(BiasLayer, self).__init__(*args, **kwargs)
        self.bias = self.add_weight('bias',
                                    shape=[units],
                                    initializer='zeros',
                                    trainable=True)

    def call(self, x):
        return x + self.bias


# Add the following as the output of the Sequential model
model.add(keras.layers.Dense(1, use_bias=False))
model.add(BiasLayer(4))
model.add(keras.layers.Activation(""sigmoid""))

Note that the number of ordinal classes here is 5, hence the $K-1$ biases.
I tested the difference in performance on actual data, and the predictive accuracy improved substantially. Hope this helps.
","**Summary:**

Binary cross-entropy loss with one-hot ordinal class encoding is not optimal for prediction accuracy. To address this, the ""Rank-consistent ordinal regression"" method suggests using a neural network with shared weights but independent biases in the output layer. This ensures that predictions maintain correct ordinal relationships among classes. Implementing a BiasLayer in TensorFlow extends the model's output layer to include independent biases for each ordinal class, improving prediction accuracy significantly."
Why are non zero-centered activation functions a problem in backpropagation?,https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation,"['neural-networks', 'deep-learning', 'backpropagation']",237169,True,237282,33322,1,48,1641700377,1474985560,60,1475032111,"$$f=\sum w_ix_i+b$$ $$\frac{df}{dw_i}=x_i$$ $$\frac{dL}{dw_i}=\frac{dL}{df}\frac{df}{dw_i}=\frac{dL}{df}x_i$$
because $x_i>0$, the gradient $\dfrac{dL}{dw_i}$ always has the same sign as $\dfrac{dL}{df}$ (all positive or all negative).
Update
Say there are two parameters $w_1$ and $w_2$. If the gradients of two dimensions are always of the same sign (i.e., either both are positive or both are negative), it means we can only move roughly in the direction of northeast or southwest in the parameter space.
If our goal happens to be in the northwest, we can only move in a zig-zagging fashion to get there, just like parallel parking in a narrow space. (forgive my drawing)
 (https://i.sstatic.net/eWcLR.png)
Therefore all-positive or all-negative activation functions (relu, sigmoid) can be difficult for gradient based optimization. To solve this problem we can normalize the data in advance  to be zero-centered as in batch/layer normalization.
Also another solution I can think of is to add a bias term for each input so the layer becomes
$$f=\sum w_i(x_i+b_i).$$
The gradients is then
$$\frac{dL}{dw_i}=\frac{dL}{df}(x_i-b_i)$$
the sign won't solely depend on $x_i$.
","In gradient-based optimization of neural networks, the update direction for weight parameters is determined by the gradient of the loss function with respect to the weight. For activation functions with all-positive or all-negative gradients, the update direction is restricted to the northeast or southwest direction in the parameter space. This can hinder optimization when the desired direction is northwest. To mitigate this issue, data normalization techniques like batch normalization can be employed to center the inputs around zero. Additionally, introducing a bias term to each input allows the gradient to depend on both the input and the bias, providing more flexibility in update directions."
Neural Networks: weight change momentum and weight decay,https://stats.stackexchange.com/questions/70101/neural-networks-weight-change-momentum-and-weight-decay,"['neural-networks', 'optimization', 'regularization', 'gradient-descent']",70101,True,70146,42474,1,47,1589313650,1379296588,51,1379344580,"Yes, it's very common to use both tricks.  They solve different problems and can work well together.
One way to think about it is that weight decay changes the function that's being optimized, while momentum changes the path you take to the optimum.
Weight decay, by shrinking your coefficients toward zero, ensures that you find a local optimum with small-magnitude parameters.  This is usually crucial for avoiding overfitting (although other kinds of constraints on the weights can work too).  As a side benefit, it can also make the model easier to optimize, by making the objective function more convex.
Once you have an objective function, you have to decide how to move around on it.  Steepest descent on the gradient is the simplest approach, but you're right that fluctuations can be a big problem. Adding momentum helps solve that problem.  If you're working with batch updates (which is usually a bad idea with neural networks) Newton-type steps are another option. The new ""hot"" approaches are based on Nesterov's accelerated gradient and so-called ""Hessian-Free"" optimization.
But regardless of which of these update rules you use (momentum, Newton, etc.), you're still working with the same objective function, which is determined by your error function (e.g. squared error) and other constraints (e.g. weight decay).  The main question when deciding which of these to use is how quickly you'll get to a good set of weights.
","Weight decay and momentum are common optimization techniques that address distinct challenges in training neural networks. Weight decay modifies the objective function to promote small-magnitude parameters, reducing overfitting and improving optimization. Momentum alters the optimization path by damping fluctuations, leading to faster convergence. While the choice of update rule (e.g., momentum, Newton, etc.) depends on the desired optimization speed, the objective function remains the same and is influenced by the error function and weight decay. Together, weight decay and momentum can effectively improve model performance and optimize training efficiency."
Neural Networks: weight change momentum and weight decay,https://stats.stackexchange.com/questions/70101/neural-networks-weight-change-momentum-and-weight-decay,"['neural-networks', 'optimization', 'regularization', 'gradient-descent']",70101,True,70146,42474,1,47,1589313650,1379296588,51,1379344580,"Yes, it's very common to use both tricks.  They solve different problems and can work well together.
One way to think about it is that weight decay changes the function that's being optimized, while momentum changes the path you take to the optimum.
Weight decay, by shrinking your coefficients toward zero, ensures that you find a local optimum with small-magnitude parameters.  This is usually crucial for avoiding overfitting (although other kinds of constraints on the weights can work too).  As a side benefit, it can also make the model easier to optimize, by making the objective function more convex.
Once you have an objective function, you have to decide how to move around on it.  Steepest descent on the gradient is the simplest approach, but you're right that fluctuations can be a big problem. Adding momentum helps solve that problem.  If you're working with batch updates (which is usually a bad idea with neural networks) Newton-type steps are another option. The new ""hot"" approaches are based on Nesterov's accelerated gradient and so-called ""Hessian-Free"" optimization.
But regardless of which of these update rules you use (momentum, Newton, etc.), you're still working with the same objective function, which is determined by your error function (e.g. squared error) and other constraints (e.g. weight decay).  The main question when deciding which of these to use is how quickly you'll get to a good set of weights.
","Weight decay modifies the optimization function to promote small-magnitude parameters, reducing overfitting. Momentum, on the other hand, influences the path to the optimal solution. It helps mitigate fluctuations in steepest descent gradient updates. Both techniques serve distinct purposes, and their combination can enhance model optimization. Regardless of the specific update rule employed, the objective function, influenced by error function and constraints like weight decay, remains constant. The choice of update rule hinges on the desired optimization speed."
What are the differences between hidden Markov models and neural networks?,https://stats.stackexchange.com/questions/20429/what-are-the-differences-between-hidden-markov-models-and-neural-networks,"['data-mining', 'algorithms', 'neural-networks', 'markov-process']",20429,True,25999,51321,3,46,1526768767,1325365415,36,1333749546,"What is hidden and what is observed
The thing that is hidden in a hidden Markov model is the same as the thing that is hidden in a discrete mixture model, so for clarity, forget about the hidden state's dynamics and stick with a finite mixture model as an example.  The 'state' in this model is the identity of the component that caused each observation.  In this class of model such causes are never observed, so 'hidden cause' is translated statistically into the claim that the observed data have marginal dependencies which are removed when the source component is known.  And the source components are estimated to be whatever makes this statistical relationship true.
The thing that is hidden in a feedforward multilayer neural network with sigmoid middle units is the states of those units, not the outputs which are the target of inference.  When the output of the network is a classification, i.e., a probability distribution over possible output categories, these hidden units values define a space within which categories are separable.  The trick in learning such a model is to make a hidden space (by adjusting the mapping out of the input units) within which the problem is linear.  Consequently, non-linear decision boundaries are possible from the system as a whole.
Generative versus discriminative
The mixture model (and HMM) is a model of the data generating process, sometimes called a likelihood or 'forward model'.  When coupled with some assumptions about the prior probabilities of each state you can infer a distribution over possible values of the hidden state using Bayes theorem (a generative approach).  Note that, while called a 'prior', both the prior and the parameters in the likelihood are usually learned from data.
In contrast to the mixture model (and HMM) the neural network learns a posterior distribution over the output categories directly (a discriminative approach).  This is possible because the output values were observed during estimation.  And since they were observed, it is not necessary to construct a posterior distribution from a prior and a specific model for the likelihood such as a mixture.  The posterior is learnt directly from data, which is more efficient and less model dependent. 
Mix and match
To make things more confusing, these approaches can be mixed together, e.g. when mixture model (or HMM) state is sometimes actually observed.  When that is true, and in some other circumstances not relevant here, it is possible to train discriminatively in an otherwise generative model.  Similarly it is possible to replace the mixture model mapping of an HMM with a more flexible forward model, e.g., a neural network.  
The questions
So it's not quite true that both models predict hidden state.  HMMs can be used to predict hidden state, albeit only of the kind that the forward model is expecting.  Neural networks can be used to predict a not yet observed state, e.g. future states for which predictors are available.  This sort of state is not hidden in principle, it just hasn't been observed yet.
When would you use one rather than the other?  Well, neural networks make rather awkward time series models in my experience.  They also assume you have observed output.  HMMs don't but you don't really have any control of what the hidden state actually is.  Nevertheless they are proper time series models.
","**Summary:**

**Hidden and Observed Factors:**

In hidden Markov models (HMMs) and discrete mixture models, the hidden state refers to the underlying cause of observed data. In neural networks, the hidden states are the activations of internal units that enable linear separation of categories.

**Generative vs. Discriminative Models:**

Mixture models (and HMMs) generate data by modeling the underlying process. Neural networks predict output categories directly from observed data, learning a posterior distribution without a prior or explicit likelihood model.

**Mixing Approaches:**

Generative and discriminative approaches can be combined when some hidden states are observed. HMMs can predict hidden states within their forward model, while neural networks can predict unobserved future states.

**Choice Criteria:**

Neural networks may be preferred for data with observed output and non-time series nature. HMMs are suitable for time series modeling but offer limited control over hidden state properties."
Softmax layer in a neural network,https://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network,['neural-networks'],79454,True,92309,65995,3,45,1512816502,1386853020,45,1396474114,"I feel a little bit bad about providing my own answer for this because it is pretty well captured by amoeba and juampa, except for maybe the final intuition about how the Jacobian (http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)  can be reduced back to a vector.
You correctly derived the gradient of the diagonal of the Jacobian matrix, which is to say that 
$ {\partial h_i \over \partial z_j}= h_i(1-h_j)\;\;\;\;\;\;: i = j $
and as amoeba stated it, you also have to derive the off diagonal entries of the Jacobian, which yield
$ {\partial h_i \over \partial z_j}= -h_ih_j\;\;\;\;\;\;: i \ne j $
These two concepts definitions can be conveniently combined using a construct called the Kronecker Delta (http://en.wikipedia.org/wiki/Kronecker_delta), so the definition of the gradient becomes
$ {\partial h_i \over \partial z_j}= h_i(\delta_{ij}-h_j) $
So the Jacobian is a square matrix $ \left[J \right]_{ij}=h_i(\delta_{ij}-h_j) $
All of the information up to this point is already covered by amoeba and juampa. The problem is of course, that we need to get the input errors from the output errors that are already computed. Since the gradient of the output error $\nabla h_i$ depends on all of the inputs, then the gradient of the input $x_i$ is
$[\nabla x]_k = \sum\limits_{i=1} \nabla h_{i,k}  $
Given the Jacobian matrix defined above, this is implemented trivially as the product of the matrix and the output error vector:
$ \vec{\sigma_l} = J\vec{\sigma_{l+1}} $
If the softmax layer is your output layer, then combining it with the cross-entropy cost model simplifies the computation to simply
$ \vec{\sigma_l} = \vec{h}-\vec{t} $
where $\vec{t}$ is the vector of labels, and $\vec{h}$ is the output from the softmax function. Not only is the simplified form convenient, it is also extremely useful from a numerical stability standpoint.
","**Summary:**

The Jacobian matrix captures the gradients of a diagonal layer with respect to its inputs. It is calculated using the Kronecker Delta, yielding:

```
[J]_{ij} = h_i(δ_{ij} - h_j)
```

To obtain the input errors from the output errors, the Jacobian is used to compute the gradient of the inputs:

```
$[\nabla x]_k = \sum_{i=1} \nabla h_{i,k}
```

This gradient calculation simplifies to a matrix-vector multiplication when using the cross-entropy cost model combined with a softmax output layer:

```
\vec{\sigma_l} = J\vec{\sigma_{l+1}} = \vec{h} - \vec{t}
```

where $\vec{\sigma}$ represents the errors, $\vec{h}$ the softmax output, and $\vec{t}$ the labels. This simplified form enhances numerical stability."
Training loss increases with time,https://stats.stackexchange.com/questions/324896/training-loss-increases-with-time,"['machine-learning', 'neural-networks', 'loss-functions', 'recurrent-neural-network', 'training-error']",324896,True,325255,117038,3,45,1573140634,1516822787,19,1517002089,"I had such a similar behavior when training a CNN, it was because I used the gradient descent with decaying learning rate for the error calculation. Have you significantly increased the number of iterations and checked if this behavior comes much later with the new low learning rate?
","While training a Convolutional Neural Network (CNN), the user encountered a behavior where the error stopped decreasing. This is attributed to using gradient descent with a decaying learning rate for error calculation.

To address this, the user is advised to:
- Significantly increase the number of iterations.
- Inspect if the behavior persists later in the training process, as the learning rate will be lower at that point.

The rationale is that increasing the number of iterations allows more time for the decaying learning rate to adjust. If the behavior still occurs later in training, it indicates that the model has reached a local minimum or plateau, and further training may not yield significant improvements."
Why is max pooling necessary in convolutional neural networks?,https://stats.stackexchange.com/questions/288261/why-is-max-pooling-necessary-in-convolutional-neural-networks,"['deep-learning', 'conv-neural-network', 'pooling']",288261,True,288445,47704,4,44,1651610822,1498872952,38,1499007908,"You can indeed do that, see Striving for Simplicity: The All Convolutional Net (https://arxiv.org/abs/1412.6806). Pooling gives you some amount of translation invariance, which may or may not be helpful. Also, pooling is faster to compute than convolutions. Still, you can always try replacing pooling by convolution with stride and see what works better. 
Some current works use average pooling (Wide Residual Networks (https://arxiv.org/abs/1605.07146), DenseNets (https://arxiv.org/abs/1608.06993)), others use convolution with stride (DelugeNets (https://arxiv.org/abs/1611.05552))
","**Summary:**

Pooling layers, while providing translation invariance and computational speed, may not always be optimal.

Recent research suggests that replacing pooling with convolutional layers with stride can improve performance. Experiments have shown success with average pooling (in Wide Residual Networks and DenseNets) and convolutional stride (in DelugeNets).

This strategy eliminates pooling's drawbacks while maintaining its benefits, allowing for more efficient and accurate convolution-based neural networks."
What exactly is the difference between a parametric and non-parametric model?,https://stats.stackexchange.com/questions/268638/what-exactly-is-the-difference-between-a-parametric-and-non-parametric-model,"['machine-learning', 'neural-networks', 'terminology', 'nonparametric']",268638,True,268646,70243,4,44,1623694514,1490018086,39,1490020547,"In a parametric model, the number of parameters is fixed with respect to the sample size.  In a nonparametric model, the (effective) number of parameters can grow with the sample size.  
In an OLS regression, the number of parameters will always be the length of $\beta$, plus one for the variance.  
A neural net with fixed architecture and no weight decay would be a parametric model.  
But if you have weight decay, then the value of the decay parameter selected by cross-validation will generally get smaller with more data.  This can be interpreted as an increase in the effective number of parameters with increasing sample size.
","**Summary:**

Parametric models have a fixed number of parameters regardless of sample size. Nonparametric models, however, allow for an increase in effective parameters as sample size increases.

In linear regression, the number of parameters is constant (one for each predictor variable plus one for constant term). In neural networks, fixed architecture and no weight adjustment result in a parametric model. However, weight decay leads to a variable number of parameters, as cross-validation selects smaller decay values with larger sample sizes.

This phenomenon in neural networks is effectively an increase in the number of parameters with sample size, making it similar to a nonparametric model."
Training loss goes down and up again. What is happening?,https://stats.stackexchange.com/questions/201129/training-loss-goes-down-and-up-again-what-is-happening,"['machine-learning', 'neural-networks', 'loss-functions', 'lstm']",201129,True,206238,112090,1,44,1497128812,1457691522,31,1460118041,"Your learning rate could be to big after the 25th epoch. This problem is easy to identify. You just need to set up a smaller value for your learning rate. If the problem related to your learning rate than NN should reach a lower error despite that it will go up again after a while. The main point is that the error rate will be lower in some point in time.
If you observed this behaviour you could use two simple solutions. First one is a simplest one. Set up a very small step and train it. The second one is to decrease your learning rate monotonically. Here is a simple formula:
$$
\alpha(t + 1) = \frac{\alpha(0)}{1 + \frac{t}{m}}
$$
Where $a$ is your learning rate, $t$ is your iteration number and $m$ is a coefficient that identifies learning rate decreasing speed. It means that your step will minimise by a factor of two when $t$ is equal to $m$.
","Excessive learning rate beyond the 25th epoch can hinder neural network (NN) performance. This can be detected by fluctuating error rates that initially decrease but later rise again. To address this issue, two solutions are proposed:

1. **Fixed Small Step:** Setting a significantly smaller learning rate from the beginning can alleviate the problem.

2. **Monotonic Learning Rate Decay:** Implementing a decay function that gradually reduces the learning rate over time can also mitigate the issue. The decay rate can be adjusted with the coefficient 'm' in the formula:

$$
\alpha(t + 1) = \frac{\alpha(0)}{1 + \frac{t}{m}}
$$

where 'α' represents the learning rate, 't' is the iteration number, and 'm' controls the decay speed. When 't' reaches 'm,' the learning rate halves."
What are the advantages of stacking multiple LSTMs?,https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms,"['classification', 'neural-networks', 'deep-learning', 'lstm', 'recurrent-neural-network']",163304,True,179817,43374,4,43,1583844632,1437962278,28,1446479118,"I think that you are referring to vertically stacked LSTM layers (assuming the horizontal axes is the time axis. 
In that case the main reason for stacking LSTM is to allow for greater model complexity. In case of a simple feedforward net we stack layers to create a hierarchical feature representation of the input data to then use for some machine learning task. The same applies for stacked LSTM's. 
At every time step an LSTM, besides the recurrent input. If the input is already the result from an LSTM layer (or a feedforward layer) then the current LSTM can create a more complex feature representation of the current input. 
Now the difference between having a feedforward layer between the feature input and the LSTM layer and having another LSTM layer is that a feed forward layer (say a fully connected layer) does not receive feedback from its previous time step and thus can not account for certain patterns. Having an LSTM in stead (e.g. using a stacked LSTM representation) more complex input patterns can be described at every layer
","Stacking LSTM layers enhances model complexity by creating hierarchical feature representations through time. Each LSTM layer leverages recurrent input and prior LSTM layer outputs, enabling more complex feature extraction. Unlike feedforward layers, stacked LSTMs retain information across time steps, allowing for the modeling of intricate patterns and dependencies in time series data."
Why is it that my colleagues and I learned opposite definitions for test and validation sets?,https://stats.stackexchange.com/questions/525697/why-is-it-that-my-colleagues-and-i-learned-opposite-definitions-for-test-and-val,"['machine-learning', 'neural-networks', 'cross-validation', 'terminology', 'validation']",525697,True,525810,5222,3,42,1621967341,1621864755,25,1621930291,"For machine learning, I've predominantly seen the usage OP describes, but I've also encountered lots of confusion coming from this usage.

Historically, I guess what happened (at least in my field, analytical chemistry) is that as models became more complex, at some point people noticed that independent data is needed for verification and validation purposes (in our terminology, almost all testing that is routinely done with models would be considered part of verification which in turn is part of the much wider task of method validation). Enter the validation set and methods such as cross validation (with its original purpose of estimating generalization error).
Later, people started to use generalization error estimates from what we call internal verification/validation such as cross validation or a random split to refine/optimize their models. Enter hyperparameter tuning.
Again, it was realized that estimating generalization error of the refined model needs independent data. And a new name was needed as well, as the usage of ""validation set"" for the data used for refining/optimizing had already been established. Enter the test set.
Thus we have the situation where a so-called validation set is used for model development/optimization/refining and is therefore not suitable any more for the purpose of model verification and validation.

Someone with e.g. an analytical chemistry (or engineering) background will certainly refer to the data they use/acquire for method validation purposes as their validation data* - and that is correct usage of the terms in these fields.
*(unless they know the different use of terminology in machine learning, in which case they'd usually explain what exactly they are talking about).

Personally, in order to avoid the ongoing confusion that comes from this clash of terminology between fields, I've moved to using ""optimization data/set"" for the data used for hyperparameter tuning (Andrew Ng's development set is fine with me as well) and ""verification data/set"" for the final independent test data (the testing we typically do is actually verification rather than validation, so that avoids another common mistake: the testing we typically do is not even close to a full method validation in analytical chemistry, and it's good to be aware of that)
Another strategy I find helpful to avoid confusion is moving from splitting into 3 data sets back to splitting into training and verification data, and then describing the hyperparameter tuning as part of the training procedure which happens to include another split into data used to fit the model parameters and data used to optimize the hyperparameters.
","In machine learning, the historical use of ""validation sets"" has shifted. These sets were initially used for model verification and validation, but as models grew more complex, they were also adopted for model optimization (hyperparameter tuning). However, this dual use created confusion, prompting the introduction of separate ""test sets"" for independent model verification.

In fields like analytical chemistry, ""validation data"" remains the preferred term for data used in independent testing. To address the terminology clash, some machine learning practitioners now use ""optimization data"" for hyperparameter tuning and ""verification data"" for final testing. Alternatively, splitting data into training and verification sets is recommended, with hyperparameter tuning viewed as part of the training process. This simplified approach helps prevent misunderstandings and aligns with the typical practice of verification rather than full validation in machine learning."
Difference between feedback RNN and LSTM/GRU,https://stats.stackexchange.com/questions/222584/difference-between-feedback-rnn-and-lstm-gru,"['neural-networks', 'lstm', 'recurrent-neural-network', 'gru']",222584,True,222587,100674,5,41,1633072823,1467896015,59,1467897431,"All RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies. GRUs are similar to LSTMs, but use a simplified structure. They also use a set of gates to control the flow of information, but they don't use separate memory cells, and they use fewer gates.
This paper gives a good overview:

Chung et al. (2014) (http://arxiv.org/abs/1412.3555).  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.

","Recurrent Neural Networks (RNNs) leverage feedback loops, enabling them to memorize information over time. However, training standard RNNs can be challenging due to the vanishing gradient problem. LSTM networks address this issue by introducing Long Short-Term Memory (LSTM) units, which include a memory cell to store long-term information. Gates control access to and removal from memory, enabling the network to learn more extended temporal dependencies. Gated Recurrent Units (GRUs) adopt a simplified structure with controlling gates but lack separate memory cells.

In summary, LSTM and GRU networks are types of RNNs that overcome the limitations of standard RNNs in handling long-term dependencies. LSTM networks excel in this regard with their memory cells, while GRUs offer a more efficient alternative with a simpler design. Both architectures have found widespread applications in various areas of machine learning, including natural language processing and time series analysis."
Why do we need to normalize the images before we put them into CNN?,https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn,"['deep-learning', 'conv-neural-network', 'image-processing']",185853,True,185857,62894,1,41,1539676793,1449644046,55,1449645855,"First note: you really should be also dividing by the standard deviation of each feature (pixel) value as well. Subtracting the mean centers the input to 0, and dividing by the standard deviation makes any scaled feature value the number of standard deviations away from the mean. 
To answer your question: Consider how a neural network learns its weights. C(NN)s learn by continually adding gradient error vectors (multiplied by a learning rate) computed from backpropagation to various weight matrices throughout the network as training examples are passed through. 
The thing to notice here is the ""multiplied by a learning rate"". 
If we didn't scale our input training vectors, the ranges of our distributions of feature values would likely be different for each feature, and thus the learning rate would cause corrections in each dimension that would differ (proportionally speaking) from one another. We might be over compensating a correction in one weight dimension while undercompensating in another. 
This is non-ideal as we might find ourselves in a oscillating (unable to center onto a better maxima in cost(weights) space) state or in a slow moving (traveling too slow to get to a better maxima) state. 
It is of course possible to have a per-weight learning rate, but it's yet more hyperparameters to introduce into an already complicated network that we'd also have to optimize to find. Generally learning rates are scalars. 
Thus we try to normalize images before using them as input into NN (or any gradient based) algorithm. 
","**Summary:**

To optimize neural network training, input data is typically normalized by subtracting the mean and dividing by the standard deviation for each feature. This ensures that feature values are centered around zero and scaled to a common range.

Normalizing the input data helps address the problem of varying ranges in feature values, which can lead to disproportionate weight adjustments during training. By making all feature values comparable, the learning rate can be applied consistently, reducing the risk of oscillations or slow convergence. This allows the network to efficiently find optimal weight configurations and improve performance."
"What&#39;s the relation between hierarchical models, neural networks, graphical models, bayesian networks?",https://stats.stackexchange.com/questions/4498/whats-the-relation-between-hierarchical-models-neural-networks-graphical-mode,"['causality', 'neural-networks', 'multilevel-analysis', 'graphical-model']",4498,True,4502,9873,3,41,1449584398,1289626995,34,1289637446,"A Bayesian network is a type of graphical model. The other ""big"" type of graphical model is a Markov Random Field (MRF).  Graphical models are used for inference, estimation and in general, to model the world.
The term hierarchical model is used to mean many things in different areas.
While neural networks come with ""graphs"" they generally don't encode dependence information, and the nodes don't represent random variables. NNs are different because they are discriminative. Popular neural networks are used for classification and regression.
Kevin Murphy has an excellent introduction to these topics available here (http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html).
","Graphical models, including Bayesian networks and Markov Random Fields, depict relationships between random variables using graphs. Bayesian networks excel in probabilistic inference and estimation, while Markov Random Fields emphasize spatial or temporal dependencies.

Despite the term ""hierarchical model"" having diverse meanings, it generally encompasses models with nested structures.

Neural networks, though containing graphs, differ in that their nodes do not represent random variables and they are primarily used for classification and regression tasks. This distinction arises from their discriminative nature, contrasting with the generative nature of graphical models."
"What&#39;s the relation between hierarchical models, neural networks, graphical models, bayesian networks?",https://stats.stackexchange.com/questions/4498/whats-the-relation-between-hierarchical-models-neural-networks-graphical-mode,"['causality', 'neural-networks', 'multilevel-analysis', 'graphical-model']",4498,True,4502,9873,3,41,1449584398,1289626995,34,1289637446,"A Bayesian network is a type of graphical model. The other ""big"" type of graphical model is a Markov Random Field (MRF).  Graphical models are used for inference, estimation and in general, to model the world.
The term hierarchical model is used to mean many things in different areas.
While neural networks come with ""graphs"" they generally don't encode dependence information, and the nodes don't represent random variables. NNs are different because they are discriminative. Popular neural networks are used for classification and regression.
Kevin Murphy has an excellent introduction to these topics available here (http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html).
","Bayesian networks and Markov Random Fields (MRFs) are two classes of graphical models that represent dependencies between random variables. Bayesian networks excel in modeling complex relationships, while MRFs focus on spatial or temporal dependencies.

The concept of ""hierarchical models"" has multiple interpretations, while neural networks differ from graphical models as they are discriminative and specialize in classification and regression tasks.

For a comprehensive introduction to these topics, refer to Kevin Murphy's work, which provides insights into the distinct characteristics and applications of Bayesian networks, MRFs, neural networks, and related modeling techniques."
How do bottleneck architectures work in neural networks?,https://stats.stackexchange.com/questions/205150/how-do-bottleneck-architectures-work-in-neural-networks,"['residuals', 'deep-learning', 'conv-neural-network']",205150,True,221466,45330,3,41,1621935154,1459624000,12,1467294310,"The bottleneck architecture is used in very deep networks due to computational considerations.
To answer your questions:

56x56 feature maps are not represented in the above image. This block is taken from a ResNet with input size 224x224. 56x56 is the downsampled version of the input at some intermediate layer.
64-d refers to the number of feature maps(filters). The bottleneck architecture has 256-d, simply because it is meant for much deeper network, which possibly take higher resolution image as input and hence require more feature maps.
Refer this figure (http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) for parameters of each bottleneck layer in ResNet 50.

","The bottleneck architecture, employed in deep neural networks, aims to reduce computational load. This approach involves a sequential arrangement of layers with varying dimensions.

In a bottleneck block, the input feature maps are reduced in size to minimize dimensionality and computational cost. The block is characterized by a specific number of feature maps (e.g., 64-d), which represents the number of filters employed in the convolution operations.

Variations in the bottleneck architecture exist, such as the ResNet architecture, which utilizes 256-d feature maps. This difference stems from the network's depth and the resolution of the input images. Deeper networks with higher resolution inputs require more feature maps to process the increased data volume.

To further illustrate these concepts, a reference figure can be consulted, providing detailed parameters for each bottleneck layer in the ResNet 50 architecture. By studying this figure, one can gain insights into the specific dimensions and configurations used in this popular deep neural network model."
Is overfitting &quot;better&quot; than underfitting?,https://stats.stackexchange.com/questions/521835/is-overfitting-better-than-underfitting,"['machine-learning', 'neural-networks', 'overfitting', 'bias-variance-tradeoff']",521835,True,521842,9861,9,40,1619799926,1619607816,67,1619611071,"Overfitting is likely to be worse than underfitting.  The reason is that there is no real upper limit to the degradation of generalisation performance that can result from over-fitting, whereas there is for underfitting.
Consider a non-linear regression model, such as a neural network or polynomial model.  Assume we have standardised the response variable.  A maximally underfitted solution might completely ignore the training set and have a constant output regardless of the input variables.  In this case the expected mean squared error on test data will be approximately the variance of the response variable in the training set.
Now consider an over-fitted model that exactly interpolates the training data.  To do so, this may require large excursions from the true conditional mean of the data generating process between points in the training set, for example the spurious peak at about x = -5.  If the first three training points were closer together on the x-axis, the peak would be likely to be even higher.  As a result, the test error for such points can be arbitrarily large, and hence the expected MSE on test data can similarly be arbitrarily large.
 (https://i.sstatic.net/hxmlA.png)
Source: https://en.wikipedia.org/wiki/Overfitting (https://en.wikipedia.org/wiki/Overfitting) (it is actually a polynomial model in this case, but see below for an MLP example)
Edit: As @Accumulation suggests, here is an example where the extent of overfitting is much greater (10 randomly selected data points from a linear model with Gaussian noise, fitted by a 10th order polynomial fitted to the utmost degree).  Happily the random number generator gave some points that were not very well spaced out first time!
 (https://i.sstatic.net/oJOq8.png)
It is worth making a distinction between ""overfitting"" and ""overparameterisation"".  Overparameterisation means you have used a model class that is more flexible than necessary to represent the underlying structure of the data, which normally implies a larger number of parameters.  ""Overfitting"" means that you have optimised the parameters of a model in a way that gives a better ""fit"" to the training sample (i.e. a better value of the training criterion), but to the detriment of generalisation performance.  You can have an over-parameterised model that does not overfit the data.  Unfortunately the two terms are often used interchangeably, perhaps because in earlier times the only real control of overfitting was achieved by limiting the number of parameters in the model (e.g. feature selection for linear regression models).  However regularisation (c.f. ridge regression) decouples overparameterisation from overfitting, but our use of the terminology has not reliably adapted to that change (even though ridge regression is almost as old as I am!).
Here is an example that was actually generated using an (overparameterised) MLP
 (https://i.sstatic.net/XINi2.png)
","Overfitting, where a model excessively conforms to training data, poses a greater risk than underfitting. Unlike underfitting, overfitting has no inherent limit to the potential degradation of generalization performance. In non-linear models, overfitting can cause large deviations from the true data-generating process, leading to arbitrarily large test errors. While overparameterization refers to using a highly flexible model class, overfitting specifically denotes optimizing parameters detrimentally affecting generalization performance. Regularization techniques can mitigate overfitting by decoupling overparameterization from overfitting, allowing for more flexible models without compromising accuracy on unseen data."
What does kernel size mean?,https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean,"['machine-learning', 'neural-networks']",296679,True,296701,101595,2,40,1586520801,1502131280,37,1502138617,"Deep neural networks, more concretely convolutional neural networks (CNN), are basically a stack of layers which are defined by the action of a number of filters on the input. Those filters are usually called kernels.
For example, the kernels in the convolutional layer, are the convolutional filters. Actually no convolution is performed, but a cross-correlation. The kernel size here refers to the widthxheight of the filter mask.
The max pooling layer, for example, returns the pixel with maximum value from a set of pixels within a mask (kernel). That kernel is swept across the input, subsampling it.
So nothing to do with the concept of kernels in support vector machines or regularization networks. You can think of them as feature extractors.
","Deep neural networks, particularly convolutional neural networks (CNNs), consist of layers defined by the application of filters (kernels) on the input. Kernels in convolutional layers perform cross-correlation, not convolution, and determine the filter's size. Max pooling layers select the pixel with the highest value from a set defined by the kernel, subsampling the input. Unlike kernels in support vector machines or regularization networks, CNN kernels are feature extractors, identifying patterns and features in the data."
What is the &quot;capacity&quot; of a machine learning model?,https://stats.stackexchange.com/questions/312424/what-is-the-capacity-of-a-machine-learning-model,"['machine-learning', 'deep-learning', 'autoencoders', 'variational-bayes']",312424,True,312578,30695,3,40,1692802473,1510054001,48,1510123776,"Capacity is an informal term. It's very close (if not a synonym) for model complexity. It's a way to talk about how complicated a pattern or relationship a model can express. You could expect a model with higher capacity to be able to model more relationships between more variables than a model with a lower capacity.
Drawing an analogy from the colloquial definition of capacity, you can think of it as the ability of a model to learn from more and more data, until it's been completely ""filled up"" with information.
There are various ways to formalize capacity and compute a numerical value for it, but importantly these are just some possible ""operationalizations"" of capacity (in much the same way that, if someone came up with a formula to compute beauty, you would realize that the formula is just one fallible interpretation of beauty).

VC dimension (https://en.wikipedia.org/wiki/VC_dimension) is a mathematically rigorous formulation of capacity. However, there can be a large gap between the VC dimension of a model and the model's actual ability to fit the data. Even though knowing the VC dim gives a bound on the generalization error of the model, this is usually too loose to be useful with neural networks.
Another line of research see here (http://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf) is to use the spectral norm of the weight matrices in a neural network as a measure of capacity. One way to understand this is that the spectral norm bounds the Lipschitz constant of the network. 
The most common way to estimate the capacity of a model is to count the number of parameters. The more parameters, the higher the capacity in general. Of course, often a smaller network learns to model more complex data better than a larger network, so this measure is also far from perfect.
Another way to measure capacity might be to train your model with random labels (Neyshabur et. al (https://arxiv.org/abs/1706.08947)) -- if your network can correctly remember a bunch of inputs along with random labels, it essentially shows that the model has the ability to remember all those data points individually. The more input/output pairs which can be ""learned"", the higher the capacity. 
Adapting this to an auto-encoder, you might 
generate random inputs, train the network to reconstruct them, and then count how many random inputs you can successfully reconstruct with less than $\epsilon$ error. 
","Capacity, akin to model complexity, measures the model's ability to capture intricate relationships between variables. It is analogous to a model's capacity to learn from data before reaching its full potential.

VC Dimension provides a mathematical framework for assessing capacity, but its utility is limited for neural networks. Other methods include utilizing the spectral norm of weight matrices or counting the number of network parameters.

To estimate capacity, researchers have proposed training models with random labels to evaluate their ability to memorize random input-label pairs. Additionally, measuring the number of random inputs that can be reconstructed with minimal error using an autoencoder provides an alternative metric.

Despite these measures, capacity remains an elusive concept, with no perfect or universally applicable method for its estimation."
Why should we shuffle data while training a neural network?,https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network,"['machine-learning', 'neural-networks']",245502,True,311318,47251,6,38,1583788951,1478905829,33,1509566175,"Note: throughout this answer I refer to minimization of training loss and I do not discuss stopping criteria such as validation loss. The choice of stopping criteria does not affect the process/concepts described below.
The process of training a neural network is to find the minimum value of a loss function $ℒ_X(W)$, where $W$ represents a matrix (or several matrices) of weights between neurons and $X$ represents the training dataset. I use a subscript for $X$ to indicate that our minimization of $ℒ$ occurs only over the weights $W$ (that is, we are looking for $W$ such that $ℒ$ is minimized) while $X$ is fixed.
Now, if we assume that we have $P$ elements in $W$ (that is, there are $P$ weights in the network), $ℒ$ is a surface in a $P+1$-dimensional space. To give a visual analogue, imagine that we have only two neuron weights ($P=2$). Then $ℒ$ has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights $W$, the loss function can be evaluated on $X$ and that value becomes the elevation of the surface.
But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming ""stuck"" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if $X$ is unchanged over all training iterations, because the surface is fixed for a given $X$; all its features are static, including its various minima.
A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, $X$ changes with every iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same $X$. The effect is that the solver can easily ""bounce"" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration $i$ with training mini-batch $X_i$. This local minimum corresponds to $ℒ$ evaluated at a particular value of weights; we'll call it $ℒ_{X_i}(W_i)$. On the next iteration the shape of our loss surface actually changes because we are using $X_{i+1}$, that is, $ℒ_{X_{i+1}}(W_i)$ may take on a very different value from $ℒ_{X_i}(W_i)$ and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of $ℒ_{X_{i+1}}$ will -- in general -- be different from that of $ℒ_{X_{i}}$. Note that here I am referring to the loss function $ℒ$ evaluated on a training set $X$; it is a complete surface defined over all possible values of $W$, rather than the evaluation of that loss (which is just a scalar) for a specific value of $W$. Note also that if mini-batches are used without shuffling there is still a degree of ""diversification"" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch).
One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because $ℒ$ is evaluated by computing a value for each row of $X$ (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices $W$, the arrangement of the rows of $X$ has no effect when using full-batch gradient descent (that is, when each batch is the full $X$, and iterations and epochs are the same thing).
","Neural network training aims to minimize a loss function over trainable weights. However, non-convexity leads to the challenge of getting stuck in local minima.

Mini-batch training addresses this by iteratively shuffling and using subsets of the data, resulting in dynamic loss surfaces that change with each iteration. This ""bouncing"" effect reduces the likelihood of settling in local minima.

Using full-batch gradient descent, the order of data rows does not affect the loss function evaluation, as it considers the entire dataset at once."
What are the differences between sparse coding and autoencoder?,https://stats.stackexchange.com/questions/118199/what-are-the-differences-between-sparse-coding-and-autoencoder,"['machine-learning', 'neural-networks', 'unsupervised-learning', 'deep-learning', 'autoencoders']",118199,True,118490,34596,4,38,1509118246,1412703894,36,1412880717,"Finding the differences can be done by looking at the models. Let's look at sparse coding first. 
Sparse coding
Sparse coding minimizes the objective
$$
\mathcal{L}_{\text{sc}} = \underbrace{||WH - X||_2^2}_{\text{reconstruction term}} + \underbrace{\lambda  ||H||_1}_{\text{sparsity term}}
$$
where $W$ is a matrix of bases, H is a matrix of codes and $X$ is a matrix of the data we wish to represent. $\lambda$ implements a trade of between sparsity and reconstruction. Note that if we are given $H$, estimation of $W$ is easy via least squares. 
In the beginning, we do not have $H$ however. Yet, many algorithms exist that can solve the objective above with respect to $H$. Actually, this is how we do inference: we need to solve an optimisation problem if we want to know the $h$ belonging to an unseen $x$.
Auto encoders
Auto encoders are a family of unsupervised neural networks. There are quite a lot of them, e.g. deep auto encoders or those having different regularisation tricks attached--e.g. denoising, contractive, sparse. There even exist probabilistic ones, such as generative stochastic networks or the variational auto encoder. Their most abstract form is
$$
D(d(e(x;\theta^r); \theta^d), x)
$$
but we will go along with a much simpler one for now:
$$
\mathcal{L}_{\text{ae}} = ||W\sigma(W^TX) - X||^2
$$
where $\sigma$ is a nonlinear function such as the logistic sigmoid $\sigma(x) = {1 \over 1 + \exp(-x)}$. 
Similarities
Note that $\mathcal{L}_{sc}$ looks almost like $\mathcal{L}_{ae}$ once we set $H = \sigma(W^TX)$. The difference of both is that i) auto encoders do not encourage sparsity in their general form ii) an autoencoder uses a model for finding the codes, while sparse coding does so by means of optimisation.
For natural image data, regularized auto encoders and sparse coding tend to yield very similar $W$. However, auto encoders are much more efficient and are easily generalized to much more complicated models. E.g. the decoder can be highly nonlinear, e.g. a deep neural network. Furthermore, one is not tied to the squared loss (on which the estimation of $W$ for $\mathcal{L}_{sc}$ depends.)
Also, the different methods of regularisation yield representations with different characteristica. Denoising auto encoders have also been shown to be equivalent to a certain form of RBMs etc.
But why?
If you want to solve a prediction problem, you will not need auto encoders unless you have only little labeled data and a lot of unlabeled data. Then you will generally be better of to train a deep auto encoder and put a linear SVM on top instead of training a deep neural net.
However, they are very powerful models for capturing characteristica of distributions. This is vague, but research turning this into hard statistical facts is currently conducted. Deep latent Gaussian models aka Variational Auto encoders or generative stochastic networks are pretty interesting ways of obtaining auto encoders which provably estimate the underlying data distribution.
","Sparse coding and auto encoders are unsupervised learning techniques that aim to extract meaningful representations from data. Sparse coding enforces sparsity in the representations, leading to a trade-off between reconstruction accuracy and sparsity. On the other hand, auto encoders utilize nonlinear activation functions to learn complex representations without explicitly promoting sparsity.

Auto encoders, despite not explicitly enforcing sparsity, often yield similar representations to sparse coding for natural image data. However, auto encoders are computationally more efficient and offer greater flexibility. They can be extended to include complex decoder networks and different loss functions. While auto encoders are not essential for prediction problems with labeled data, they excel in capturing distribution characteristics, especially when data is limited and unlabeled. Deep latent Gaussian models (DGMs) are a promising variation of auto encoders that provably estimate the underlying data distribution."
Gradient backpropagation through ResNet skip connections,https://stats.stackexchange.com/questions/268820/gradient-backpropagation-through-resnet-skip-connections,"['machine-learning', 'neural-networks', 'conv-neural-network', 'gradient-descent', 'backpropagation']",268820,True,268824,24588,2,38,1628862856,1490083792,24,1490085777,"Add sends the gradient back equally to both inputs. You can convince yourself of this by running the following in tensorflow:
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    x1_tf = tf.Variable(1.5, name='x1')
    x2_tf = tf.Variable(3.5, name='x2')
    out_tf = x1_tf + x2_tf

    grads_tf = tf.gradients(ys=[out_tf], xs=[x1_tf, x2_tf])
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        fd = {
            out_tf: 10.0
        }
        print(sess.run(grads_tf, feed_dict=fd))

Output:
[1.0, 1.0]

So, the gradient will be:

passed back to previous layers, unchanged, via the skip-layer connection, and also
passed to the block with weights, and used to update those weights

Edit: there is a question: ""what is the operation at the point where the highway connection and the neural net block join back together again, at the bottom of Figure 2?""
There answer is: they are summed. You can see this from Figure 2's formula:
$$
\mathbf{\text{output}} \leftarrow \mathcal{F}(\mathbf{x}) + \mathbf{x}
$$
What this says is that:

the values in the bus ($\mathbf{x}$)
are added to the results of passing the bus values, $\mathbf{x}$, through the network, ie $\mathcal{F}(\mathbf{x})$
to give the output from the residual block, which I've labelled here as $\mathbf{\text{output}}$

Edit 2:
Rewriting in slightly different words:

in the forwards direction, the input data flows down the bus


at points along the bus, residual blocks can learn to add/remove values to the bus vector

in the backwards direction, the gradients flow back down the bus


along the way, the gradients update the residual blocks they move past
the residual blocks will themselves modify the gradients slightly too


The residual blocks do modify the gradients flowing backwards, but there are no 'squashing' or 'activation' functions that the gradients flow through. 'squashing'/'activation' functions are what causes the exploding/vanishing gradient problem, so by removing those from the bus itself, we mitigate this problem considerably.
Edit 3: Personally I imagine a resnet in my head as the following diagram. Its topologically identical to figure 2, but it shows more clearly perhaps how the bus just flows straight through the network, whilst the residual blocks just tap the values from it, and add/remove some small vector against the bus:
 (https://i.sstatic.net/ei9GQ.png)
","ResNet architectures employ ""highway"" connections that pass gradients unchanged to both inputs. This allows gradients to flow directly through the network without encountering squashing or activation functions, thereby mitigating the exploding/vanishing gradient problem.

The residual block operation at the point where the highway connection and neural net block rejoin involves summing the original input values with the output of the neural net block. This effectively allows residual blocks to modify the input data by adding or removing elements.

During forward propagation, data flows down the ""bus"" (highway connection), and residual blocks can modify the data. During backpropagation, gradients flow back down the ""bus,"" updating residual blocks along the way. The residual blocks modify the gradients slightly, but the absence of squashing functions prevents the gradients from becoming unstable."
How to visualize/understand what a neural network is doing?,https://stats.stackexchange.com/questions/11764/how-to-visualize-understand-what-a-neural-network-is-doing,"['data-visualization', 'neural-networks']",11764,True,11790,8732,5,38,1455682217,1307639959,12,1307687345,"Neural networks are sometimes called ""differentiable function approximators"". So what you can do is to differentiate any unit with respect to any other unit to see what their relationshsip is.
You can check how sensitive the error of the network is wrt to a specific input as well with this.
Then, there is something called ""receptive fields"", which is just the visualization of the connections going into a hidden unit. This makes it easy to understand what particular units do for image data, for example. This can be done for higher levels as well. See Visualizing Higher-Level Features of a Deep Network (http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247).
","Neural networks, known as differentiable function approximators, allow for the analysis of relationships between units by evaluating derivatives. This enables the examination of the network's error sensitivity to specific inputs.

The concept of ""receptive fields"" visualizes the connections feeding into a hidden unit, providing insights into the functionality of individual units within image data processing. This visualization extends to higher network levels, enabling the understanding of feature extraction and representation in complex networks."
"What is the difference between kernel, bias, and activity regulizers, and when to use which?",https://stats.stackexchange.com/questions/383310/what-is-the-difference-between-kernel-bias-and-activity-regulizers-and-when-t,"['neural-networks', 'regularization', 'keras']",383310,True,383326,30962,3,37,1599410632,1545003348,51,1545013207,"What is the difference between them?
You have the regression equation $y = Wx+b$, where $x$ is the input, $W$ the weights matrix and $b$ the bias.

Kernel Regularizer: Tries to reduce the weights $W$ (excluding bias).
Bias Regularizer: Tries to reduce the bias $b$.
Activity Regularizer: Tries to reduce the layer's output $y$, thus will reduce the weights and adjust bias so $Wx+b$ is smallest.

When to use which?
Usually, if you have no prior on the distribution that you wish to model, you would only use the kernel regularizer, since a large enough network can still model your function even if the regularization on the weights are big.
If you want the output function to pass through (or have an intercept closer to) the origin, you can use the bias regularizer.
If you want the output to be smaller (or closer to 0), you can use the activity regularizer.
$L_1$ versus $L_2$ regularization
Now, for the $L_1$ versus $L_2$ loss for weight decay (not to be confused with the outputs loss function).

$L_2$ loss is defined as $w^2$
$L_1$ loss is defined as $|w|$.

where $w$ is a component of the matrix $W$.

The gradient of $L_2$ will be: $2w$
The gradient of $L_1$ will be: $sign(w)$

Thus, for each gradient update with a learning rate $a$, in $L_2$ loss, the weights will be subtracted by $aW$, while in $L_1$ loss they will be subtracted by $a \cdot sign(W)$.
The effect of $L_2$ loss on the weights is a reduction of large components in the matrix $W$, while $L_1$ loss will make the weights matrix sparse, with many zero values. The same applies to the bias and output respectively using the bias and activity regularizer.
","**Summary:**

Regularization techniques in machine learning reduce overfitting by penalizing specific model parameters. Different types of regularization target different model components:

* **Kernel Regularizer:** Reduces weights ($W$) to minimize overfitting.
* **Bias Regularizer:** Reduces bias ($b$) to ensure the output function passes through or is close to the origin.
* **Activity Regularizer:** Reduces the layer output ($y$) to decrease weights and adjust bias.

In $L_1$ and $L_2$ weight decay, the loss functions diverge. $L_2$ gradually reduces large weights, while $L_1$ encourages sparsity in the weight matrix. The bias and activity regularizers have similar effects on their respective parameters using $L_1$ or $L_2$ loss.

When choosing a regularization technique, consider the desired behavior: reducing overfitting, enforcing origin-centered output, or minimizing output values. $L_1$ vs. $L_2$ regularization affects the weight distribution, with $L_1$ promoting sparsity and $L_2$ reducing large weights."
What are variational autoencoders and to what learning tasks are they used?,https://stats.stackexchange.com/questions/321841/what-are-variational-autoencoders-and-to-what-learning-tasks-are-they-used,"['machine-learning', 'bayesian', 'deep-learning', 'autoencoders', 'variational-bayes']",321841,True,328181,9213,2,37,1696412147,1515249119,58,1518445757,"Even though variational autoencoders (VAEs) are easy to implement and train, explaining them is not simple at all, because they blend concepts from Deep Learning and Variational Bayes, and the Deep Learning and Probabilistic Modeling communities use different terms for the same concepts. Thus when explaining VAEs you risk either concentrating on the statistical model part, leaving the reader without a clue about how to actually implement it, or vice versa to concentrate on the network architecture and loss function, in which the Kullback-Leibler term seems to be pulled out of thin air. I'll try to strike a middle ground here, starting from the model but giving enough details to actually implement it in practice, or understand someone's else implementation.
VAEs are generative models
Unlike classical (sparse, denoising, etc.) autoencoders, VAEs are generative models, like GANs. With generative model I mean a model which learns the probability distribution $p(\mathbf{x})$ over the input space $\mathcal{x}$. This means that after we have trained such a model, we can then sample from (our approximation of) $p(\mathbf{x})$. If our training set is made of handwritten digits (MNIST), then after training the generative model is able to create images which look like handwritten digits, even though they're not ""copies"" of the images in the training set.
Learning the distribution of the images in the training set implies that images which look like handwritten digits should have an high probability of being generated, while images which look like the Jolly Roger or random noise should have a low probability. In other words, it means learning about the dependencies among pixels: if our image is a $28\times 28=784$ pixels grayscale image from MNIST, the model should learn that if a pixel is very bright, then there's a significant probability that some neighboring pixels are bright too, that if we have a long, slanted line of bright pixels we may have another smaller, horizontal line of pixels above this one (a 7), etc.
VAEs are latent variable models
The VAE is a latent variables model: this means that $\mathbf{x}$, the random vector of the 784 pixel intensities (the observed variables), is modeled as a (possibly very complicated) function of a random vector $\mathbf{z}\in\mathcal{Z}$ of lower dimensionality, whose components are unobserved (latent) variables. When does such a model make sense? For example, in the MNIST case we think that the handwritten digits belong to a manifold of dimension much smaller than the dimension of $\mathcal{x}$, because the vast majority of random arrangements of 784 pixel intensities, don't look at all like handwritten digit. Intuitively we would expect the dimension to be at least 10 (the number of digits), but it's most likely larger because each digit can be written in different ways. Some differences are unimportant for the quality of the final image (for example, global rotations and translations), but others are important. So in this case the latent model makes sense. More on this later. Note that, amazingly, even if our intuition tells us that the dimension should about 10, we can definitely use just 2 latent variables to encode the MNIST dataset with a VAE (though results won't be pretty). The reason is that even a single real variable can encode infinitely many classes, because it can assume all possible integer values and more. Of course, if the classes have significant overlap among them (such as 9 and 8 or 7 and I in MNIST), even the most complicated function of just two latent variables will do a poor job of generating clearly discernible samples for each class. More on this later.
VAEs assume a multivariate parametric distribution $q(\mathbf{z}\vert\mathbf{x},\boldsymbol{\lambda})$ (where $\boldsymbol{\lambda}$ are the parameters of $q$), and they learn the parameters of the multivariate distribution. The use of a parametric pdf for $\mathbf{z}$, which prevents the number of parameters of a VAE to grow without bounds with the growth of the training set, is called amortization in VAE lingo (yeah, I know...).
The decoder network
 (https://i.sstatic.net/Hqauz.png)
We start from the decoder network because the VAE is a generative model, and the only part of the VAE which is actually used to generate new images is the decoder. The encoder network is only used at inference (training) time.
The goal of the decoder network is to generate new random vectors $\mathbf{x}$ belonging to the input space $\mathcal{X}$, i.e., new images, starting from realizations of the latent vector $\mathbf{z}$. This means clearly that it must learn the conditional distribution $p(\mathbf{x}\vert\mathbf{z})$. For VAEs this distribution is often assumed to be a multivariate Gaussian1:
$$p_{\boldsymbol{\phi}}(\mathbf{x}\vert\mathbf{z}) = \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}(\mathbf{z}; \boldsymbol{\phi}), \boldsymbol{\sigma}(\mathbf{z}; \boldsymbol{\phi})^2I) $$
$\boldsymbol{\phi}$ is the vector of weights (and biases) of the encoder network. The vectors $\boldsymbol{\mu}(\mathbf{z};\boldsymbol{\phi})$ and $\boldsymbol{\sigma}(\mathbf{z}; \boldsymbol{\phi})$ are complex, unknown nonlinear functions, modeled by the decoder network: neural networks are powerful nonlinear functions approximators.

As noted by @amoeba in the comments, there is a striking similarity between the decoder and a classic latent variables model: Factor Analysis. In Factor Analysis, you assume the model:
$$ \mathbf{x}\vert\mathbf{z}\sim\mathcal{N}(\mathbf{W}\mathbf{z}+\boldsymbol{\mu}, \boldsymbol{\sigma}^2I),\ \mathbf{z}\sim\mathcal{N}(0,I)$$
Both models (FA & the decoder) assume that the conditional distribution of the observable variables $\mathbf{x}$ on the latent variables $\mathbf{z}$ is Gaussian, and that the $\mathbf{z}$ themselves are standard Gaussians. The difference is that the decoder doesn't assume that the mean of $p(\mathbf{x}|\mathbf{z})$ is linear in $\mathbf{z}$, nor it assumes that the standard deviation is a constant vector. On the contrary, it models them as  complex nonlinear functions of the $\mathbf{z}$. In this respect, it can be seen as nonlinear Factor Analysis. See here (https://www.cs.cmu.edu/%7Ebhiksha/courses/deeplearning/Spring.2017/slides/lec12.vae.pdf) for an insightful discussion of this connection between FA and VAE. Since FA with an isotropic covariance matrix is just PPCA, this also ties in to the well-known result that a linear autoencoder reduces to PCA.

Let's go back to the decoder: how do we learn $\boldsymbol{\phi}$? Intuitively we want latent variables $\mathbf{z}$ which maximize the likelihood of generating the $\mathbf{x}_i$ in the training set $D_n$. In other words we want to compute the posterior probability distribution of the $\mathbf{z}$, given the data:
$$p(\mathbf{z}\vert\mathbf{x})=\frac{p_{\boldsymbol{\phi}}(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})}{p(\mathbf{x})}$$
We assume a $\mathcal{N}(0,I)$ prior on $\mathbf{z}$, and we're left with the usual issue in Bayesian inference that computing $p(\mathbf{x})$ (the evidence) is hard (a multidimensional integral). What's more, since here $\boldsymbol{\mu}(\mathbf{z};\boldsymbol{\phi})$ is unknown, we can't compute it anyway. Enter Variational Inference, the tool which gives Variational Autoencoders their name.
Variational Inference for the VAE model
Variational Inference is a tool to perform approximate Bayesian Inference for very complex models. It's not an overly complex tool, but my answer is already too long and I won't go into a detailed explanation of VI. You can have a look at this answer and the references therein if you're curious:
https://stats.stackexchange.com/a/270569/58675 (https://stats.stackexchange.com/a/270569/58675)
It suffices to say that VI looks for an approximation to $p(\mathbf{z}\vert \mathbf{x})$ in a parametric family of distributions $q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})$, where, as noted above, $\boldsymbol{\lambda}$ are the parameters of the family. We look for the parameters which minimize the Kullback-Leibler divergence between our target distribution $p(\mathbf{z}\vert \mathbf{x})$ and $q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})$:
$$\min_{\boldsymbol{\lambda}}\mathcal{D}[p(\mathbf{z}\vert \mathbf{x})\vert\vert q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda})]$$
Again, we cannot minimize this directly because the definition of Kullback-Leibler divergence includes the evidence. Introducing the ELBO (Evidence Lower BOund) and after some algebraic manipulations, we finally get at:
$$ELBO(\boldsymbol{\lambda})= E_{q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})}[\log p(\mathbf{x}\vert\boldsymbol{z})]-\mathcal{D}[(q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})\vert\vert p(\boldsymbol{z})]$$
Since the ELBO is a lower bound on evidence (see the above link), maximizing the ELBO is not exactly equivalent to maximizing the likelihood of data given $\boldsymbol{\lambda}$ (after all, VI is a tool for approximate Bayesian inference), but it goes in the right direction.
In order to make inference, we need to specify the parametric family $q(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$. In most VAEs we choose a multivariate, uncorrelated Gaussian distribution
$$q(\mathbf{z}\vert \mathbf{x},\boldsymbol{\lambda}) = \mathcal{N}(\mathbf{z}\vert\boldsymbol{\mu}(\mathbf{x}), \boldsymbol{\sigma}^2(\mathbf{x})I) $$
This is the same choice we made for $p(\mathbf{x}\vert\mathbf{z})$, though we may have chosen a different parametric family. As before, we can estimate these complex nonlinear functions by introducing a neural network model. Since this model accepts input images and returns parameters of the distribution of the latent variables we call it the encoder network.
The encoder network
Also called the inference network, this is only used at training time.
 (https://i.sstatic.net/XHxi8.png)
As noted above, the encoder must approximate $\boldsymbol{\mu}(\mathbf{x})$ and  $\boldsymbol{\sigma}(\mathbf{x})$, thus if we have, say, 24 latent variables, the output of the encoder is a $d=48$ vector. The encoder has weights (and biases) $\boldsymbol{\theta}$. To learn $\boldsymbol{\theta}$, we can finally write the ELBO in terms of the parameters $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ of the encoder and decoder network, as well as the training set points:
$$ELBO(\boldsymbol{\theta},\boldsymbol{\phi})= \sum_i E_{q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})}[\log p_{\boldsymbol{\phi}}(\mathbf{x}_i\vert\boldsymbol{z})]-\mathcal{D}[(q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})\vert\vert p(\boldsymbol{z})]$$
We can finally conclude. The opposite of the ELBO, as a function of $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$, is used as the loss function of the VAE. We use SGD to minimize this loss, i.e., maximize the ELBO. Since the ELBO is a lower bound on the evidence, this goes in the direction of maximizing the evidence, and thus generating new images which are optimally similar to those in the training set. The first term in the ELBO is the expected negative log-likelihood of the training set points, thus it encourages the decoder to produce images which are similar to the training ones. The second term can be interpreted as a regularizer: it encourages the encoder to generate a distribution for the latent variables which is similar to $p(\boldsymbol{z})=\mathcal{N}(0,I)$. But by introducing the probability model first, we understood where the whole expression comes from: the minimization of the Kullabck-Leibler divergence between the approximate posterior $q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$ and the model posterior $p(\boldsymbol{z}\vert \mathbf{x},\boldsymbol{\lambda})$.2
Once we have learned $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ by maximizing $ELBO(\boldsymbol{\theta},\boldsymbol{\phi})$, we can throw away the encoder. From now on, to generate new images just sample $\boldsymbol{z}\sim \mathcal{N}(0,I)$ and propagate it through the decoder. The decoder outputs will be images similar to those in the training set.
References and further reading

the original paper: Auto-Encoding Variational Bayes (https://arxiv.org/abs/1312.6114)
a nice tutorial, with a few minor imprecisions: Tutorial on Variational Autoencoders (https://arxiv.org/pdf/1606.05908.pdf)
how to reduce the blurriness of the images generated by your VAE, while at the same time getting latent variables which have a visual (perceptual) meaning, so that you can ""add"" features (smile, sunglasses, etc.) to your generated images: Deep Feature Consistent Variational
Autoencoder (https://arxiv.org/pdf/1610.00291.pdf)
improving the quality of VAE-generated images even more, by using Gaussian versions of autoregressive autoencoders:Improved Variational Inference
with Inverse Autoregressive Flow (https://arxiv.org/pdf/1606.04934.pdf)
new directions of research and a deeper understanding of pros & cons of the VAE model: Towards a Deeper Understanding of Variational Autoencoding Models (https://arxiv.org/pdf/1702.08658.pdf) & INFERENCE SUBOPTIMALITY
IN VARIATIONAL AUTOENCODERS (https://arxiv.org/pdf/1801.03558.pdf)


1 This assumption is not strictly necessary, though it simplifies our description of VAEs. However, depending on applications, you may assume a different distribution for $p_{\phi}(\mathbf{x}\vert\mathbf{z})$. For example, if $\mathbf{x}$ is a vector of binary variables, a Gaussian $p$ makes no sense, and a multivariate Bernoulli can be assumed.
2 The ELBO expression, with its mathematical elegance, conceals two major sources of pain for the VAE practitioners. One is the average term $E_{q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})}[\log p_{\boldsymbol{\phi}}(\mathbf{x}_i\vert\boldsymbol{z})]$. This effectively requires computing an expectation, which requires taking multiple samples from $q_{\boldsymbol{\theta}}(\boldsymbol{z}\vert \mathbf{x}_i,\boldsymbol{\lambda})$. Given the sizes of the involved neural networks, and the low convergence rate of the SGD algorithm, having to draw multiple random samples at each iteration (actually, for each minibatch, which is even worse) is very time-consuming. VAE users solve this problem very pragmatically by computing that expectation with a single (!) random sample. The other issue is that to train two neural networks (encoder & decoder) with the backpropagation algorithm, I need to be able to differentiate all steps involved in forward propagation from the encoder to the decoder. Since the decoder is not deterministic (evaluating its output requires drawing from a multivariate Gaussian), it doesn't even make sense to ask if it's a differentiable architecture. The solution to this is the reparametrization trick (https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important).
","Variational Autoencoders (VAEs) are generative models that learn probability distributions for data. Unlike denoising autoencoders, VAEs encode data into a latent space of lower dimensionality, enabling the generation of new samples similar to the training data.

VAE models include an encoder network that maps input data to a probability distribution over the latent space, and a decoder network that maps latent variables to generated data. The encoder and decoder are trained to minimize the Kullback-Leibler divergence between the approximate posterior distribution given the data and the prior distribution of the latent variables.

This minimization is achieved through variational inference, an approximation technique used to compute the posterior distribution. VAEs assume a multivariate Gaussian distribution for the latent variables and use neural networks to model complex nonlinear functions for the encoder and decoder.

Maximizing the Evidence Lower BOund (ELBO) serves as the training objective for VAEs, encouraging the decoder to generate data similar to the training set and the encoder to approximate the prior distribution. Once trained, the decoder can generate new data by sampling from the latent space.

VAEs offer interpretability advantages over GANs, as latent variables can provide insights into the features and variations present in the data. However, their training can be computationally expensive due to the need for multiple random samples to estimate expectations during training."
Explanation of Spikes in training loss vs. iterations with Adam Optimizer,https://stats.stackexchange.com/questions/303857/explanation-of-spikes-in-training-loss-vs-iterations-with-adam-optimizer,"['neural-networks', 'deep-learning', 'adam']",303857,True,304150,56436,2,37,1543935120,1505813424,33,1505931359,"The spikes are an unavoidable consequence of Mini-Batch Gradient Descent in Adam (batch_size=32). 
Some mini-batches have 'by chance' unlucky data for the optimization, inducing those spikes you see in your cost function using Adam. If you try stochastic gradient descent (same as using batch_size=1) you will see that there are even more spikes in the cost function. The same doesn´t happen in (Full) Batch GD because it uses all training data (i.e the batch size is equal to the cardinality of your training set) each optimization epoch. As in your first graphic the cost is monotonically decreasing smoothly it seems the title (i) With SGD) is wrong and you are using (Full) Batch Gradient Descent instead of SGD.
On his great Deep Learning course at Coursera (https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent), Andrew Ng explains in great details this using the image below:
 (https://i.sstatic.net/8xKxN.png) 
","Mini-Batch Gradient Descent (Adam) with a small batch size can result in spikes in the cost function due to unlucky data distributions in certain mini-batches. This effect is more pronounced with small batch sizes (e.g., batch_size=1). In contrast, Full Batch Gradient Descent, which uses the entire training dataset, avoids these spikes. The spikes seen in the provided graph may be due to the incorrect use of Full Batch Gradient Descent instead of SGD. Andrew Ng's Coursera course provides a detailed explanation of these concepts, using an illustrative image showing how mini-batches can lead to varying cost updates and spikes."
What is the difference between dropout and drop connect?,https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect,"['neural-networks', 'dropout']",201569,True,201891,21418,3,37,1658164203,1457951530,41,1458070846,"DropOut and DropConnect are both methods intended to prevent ""co-adaptation"" of units in a neural network. In other words, we want units to independently extract features from their inputs instead of relying on other neurons to do so. 
Suppose we have a multilayered feedforward network like this one (the topology doesn't really matter). We're worried about the yellow hidden units in the middle layer co-adapting. 
 (https://i.sstatic.net/DfKTA.png)
 DropOut 
To apply DropOut, we randomly select a subset of the units and clamp their output to zero, regardless of the input; this effectively removes those units from the model. A different subset of units is randomly selected every time we present a training example. 
Below are two possible network configurations. On the first presentation (left), the 1st and 3rd units are disabled, but the 2nd and 3rd units have been randomly selected on a subsequent presentation. At test time, we use the complete network but rescale the weights to compensate for the fact that all of them can now become active (e.g., if you drop half of the nodes, the weights should also be halved).
 (https://i.sstatic.net/CewjH.png)
DropConnect
DropConnect works similarly, except that we disable individual weights (i.e., set them to zero), instead of nodes, so a node can remain partially active. Schematically, it looks like this:
 (https://i.sstatic.net/D1QC7.png)
 Comparison 
These methods both work because they effectively let you train several models at the same time, then average across them for testing. For example, the yellow layer has four nodes, and thus 16 possible DropOut states (all enabled, #1 disabled, #1 and #2 disabled, etc). 
DropConnect is a generalization of DropOut because it produces even more possible models, since there are almost always more connections than units. However, you can get similar outcomes on an individual trial. For example, the DropConnect network on the right has effectively dropped Unit #2 since all of the incoming connections have been removed.
Further Reading
The original papers are pretty accessible and contain more details and empirical results.

DropOut: Hinton et al., 2012 (http://www.cs.toronto.edu/~hinton/absps/dropout.pdf), Srivasta et al., 2014; JMLR (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
DropConnect: Wan et al., 2013 (http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf)

","DropOut and DropConnect are regularization techniques in neural networks that aim to prevent co-adaptation of units. DropOut randomly sets a subset of units to zero during training, while DropConnect disables individual weights.

Both methods simulate training multiple networks simultaneously and averaging their outputs during testing. DropConnect allows for a greater variety of network configurations than DropOut.

The effectiveness of DropOut and DropConnect stems from their ability to promote the independent feature extraction of units, preventing over-reliance on other neurons. They achieve this by introducing randomness during training, forcing the network to learn robust representations.

DropConnect is a generalization of DropOut, producing similar outcomes on an individual trial but offering more possible network configurations due to the increased number of connections compared to units. Both methods have proven effective in improving the robustness and generalization of neural networks."
